============================= test session starts ==============================
platform linux -- Python 3.11.0rc1, pytest-8.3.5, pluggy-1.5.0 -- /localdev/jameszianxu/tt-torch/env/venv/bin/python3.11
cachedir: .pytest_cache
rootdir: /localdev/jameszianxu/tt-torch
configfile: pyproject.toml
plugins: cov-6.0.0
collecting ... collected 1 item

tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval] setup runtime intermediate cache
Neither required_atol, or relative_atol is provided. Setting required_atol=0.01.
Attempting to cache intermediate tensors
running backend with CompileDepth.COMPILE_OP_BY_OP
setup runtime intermediate cache
During call - compile depth = CompileDepth.COMPILE_OP_BY_OP
Compiling 0/80: p__fx_const_folded_attrs_0
Compiling 1/80: p__fx_const_folded_attrs_1
Compiling 2/80: p__fx_const_folded_attrs_2
Compiling 3/80: p__fx_const_folded_attrs_3
Compiling 4/80: p__fx_const_folded_attrs_4
Compiling 5/80: p__fx_const_folded_attrs_5
Compiling 6/80: p__fx_const_folded_attrs_6
Compiling 7/80: p__fx_const_folded_attrs_7
Compiling 8/80: p__fx_const_folded_attrs_8
Compiling 9/80: p__fx_const_folded_attrs_9
Compiling 10/80: p__fx_const_folded_attrs_10
Compiling 11/80: p__fx_const_folded_attrs_11
Compiling 12/80: p__fx_const_folded_attrs_12
Compiling 13/80: p__fx_const_folded_attrs_13
Compiling 14/80: p__fx_const_folded_attrs_14
Compiling 15/80: p__fx_const_folded_attrs_15
Compiling 16/80: p_const_subgraph_module_l__self___encoder_lin1_weight
Compiling 17/80: p_const_subgraph_module_l__self___encoder_lin1_bias
Compiling 18/80: p_const_subgraph_module_l__self___encoder_lin2_weight
Compiling 19/80: p_const_subgraph_module_l__self___encoder_lin2_bias
Compiling 20/80: p_const_subgraph_module_l__self___encoder_lin3_weight
Compiling 21/80: p_const_subgraph_module_l__self___encoder_lin3_bias
Compiling 22/80: p_const_subgraph_module_l__self___encoder_lin4_weight
Compiling 23/80: p_const_subgraph_module_l__self___encoder_lin4_bias
Compiling 24/80: p_const_subgraph_module_l__self___decoder_lin1_weight
Compiling 25/80: p_const_subgraph_module_l__self___decoder_lin1_bias
Compiling 26/80: p_const_subgraph_module_l__self___decoder_lin2_weight
Compiling 27/80: p_const_subgraph_module_l__self___decoder_lin2_bias
Compiling 28/80: p_const_subgraph_module_l__self___decoder_lin3_weight
Compiling 29/80: p_const_subgraph_module_l__self___decoder_lin3_bias
Compiling 30/80: p_const_subgraph_module_l__self___decoder_lin4_weight
Compiling 31/80: p_const_subgraph_module_l__self___decoder_lin4_bias
Compiling 32/80: args_0
Compiling 33/80: prims.convert_element_type.default
json len 14237
Caching runtime intermediate for convert_element_type
Compiling 34/80: aten.mm.default
json len 16107
Caching runtime intermediate for mm
Compiling 35/80: aten.mul.Tensor
json len 47727
Caching runtime intermediate for mul
Compiling 36/80: aten.add.Tensor
json len 19634
Caching runtime intermediate for add
Compiling 37/80: prims.convert_element_type.default
json len 14234
Caching runtime intermediate for convert_element_type_1
Compiling 38/80: aten.relu.default
json len 18611
Caching runtime intermediate for relu
Compiling 39/80: prims.convert_element_type.default
json len 14229
Caching runtime intermediate for convert_element_type_2
Compiling 40/80: aten.mm.default
json len 16086
Caching runtime intermediate for mm_1
Compiling 41/80: aten.mul.Tensor
json len 47688
Caching runtime intermediate for mul_1
Compiling 42/80: aten.add.Tensor
json len 19596
Caching runtime intermediate for add_1
Compiling 43/80: prims.convert_element_type.default
json len 14221
Caching runtime intermediate for convert_element_type_3
Compiling 44/80: aten.relu.default
json len 18591
Caching runtime intermediate for relu_1
Compiling 45/80: prims.convert_element_type.default
json len 14216
Caching runtime intermediate for convert_element_type_4
Compiling 46/80: aten.mm.default
json len 16072
Caching runtime intermediate for mm_2
Compiling 47/80: aten.mul.Tensor
json len 47565
Caching runtime intermediate for mul_2
Compiling 48/80: aten.add.Tensor
json len 19596
Caching runtime intermediate for add_2
Compiling 49/80: prims.convert_element_type.default
json len 14221
Caching runtime intermediate for convert_element_type_5
Compiling 50/80: aten.relu.default
json len 18591
Caching runtime intermediate for relu_2
Compiling 51/80: prims.convert_element_type.default
json len 14216
Caching runtime intermediate for convert_element_type_6
Compiling 52/80: aten.mm.default
json len 15816
Caching runtime intermediate for mm_3
Compiling 53/80: aten.mul.Tensor
json len 47536
Caching runtime intermediate for mul_3
Compiling 54/80: aten.add.Tensor
json len 19567
Caching runtime intermediate for add_3
Compiling 55/80: prims.convert_element_type.default
json len 14210
Caching runtime intermediate for convert_element_type_7
Compiling 56/80: prims.convert_element_type.default
json len 14205
Caching runtime intermediate for convert_element_type_8
Compiling 57/80: aten.mm.default
json len 15817
Caching runtime intermediate for mm_4
Compiling 58/80: aten.mul.Tensor
Caching runtime intermediate for mul_4
Compiling 59/80: aten.add.Tensor
Caching runtime intermediate for add_4
Compiling 60/80: prims.convert_element_type.default
Caching runtime intermediate for convert_element_type_9
Compiling 61/80: aten.relu.default
Caching runtime intermediate for relu_3
Compiling 62/80: prims.convert_element_type.default
Caching runtime intermediate for convert_element_type_10
Compiling 63/80: aten.mm.default
json len 15952
Caching runtime intermediate for mm_5
Compiling 64/80: aten.mul.Tensor
Caching runtime intermediate for mul_5
Compiling 65/80: aten.add.Tensor
Caching runtime intermediate for add_5
Compiling 66/80: prims.convert_element_type.default
Caching runtime intermediate for convert_element_type_11
Compiling 67/80: aten.relu.default
Caching runtime intermediate for relu_4
Compiling 68/80: prims.convert_element_type.default
Caching runtime intermediate for convert_element_type_12
Compiling 69/80: aten.mm.default
json len 16087
Caching runtime intermediate for mm_6
Compiling 70/80: aten.mul.Tensor
Caching runtime intermediate for mul_6
Compiling 71/80: aten.add.Tensor
Caching runtime intermediate for add_6
Compiling 72/80: prims.convert_element_type.default
Caching runtime intermediate for convert_element_type_13
Compiling 73/80: aten.relu.default
Caching runtime intermediate for relu_5
Compiling 74/80: prims.convert_element_type.default
Caching runtime intermediate for convert_element_type_14
Compiling 75/80: aten.mm.default
json len 16107
Caching runtime intermediate for mm_7
Compiling 76/80: aten.mul.Tensor
json len 47746
Caching runtime intermediate for mul_7
Compiling 77/80: aten.add.Tensor
json len 19651
Caching runtime intermediate for add_7
Compiling 78/80: prims.convert_element_type.default
[32m2025-04-01 15:39:52.915[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Opened PCI device 4; KMD version: 1.31.0, IOMMU: disabled

[32m2025-04-01 15:39:52.933[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Opened PCI device 4; KMD version: 1.31.0, IOMMU: disabled
[32m2025-04-01 15:39:52.936[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Harvesting mask for chip 0 is 0x40 (physical layout: 0x40, logical: 0x40, simulated harvesting mask: 0x0).
[32m2025-04-01 15:39:52.950[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Opened PCI device 4; KMD version: 1.31.0, IOMMU: disabled
[32m2025-04-01 15:39:52.951[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Detected PCI devices: [4]
[32m2025-04-01 15:39:52.951[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Using local chip ids: {0} and remote chip ids {}
[32m2025-04-01 15:39:53.018[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Software version 6.0.0, Ethernet FW version 6.10.0 (Device 0)
[32m                 Always[0m | [1m[38;5;240m   DEBUG[0m | Device grid size = { 8, 8 }
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %5 = "ttnn.from_device"(%4) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %6 = "ttnn.to_layout"(%5) <{layout = #ttnn.layout<tile>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%5) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %7 = "ttnn.to_device"(%6, %0) <{memory_config = #ttnn.memory_config<<dram>, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%6) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %8 = "ttnn.typecast"(%7) <{dtype = #tt.supportedDataTypes<si32>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%7) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %9 = "ttnn.typecast"(%arg32) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x784xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "convert_element_type"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %10 = "ttnn.matmul"(%9, %arg0) <{transpose_a = false, transpose_b = false}> : (tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<784x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<25x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "mm"])
json len 14242
Caching runtime intermediate for convert_element_type_15
Compiling 79/80: output
#####  Saving unique ops to results/models/tests/models/autoencoder_linear/test_autoencoder_linear_py__test_autoencoder_linear_full_eval___call__unique_ops.json#####
0/31 ops executed
compiling model properly now.
running backend with CompileDepth.EXECUTE
setup runtime intermediate cache
Node Location: convert_element_type
Found golden for op @ convert_element_type == convert_element_type.
output intermediate tensor tensor([[-0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4121, -0.4121, -0.4121, -0.4121,
         -0.4121, -0.3984, -0.3984, -0.3984, -0.3984, -0.3984, -0.3867, -0.3867,
         -0.3867, -0.3867, -0.3867, -0.3730, -0.3730, -0.3730, -0.3730, -0.3730,
         -0.3613, -0.3613, -0.3613, -0.3613, -0.3613, -0.3477, -0.3477, -0.3477,
         -0.3477, -0.3359, -0.3359, -0.3359, -0.3359, -0.3223, -0.3223, -0.3223,
         -0.3223, -0.3105, -0.3105, -0.3105, -0.2969, -0.2969, -0.2969, -0.2969,
         -0.2969, -0.2852, -0.2852, -0.2852, -0.2715, -0.2715, -0.2715, -0.2715,
         -0.2578, -0.2578, -0.2578, -0.2578, -0.2578, -0.2578, -0.2578, -0.2578,
         -0.2578, -0.2461, -0.2461, -0.2461, -0.2461, -0.2461, -0.2461, -0.2461,
         -0.2461, -0.2461, -0.2461, -0.2461, -0.2461, -0.2334, -0.2334, -0.2334,
         -0.2334, -0.2334, -0.2334, -0.2334, -0.2334, -0.2334, -0.2334, -0.2207,
         -0.2207, -0.2207, -0.2080, -0.2080, -0.2080, -0.2080, -0.1953, -0.1953,
         -0.1953, -0.1826, -0.1826, -0.1826, -0.1699, -0.1699, -0.1699, -0.1572,
         -0.1572, -0.1572, -0.1572, -0.1445, -0.1445, -0.1445, -0.1318, -0.1318,
         -0.1318, -0.1187, -0.1187, -0.1060, -0.1060, -0.0933, -0.0806, -0.0679,
         -0.0679, -0.0552, -0.0425, -0.0425, -0.0297, -0.0170, -0.0042, -0.0042,
          0.0085,  0.0212,  0.0212,  0.0339,  0.0466,  0.0596,  0.0596,  0.0723,
          0.0850,  0.0850,  0.0977,  0.1104,  0.1230,  0.1230,  0.1357,  0.1357,
          0.1357,  0.1357,  0.1484,  0.1484,  0.1484,  0.1484,  0.1484,  0.1611,
          0.1611,  0.1611,  0.1611,  0.1738,  0.1738,  0.1738,  0.1738,  0.1738,
          0.1865,  0.1865,  0.1865,  0.1865,  0.1865,  0.1992,  0.1992,  0.1992,
          0.1992,  0.2119,  0.2119,  0.2119,  0.2119,  0.2119,  0.2246,  0.2246,
          0.2246,  0.2246,  0.2246,  0.2373,  0.2373,  0.2373,  0.2373,  0.2373,
          0.2373,  0.2500,  0.2500,  0.2500,  0.2500,  0.2500,  0.2637,  0.2637,
          0.2637,  0.2637,  0.2637,  0.2754,  0.2754,  0.2754,  0.2891,  0.2891,
          0.3008,  0.3145,  0.3262,  0.3262,  0.3398,  0.3516,  0.3652,  0.3770,
          0.3770,  0.3906,  0.4023,  0.4160,  0.4160,  0.4277,  0.4414,  0.4531,
          0.4531,  0.4668,  0.4785,  0.4922,  0.5039,  0.5039,  0.5195,  0.5312,
          0.5430,  0.5430,  0.5547,  0.5703,  0.5820,  0.5820,  0.5938,  0.6055,
          0.6211,  0.6211,  0.6328,  0.6445,  0.6562,  0.6562,  0.6719,  0.6836,
          0.6953,  0.6953,  0.7070,  0.7227,  0.7344,  0.7344,  0.7461,  0.7578,
          0.7734,  0.7734,  0.7852,  0.7969,  0.8086,  0.8086,  0.8242,  0.8242,
          0.8242,  0.8359,  0.8359,  0.8359,  0.8477,  0.8477,  0.8477,  0.8477,
          0.8633,  0.8633,  0.8633,  0.8633,  0.8750,  0.8750,  0.8750,  0.8867,
          0.8867,  0.8867,  0.8867,  0.8984,  0.8984,  0.8984,  0.9141,  0.9141,
          0.9141,  0.9141,  0.9141,  0.8984,  0.8984,  0.8867,  0.8750,  0.8633,
          0.8633,  0.8477,  0.8359,  0.8359,  0.8242,  0.8086,  0.7969,  0.7969,
          0.7852,  0.7734,  0.7734,  0.7578,  0.7461,  0.7344,  0.7227,  0.7227,
          0.7070,  0.6953,  0.6953,  0.6836,  0.6719,  0.6562,  0.6562,  0.6562,
          0.6562,  0.6562,  0.6562,  0.6562,  0.6562,  0.6562,  0.6562,  0.6562,
          0.6562,  0.6562,  0.6562,  0.6562,  0.6562,  0.6562,  0.6562,  0.6562,
          0.6562,  0.6562,  0.6562,  0.6562,  0.6562,  0.6562,  0.6562,  0.6562,
          0.6562,  0.6562,  0.6719,  0.6719,  0.6719,  0.6836,  0.6836,  0.6836,
          0.6953,  0.6953,  0.6953,  0.7070,  0.7070,  0.7070,  0.7227,  0.7227,
          0.7227,  0.7344,  0.7344,  0.7344,  0.7461,  0.7461,  0.7461,  0.7578,
          0.7578,  0.7578,  0.7734,  0.7734,  0.7734,  0.7852,  0.7852,  0.7852,
          0.7969,  0.7969,  0.7969,  0.8086,  0.8086,  0.8086,  0.8086,  0.8242,
          0.8242,  0.8242,  0.8359,  0.8359,  0.8359,  0.8477,  0.8477,  0.8477,
          0.8477,  0.8633,  0.8633,  0.8633,  0.8750,  0.8750,  0.8750,  0.8867,
          0.8867,  0.8867,  0.8867,  0.8750,  0.8477,  0.8359,  0.8242,  0.8086,
          0.7852,  0.7734,  0.7578,  0.7461,  0.7344,  0.7070,  0.6953,  0.6836,
          0.6719,  0.6445,  0.6328,  0.6211,  0.6055,  0.5938,  0.5703,  0.5547,
          0.5430,  0.5312,  0.5039,  0.4922,  0.4785,  0.4668,  0.4531,  0.4414,
          0.4277,  0.4160,  0.4023,  0.3906,  0.3770,  0.3770,  0.3652,  0.3516,
          0.3398,  0.3262,  0.3145,  0.3008,  0.2891,  0.2891,  0.2754,  0.2637,
          0.2500,  0.2373,  0.2246,  0.2119,  0.2119,  0.1992,  0.1865,  0.1738,
          0.1611,  0.1484,  0.1357,  0.1230,  0.1230,  0.1104,  0.0977,  0.0850,
          0.0723,  0.0596,  0.0466,  0.0339,  0.0339,  0.0212,  0.0085, -0.0042,
         -0.0170, -0.0297, -0.0425, -0.0552, -0.0552, -0.0679, -0.0806, -0.0933,
         -0.1060, -0.1187, -0.1318, -0.1445, -0.1572, -0.1699, -0.1699, -0.1699,
         -0.1699, -0.1699, -0.1699, -0.1699, -0.1699, -0.1699, -0.1699, -0.1699,
         -0.1699, -0.1699, -0.1699, -0.1699, -0.1826, -0.1826, -0.1826, -0.1826,
         -0.1826, -0.1826, -0.1826, -0.1826, -0.1826, -0.1826, -0.1826, -0.1826,
         -0.1826, -0.1826, -0.1826, -0.1826, -0.1826, -0.1826, -0.1826, -0.1953,
         -0.1953, -0.1953, -0.1953, -0.1953, -0.1953, -0.1953, -0.1953, -0.1953,
         -0.1953, -0.1953, -0.2080, -0.2080, -0.2080, -0.2080, -0.2080, -0.2080,
         -0.2080, -0.2080, -0.2080, -0.2080, -0.2080, -0.2207, -0.2207, -0.2207,
         -0.2207, -0.2334, -0.2334, -0.2461, -0.2461, -0.2461, -0.2578, -0.2578,
         -0.2578, -0.2715, -0.2715, -0.2715, -0.2852, -0.2852, -0.2852, -0.2969,
         -0.2969, -0.3105, -0.3105, -0.3105, -0.3223, -0.3223, -0.3223, -0.3359,
         -0.3359, -0.3359, -0.3477, -0.3477, -0.3477, -0.3477, -0.3613, -0.3613,
         -0.3613, -0.3613, -0.3730, -0.3730, -0.3730, -0.3730, -0.3730, -0.3867,
         -0.3867, -0.3867, -0.3867, -0.3984, -0.3984, -0.3984, -0.3984, -0.4121,
         -0.4121, -0.4121, -0.4121, -0.4121, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238,
         -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238, -0.4238]])
Node Location: mm
Found golden for op @ mm == mm.
output intermediate tensor [32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "mm"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %11 = "ttnn.typecast"(%8) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "mul"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%8) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "mul"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %12 = "ttnn.reshape"(%11) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "mul"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %13 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "mul"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %14 = "ttnn.add"(%12, %13) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "mul"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "mul"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "mul"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %15 = "ttnn.multiply"(%10, %14) : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "mul"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "mul"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %16 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "add"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %17 = "ttnn.add"(%15, %16) : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "add"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "add"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "add"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %18 = "ttnn.typecast"(%17) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "convert_element_type_1"])
tensor([[ 0.2980, -0.3990, -0.0264, -0.2652, -0.7404, -0.0941,  0.2699,  0.4175,
          0.1316,  0.2536, -0.2265, -0.0452, -0.1846,  0.1695, -0.0931,  0.2860,
         -0.0709,  0.1267, -0.3433,  0.1271, -0.0712,  0.0328,  0.2897, -0.2525,
          0.3208,  0.3657, -0.1259, -0.2435,  0.0159,  0.1304, -0.3046, -0.4043,
          0.5150, -0.0123,  0.2250,  0.2214,  0.1947,  0.1739, -0.2806,  0.1926,
         -0.1823,  0.1016, -0.0043,  0.3385, -0.0831, -0.2552,  0.1941, -0.4723,
         -0.0899, -0.2912,  0.3679,  0.3099, -0.0766, -0.1090,  0.1553, -0.1410,
         -0.3601, -0.0635, -0.1515, -0.4223, -0.0249,  0.2659, -0.0125,  0.0767,
          0.4004, -0.0460,  0.1053,  0.0390,  0.2216,  0.2241, -0.1222,  0.3706,
         -0.2928,  0.0587, -0.1024, -0.0070, -0.0715, -0.1010, -0.0268, -0.2626,
          0.1057, -0.1349, -0.3611, -0.2193, -0.4205,  0.0656, -0.0150,  0.1161,
         -1.0329,  0.1219, -0.2010,  0.0512,  0.1591,  0.2445, -0.1047, -0.4075,
          0.3395,  0.0465, -0.1801,  0.0657,  0.2421, -0.1420,  0.5152, -0.3956,
         -0.2968, -0.0841,  0.2793,  0.0076,  0.0616,  0.6554,  0.7926,  0.4128,
          0.0465, -0.2346, -0.3442,  0.2837,  0.1478, -0.3757,  0.5747,  0.7171,
          0.0871,  0.3730,  0.0823, -0.0590,  0.1142,  0.1976, -0.3269,  0.1734]])
Node Location: mm
Found golden for op @ mm == mm.
output intermediate tensor None
Node Location: mul
Found golden for op @ mul == mul.
output intermediate tensor tensor([1.])
Node Location: mul
Found golden for op @ mul == mul.
output intermediate tensor None
Node Location: mul
Found golden for op @ mul == mul.
output intermediate tensor tensor([[1.]])
Node Location: mul
Found golden for op @ mul == mul.
output intermediate tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0.]])
Node Location: mul
Found golden for op @ mul == mul.
output intermediate tensor tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1.]])
Node Location: mul
Found golden for op @ mul == mul.
output intermediate tensor None
Node Location: mul
Found golden for op @ mul == mul.
output intermediate tensor None
Node Location: mul
Found golden for op @ mul == mul.
output intermediate tensor tensor([[ 0.2980, -0.3990, -0.0264, -0.2652, -0.7404, -0.0941,  0.2699,  0.4175,
          0.1316,  0.2536, -0.2265, -0.0452, -0.1846,  0.1695, -0.0931,  0.2860,
         -0.0709,  0.1267, -0.3433,  0.1271, -0.0712,  0.0328,  0.2897, -0.2525,
          0.3208,  0.3657, -0.1259, -0.2435,  0.0159,  0.1304, -0.3046, -0.4043,
          0.5150, -0.0123,  0.2250,  0.2214,  0.1947,  0.1739, -0.2806,  0.1926,
         -0.1823,  0.1016, -0.0043,  0.3385, -0.0831, -0.2552,  0.1941, -0.4723,
         -0.0899, -0.2912,  0.3679,  0.3099, -0.0766, -0.1090,  0.1553, -0.1410,
         -0.3601, -0.0635, -0.1515, -0.4223, -0.0249,  0.2659, -0.0125,  0.0767,
          0.4004, -0.0460,  0.1053,  0.0390,  0.2216,  0.2241, -0.1222,  0.3706,
         -0.2928,  0.0587, -0.1024, -0.0070, -0.0715, -0.1010, -0.0268, -0.2626,
          0.1057, -0.1349, -0.3611, -0.2193, -0.4205,  0.0656, -0.0150,  0.1161,
         -1.0329,  0.1219, -0.2010,  0.0512,  0.1591,  0.2445, -0.1047, -0.4075,
          0.3395,  0.0465, -0.1801,  0.0657,  0.2421, -0.1420,  0.5152, -0.3956,
         -0.2968, -0.0841,  0.2793,  0.0076,  0.0616,  0.6554,  0.7926,  0.4128,
          0.0465, -0.2346, -0.3442,  0.2837,  0.1478, -0.3757,  0.5747,  0.7171,
          0.0871,  0.3730,  0.0823, -0.0590,  0.1142,  0.1976, -0.3269,  0.1734]])
Node Location: mul
Found golden for op @ mul == mul.
output intermediate tensor None
Node Location: add
Found golden for op @ add == add.
output intermediate tensor tensor([[-0.0320, -0.0053,  0.0045, -0.0211,  0.0225,  0.0130,  0.0157,  0.0272,
          0.0239, -0.0156, -0.0089,  0.0221,  0.0055,  0.0070, -0.0031, -0.0249,
         -0.0334, -0.0024, -0.0123,  0.0129, -0.0047, -0.0190, -0.0051,  0.0227,
          0.0325, -0.0356,  0.0029, -0.0081,  0.0255, -0.0104, -0.0205, -0.0056,
          0.0217, -0.0106,  0.0032,  0.0339, -0.0189, -0.0079, -0.0206, -0.0077,
         -0.0113,  0.0232,  0.0033, -0.0164, -0.0082,  0.0204, -0.0322, -0.0244,
         -0.0153,  0.0347,  0.0135, -0.0112, -0.0146,  0.0299, -0.0162,  0.0146,
         -0.0339, -0.0219, -0.0154, -0.0079,  0.0228, -0.0354, -0.0325,  0.0356,
         -0.0309,  0.0183,  0.0029, -0.0067,  0.0194, -0.0258,  0.0013, -0.0124,
          0.0356,  0.0184,  0.0089, -0.0132,  0.0187,  0.0097,  0.0099,  0.0055,
          0.0237, -0.0239,  0.0065,  0.0259,  0.0229, -0.0256, -0.0293,  0.0177,
          0.0294,  0.0025, -0.0016,  0.0016, -0.0095,  0.0272, -0.0003, -0.0101,
          0.0052,  0.0150, -0.0171, -0.0068, -0.0126,  0.0300,  0.0034,  0.0108,
         -0.0074, -0.0145, -0.0192,  0.0016,  0.0270,  0.0165, -0.0327,  0.0272,
         -0.0132,  0.0253,  0.0232,  0.0312, -0.0302, -0.0205,  0.0322,  0.0154,
          0.0050, -0.0181, -0.0215,  0.0294,  0.0033,  0.0150,  0.0043,  0.0334]])
Node Location: add
Found golden for op @ add == add.
output intermediate tensor tensor([[ 0.2661, -0.4042, -0.0219, -0.2863, -0.7179, -0.0811,  0.2857,  0.4447,
          0.1555,  0.2380, -0.2353, -0.0232, -0.1791,  0.1765, -0.0963,  0.2611,
         -0.1043,  0.1243, -0.3556,  0.1399, -0.0759,  0.0137,  0.2846, -0.2298,
          0.3532,  0.3300, -0.1231, -0.2516,  0.0414,  0.1200, -0.3251, -0.4099,
          0.5367, -0.0229,  0.2282,  0.2554,  0.1758,  0.1660, -0.3012,  0.1849,
         -0.1936,  0.1248, -0.0011,  0.3221, -0.0913, -0.2348,  0.1618, -0.4967,
         -0.1051, -0.2565,  0.3813,  0.2987, -0.0912, -0.0791,  0.1391, -0.1264,
         -0.3940, -0.0853, -0.1669, -0.4302, -0.0021,  0.2305, -0.0449,  0.1123,
          0.3695, -0.0277,  0.1082,  0.0322,  0.2410,  0.1983, -0.1209,  0.3582,
         -0.2572,  0.0771, -0.0935, -0.0203, -0.0529, -0.0912, -0.0169, -0.2571,
          0.1294, -0.1588, -0.3547, -0.1935, -0.3975,  0.0400, -0.0443,  0.1338,
         -1.0035,  0.1244, -0.2027,  0.0528,  0.1495,  0.2717, -0.1049, -0.4176,
          0.3446,  0.0615, -0.1972,  0.0588,  0.2295, -0.1119,  0.5186, -0.3848,
         -0.3042, -0.0986,  0.2602,  0.0092,  0.0885,  0.6719,  0.7599,  0.4401,
          0.0333, -0.2093, -0.3210,  0.3150,  0.1176, -0.3962,  0.6070,  0.7324,
          0.0921,  0.3550,  0.0608, -0.0296,  0.1175,  0.2126, -0.3226,  0.2068]])
Node Location: add
Found golden for op @ add == add.
output intermediate tensor None
Node Location: add
Found golden for op @ add == add.
output intermediate tensor None
Node Location: convert_element_type_1
Found golden for op @ convert_element_type_1 == convert_element_type_1.
output intermediate tensor [32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":35:0, "convert_element_type_1"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %19 = "ttnn.maximum"(%18, %1) : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":36:0, "relu"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":36:0, "relu"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %20 = "ttnn.typecast"(%19) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "convert_element_type_2"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "convert_element_type_2"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %21 = "ttnn.matmul"(%20, %arg2) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "mm_1"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "mm_1"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %22 = "ttnn.reshape"(%11) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "mul_1"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %23 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "mul_1"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %24 = "ttnn.add"(%22, %23) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "mul_1"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "mul_1"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "mul_1"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %25 = "ttnn.multiply"(%21, %24) : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "mul_1"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "mul_1"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %26 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "add_1"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %27 = "ttnn.add"(%25, %26) : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "add_1"])
tensor([[ 0.2656, -0.4043, -0.0220, -0.2871, -0.7188, -0.0811,  0.2852,  0.4453,
          0.1553,  0.2383, -0.2354, -0.0232, -0.1787,  0.1768, -0.0962,  0.2617,
         -0.1045,  0.1240, -0.3555,  0.1396, -0.0762,  0.0137,  0.2852, -0.2295,
          0.3535,  0.3301, -0.1230, -0.2520,  0.0415,  0.1201, -0.3242, -0.4102,
          0.5352, -0.0229,  0.2285,  0.2559,  0.1758,  0.1660, -0.3008,  0.1846,
         -0.1934,  0.1250, -0.0011,  0.3223, -0.0913, -0.2344,  0.1621, -0.4961,
         -0.1050, -0.2559,  0.3809,  0.2988, -0.0913, -0.0791,  0.1387, -0.1260,
         -0.3945, -0.0854, -0.1670, -0.4297, -0.0021,  0.2305, -0.0449,  0.1123,
          0.3691, -0.0277,  0.1084,  0.0322,  0.2412,  0.1982, -0.1211,  0.3574,
         -0.2578,  0.0771, -0.0938, -0.0203, -0.0530, -0.0913, -0.0168, -0.2578,
          0.1299, -0.1592, -0.3555, -0.1934, -0.3984,  0.0400, -0.0444,  0.1338,
         -1.0000,  0.1245, -0.2031,  0.0527,  0.1494,  0.2715, -0.1050, -0.4180,
          0.3438,  0.0615, -0.1973,  0.0588,  0.2295, -0.1118,  0.5195, -0.3848,
         -0.3047, -0.0986,  0.2598,  0.0092,  0.0884,  0.6719,  0.7617,  0.4395,
          0.0334, -0.2090, -0.3203,  0.3145,  0.1177, -0.3965,  0.6055,  0.7344,
          0.0923,  0.3555,  0.0608, -0.0295,  0.1177,  0.2129, -0.3223,  0.2070]],
       dtype=torch.bfloat16)
Node Location: convert_element_type_1
Found golden for op @ convert_element_type_1 == convert_element_type_1.
output intermediate tensor None
Node Location: relu
Found golden for op @ relu == relu.
output intermediate tensor tensor([[0.2656, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2852, 0.4453, 0.1553,
         0.2383, 0.0000, 0.0000, 0.0000, 0.1768, 0.0000, 0.2617, 0.0000, 0.1240,
         0.0000, 0.1396, 0.0000, 0.0137, 0.2852, 0.0000, 0.3535, 0.3301, 0.0000,
         0.0000, 0.0415, 0.1201, 0.0000, 0.0000, 0.5352, 0.0000, 0.2285, 0.2559,
         0.1758, 0.1660, 0.0000, 0.1846, 0.0000, 0.1250, 0.0000, 0.3223, 0.0000,
         0.0000, 0.1621, 0.0000, 0.0000, 0.0000, 0.3809, 0.2988, 0.0000, 0.0000,
         0.1387, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2305, 0.0000,
         0.1123, 0.3691, 0.0000, 0.1084, 0.0322, 0.2412, 0.1982, 0.0000, 0.3574,
         0.0000, 0.0771, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1299,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0400, 0.0000, 0.1338, 0.0000, 0.1245,
         0.0000, 0.0527, 0.1494, 0.2715, 0.0000, 0.0000, 0.3438, 0.0615, 0.0000,
         0.0588, 0.2295, 0.0000, 0.5195, 0.0000, 0.0000, 0.0000, 0.2598, 0.0092,
         0.0884, 0.6719, 0.7617, 0.4395, 0.0334, 0.0000, 0.0000, 0.3145, 0.1177,
         0.0000, 0.6055, 0.7344, 0.0923, 0.3555, 0.0608, 0.0000, 0.1177, 0.2129,
         0.0000, 0.2070]], dtype=torch.bfloat16)
Node Location: relu
Found golden for op @ relu == relu.
output intermediate tensor None
Node Location: convert_element_type_2
Found golden for op @ convert_element_type_2 == convert_element_type_2.
output intermediate tensor tensor([[0.2656, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2852, 0.4453, 0.1553,
         0.2383, 0.0000, 0.0000, 0.0000, 0.1768, 0.0000, 0.2617, 0.0000, 0.1240,
         0.0000, 0.1396, 0.0000, 0.0137, 0.2852, 0.0000, 0.3535, 0.3301, 0.0000,
         0.0000, 0.0415, 0.1201, 0.0000, 0.0000, 0.5352, 0.0000, 0.2285, 0.2559,
         0.1758, 0.1660, 0.0000, 0.1846, 0.0000, 0.1250, 0.0000, 0.3223, 0.0000,
         0.0000, 0.1621, 0.0000, 0.0000, 0.0000, 0.3809, 0.2988, 0.0000, 0.0000,
         0.1387, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2305, 0.0000,
         0.1123, 0.3691, 0.0000, 0.1084, 0.0322, 0.2412, 0.1982, 0.0000, 0.3574,
         0.0000, 0.0771, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1299,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0400, 0.0000, 0.1338, 0.0000, 0.1245,
         0.0000, 0.0527, 0.1494, 0.2715, 0.0000, 0.0000, 0.3438, 0.0615, 0.0000,
         0.0588, 0.2295, 0.0000, 0.5195, 0.0000, 0.0000, 0.0000, 0.2598, 0.0092,
         0.0884, 0.6719, 0.7617, 0.4395, 0.0334, 0.0000, 0.0000, 0.3145, 0.1177,
         0.0000, 0.6055, 0.7344, 0.0923, 0.3555, 0.0608, 0.0000, 0.1177, 0.2129,
         0.0000, 0.2070]])
Node Location: convert_element_type_2
Found golden for op @ convert_element_type_2 == convert_element_type_2.
output intermediate tensor None
Node Location: mm_1
Found golden for op @ mm_1 == mm_1.
output intermediate tensor tensor([[ 0.0081,  0.0983, -0.1623,  0.0418, -0.0215,  0.0133, -0.1687, -0.0536,
         -0.0720,  0.1075, -0.0820,  0.1444,  0.0782, -0.0777, -0.0211,  0.2532,
          0.1611,  0.1604,  0.1118, -0.2608, -0.0952, -0.0078, -0.0747,  0.0342,
         -0.0377, -0.0032,  0.1567, -0.0667, -0.1439, -0.3095,  0.2683, -0.0365,
          0.0735,  0.0038, -0.0436,  0.1862, -0.0300, -0.2254, -0.1851,  0.0244,
         -0.1050,  0.0381,  0.1042, -0.1783, -0.1164,  0.0520, -0.0717,  0.0326,
         -0.2298, -0.0865,  0.1449,  0.1431, -0.0658,  0.1658, -0.1007,  0.0497,
         -0.0390, -0.0139, -0.1908,  0.1435,  0.0370,  0.2084,  0.1090,  0.0943]])
Node Location: mm_1
Found golden for op @ mm_1 == mm_1.
output intermediate tensor None
Node Location: mul_1
Found golden for op @ mul_1 == mul_1.
output intermediate tensor tensor([[1.]])
Node Location: mul_1
Found golden for op @ mul_1 == mul_1.
output intermediate tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Node Location: mul_1
Found golden for op @ mul_1 == mul_1.
output intermediate tensor tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])
Node Location: mul_1
Found golden for op @ mul_1 == mul_1.
output intermediate tensor None
Node Location: mul_1
Found golden for op @ mul_1 == mul_1.
output intermediate tensor None
Node Location: mul_1
Found golden for op @ mul_1 == mul_1.
output intermediate tensor tensor([[ 0.0081,  0.0983, -0.1623,  0.0418, -0.0215,  0.0133, -0.1687, -0.0536,
         -0.0720,  0.1075, -0.0820,  0.1444,  0.0782, -0.0777, -0.0211,  0.2532,
          0.1611,  0.1604,  0.1118, -0.2608, -0.0952, -0.0078, -0.0747,  0.0342,
         -0.0377, -0.0032,  0.1567, -0.0667, -0.1439, -0.3095,  0.2683, -0.0365,
          0.0735,  0.0038, -0.0436,  0.1862, -0.0300, -0.2254, -0.1851,  0.0244,
         -0.1050,  0.0381,  0.1042, -0.1783, -0.1164,  0.0520, -0.0717,  0.0326,
         -0.2298, -0.0865,  0.1449,  0.1431, -0.0658,  0.1658, -0.1007,  0.0497,
         -0.0390, -0.0139, -0.1908,  0.1435,  0.0370,  0.2084,  0.1090,  0.0943]])
Node Location: mul_1
Found golden for op @ mul_1 == mul_1.
output intermediate tensor None
Node Location: add_1
Found golden for op @ add_1 == add_1.
output intermediate tensor tensor([[ 0.0854,  0.0854, -0.0280,  0.0430,  0.0879, -0.0371,  0.0317, -0.0801,
         -0.0247, -0.0184,  0.0613,  0.0032, -0.0601, -0.0053, -0.0598, -0.0038,
         -0.0195, -0.0232, -0.0129, -0.0271,  0.0300, -0.0559, -0.0464, -0.0732,
         -0.0176,  0.0103, -0.0055,  0.0728, -0.0315,  0.0408,  0.0776,  0.0874,
         -0.0413, -0.0513, -0.0238, -0.0491, -0.0383, -0.0471, -0.0835, -0.0581,
          0.0610, -0.0552, -0.0019,  0.0415,  0.0298, -0.0635, -0.0356,  0.0796,
          0.0513, -0.0116,  0.0118, -0.0850,  0.0320, -0.0859,  0.0845,  0.0476,
          0.0037,  0.0306,  0.0284, -0.0640, -0.0074,  0.0625,  0.0649,  0.0581]])
Node Location: add_1
Found golden for op @ add_1 == add_1.
output intermediate tensor [32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "add_1"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "add_1"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %28 = "ttnn.typecast"(%27) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "convert_element_type_3"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%27) <{force = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":37:0, "convert_element_type_3"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %29 = "ttnn.maximum"(%28, %2) : (tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":38:0, "relu_1"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%28) <{force = false}> : (tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":38:0, "relu_1"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %30 = "ttnn.typecast"(%29) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "convert_element_type_4"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "convert_element_type_4"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %31 = "ttnn.matmul"(%30, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<64x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "mm_2"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "mm_2"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %32 = "ttnn.reshape"(%11) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "mul_2"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %33 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "mul_2"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %34 = "ttnn.add"(%32, %33) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "mul_2"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "mul_2"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "mul_2"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %35 = "ttnn.multiply"(%31, %34) : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "mul_2"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "mul_2"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %36 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 12 : i32]}> : (tensor<12xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "add_2"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %37 = "ttnn.add"(%35, %36) : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "add_2"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "add_2"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "add_2"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %38 = "ttnn.typecast"(%37) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "convert_element_type_5"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":39:0, "convert_element_type_5"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %39 = "ttnn.maximum"(%38, %3) : (tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":40:0, "relu_2"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":40:0, "relu_2"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %40 = "ttnn.typecast"(%39) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "convert_element_type_6"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "convert_element_type_6"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %41 = "ttnn.matmul"(%40, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "mm_3"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "mm_3"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %42 = "ttnn.reshape"(%11) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "mul_3"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %43 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "mul_3"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %44 = "ttnn.add"(%42, %43) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "mul_3"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "mul_3"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "mul_3"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %45 = "ttnn.multiply"(%41, %44) : (tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "mul_3"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "mul_3"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "mul_3"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %46 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 3 : i32]}> : (tensor<3xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "add_3"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %47 = "ttnn.add"(%45, %46) : (tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "add_3"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "add_3"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "add_3"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %48 = "ttnn.typecast"(%47) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "convert_element_type_7"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":41:0, "convert_element_type_7"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
tensor([[ 0.0935,  0.1837, -0.1903,  0.0848,  0.0664, -0.0238, -0.1370, -0.1336,
         -0.0966,  0.0891, -0.0208,  0.1475,  0.0182, -0.0830, -0.0809,  0.2494,
          0.1415,  0.1372,  0.0989, -0.2879, -0.0652, -0.0637, -0.1211, -0.0390,
         -0.0552,  0.0071,  0.1512,  0.0060, -0.1754, -0.2687,  0.3459,  0.0509,
          0.0322, -0.0475, -0.0674,  0.1372, -0.0684, -0.2725, -0.2686, -0.0337,
         -0.0440, -0.0171,  0.1023, -0.1368, -0.0866, -0.0115, -0.1073,  0.1122,
         -0.1785, -0.0981,  0.1567,  0.0582, -0.0339,  0.0798, -0.0162,  0.0974,
         -0.0353,  0.0168, -0.1624,  0.0796,  0.0296,  0.2709,  0.1739,  0.1524]])
Node Location: add_1
Found golden for op @ add_1 == add_1.
output intermediate tensor None
Node Location: add_1
Found golden for op @ add_1 == add_1.
output intermediate tensor None
Node Location: convert_element_type_3
Found golden for op @ convert_element_type_3 == convert_element_type_3.
output intermediate tensor tensor([[ 0.0938,  0.1836, -0.1904,  0.0850,  0.0664, -0.0238, -0.1367, -0.1338,
         -0.0967,  0.0889, -0.0208,  0.1475,  0.0182, -0.0830, -0.0811,  0.2490,
          0.1416,  0.1377,  0.0991, -0.2871, -0.0654, -0.0635, -0.1211, -0.0391,
         -0.0552,  0.0071,  0.1514,  0.0060, -0.1758, -0.2695,  0.3457,  0.0508,
          0.0322, -0.0474, -0.0674,  0.1367, -0.0684, -0.2734, -0.2695, -0.0337,
         -0.0439, -0.0171,  0.1025, -0.1367, -0.0864, -0.0115, -0.1074,  0.1123,
         -0.1787, -0.0981,  0.1562,  0.0581, -0.0339,  0.0801, -0.0162,  0.0972,
         -0.0352,  0.0167, -0.1621,  0.0796,  0.0295,  0.2715,  0.1738,  0.1523]],
       dtype=torch.bfloat16)
Node Location: convert_element_type_3
Found golden for op @ convert_element_type_3 == convert_element_type_3.
output intermediate tensor None
Node Location: relu_1
Found golden for op @ relu_1 == relu_1.
output intermediate tensor tensor([[0.0938, 0.1836, 0.0000, 0.0850, 0.0664, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0889, 0.0000, 0.1475, 0.0182, 0.0000, 0.0000, 0.2490, 0.1416, 0.1377,
         0.0991, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0071, 0.1514,
         0.0060, 0.0000, 0.0000, 0.3457, 0.0508, 0.0322, 0.0000, 0.0000, 0.1367,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1025, 0.0000, 0.0000,
         0.0000, 0.0000, 0.1123, 0.0000, 0.0000, 0.1562, 0.0581, 0.0000, 0.0801,
         0.0000, 0.0972, 0.0000, 0.0167, 0.0000, 0.0796, 0.0295, 0.2715, 0.1738,
         0.1523]], dtype=torch.bfloat16)
Node Location: relu_1
Found golden for op @ relu_1 == relu_1.
output intermediate tensor None
Node Location: convert_element_type_4
Found golden for op @ convert_element_type_4 == convert_element_type_4.
output intermediate tensor tensor([[0.0938, 0.1836, 0.0000, 0.0850, 0.0664, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0889, 0.0000, 0.1475, 0.0182, 0.0000, 0.0000, 0.2490, 0.1416, 0.1377,
         0.0991, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0071, 0.1514,
         0.0060, 0.0000, 0.0000, 0.3457, 0.0508, 0.0322, 0.0000, 0.0000, 0.1367,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1025, 0.0000, 0.0000,
         0.0000, 0.0000, 0.1123, 0.0000, 0.0000, 0.1562, 0.0581, 0.0000, 0.0801,
         0.0000, 0.0972, 0.0000, 0.0167, 0.0000, 0.0796, 0.0295, 0.2715, 0.1738,
         0.1523]])
Node Location: convert_element_type_4
Found golden for op @ convert_element_type_4 == convert_element_type_4.
output intermediate tensor None
Node Location: mm_2
Found golden for op @ mm_2 == mm_2.
output intermediate tensor tensor([[ 0.0533, -0.0234, -0.0172,  0.0112,  0.0810,  0.0315, -0.1048,  0.0656,
         -0.0111, -0.0541,  0.0314,  0.0898]])
Node Location: mm_2
Found golden for op @ mm_2 == mm_2.
output intermediate tensor None
Node Location: mul_2
Found golden for op @ mul_2 == mul_2.
output intermediate tensor tensor([[1.]])
Node Location: mul_2
Found golden for op @ mul_2 == mul_2.
output intermediate tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Node Location: mul_2
Found golden for op @ mul_2 == mul_2.
output intermediate tensor tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])
Node Location: mul_2
Found golden for op @ mul_2 == mul_2.
output intermediate tensor None
Node Location: mul_2
Found golden for op @ mul_2 == mul_2.
output intermediate tensor None
Node Location: mul_2
Found golden for op @ mul_2 == mul_2.
output intermediate tensor tensor([[ 0.0533, -0.0234, -0.0172,  0.0112,  0.0810,  0.0315, -0.1048,  0.0656,
         -0.0111, -0.0541,  0.0314,  0.0898]])
Node Location: mul_2
Found golden for op @ mul_2 == mul_2.
output intermediate tensor None
Node Location: add_2
Found golden for op @ add_2 == add_2.
output intermediate tensor tensor([[-4.5898e-02,  6.5918e-02,  8.3008e-02, -1.1475e-01,  4.2725e-02,
          5.5176e-02, -1.0840e-01,  1.2305e-01,  1.1377e-01,  9.5844e-05,
          9.0820e-02,  2.8442e-02]])
Node Location: add_2
Found golden for op @ add_2 == add_2.
output intermediate tensor tensor([[ 0.0074,  0.0425,  0.0658, -0.1036,  0.1237,  0.0867, -0.2132,  0.1887,
          0.1026, -0.0540,  0.1222,  0.1183]])
Node Location: add_2
Found golden for op @ add_2 == add_2.
output intermediate tensor None
Node Location: add_2
Found golden for op @ add_2 == add_2.
output intermediate tensor None
Node Location: convert_element_type_5
Found golden for op @ convert_element_type_5 == convert_element_type_5.
output intermediate tensor tensor([[ 0.0074,  0.0425,  0.0659, -0.1035,  0.1235,  0.0869, -0.2129,  0.1885,
          0.1025, -0.0540,  0.1221,  0.1182]], dtype=torch.bfloat16)
Node Location: convert_element_type_5
Found golden for op @ convert_element_type_5 == convert_element_type_5.
output intermediate tensor None
Node Location: relu_2
Found golden for op @ relu_2 == relu_2.
output intermediate tensor tensor([[0.0074, 0.0425, 0.0659, 0.0000, 0.1235, 0.0869, 0.0000, 0.1885, 0.1025,
         0.0000, 0.1221, 0.1182]], dtype=torch.bfloat16)
Node Location: relu_2
Found golden for op @ relu_2 == relu_2.
output intermediate tensor None
Node Location: convert_element_type_6
Found golden for op @ convert_element_type_6 == convert_element_type_6.
output intermediate tensor tensor([[0.0074, 0.0425, 0.0659, 0.0000, 0.1235, 0.0869, 0.0000, 0.1885, 0.1025,
         0.0000, 0.1221, 0.1182]])
Node Location: convert_element_type_6
Found golden for op @ convert_element_type_6 == convert_element_type_6.
output intermediate tensor None
Node Location: mm_3
Found golden for op @ mm_3 == mm_3.
output intermediate tensor tensor([[-0.0242,  0.0872,  0.0593]])
Node Location: mm_3
Found golden for op @ mm_3 == mm_3.
output intermediate tensor None
Node Location: mul_3
Found golden for op @ mul_3 == mul_3.
output intermediate tensor tensor([[1.]])
Node Location: mul_3
Found golden for op @ mul_3 == mul_3.
output intermediate tensor tensor([[0., 0., 0.]])
Node Location: mul_3
Found golden for op @ mul_3 == mul_3.
output intermediate tensor tensor([[1., 1., 1.]])
Node Location: mul_3
Found golden for op @ mul_3 == mul_3.
output intermediate tensor None
Node Location: mul_3
Found golden for op @ mul_3 == mul_3.
output intermediate tensor None
Node Location: mul_3
Found golden for op @ mul_3 == mul_3.
output intermediate tensor tensor([[-0.0242,  0.0872,  0.0593]])
Node Location: mul_3
Found golden for op @ mul_3 == mul_3.
output intermediate tensor None
Node Location: mul_3
Found golden for op @ mul_3 == mul_3.
output intermediate tensor None
Node Location: add_3
Found golden for op @ add_3 == add_3.
output intermediate tensor tensor([[ 0.0820, -0.0742, -0.1001]])
Node Location: add_3
Found golden for op @ add_3 == add_3.
output intermediate tensor tensor([[ 0.0579,  0.0130, -0.0408]])
Node Location: add_3
Found golden for op @ add_3 == add_3.
output intermediate tensor None
Node Location: add_3
Found golden for op @ add_3 == add_3.
output intermediate tensor None
Node Location: convert_element_type_7
Found golden for op @ convert_element_type_7 == convert_element_type_7.
output intermediate tensor tensor([[ 0.0579,  0.0129, -0.0408]], dtype=torch.bfloat16)
Node Location: convert_element_type_7
Found golden for op @ convert_element_type_7 == convert_element_type_7.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %49 = "ttnn.typecast"(%48) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x3xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":44:0, "convert_element_type_8"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x3xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":44:0, "convert_element_type_8"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %50 = "ttnn.matmul"(%49, %arg8) <{transpose_a = false, transpose_b = false}> : (tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":44:0, "mm_4"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x3xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":44:0, "mm_4"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %51 = "ttnn.multiply"(%50, %34) : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":44:0, "mul_4"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":44:0, "mul_4"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":44:0, "mul_4"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %52 = "ttnn.reshape"(%arg9) <{shape = [1 : i32, 12 : i32]}> : (tensor<12xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":44:0, "add_4"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %53 = "ttnn.add"(%51, %52) : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":44:0, "add_4"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":44:0, "add_4"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":44:0, "add_4"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %54 = "ttnn.typecast"(%53) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":44:0, "convert_element_type_9"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":44:0, "convert_element_type_9"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %55 = "ttnn.maximum"(%54, %3) : (tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":45:0, "relu_3"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":45:0, "relu_3"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":45:0, "relu_3"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %56 = "ttnn.typecast"(%55) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":46:0, "convert_element_type_10"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x12xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":46:0, "convert_element_type_10"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %57 = "ttnn.matmul"(%56, %arg10) <{transpose_a = false, transpose_b = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":46:0, "mm_5"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":46:0, "mm_5"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %58 = "ttnn.multiply"(%57, %24) : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":46:0, "mul_5"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":46:0, "mul_5"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":46:0, "mul_5"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %59 = "ttnn.reshape"(%arg11) <{shape = [1 : i32, 64 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":46:0, "add_5"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %60 = "ttnn.add"(%58, %59) : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":46:0, "add_5"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":46:0, "add_5"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":46:0, "add_5"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %61 = "ttnn.typecast"(%60) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":46:0, "convert_element_type_11"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":46:0, "convert_element_type_11"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %62 = "ttnn.maximum"(%61, %2) : (tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":47:0, "relu_4"])
output intermediate tensor None
Node Location: convert_element_type_8
Found golden for op @ convert_element_type_8 == convert_element_type_8.
output intermediate tensor tensor([[ 0.0579,  0.0129, -0.0408]])
Node Location: convert_element_type_8
Found golden for op @ convert_element_type_8 == convert_element_type_8.
output intermediate tensor None
Node Location: mm_4
Found golden for op @ mm_4 == mm_4.
output intermediate tensor tensor([[ 0.0180,  0.0125, -0.0244,  0.0028, -0.0244,  0.0061, -0.0106,  0.0251,
         -0.0040,  0.0226, -0.0464, -0.0218]])
Node Location: mm_4
Found golden for op @ mm_4 == mm_4.
output intermediate tensor None
Node Location: mul_4
Found golden for op @ mul_4 == mul_4.
output intermediate tensor tensor([[ 0.0180,  0.0125, -0.0244,  0.0028, -0.0244,  0.0061, -0.0106,  0.0251,
         -0.0040,  0.0226, -0.0464, -0.0218]])
Node Location: mul_4
Found golden for op @ mul_4 == mul_4.
output intermediate tensor None
Node Location: mul_4
Found golden for op @ mul_4 == mul_4.
output intermediate tensor None
Node Location: add_4
Found golden for op @ add_4 == add_4.
output intermediate tensor tensor([[-0.3984, -0.1123, -0.0640, -0.1177, -0.2041,  0.4238,  0.0344, -0.0522,
          0.4141,  0.0522, -0.4336, -0.4609]])
Node Location: add_4
Found golden for op @ add_4 == add_4.
output intermediate tensor tensor([[-0.3805, -0.0998, -0.0883, -0.1149, -0.2285,  0.4299,  0.0239, -0.0271,
          0.4100,  0.0748, -0.4800, -0.4828]])
Node Location: add_4
Found golden for op @ add_4 == add_4.
output intermediate tensor None
Node Location: add_4
Found golden for op @ add_4 == add_4.
output intermediate tensor None
Node Location: convert_element_type_9
Found golden for op @ convert_element_type_9 == convert_element_type_9.
output intermediate tensor tensor([[-0.3809, -0.0996, -0.0884, -0.1147, -0.2285,  0.4297,  0.0238, -0.0271,
          0.4102,  0.0747, -0.4805, -0.4824]], dtype=torch.bfloat16)
Node Location: convert_element_type_9
Found golden for op @ convert_element_type_9 == convert_element_type_9.
output intermediate tensor None
Node Location: relu_3
Found golden for op @ relu_3 == relu_3.
output intermediate tensor tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4297, 0.0238, 0.0000, 0.4102,
         0.0747, 0.0000, 0.0000]], dtype=torch.bfloat16)
Node Location: relu_3
Found golden for op @ relu_3 == relu_3.
output intermediate tensor None
Node Location: relu_3
Found golden for op @ relu_3 == relu_3.
output intermediate tensor None
Node Location: convert_element_type_10
Found golden for op @ convert_element_type_10 == convert_element_type_10.
output intermediate tensor tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4297, 0.0238, 0.0000, 0.4102,
         0.0747, 0.0000, 0.0000]])
Node Location: convert_element_type_10
Found golden for op @ convert_element_type_10 == convert_element_type_10.
output intermediate tensor None
Node Location: mm_5
Found golden for op @ mm_5 == mm_5.
output intermediate tensor tensor([[-9.6622e-03, -1.0696e-01,  1.9184e-02, -1.2930e-02,  9.6392e-02,
          1.4741e-01, -7.3139e-02, -9.0573e-02,  9.2724e-02,  1.8909e-01,
          8.3528e-02, -9.1249e-04,  2.1328e-01, -9.1101e-02,  1.4486e-01,
         -3.0941e-02,  4.7253e-02,  1.4478e-02, -1.1212e-02,  1.2857e-01,
          5.1332e-02, -2.6054e-03, -9.1854e-02, -2.0180e-01, -2.9163e-02,
          4.3667e-02,  1.3536e-03,  1.6212e-01,  1.8576e-01, -3.1312e-02,
          1.6470e-01, -6.0232e-02,  8.6628e-02,  1.9035e-01, -1.1604e-02,
          1.3089e-04,  6.8741e-02,  4.5784e-02,  1.4270e-01,  9.3707e-02,
         -1.0527e-01,  1.4784e-01, -1.3362e-01, -1.2875e-01, -6.5962e-02,
         -8.4507e-02,  6.8113e-03, -1.2686e-01, -3.3205e-02,  4.9038e-03,
          1.0848e-01,  2.9250e-02, -4.7336e-02, -4.1415e-02, -3.5197e-02,
          7.8448e-02,  1.2112e-01, -1.7717e-01, -1.4413e-02, -6.7333e-02,
          7.9175e-02,  1.5714e-02,  4.5015e-02, -9.0979e-02]])
Node Location: mm_5
Found golden for op @ mm_5 == mm_5.
output intermediate tensor None
Node Location: mul_5
Found golden for op @ mul_5 == mul_5.
output intermediate tensor tensor([[-9.6622e-03, -1.0696e-01,  1.9184e-02, -1.2930e-02,  9.6392e-02,
          1.4741e-01, -7.3139e-02, -9.0573e-02,  9.2724e-02,  1.8909e-01,
          8.3528e-02, -9.1249e-04,  2.1328e-01, -9.1101e-02,  1.4486e-01,
         -3.0941e-02,  4.7253e-02,  1.4478e-02, -1.1212e-02,  1.2857e-01,
          5.1332e-02, -2.6054e-03, -9.1854e-02, -2.0180e-01, -2.9163e-02,
          4.3667e-02,  1.3536e-03,  1.6212e-01,  1.8576e-01, -3.1312e-02,
          1.6470e-01, -6.0232e-02,  8.6628e-02,  1.9035e-01, -1.1604e-02,
          1.3089e-04,  6.8741e-02,  4.5784e-02,  1.4270e-01,  9.3707e-02,
         -1.0527e-01,  1.4784e-01, -1.3362e-01, -1.2875e-01, -6.5962e-02,
         -8.4507e-02,  6.8113e-03, -1.2686e-01, -3.3205e-02,  4.9038e-03,
          1.0848e-01,  2.9250e-02, -4.7336e-02, -4.1415e-02, -3.5197e-02,
          7.8448e-02,  1.2112e-01, -1.7717e-01, -1.4413e-02, -6.7333e-02,
          7.9175e-02,  1.5714e-02,  4.5015e-02, -9.0979e-02]])
Node Location: mul_5
Found golden for op @ mul_5 == mul_5.
output intermediate tensor None
Node Location: mul_5
Found golden for op @ mul_5 == mul_5.
output intermediate tensor None
Node Location: add_5
Found golden for op @ add_5 == add_5.
output intermediate tensor tensor([[-0.1777, -0.0601, -0.2754,  0.2354,  0.2441, -0.2119,  0.0708,  0.2314,
         -0.0762, -0.0796, -0.2812,  0.2656,  0.1816, -0.0317,  0.1367, -0.2168,
          0.1055,  0.0466,  0.1338,  0.1543, -0.0591,  0.0786,  0.0752,  0.2812,
         -0.1924,  0.0325, -0.1523,  0.0374, -0.2129,  0.0962, -0.1592, -0.1357,
         -0.1855,  0.1758,  0.0762,  0.1187, -0.0625,  0.1826,  0.0233, -0.1357,
          0.1270,  0.2617,  0.1201, -0.2422, -0.1787, -0.2207, -0.0258, -0.0518,
         -0.0850,  0.2734,  0.0017,  0.2441,  0.0679, -0.1719,  0.0640,  0.2490,
         -0.1582,  0.2354, -0.1895, -0.0938,  0.0762, -0.0776, -0.2773, -0.2402]])
Node Location: add_5
Found golden for op @ add_5 == add_5.
output intermediate tensor tensor([[-0.1874, -0.1670, -0.2562,  0.2224,  0.3405, -0.0645, -0.0023,  0.1409,
          0.0166,  0.1095, -0.1977,  0.2647,  0.3949, -0.1228,  0.2816, -0.2477,
          0.1527,  0.0611,  0.1226,  0.2829, -0.0078,  0.0760, -0.0167,  0.0794,
         -0.2215,  0.0761, -0.1510,  0.1995, -0.0271,  0.0649,  0.0055, -0.1960,
         -0.0989,  0.3661,  0.0646,  0.1188,  0.0062,  0.2284,  0.1660, -0.0420,
          0.0217,  0.4096, -0.0135, -0.3709, -0.2447, -0.3052, -0.0189, -0.1786,
         -0.1182,  0.2783,  0.1102,  0.2734,  0.0205, -0.2133,  0.0288,  0.3275,
         -0.0371,  0.0582, -0.2039, -0.1611,  0.1553, -0.0619, -0.2323, -0.3312]])
Node Location: add_5
Found golden for op @ add_5 == add_5.
output intermediate tensor None
Node Location: add_5
Found golden for op @ add_5 == add_5.
output intermediate tensor None
Node Location: convert_element_type_11
Found golden for op @ convert_element_type_11 == convert_element_type_11.
output intermediate tensor tensor([[-0.1875, -0.1670, -0.2559,  0.2227,  0.3398, -0.0645, -0.0023,  0.1406,
          0.0166,  0.1094, -0.1973,  0.2656,  0.3945, -0.1230,  0.2812, -0.2480,
          0.1523,  0.0610,  0.1226,  0.2832, -0.0078,  0.0762, -0.0166,  0.0796,
         -0.2217,  0.0762, -0.1514,  0.1992, -0.0271,  0.0649,  0.0055, -0.1963,
         -0.0991,  0.3652,  0.0645,  0.1187,  0.0062,  0.2285,  0.1660, -0.0420,
          0.0217,  0.4102, -0.0135, -0.3711, -0.2451, -0.3047, -0.0189, -0.1787,
         -0.1182,  0.2793,  0.1104,  0.2734,  0.0205, -0.2129,  0.0288,  0.3281,
         -0.0371,  0.0581, -0.2041, -0.1611,  0.1553, -0.0620, -0.2324, -0.3320]],
       dtype=torch.bfloat16)
Node Location: convert_element_type_11
Found golden for op @ convert_element_type_11 == convert_element_type_11.
output intermediate tensor None
Node Location: relu_4
Found golden for op @ relu_4 == relu_4.
output intermediate tensor [32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":47:0, "relu_4"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":47:0, "relu_4"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %63 = "ttnn.typecast"(%62) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":48:0, "convert_element_type_12"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x64xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":48:0, "convert_element_type_12"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %64 = "ttnn.matmul"(%63, %arg12) <{transpose_a = false, transpose_b = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<64x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":48:0, "mm_6"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x64xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":48:0, "mm_6"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %65 = "ttnn.multiply"(%64, %14) : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":48:0, "mul_6"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":48:0, "mul_6"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":48:0, "mul_6"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %66 = "ttnn.reshape"(%arg13) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":48:0, "add_6"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %67 = "ttnn.add"(%65, %66) : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":48:0, "add_6"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":48:0, "add_6"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":48:0, "add_6"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %68 = "ttnn.typecast"(%67) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":48:0, "convert_element_type_13"])
tensor([[0.0000, 0.0000, 0.0000, 0.2227, 0.3398, 0.0000, 0.0000, 0.1406, 0.0166,
         0.1094, 0.0000, 0.2656, 0.3945, 0.0000, 0.2812, 0.0000, 0.1523, 0.0610,
         0.1226, 0.2832, 0.0000, 0.0762, 0.0000, 0.0796, 0.0000, 0.0762, 0.0000,
         0.1992, 0.0000, 0.0649, 0.0055, 0.0000, 0.0000, 0.3652, 0.0645, 0.1187,
         0.0062, 0.2285, 0.1660, 0.0000, 0.0217, 0.4102, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.2793, 0.1104, 0.2734, 0.0205, 0.0000,
         0.0288, 0.3281, 0.0000, 0.0581, 0.0000, 0.0000, 0.1553, 0.0000, 0.0000,
         0.0000]], dtype=torch.bfloat16)
Node Location: relu_4
Found golden for op @ relu_4 == relu_4.
output intermediate tensor None
Node Location: relu_4
Found golden for op @ relu_4 == relu_4.
output intermediate tensor None
Node Location: convert_element_type_12
Found golden for op @ convert_element_type_12 == convert_element_type_12.
output intermediate tensor tensor([[0.0000, 0.0000, 0.0000, 0.2227, 0.3398, 0.0000, 0.0000, 0.1406, 0.0166,
         0.1094, 0.0000, 0.2656, 0.3945, 0.0000, 0.2812, 0.0000, 0.1523, 0.0610,
         0.1226, 0.2832, 0.0000, 0.0762, 0.0000, 0.0796, 0.0000, 0.0762, 0.0000,
         0.1992, 0.0000, 0.0649, 0.0055, 0.0000, 0.0000, 0.3652, 0.0645, 0.1187,
         0.0062, 0.2285, 0.1660, 0.0000, 0.0217, 0.4102, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.2793, 0.1104, 0.2734, 0.0205, 0.0000,
         0.0288, 0.3281, 0.0000, 0.0581, 0.0000, 0.0000, 0.1553, 0.0000, 0.0000,
         0.0000]])
Node Location: convert_element_type_12
Found golden for op @ convert_element_type_12 == convert_element_type_12.
output intermediate tensor None
Node Location: mm_6
Found golden for op @ mm_6 == mm_6.
output intermediate tensor tensor([[-0.0587, -0.0798, -0.0378,  0.0850,  0.0451,  0.0933,  0.1044, -0.1355,
         -0.0539,  0.1694, -0.0568, -0.0761, -0.0278,  0.0166,  0.0536,  0.0211,
         -0.0623, -0.0241,  0.0317,  0.0126,  0.0044, -0.0338,  0.0763, -0.0777,
         -0.0555, -0.0271, -0.0180, -0.0400,  0.0319, -0.0450,  0.1495, -0.1272,
         -0.0695, -0.0877,  0.0033,  0.1117, -0.0027, -0.0613,  0.0870,  0.0048,
         -0.0199, -0.0459, -0.0221, -0.0288,  0.0187,  0.0046, -0.0472, -0.1329,
          0.1177,  0.0037, -0.0151,  0.0747, -0.0268,  0.0632, -0.0456, -0.0775,
          0.0474, -0.1235,  0.0655,  0.0629,  0.0970,  0.0648, -0.0351, -0.1411,
          0.0711,  0.0972, -0.0964,  0.1317,  0.0329,  0.1390, -0.0064, -0.0196,
         -0.0883,  0.2185,  0.1271, -0.0206, -0.0528, -0.0895, -0.0388,  0.0430,
          0.0003,  0.0510, -0.0021, -0.1491,  0.0724, -0.1090,  0.0173, -0.0687,
         -0.0094,  0.0977,  0.1337,  0.0780, -0.2126, -0.1580,  0.0675,  0.0400,
         -0.0152, -0.0614, -0.1473, -0.0141,  0.1327, -0.1283, -0.0682,  0.1504,
         -0.0597, -0.0137,  0.0831,  0.0780,  0.0897, -0.0935,  0.1012, -0.0355,
         -0.0779, -0.0627, -0.0247,  0.1424,  0.0826,  0.0800,  0.1562, -0.0236,
          0.1680, -0.1119, -0.1148,  0.0629,  0.0925,  0.0427,  0.0054,  0.0196]])
Node Location: mm_6
Found golden for op @ mm_6 == mm_6.
output intermediate tensor None
Node Location: mul_6
Found golden for op @ mul_6 == mul_6.
output intermediate tensor tensor([[-0.0587, -0.0798, -0.0378,  0.0850,  0.0451,  0.0933,  0.1044, -0.1355,
         -0.0539,  0.1694, -0.0568, -0.0761, -0.0278,  0.0166,  0.0536,  0.0211,
         -0.0623, -0.0241,  0.0317,  0.0126,  0.0044, -0.0338,  0.0763, -0.0777,
         -0.0555, -0.0271, -0.0180, -0.0400,  0.0319, -0.0450,  0.1495, -0.1272,
         -0.0695, -0.0877,  0.0033,  0.1117, -0.0027, -0.0613,  0.0870,  0.0048,
         -0.0199, -0.0459, -0.0221, -0.0288,  0.0187,  0.0046, -0.0472, -0.1329,
          0.1177,  0.0037, -0.0151,  0.0747, -0.0268,  0.0632, -0.0456, -0.0775,
          0.0474, -0.1235,  0.0655,  0.0629,  0.0970,  0.0648, -0.0351, -0.1411,
          0.0711,  0.0972, -0.0964,  0.1317,  0.0329,  0.1390, -0.0064, -0.0196,
         -0.0883,  0.2185,  0.1271, -0.0206, -0.0528, -0.0895, -0.0388,  0.0430,
          0.0003,  0.0510, -0.0021, -0.1491,  0.0724, -0.1090,  0.0173, -0.0687,
         -0.0094,  0.0977,  0.1337,  0.0780, -0.2126, -0.1580,  0.0675,  0.0400,
         -0.0152, -0.0614, -0.1473, -0.0141,  0.1327, -0.1283, -0.0682,  0.1504,
         -0.0597, -0.0137,  0.0831,  0.0780,  0.0897, -0.0935,  0.1012, -0.0355,
         -0.0779, -0.0627, -0.0247,  0.1424,  0.0826,  0.0800,  0.1562, -0.0236,
          0.1680, -0.1119, -0.1148,  0.0629,  0.0925,  0.0427,  0.0054,  0.0196]])
Node Location: mul_6
Found golden for op @ mul_6 == mul_6.
output intermediate tensor None
Node Location: mul_6
Found golden for op @ mul_6 == mul_6.
output intermediate tensor None
Node Location: add_6
Found golden for op @ add_6 == add_6.
output intermediate tensor tensor([[ 0.0884, -0.0564, -0.0884,  0.1030, -0.0102,  0.0400,  0.0623, -0.0513,
         -0.0737, -0.1055, -0.0403,  0.0908,  0.1089,  0.1138,  0.0176,  0.0364,
         -0.0547, -0.0449, -0.0820,  0.0442, -0.0830, -0.0723,  0.0157,  0.0486,
         -0.0376,  0.1030,  0.0957,  0.0928, -0.0532,  0.0898, -0.1011, -0.0090,
          0.0403, -0.0496, -0.0330,  0.1235,  0.0129, -0.0255,  0.0718, -0.0654,
          0.1221, -0.1235, -0.0461,  0.0762, -0.0776,  0.0640,  0.0591, -0.0221,
          0.0179,  0.0459,  0.1187,  0.0991,  0.0718, -0.1245,  0.0447,  0.0554,
         -0.0559, -0.0132,  0.1035, -0.0576,  0.1060, -0.0542, -0.0334, -0.1201,
         -0.1230,  0.0532,  0.0903,  0.0320, -0.0520,  0.0073, -0.0125,  0.0306,
          0.0393,  0.0134,  0.0182,  0.1035, -0.1069,  0.0103, -0.0347, -0.0020,
          0.0542, -0.1094,  0.0281,  0.0630, -0.1147, -0.0942,  0.1016,  0.1221,
         -0.1240,  0.0203,  0.0933, -0.0166, -0.0229, -0.0486,  0.0182,  0.0190,
          0.1230,  0.0284,  0.1040, -0.0280, -0.0107, -0.0771, -0.0270, -0.1011,
          0.0288, -0.0106,  0.0251,  0.0515,  0.0569,  0.0557,  0.1104, -0.0923,
         -0.0364, -0.0520,  0.0010, -0.0231, -0.0132,  0.0703,  0.0093, -0.0361,
         -0.1104,  0.1089, -0.0723, -0.0132,  0.0172, -0.0781,  0.0054, -0.0103]])
Node Location: add_6
Found golden for op @ add_6 == add_6.
output intermediate tensor tensor([[ 0.0297, -0.1362, -0.1261,  0.1880,  0.0349,  0.1333,  0.1666, -0.1868,
         -0.1277,  0.0639, -0.0971,  0.0147,  0.0810,  0.1304,  0.0712,  0.0574,
         -0.1170, -0.0690, -0.0503,  0.0568, -0.0786, -0.1061,  0.0921, -0.0291,
         -0.0931,  0.0759,  0.0777,  0.0528, -0.0213,  0.0449,  0.0484, -0.1362,
         -0.0292, -0.1373, -0.0297,  0.2353,  0.0101, -0.0868,  0.1588, -0.0606,
          0.1022, -0.1695, -0.0682,  0.0474, -0.0590,  0.0686,  0.0119, -0.1550,
          0.1356,  0.0496,  0.1036,  0.1739,  0.0449, -0.0613, -0.0009, -0.0220,
         -0.0085, -0.1367,  0.1690,  0.0053,  0.2029,  0.0106, -0.0686, -0.2612,
         -0.0519,  0.1505, -0.0060,  0.1637, -0.0191,  0.1464, -0.0189,  0.0111,
         -0.0490,  0.2319,  0.1453,  0.0830, -0.1597, -0.0792, -0.0735,  0.0410,
          0.0545, -0.0584,  0.0260, -0.0861, -0.0424, -0.2032,  0.1189,  0.0534,
         -0.1334,  0.1180,  0.2269,  0.0614, -0.2355, -0.2066,  0.0857,  0.0591,
          0.1078, -0.0330, -0.0433, -0.0420,  0.1220, -0.2054, -0.0952,  0.0493,
         -0.0309, -0.0244,  0.1083,  0.1295,  0.1466, -0.0378,  0.2115, -0.1278,
         -0.1142, -0.1147, -0.0237,  0.1193,  0.0694,  0.1503,  0.1655, -0.0597,
          0.0577, -0.0030, -0.1871,  0.0497,  0.1097, -0.0354,  0.0108,  0.0093]])
Node Location: add_6
Found golden for op @ add_6 == add_6.
output intermediate tensor None
Node Location: add_6
Found golden for op @ add_6 == add_6.
output intermediate tensor None
Node Location: convert_element_type_13
Found golden for op @ convert_element_type_13 == convert_element_type_13.
output intermediate tensor [32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":48:0, "convert_element_type_13"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %69 = "ttnn.maximum"(%68, %1) : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":49:0, "relu_5"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":49:0, "relu_5"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":49:0, "relu_5"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %70 = "ttnn.typecast"(%69) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "convert_element_type_14"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "convert_element_type_14"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %71 = "ttnn.matmul"(%70, %arg14) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "mm_7"])
tensor([[ 0.0297, -0.1357, -0.1260,  0.1885,  0.0349,  0.1328,  0.1670, -0.1865,
         -0.1279,  0.0640, -0.0972,  0.0148,  0.0811,  0.1309,  0.0713,  0.0574,
         -0.1172, -0.0688, -0.0503,  0.0569, -0.0786, -0.1060,  0.0923, -0.0292,
         -0.0933,  0.0757,  0.0776,  0.0527, -0.0212,  0.0449,  0.0483, -0.1357,
         -0.0293, -0.1377, -0.0297,  0.2354,  0.0101, -0.0869,  0.1592, -0.0605,
          0.1021, -0.1699, -0.0684,  0.0474, -0.0591,  0.0684,  0.0118, -0.1553,
          0.1357,  0.0496,  0.1035,  0.1738,  0.0449, -0.0613, -0.0009, -0.0221,
         -0.0085, -0.1367,  0.1689,  0.0053,  0.2031,  0.0106, -0.0684, -0.2617,
         -0.0520,  0.1504, -0.0060,  0.1641, -0.0192,  0.1465, -0.0189,  0.0110,
         -0.0491,  0.2314,  0.1455,  0.0830, -0.1602, -0.0791, -0.0737,  0.0410,
          0.0544, -0.0583,  0.0260, -0.0859, -0.0425, -0.2031,  0.1187,  0.0535,
         -0.1338,  0.1182,  0.2266,  0.0615, -0.2354, -0.2070,  0.0859,  0.0591,
          0.1079, -0.0330, -0.0432, -0.0420,  0.1221, -0.2051, -0.0952,  0.0493,
         -0.0309, -0.0243,  0.1084,  0.1299,  0.1465, -0.0378,  0.2119, -0.1279,
         -0.1143, -0.1147, -0.0237,  0.1191,  0.0693,  0.1504,  0.1650, -0.0598,
          0.0576, -0.0030, -0.1875,  0.0496,  0.1099, -0.0354,  0.0109,  0.0093]],
       dtype=torch.bfloat16)
Node Location: convert_element_type_13
Found golden for op @ convert_element_type_13 == convert_element_type_13.
output intermediate tensor None
Node Location: relu_5
Found golden for op @ relu_5 == relu_5.
output intermediate tensor tensor([[0.0297, 0.0000, 0.0000, 0.1885, 0.0349, 0.1328, 0.1670, 0.0000, 0.0000,
         0.0640, 0.0000, 0.0148, 0.0811, 0.1309, 0.0713, 0.0574, 0.0000, 0.0000,
         0.0000, 0.0569, 0.0000, 0.0000, 0.0923, 0.0000, 0.0000, 0.0757, 0.0776,
         0.0527, 0.0000, 0.0449, 0.0483, 0.0000, 0.0000, 0.0000, 0.0000, 0.2354,
         0.0101, 0.0000, 0.1592, 0.0000, 0.1021, 0.0000, 0.0000, 0.0474, 0.0000,
         0.0684, 0.0118, 0.0000, 0.1357, 0.0496, 0.1035, 0.1738, 0.0449, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.1689, 0.0053, 0.2031, 0.0106, 0.0000,
         0.0000, 0.0000, 0.1504, 0.0000, 0.1641, 0.0000, 0.1465, 0.0000, 0.0110,
         0.0000, 0.2314, 0.1455, 0.0830, 0.0000, 0.0000, 0.0000, 0.0410, 0.0544,
         0.0000, 0.0260, 0.0000, 0.0000, 0.0000, 0.1187, 0.0535, 0.0000, 0.1182,
         0.2266, 0.0615, 0.0000, 0.0000, 0.0859, 0.0591, 0.1079, 0.0000, 0.0000,
         0.0000, 0.1221, 0.0000, 0.0000, 0.0493, 0.0000, 0.0000, 0.1084, 0.1299,
         0.1465, 0.0000, 0.2119, 0.0000, 0.0000, 0.0000, 0.0000, 0.1191, 0.0693,
         0.1504, 0.1650, 0.0000, 0.0576, 0.0000, 0.0000, 0.0496, 0.1099, 0.0000,
         0.0109, 0.0093]], dtype=torch.bfloat16)
Node Location: relu_5
Found golden for op @ relu_5 == relu_5.
output intermediate tensor None
Node Location: relu_5
Found golden for op @ relu_5 == relu_5.
output intermediate tensor None
Node Location: convert_element_type_14
Found golden for op @ convert_element_type_14 == convert_element_type_14.
output intermediate tensor tensor([[0.0297, 0.0000, 0.0000, 0.1885, 0.0349, 0.1328, 0.1670, 0.0000, 0.0000,
         0.0640, 0.0000, 0.0148, 0.0811, 0.1309, 0.0713, 0.0574, 0.0000, 0.0000,
         0.0000, 0.0569, 0.0000, 0.0000, 0.0923, 0.0000, 0.0000, 0.0757, 0.0776,
         0.0527, 0.0000, 0.0449, 0.0483, 0.0000, 0.0000, 0.0000, 0.0000, 0.2354,
         0.0101, 0.0000, 0.1592, 0.0000, 0.1021, 0.0000, 0.0000, 0.0474, 0.0000,
         0.0684, 0.0118, 0.0000, 0.1357, 0.0496, 0.1035, 0.1738, 0.0449, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.1689, 0.0053, 0.2031, 0.0106, 0.0000,
         0.0000, 0.0000, 0.1504, 0.0000, 0.1641, 0.0000, 0.1465, 0.0000, 0.0110,
         0.0000, 0.2314, 0.1455, 0.0830, 0.0000, 0.0000, 0.0000, 0.0410, 0.0544,
         0.0000, 0.0260, 0.0000, 0.0000, 0.0000, 0.1187, 0.0535, 0.0000, 0.1182,
         0.2266, 0.0615, 0.0000, 0.0000, 0.0859, 0.0591, 0.1079, 0.0000, 0.0000,
         0.0000, 0.1221, 0.0000, 0.0000, 0.0493, 0.0000, 0.0000, 0.1084, 0.1299,
         0.1465, 0.0000, 0.2119, 0.0000, 0.0000, 0.0000, 0.0000, 0.1191, 0.0693,
         0.1504, 0.1650, 0.0000, 0.0576, 0.0000, 0.0000, 0.0496, 0.1099, 0.0000,
         0.0109, 0.0093]])
Node Location: convert_element_type_14
Found golden for op @ convert_element_type_14 == convert_element_type_14.
output intermediate tensor None
Node Location: mm_7
Found golden for op @ mm_7 == mm_7.
output intermediate tensor tensor([[-2.1867e-02,  1.6205e-02,  6.3003e-02,  1.1197e-02,  9.8905e-02,
          3.9001e-02,  3.3765e-02, -7.6523e-02,  2.3547e-02, -7.3803e-02,
          2.2773e-02, -1.0224e-02, -4.8335e-02,  6.6071e-02,  2.9627e-02,
         -5.0449e-02,  5.2697e-02, -1.8070e-02,  1.4521e-02,  8.2331e-02,
         -6.5785e-02,  4.1933e-02,  3.2504e-02, -5.0024e-03,  8.0457e-02,
         -9.8755e-03, -6.7937e-04, -6.8752e-02, -2.8420e-02, -1.3168e-01,
         -8.9696e-03, -2.7613e-02,  7.3045e-02, -3.2598e-02, -1.5196e-02,
          5.6574e-02,  9.1809e-03,  7.7844e-02,  4.7451e-02,  3.9601e-02,
          5.8309e-03, -6.3553e-03,  4.6159e-02, -4.1799e-02, -4.5876e-02,
         -5.7166e-03,  3.0591e-02,  3.4748e-05,  5.7221e-02,  4.4670e-02,
          1.0860e-01, -4.8332e-02,  4.1697e-02, -2.0241e-02, -1.1006e-02,
         -1.4917e-02, -6.4201e-03,  5.2452e-02, -4.2877e-02,  1.1859e-01,
         -1.3718e-02,  6.8313e-02, -8.0866e-02,  7.4283e-02, -2.1210e-02,
          5.2469e-02,  5.4199e-02,  8.3516e-03, -9.4177e-03, -3.5116e-02,
         -4.8095e-02, -2.0728e-02, -1.1799e-02,  1.0981e-01,  8.6170e-03,
          2.2958e-02,  4.0474e-03, -5.9987e-03, -1.7862e-02, -1.5749e-03,
         -8.8817e-02,  3.0604e-02,  2.1793e-02,  8.3852e-02, -4.9835e-02,
         -9.4274e-03,  5.6316e-02,  4.7951e-02,  1.1921e-03, -8.1622e-03,
          7.8802e-02,  7.2501e-02, -9.4317e-04,  7.7507e-02, -9.0662e-02,
          4.3858e-02, -3.5625e-02,  9.0981e-02,  4.7492e-02,  5.9240e-02,
          5.4568e-02, -3.2204e-02,  4.0080e-02,  6.0653e-02, -5.7020e-02,
          2.9619e-02, -1.6354e-03, -1.8280e-02, -3.3730e-02, -7.8668e-03,
          7.9308e-02,  1.0370e-02, -5.1930e-02, -3.1811e-02, -8.4156e-02,
          1.2618e-02,  2.1075e-02,  4.0265e-02, -1.0704e-02, -7.8192e-02,
         -7.3217e-02, -2.5120e-02, -1.3720e-03, -2.3050e-02,  1.4544e-02,
          6.6711e-02, -4.8433e-03, -6.1242e-02, -1.1343e-01, -1.0712e-02,
         -6.0254e-02, -3.3956e-02, -3.9896e-02, -7.1577e-02,  3.8434e-02,
          1.4684e-02, -7.7637e-02, -1.2789e-02,  1.6681e-02, -1.0389e-01,
         -3.6872e-02, -2.4109e-02, -2.4454e-02, -8.5753e-02, -1.0690e-02,
          6.3967e-02, -8.0134e-03, -1.1400e-02, -1.7424e-02, -9.7534e-02,
         -2.6066e-02, -3.0822e-02,  3.4854e-02,  2.9402e-02, -7.0207e-02,
         -4.4306e-02,  7.1867e-02, -5.0621e-02,  5.8532e-02,  1.5131e-02,
         -3.4712e-02, -2.0178e-02, -1.9277e-02, -4.6310e-03,  5.5671e-02,
          1.9984e-03,  3.2824e-02,  9.3048e-02,  1.7512e-02,  2.2494e-02,
          4.8699e-02, -1.0961e-02,  7.9078e-03, -3.1486e-02,  1.0796e-01,
         -3.8977e-02,  5.2580e-02, -2.3119e-02, -6.0858e-02, -4.9519e-02,
          2.7477e-02, -2.1025e-02, -7.7293e-02, -2.4662e-04,  5.8727e-02,
         -6.8490e-03, -1.9502e-02, -7.4705e-02, -2.6355e-02,  1.3003e-02,
          1.7312e-03,  3.5494e-02, -4.3808e-02, -6.3494e-02,  1.7688e-05,
         -6.6969e-03,  7.7560e-02, -3.2886e-03, -9.8268e-03,  5.9610e-02,
         -3.5992e-02, -2.9289e-02, -6.3399e-02,  9.3395e-02, -1.2467e-02,
          5.4645e-02, -1.6076e-02, -1.0745e-01,  3.5731e-02,  4.3400e-02,
         -8.6617e-02, -1.0603e-02,  3.0753e-02,  2.5776e-02,  1.9655e-02,
         -4.4262e-02,  9.3943e-02, -4.9012e-02,  7.6971e-02,  8.5157e-03,
         -4.4545e-02, -4.6715e-02, -2.2252e-03,  6.8201e-02, -1.9713e-02,
         -2.1201e-02, -2.2755e-02,  1.9216e-02, -8.9677e-03, -6.1830e-02,
          5.2686e-02, -6.5465e-04, -2.9457e-02,  2.4575e-02,  9.4123e-02,
          4.2173e-02,  9.9358e-03,  1.6318e-02,  6.3822e-02, -3.6221e-02,
          3.0403e-02, -7.0852e-02,  4.9005e-02,  1.2983e-01, -2.2655e-02,
          6.1767e-02,  9.9113e-03, -2.6694e-02,  2.6931e-02, -9.7946e-02,
          1.6225e-02,  3.8090e-02, -2.2181e-02, -9.9520e-02, -3.3877e-02,
          3.6034e-03,  4.4822e-02, -3.0268e-03, -2.7812e-03, -6.3842e-02,
          1.2728e-01,  1.2802e-02, -7.0463e-02,  2.5584e-02,  4.2579e-03,
         -6.8253e-02, -1.9896e-03, -4.3270e-02,  1.5454e-02,  5.1622e-03,
         -4.9810e-03, -3.3244e-02, -3.3651e-02, -7.5603e-02, -4.6014e-02,
         -4.5997e-02,  7.0819e-02, -1.9695e-02, -8.8442e-03, -4.4107e-03,
         -4.9587e-02,  2.8735e-03,  4.3315e-02, -1.7421e-02, -1.4088e-02,
         -9.3233e-03, -3.4510e-04,  1.3284e-01, -1.0162e-02,  3.2375e-02,
          4.4248e-02,  3.0136e-02, -6.1741e-02,  4.2480e-02,  2.8084e-02,
         -6.4014e-03,  3.7673e-02, -7.9390e-04,  5.3276e-02, -7.8646e-02,
         -1.8391e-02,  1.7545e-02, -9.4050e-03,  5.2716e-02,  2.4759e-02,
         -3.9224e-02,  2.3971e-02,  2.1829e-02,  2.4578e-02, -1.0129e-02,
         -6.2078e-02,  4.2819e-02, -1.5543e-02,  6.7439e-02,  6.6150e-02,
          6.0676e-02, -9.2658e-03,  1.2331e-01,  4.3346e-02, -1.4736e-01,
         -6.4869e-02,  4.4967e-02,  1.8466e-02, -3.1728e-02,  3.8505e-02,
          7.5659e-03,  1.2593e-01, -1.9607e-02,  2.4981e-02,  3.9745e-02,
          6.3069e-02,  9.6587e-02,  6.2396e-02,  4.3034e-02, -3.9507e-02,
          4.8780e-02,  3.4382e-03,  4.5349e-02,  4.9338e-02,  2.5835e-02,
         -2.7249e-02, -1.5007e-03,  5.7901e-02, -6.6060e-02, -8.1632e-02,
         -5.6136e-02, -6.5154e-04, -2.2815e-02, -3.1216e-02, -7.6761e-02,
          7.1771e-02, -2.2063e-02, -2.2843e-02, -8.4754e-02,  7.2376e-02,
         -1.0019e-02,  7.2387e-02, -3.3114e-03, -8.0407e-03, -7.9469e-02,
          2.3508e-02, -3.5797e-02, -4.4368e-02,  3.5663e-02,  4.3086e-02,
          3.3802e-02,  3.6470e-02,  3.6603e-02,  1.1450e-02,  2.9197e-02,
          1.0598e-02,  5.7946e-02,  3.3142e-02, -3.4517e-02,  4.7247e-02,
         -1.1176e-02,  6.1748e-02,  4.5187e-02,  5.1588e-02,  3.0208e-03,
         -1.7023e-02, -7.1375e-02, -9.3564e-03, -4.5449e-03, -3.7865e-03,
          7.6839e-02,  1.9916e-03,  7.8029e-02,  6.9980e-03,  1.3264e-02,
         -1.1534e-02,  2.9146e-02,  4.4205e-02,  9.1889e-03,  8.3998e-02,
          2.6744e-02, -4.6097e-02, -8.8736e-03,  5.8102e-02,  4.0500e-02,
          3.3775e-02,  1.0641e-01,  2.8177e-02,  3.0781e-02,  3.6318e-02,
          1.1245e-02, -4.9240e-02,  6.3385e-02, -1.3706e-02, -2.7789e-02,
          2.6541e-02,  5.0208e-02,  3.3632e-03,  5.5701e-03,  4.5007e-02,
         -7.1634e-02, -5.1880e-02,  4.4280e-03,  2.3320e-02, -8.5199e-02,
         -6.3991e-03,  2.9584e-02,  3.2744e-02,  1.1021e-02, -3.5886e-02,
          1.1385e-02,  9.2630e-02,  1.1615e-01,  1.4175e-02,  7.2323e-02,
         -9.5974e-03, -5.6565e-02, -6.2628e-02, -7.2754e-02, -3.2353e-02,
          8.4870e-02,  2.1316e-02,  4.6134e-02, -6.5736e-02, -6.7315e-02,
          1.1181e-01,  2.5173e-02, -5.9832e-02,  6.3287e-02,  5.3735e-02,
         -7.0221e-03,  4.0223e-02,  3.8097e-02,  1.1341e-02,  7.8361e-02,
         -1.5003e-02,  9.9112e-02, -2.3413e-02, -6.1398e-02,  5.8485e-02,
         -4.2263e-02, -3.5892e-03,  8.6281e-02, -7.0021e-02, -8.6394e-02,
         -3.8126e-02,  1.9907e-02,  4.4801e-02, -4.8093e-02,  2.0689e-02,
         -5.0610e-02, -3.2297e-02, -1.4243e-02, -5.2936e-02,  1.1232e-02,
         -4.1823e-02, -4.4992e-02, -4.5025e-02,  2.1656e-02,  4.0497e-03,
          9.1451e-02, -9.7055e-02, -3.6912e-03, -2.6062e-02, -4.1930e-02,
          8.4903e-02,  3.2870e-03,  1.0319e-01, -2.9562e-02,  2.1113e-02,
          1.4265e-02, -8.7659e-04,  2.2643e-02, -3.1171e-02, -2.0497e-02,
          2.7060e-02, -2.3317e-02, -9.4348e-02, -7.2554e-02,  1.2664e-02,
          2.3988e-02, -5.3871e-02, -1.1874e-02, -2.2104e-02, -3.7835e-02,
          9.3281e-02,  2.2699e-02,  2.9899e-02,  7.2557e-02, -9.1307e-02,
         -3.8036e-02, -5.5625e-02, -3.0233e-02, -2.1350e-02, -4.4730e-02,
         -2.1907e-02,  6.3717e-02,  6.4224e-02, -2.2449e-02,  1.6543e-02,
          1.4945e-01, -4.8132e-02, -2.6880e-02,  1.7116e-02,  5.0575e-03,
          2.2049e-03, -1.3965e-02,  9.5415e-02, -3.4472e-02,  4.1873e-02,
         -2.9079e-02, -1.0357e-01,  1.3686e-02, -1.3428e-02, -1.1462e-02,
         -5.8853e-02, -5.8698e-02, -6.6721e-02,  7.0808e-02, -2.7025e-02,
          5.3121e-02, -5.1488e-02, -1.6434e-02,  8.9981e-02, -3.2935e-02,
          4.3226e-02,  4.2374e-02, -1.5446e-01,  2.0149e-02,  6.8123e-02,
          1.8831e-02, -1.6452e-02, -7.9063e-02,  8.7260e-03, -2.9881e-02,
          5.7900e-02, -5.2470e-03,  3.4879e-02,  5.8433e-02,  3.6188e-02,
         -1.8300e-02,  2.6676e-02, -3.9517e-02, -2.3515e-02, -3.6388e-02,
         -6.6857e-02,  6.9749e-02,  7.7036e-03,  2.9985e-02,  3.5563e-02,
          7.0302e-02,  5.9337e-02,  5.5175e-02, -4.0274e-02,  4.5649e-02,
          6.5965e-02, -4.7472e-02, -1.3202e-01, -3.7983e-02,  1.6317e-02,
          9.0977e-02, -4.8312e-02, -3.5032e-02, -1.6887e-02, -6.2459e-02,
         -3.8862e-05,  1.4065e-03, -6.9115e-03, -8.3156e-02,  2.2420e-02,
          7.6483e-02, -3.1598e-02, -5.9497e-02, -8.5641e-04, -4.0194e-02,
         -3.8950e-02, -4.2209e-02,  5.9899e-02,  3.6407e-02,  1.1351e-02,
          3.1418e-02, -1.4075e-01,  6.6509e-02,  3.5534e-02,  4.1917e-02,
         -4.6928e-02, -2.3440e-03,  1.3901e-03,  7.3355e-02, -2.3355e-02,
         -6.3678e-04, -6.3962e-02,  3.8865e-02, -1.9514e-03,  4.2637e-02,
          6.8380e-02, -5.6464e-02, -1.1133e-01,  2.3496e-03,  2.6129e-02,
         -4.1478e-03, -6.8461e-02,  3.8547e-02,  1.7306e-02,  3.1596e-02,
         -6.8402e-02, -2.6889e-03,  4.1394e-02, -2.6522e-02,  4.9983e-02,
          1.5654e-02,  1.8225e-02, -2.4615e-02, -1.1707e-01,  2.1064e-03,
          2.7089e-02,  1.1160e-01, -8.7038e-02, -1.0155e-02,  5.5580e-02,
         -5.6851e-03,  8.8689e-04, -3.2922e-03,  8.2931e-03, -4.4664e-02,
         -5.3130e-03, -7.5256e-02,  4.1979e-02,  2.9963e-02, -9.6122e-03,
         -7.7899e-02,  9.0137e-02, -1.8766e-02, -4.6912e-02, -4.5487e-02,
         -6.7400e-02,  2.2556e-03,  1.0750e-01, -5.3664e-02,  8.6061e-02,
          7.3590e-02,  1.0592e-01,  8.2579e-02,  1.2817e-02,  1.4205e-02,
          2.1564e-02, -2.5766e-02, -3.3867e-02, -1.1801e-02, -2.4716e-02,
         -9.5514e-02, -3.0419e-02, -5.5502e-02, -5.0532e-02,  9.3172e-03,
          8.4603e-02, -4.5365e-02,  2.7727e-02, -1.0342e-01,  3.4726e-02,
         -2.0326e-02, -5.0588e-02, -3.4170e-03,  3.7002e-02, -4.5611e-02,
         -2.0691e-02,  1.1925e-01, -2.6811e-02, -6.7428e-02,  4.8495e-02,
         -4.6894e-03, -1.4914e-02,  4.6732e-02,  1.6072e-02,  2.8079e-02,
         -1.8799e-02,  6.8982e-02,  3.2881e-02, -3.3981e-02, -1.4435e-02,
         -1.4763e-02, -3.5870e-02, -1.0857e-01, -3.7904e-02,  4.3088e-02,
         -2.5842e-02,  5.1125e-02,  6.9557e-03, -1.8920e-03, -1.3198e-03,
         -2.7923e-03,  4.3067e-02,  5.8530e-02,  1.4762e-02,  3.2492e-02,
          8.8290e-02, -4.5041e-02,  9.9064e-03, -9.9754e-03,  1.6425e-03,
          5.0690e-02, -5.3533e-02,  4.0616e-02, -3.4650e-02, -5.8457e-02,
          4.2119e-02, -2.6743e-02,  4.0479e-02,  3.2411e-02, -5.4007e-02,
          1.8820e-02, -5.1342e-02,  5.1863e-03, -5.9801e-02,  2.8434e-02,
         -4.1085e-02, -2.4721e-03, -4.9403e-02, -3.4701e-02,  7.2859e-03,
          3.5997e-02,  5.6898e-03, -4.2201e-02,  3.6315e-03, -1.1761e-02,
          1.1783e-01,  4.6129e-03,  9.5060e-02,  6.0231e-02, -2.8027e-02,
          3.1596e-02,  9.1948e-02, -2.3928e-03,  1.3219e-02, -2.7330e-02,
          1.3213e-02,  6.1465e-02,  5.0244e-03, -7.1757e-02,  4.4519e-04,
         -5.4610e-02,  4.2450e-03,  1.3868e-02, -2.5351e-02, -4.1577e-02,
          3.2965e-02, -8.0214e-03, -2.5759e-02, -5.7063e-02,  5.0822e-02,
          8.6300e-02, -5.5149e-02, -3.0356e-02,  2.8373e-02,  5.2009e-02,
         -4.9847e-03,  1.7458e-02, -2.6678e-02, -1.3340e-02,  3.3918e-02,
         -9.4888e-04,  5.5630e-04, -4.5505e-02, -8.3864e-02,  7.7160e-02,
         -3.9953e-02, -1.3025e-02, -7.3682e-02, -3.1736e-02]])[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "mm_7"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %72 = "ttnn.reshape"(%11) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "mul_7"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%11) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "mul_7"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %73 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "mul_7"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %74 = "ttnn.add"(%72, %73) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "mul_7"])
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "mul_7"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "mul_7"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %75 = "ttnn.multiply"(%71, %74) : (tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "mul_7"])

Node Location: mm_7
Found golden for op @ mm_7 == mm_7.
output intermediate tensor None
Node Location: mul_7
Found golden for op @ mul_7 == mul_7.
output intermediate tensor tensor([[1.]])
Node Location: mul_7
Found golden for op @ mul_7 == mul_7.
output intermediate tensor None
Node Location: mul_7
Found golden for op @ mul_7 == mul_7.
output intermediate tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Node Location: mul_7
Found golden for op @ mul_7 == mul_7.
output intermediate tensor tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])
Node Location: mul_7
Found golden for op @ mul_7 == mul_7.
output intermediate tensor None
Node Location: mul_7
Found golden for op @ mul_7 == mul_7.
output intermediate tensor None
Node Location: mul_7
Found golden for op @ mul_7 == mul_7.
output intermediate tensor tensor([[-2.1867e-02,  1.6205e-02,  6.3003e-02,  1.1197e-02,  9.8905e-02,
          3.9001e-02,  3.3765e-02, -7.6523e-02,  2.3547e-02, -7.3803e-02,
          2.2773e-02, -1.0224e-02, -4.8335e-02,  6.6071e-02,  2.9627e-02,
         -5.0449e-02,  5.2697e-02, -1.8070e-02,  1.4521e-02,  8.2331e-02,
         -6.5785e-02,  4.1933e-02,  3.2504e-02, -5.0024e-03,  8.0457e-02,
         -9.8755e-03, -6.7937e-04, -6.8752e-02, -2.8420e-02, -1.3168e-01,
         -8.9696e-03, -2.7613e-02,  7.3045e-02, -3.2598e-02, -1.5196e-02,
          5.6574e-02,  9.1809e-03,  7.7844e-02,  4.7451e-02,  3.9601e-02,
          5.8309e-03, -6.3553e-03,  4.6159e-02, -4.1799e-02, -4.5876e-02,
         -5.7166e-03,  3.0591e-02,  3.4748e-05,  5.7221e-02,  4.4670e-02,
          1.0860e-01, -4.8332e-02,  4.1697e-02, -2.0241e-02, -1.1006e-02,
         -1.4917e-02, -6.4201e-03,  5.2452e-02, -4.2877e-02,  1.1859e-01,
         -1.3718e-02,  6.8313e-02, -8.0866e-02,  7.4283e-02, -2.1210e-02,
          5.2469e-02,  5.4199e-02,  8.3516e-03, -9.4177e-03, -3.5116e-02,
         -4.8095e-02, -2.0728e-02, -1.1799e-02,  1.0981e-01,  8.6170e-03,
          2.2958e-02,  4.0474e-03, -5.9987e-03, -1.7862e-02, -1.5749e-03,
         -8.8817e-02,  3.0604e-02,  2.1793e-02,  8.3852e-02, -4.9835e-02,
         -9.4274e-03,  5.6316e-02,  4.7951e-02,  1.1921e-03, -8.1622e-03,
          7.8802e-02,  7.2501e-02, -9.4317e-04,  7.7507e-02, -9.0662e-02,
          4.3858e-02, -3.5625e-02,  9.0981e-02,  4.7492e-02,  5.9240e-02,
          5.4568e-02, -3.2204e-02,  4.0080e-02,  6.0653e-02, -5.7020e-02,
          2.9619e-02, -1.6354e-03, -1.8280e-02, -3.3730e-02, -7.8668e-03,
          7.9308e-02,  1.0370e-02, -5.1930e-02, -3.1811e-02, -8.4156e-02,
          1.2618e-02,  2.1075e-02,  4.0265e-02, -1.0704e-02, -7.8192e-02,
         -7.3217e-02, -2.5120e-02, -1.3720e-03, -2.3050e-02,  1.4544e-02,
          6.6711e-02, -4.8433e-03, -6.1242e-02, -1.1343e-01, -1.0712e-02,
         -6.0254e-02, -3.3956e-02, -3.9896e-02, -7.1577e-02,  3.8434e-02,
          1.4684e-02, -7.7637e-02, -1.2789e-02,  1.6681e-02, -1.0389e-01,
         -3.6872e-02, -2.4109e-02, -2.4454e-02, -8.5753e-02, -1.0690e-02,
          6.3967e-02, -8.0134e-03, -1.1400e-02, -1.7424e-02, -9.7534e-02,
         -2.6066e-02, -3.0822e-02,  3.4854e-02,  2.9402e-02, -7.0207e-02,
         -4.4306e-02,  7.1867e-02, -5.0621e-02,  5.8532e-02,  1.5131e-02,
         -3.4712e-02, -2.0178e-02, -1.9277e-02, -4.6310e-03,  5.5671e-02,
          1.9984e-03,  3.2824e-02,  9.3048e-02,  1.7512e-02,  2.2494e-02,
          4.8699e-02, -1.0961e-02,  7.9078e-03, -3.1486e-02,  1.0796e-01,
         -3.8977e-02,  5.2580e-02, -2.3119e-02, -6.0858e-02, -4.9519e-02,
          2.7477e-02, -2.1025e-02, -7.7293e-02, -2.4662e-04,  5.8727e-02,
         -6.8490e-03, -1.9502e-02, -7.4705e-02, -2.6355e-02,  1.3003e-02,
          1.7312e-03,  3.5494e-02, -4.3808e-02, -6.3494e-02,  1.7688e-05,
         -6.6969e-03,  7.7560e-02, -3.2886e-03, -9.8268e-03,  5.9610e-02,
         -3.5992e-02, -2.9289e-02, -6.3399e-02,  9.3395e-02, -1.2467e-02,
          5.4645e-02, -1.6076e-02, -1.0745e-01,  3.5731e-02,  4.3400e-02,
         -8.6617e-02, -1.0603e-02,  3.0753e-02,  2.5776e-02,  1.9655e-02,
         -4.4262e-02,  9.3943e-02, -4.9012e-02,  7.6971e-02,  8.5157e-03,
         -4.4545e-02, -4.6715e-02, -2.2252e-03,  6.8201e-02, -1.9713e-02,
         -2.1201e-02, -2.2755e-02,  1.9216e-02, -8.9677e-03, -6.1830e-02,
          5.2686e-02, -6.5465e-04, -2.9457e-02,  2.4575e-02,  9.4123e-02,
          4.2173e-02,  9.9358e-03,  1.6318e-02,  6.3822e-02, -3.6221e-02,
          3.0403e-02, -7.0852e-02,  4.9005e-02,  1.2983e-01, -2.2655e-02,
          6.1767e-02,  9.9113e-03, -2.6694e-02,  2.6931e-02, -9.7946e-02,
          1.6225e-02,  3.8090e-02, -2.2181e-02, -9.9520e-02, -3.3877e-02,
          3.6034e-03,  4.4822e-02, -3.0268e-03, -2.7812e-03, -6.3842e-02,
          1.2728e-01,  1.2802e-02, -7.0463e-02,  2.5584e-02,  4.2579e-03,
         -6.8253e-02, -1.9896e-03, -4.3270e-02,  1.5454e-02,  5.1622e-03,
         -4.9810e-03, -3.3244e-02, -3.3651e-02, -7.5603e-02, -4.6014e-02,
         -4.5997e-02,  7.0819e-02, -1.9695e-02, -8.8442e-03, -4.4107e-03,
         -4.9587e-02,  2.8735e-03,  4.3315e-02, -1.7421e-02, -1.4088e-02,
         -9.3233e-03, -3.4510e-04,  1.3284e-01, -1.0162e-02,  3.2375e-02,
          4.4248e-02,  3.0136e-02, -6.1741e-02,  4.2480e-02,  2.8084e-02,
         -6.4014e-03,  3.7673e-02, -7.9390e-04,  5.3276e-02, -7.8646e-02,
         -1.8391e-02,  1.7545e-02, -9.4050e-03,  5.2716e-02,  2.4759e-02,
         -3.9224e-02,  2.3971e-02,  2.1829e-02,  2.4578e-02, -1.0129e-02,
         -6.2078e-02,  4.2819e-02, -1.5543e-02,  6.7439e-02,  6.6150e-02,
          6.0676e-02, -9.2658e-03,  1.2331e-01,  4.3346e-02, -1.4736e-01,
         -6.4869e-02,  4.4967e-02,  1.8466e-02, -3.1728e-02,  3.8505e-02,
          7.5659e-03,  1.2593e-01, -1.9607e-02,  2.4981e-02,  3.9745e-02,
          6.3069e-02,  9.6587e-02,  6.2396e-02,  4.3034e-02, -3.9507e-02,
          4.8780e-02,  3.4382e-03,  4.5349e-02,  4.9338e-02,  2.5835e-02,
         -2.7249e-02, -1.5007e-03,  5.7901e-02, -6.6060e-02, -8.1632e-02,
         -5.6136e-02, -6.5154e-04, -2.2815e-02, -3.1216e-02, -7.6761e-02,
          7.1771e-02, -2.2063e-02, -2.2843e-02, -8.4754e-02,  7.2376e-02,
         -1.0019e-02,  7.2387e-02, -3.3114e-03, -8.0407e-03, -7.9469e-02,
          2.3508e-02, -3.5797e-02, -4.4368e-02,  3.5663e-02,  4.3086e-02,
          3.3802e-02,  3.6470e-02,  3.6603e-02,  1.1450e-02,  2.9197e-02,
          1.0598e-02,  5.7946e-02,  3.3142e-02, -3.4517e-02,  4.7247e-02,
         -1.1176e-02,  6.1748e-02,  4.5187e-02,  5.1588e-02,  3.0208e-03,
         -1.7023e-02, -7.1375e-02, -9.3564e-03, -4.5449e-03, -3.7865e-03,
          7.6839e-02,  1.9916e-03,  7.8029e-02,  6.9980e-03,  1.3264e-02,
         -1.1534e-02,  2.9146e-02,  4.4205e-02,  9.1889e-03,  8.3998e-02,
          2.6744e-02, -4.6097e-02, -8.8736e-03,  5.8102e-02,  4.0500e-02,
          3.3775e-02,  1.0641e-01,  2.8177e-02,  3.0781e-02,  3.6318e-02,
          1.1245e-02, -4.9240e-02,  6.3385e-02, -1.3706e-02, -2.7789e-02,
          2.6541e-02,  5.0208e-02,  3.3632e-03,  5.5701e-03,  4.5007e-02,
         -7.1634e-02, -5.1880e-02,  4.4280e-03,  2.3320e-02, -8.5199e-02,
         -6.3991e-03,  2.9584e-02,  3.2744e-02,  1.1021e-02, -3.5886e-02,
          1.1385e-02,  9.2630e-02,  1.1615e-01,  1.4175e-02,  7.2323e-02,
         -9.5974e-03, -5.6565e-02, -6.2628e-02, -7.2754e-02, -3.2353e-02,
          8.4870e-02,  2.1316e-02,  4.6134e-02, -6.5736e-02, -6.7315e-02,
          1.1181e-01,  2.5173e-02, -5.9832e-02,  6.3287e-02,  5.3735e-02,
         -7.0221e-03,  4.0223e-02,  3.8097e-02,  1.1341e-02,  7.8361e-02,
         -1.5003e-02,  9.9112e-02, -2.3413e-02, -6.1398e-02,  5.8485e-02,
         -4.2263e-02, -3.5892e-03,  8.6281e-02, -7.0021e-02, -8.6394e-02,
         -3.8126e-02,  1.9907e-02,  4.4801e-02, -4.8093e-02,  2.0689e-02,
         -5.0610e-02, -3.2297e-02, -1.4243e-02, -5.2936e-02,  1.1232e-02,
         -4.1823e-02, -4.4992e-02, -4.5025e-02,  2.1656e-02,  4.0497e-03,
          9.1451e-02, -9.7055e-02, -3.6912e-03, -2.6062e-02, -4.1930e-02,
          8.4903e-02,  3.2870e-03,  1.0319e-01, -2.9562e-02,  2.1113e-02,
          1.4265e-02, -8.7659e-04,  2.2643e-02, -3.1171e-02, -2.0497e-02,
          2.7060e-02, -2.3317e-02, -9.4348e-02, -7.2554e-02,  1.2664e-02,
          2.3988e-02, -5.3871e-02, -1.1874e-02, -2.2104e-02, -3.7835e-02,
          9.3281e-02,  2.2699e-02,  2.9899e-02,  7.2557e-02, -9.1307e-02,
         -3.8036e-02, -5.5625e-02, -3.0233e-02, -2.1350e-02, -4.4730e-02,
         -2.1907e-02,  6.3717e-02,  6.4224e-02, -2.2449e-02,  1.6543e-02,
          1.4945e-01, -4.8132e-02, -2.6880e-02,  1.7116e-02,  5.0575e-03,
          2.2049e-03, -1.3965e-02,  9.5415e-02, -3.4472e-02,  4.1873e-02,
         -2.9079e-02, -1.0357e-01,  1.3686e-02, -1.3428e-02, -1.1462e-02,
         -5.8853e-02, -5.8698e-02, -6.6721e-02,  7.0808e-02, -2.7025e-02,
          5.3121e-02, -5.1488e-02, -1.6434e-02,  8.9981e-02, -3.2935e-02,
          4.3226e-02,  4.2374e-02, -1.5446e-01,  2.0149e-02,  6.8123e-02,
          1.8831e-02, -1.6452e-02, -7.9063e-02,  8.7260e-03, -2.9881e-02,
          5.7900e-02, -5.2470e-03,  3.4879e-02,  5.8433e-02,  3.6188e-02,
         -1.8300e-02,  2.6676e-02, -3.9517e-02, -2.3515e-02, -3.6388e-02,
         -6.6857e-02,  6.9749e-02,  7.7036e-03,  2.9985e-02,  3.5563e-02,
          7.0302e-02,  5.9337e-02,  5.5175e-02, -4.0274e-02,  4.5649e-02,
          6.5965e-02, -4.7472e-02, -1.3202e-01, -3.7983e-02,  1.6317e-02,
          9.0977e-02, -4.8312e-02, -3.5032e-02, -1.6887e-02, -6.2459e-02,
         -3.8862e-05,  1.4065e-03, -6.9115e-03, -8.3156e-02,  2.2420e-02,
          7.6483e-02, -3.1598e-02, -5.9497e-02, -8.5641e-04, -4.0194e-02,
         -3.8950e-02, -4.2209e-02,  5.9899e-02,  3.6407e-02,  1.1351e-02,
          3.1418e-02, -1.4075e-01,  6.6509e-02,  3.5534e-02,  4.1917e-02,
         -4.6928e-02, -2.3440e-03,  1.3901e-03,  7.3355e-02, -2.3355e-02,
         -6.3678e-04, -6.3962e-02,  3.8865e-02, -1.9514e-03,  4.2637e-02,
          6.8380e-02, -5.6464e-02, -1.1133e-01,  2.3496e-03,  2.6129e-02,
         -4.1478e-03, -6.8461e-02,  3.8547e-02,  1.7306e-02,  3.1596e-02,
         -6.8402e-02, -2.6889e-03,  4.1394e-02, -2.6522e-02,  4.9983e-02,
          1.5654e-02,  1.8225e-02, -2.4615e-02, -1.1707e-01,  2.1064e-03,
          2.7089e-02,  1.1160e-01, -8.7038e-02, -1.0155e-02,  5.5580e-02,
         -5.6851e-03,  8.8689e-04, -3.2922e-03,  8.2931e-03, -4.4664e-02,
         -5.3130e-03, -7.5256e-02,  4.1979e-02,  2.9963e-02, -9.6122e-03,
         -7.7899e-02,  9.0137e-02, -1.8766e-02, -4.6912e-02, -4.5487e-02,
         -6.7400e-02,  2.2556e-03,  1.0750e-01, -5.3664e-02,  8.6061e-02,
          7.3590e-02,  1.0592e-01,  8.2579e-02,  1.2817e-02,  1.4205e-02,
          2.1564e-02, -2.5766e-02, -3.3867e-02, -1.1801e-02, -2.4716e-02,
         -9.5514e-02, -3.0419e-02, -5.5502e-02, -5.0532e-02,  9.3172e-03,
          8.4603e-02, -4.5365e-02,  2.7727e-02, -1.0342e-01,  3.4726e-02,
         -2.0326e-02, -5.0588e-02, -3.4170e-03,  3.7002e-02, -4.5611e-02,
         -2.0691e-02,  1.1925e-01, -2.6811e-02, -6.7428e-02,  4.8495e-02,
         -4.6894e-03, -1.4914e-02,  4.6732e-02,  1.6072e-02,  2.8079e-02,
         -1.8799e-02,  6.8982e-02,  3.2881e-02, -3.3981e-02, -1.4435e-02,
         -1.4763e-02, -3.5870e-02, -1.0857e-01, -3.7904e-02,  4.3088e-02,
         -2.5842e-02,  5.1125e-02,  6.9557e-03, -1.8920e-03, -1.3198e-03,
         -2.7923e-03,  4.3067e-02,  5.8530e-02,  1.4762e-02,  3.2492e-02,
          8.8290e-02, -4.5041e-02,  9.9064e-03, -9.9754e-03,  1.6425e-03,
          5.0690e-02, -5.3533e-02,  4.0616e-02, -3.4650e-02, -5.8457e-02,
          4.2119e-02, -2.6743e-02,  4.0479e-02,  3.2411e-02, -5.4007e-02,
          1.8820e-02, -5.1342e-02,  5.1863e-03, -5.9801e-02,  2.8434e-02,
         -4.1085e-02, -2.4721e-03, -4.9403e-02, -3.4701e-02,  7.2859e-03,
          3.5997e-02,  5.6898e-03, -4.2201e-02,  3.6315e-03, -1.1761e-02,
          1.1783e-01,  4.6129e-03,  9.5060e-02,  6.0231e-02, -2.8027e-02,
          3.1596e-02,  9.1948e-02, -2.3928e-03,  1.3219e-02, -2.7330e-02,
          1.3213e-02,  6.1465e-02,  5.0244e-03, -7.1757e-02,  4.4519e-04,
         -5.4610e-02,  4.2450e-03,  1.3868e-02, -2.5351e-02, -4.1577e-02,
          3.2965e-02, -8.0214e-03, -2.5759e-02, -5.7063e-02,  5.0822e-02,
          8.6300e-02, -5.5149e-02, -3.0356e-02,  2.8373e-02,  5.2009e-02,
         -4.9847e-03,  1.7458e-02, -2.6678e-02, -1.3340e-02,  3.3918e-02,
         -9.4888e-04,  5.5630e-04, -4.5505e-02, -8.3864e-02,  7.7160e-02,
         -3.9953e-02, -1.3025e-02, -7.3682e-02, -3.1736e-02]])[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "mul_7"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "mul_7"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %76 = "ttnn.reshape"(%arg15) <{shape = [1 : i32, 784 : i32]}> : (tensor<784xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "add_7"])

Node Location: mul_7
Found golden for op @ mul_7 == mul_7.
output intermediate tensor None
Node Location: mul_7
Found golden for op @ mul_7 == mul_7.
output intermediate tensor None
Node Location: add_7
Found golden for op @ add_7 == add_7.
output intermediate tensor tensor([[-6.5430e-02,  5.9814e-02, -6.7139e-03, -7.9590e-02,  5.6152e-02,
          4.4678e-02, -5.6076e-04,  7.9102e-02,  1.4648e-02, -7.5195e-02,
         -8.5938e-02, -1.7700e-02, -1.1292e-02, -1.4282e-02, -2.9785e-02,
         -5.9570e-02, -1.1841e-02, -6.9824e-02,  9.9487e-03,  1.4465e-02,
         -3.8910e-03,  4.4189e-02, -5.7861e-02,  5.1270e-02,  4.0039e-02,
          2.6489e-02,  3.8330e-02, -2.0508e-02, -6.1768e-02,  4.0527e-02,
          1.3184e-02,  1.0925e-02,  2.0630e-02,  4.2725e-03,  7.6660e-02,
          9.9945e-04,  6.0791e-02, -8.3496e-02,  2.3560e-02, -7.8125e-02,
         -5.8105e-02,  5.5420e-02, -7.6660e-02, -7.6660e-02,  6.4941e-02,
         -3.3203e-02,  6.8848e-02, -7.7148e-02,  7.6660e-02,  7.9102e-02,
          6.0654e-04,  3.6469e-03, -7.9102e-02,  9.3384e-03, -7.2754e-02,
          5.8594e-03, -6.8359e-02,  6.1951e-03, -1.0193e-02, -1.9287e-02,
         -3.6377e-02, -6.9824e-02, -7.6172e-02,  8.3984e-02,  8.6426e-02,
         -6.8359e-02, -4.7119e-02, -5.5847e-03,  8.6426e-02,  1.3306e-02,
         -8.7891e-02,  6.6406e-02,  5.0781e-02, -8.4473e-02,  5.8838e-02,
          2.2583e-02,  8.4473e-02, -4.3945e-02, -7.6172e-02,  5.4932e-02,
         -7.4219e-02, -3.7354e-02, -2.2583e-02, -2.6978e-02,  2.0294e-03,
         -4.9805e-02,  6.4453e-02,  6.7871e-02, -8.4473e-02, -8.3496e-02,
          8.2031e-02, -2.7588e-02, -7.7637e-02, -3.5889e-02,  1.3916e-02,
         -2.5757e-02, -5.5664e-02, -1.8311e-02, -3.6377e-02, -7.8125e-02,
          8.3984e-02, -2.6550e-03, -6.7871e-02,  1.2268e-02, -8.3496e-02,
          8.0078e-02, -7.0801e-02,  1.7319e-03, -3.0151e-02, -6.1035e-02,
         -8.3618e-03, -6.0547e-02,  7.2754e-02,  6.8848e-02, -1.8539e-03,
          6.8848e-02,  2.8687e-02,  3.3691e-02,  1.0193e-02, -3.7109e-02,
          6.2256e-02,  3.8818e-02,  7.0312e-02,  6.5918e-02,  7.2266e-02,
          7.0312e-02,  3.7842e-02,  3.1494e-02,  3.3691e-02, -1.9653e-02,
         -8.6426e-02, -9.1553e-03, -5.2490e-02, -7.9102e-02,  5.5908e-02,
          5.2490e-02, -5.5176e-02,  3.7354e-02,  6.9336e-02,  3.3417e-03,
         -1.6968e-02,  5.1025e-02, -3.3203e-02,  6.9336e-02, -3.4424e-02,
         -7.4707e-02,  3.5156e-02,  2.5757e-02,  5.8594e-02, -1.1719e-02,
         -5.3955e-02,  5.8594e-02, -1.6235e-02, -3.2471e-02,  2.7344e-02,
         -4.5410e-02, -5.8105e-02, -9.7046e-03, -2.1606e-02,  6.4941e-02,
          8.5449e-02, -5.6152e-02, -4.3213e-02,  7.7148e-02,  4.1992e-02,
         -4.7852e-02,  2.6855e-02,  3.4912e-02, -1.7456e-02,  5.8594e-02,
         -8.3496e-02,  3.7109e-02, -6.8359e-02,  1.4343e-02,  5.1270e-02,
          4.8584e-02,  4.3457e-02, -3.3447e-02,  4.4922e-02,  3.6865e-02,
         -3.0029e-02, -8.6426e-02,  5.0537e-02,  1.4160e-02,  3.7598e-02,
         -5.4199e-02, -2.5024e-02, -4.9805e-02, -6.8848e-02, -8.0566e-02,
         -2.5513e-02, -5.7373e-02,  8.4473e-02,  6.6895e-02, -5.6641e-02,
          4.5166e-03, -5.4443e-02, -5.0049e-03,  4.8096e-02,  3.1250e-02,
         -1.4954e-02, -8.0078e-02, -5.2490e-02,  6.0547e-02, -7.3242e-02,
         -7.8125e-02, -3.5645e-02, -8.7402e-02, -8.7280e-03, -6.8848e-02,
          5.1758e-02,  3.8086e-02, -3.8757e-03, -1.2329e-02,  3.4424e-02,
          4.8096e-02, -5.4443e-02, -4.3701e-02, -4.9316e-02, -2.5879e-02,
          6.6895e-02, -2.7588e-02,  4.7607e-02, -2.9785e-02, -2.7618e-03,
          1.9897e-02, -5.9326e-02, -8.2031e-02,  1.3611e-02,  6.3477e-02,
         -3.9101e-05,  7.1777e-02, -7.7637e-02, -1.3123e-02, -6.6406e-02,
          4.7119e-02, -7.1289e-02,  4.2236e-02,  1.6235e-02,  3.6377e-02,
         -5.6396e-02, -6.1035e-02, -4.5898e-02, -5.7617e-02,  2.4170e-02,
          2.1118e-02,  6.1523e-02,  7.7637e-02, -4.2236e-02,  5.4688e-02,
          4.1016e-02, -2.7954e-02,  2.1973e-02, -7.6172e-02, -3.5156e-02,
         -1.8188e-02,  7.0801e-02, -2.0874e-02,  4.0771e-02, -1.4587e-02,
          8.2520e-02, -4.2480e-02,  5.7861e-02,  6.0791e-02, -4.8828e-02,
         -1.5793e-03,  8.7402e-02,  3.2227e-02,  1.0620e-02, -6.3477e-02,
          3.6133e-02, -2.4414e-02, -4.3701e-02,  2.6733e-02, -1.2939e-02,
          7.4219e-02, -4.3457e-02, -8.4961e-02,  7.3730e-02,  6.8359e-02,
          6.6406e-02, -6.9824e-02,  5.2734e-02,  8.8501e-03,  1.1597e-02,
         -4.2236e-02,  5.2734e-02, -8.3984e-02,  7.9346e-03, -4.2480e-02,
         -5.1575e-03, -5.8838e-02, -3.5889e-02, -1.2878e-02, -3.7109e-02,
          1.3367e-02,  6.3965e-02,  5.1758e-02, -8.5449e-02,  3.1738e-02,
         -5.5176e-02,  1.4648e-02,  8.6426e-02, -4.0039e-02,  1.8311e-03,
         -4.0283e-02, -1.0193e-02, -1.5259e-02,  1.3000e-02, -5.5420e-02,
          2.0752e-02, -5.6396e-02,  8.3984e-02,  4.5166e-02,  6.5430e-02,
         -1.7944e-02,  7.7148e-02,  4.5654e-02, -6.1035e-02,  9.9182e-04,
          2.1118e-02, -1.5198e-02,  1.6235e-02, -4.2969e-02,  7.6660e-02,
          3.1982e-02, -1.2695e-02,  4.9316e-02,  8.3984e-02,  7.3242e-02,
         -8.5938e-02, -4.2725e-02,  2.4902e-02,  6.4453e-02, -2.1729e-02,
          4.5166e-02, -1.4526e-02, -6.7871e-02,  1.0742e-02,  8.0078e-02,
          1.0986e-02, -7.7637e-02,  1.0605e-03,  7.6660e-02,  4.9072e-02,
          1.6479e-02, -1.1597e-02,  1.4465e-02,  1.6968e-02,  5.8838e-02,
         -2.3804e-02,  9.5215e-03, -1.2024e-02, -5.6396e-02, -1.2283e-03,
         -4.3213e-02,  4.5410e-02, -2.1362e-02, -4.6875e-02,  6.2466e-05,
          5.8289e-03,  7.3730e-02, -4.6387e-02,  2.6703e-03,  6.5613e-03,
         -6.9336e-02,  5.1270e-02,  7.3730e-02, -5.5664e-02,  4.2969e-02,
          8.6426e-02, -3.8330e-02,  2.1057e-03,  8.3496e-02,  7.0801e-03,
         -4.7607e-02, -1.8921e-02, -3.1128e-02,  4.0039e-02,  4.1260e-02,
          4.5166e-02, -8.5449e-03, -7.7637e-02, -6.8848e-02,  7.2754e-02,
         -3.0273e-02, -5.1758e-02, -4.5654e-02, -5.6152e-02,  4.7363e-02,
         -4.9133e-03,  7.7148e-02,  8.6914e-02,  7.9590e-02, -2.5635e-02,
          2.7832e-02, -1.9653e-02, -7.7637e-02,  3.3447e-02, -1.2573e-02,
          1.3428e-02,  7.7820e-03,  8.6060e-03,  4.8218e-03,  5.2246e-02,
         -1.7944e-02,  7.8125e-02, -2.6611e-02, -1.1658e-02, -3.2227e-02,
          2.5879e-02, -3.0151e-02, -3.8086e-02,  4.7363e-02,  2.1118e-02,
          2.1484e-02,  6.2012e-02,  8.5449e-02, -2.6367e-02,  8.5449e-02,
          6.4941e-02, -6.0303e-02, -5.5176e-02,  7.0312e-02, -5.8594e-02,
          8.3984e-02, -5.7373e-02, -8.0566e-02,  1.2512e-02,  2.1118e-02,
         -2.8442e-02,  3.1494e-02, -7.7209e-03, -5.8350e-02,  4.7607e-02,
         -2.0264e-02, -1.8921e-02,  8.1177e-03, -2.1362e-02,  8.2520e-02,
         -2.6489e-02,  5.6641e-02,  5.5908e-02, -8.0078e-02, -6.0791e-02,
         -4.2236e-02, -1.8188e-02,  4.5654e-02,  4.6143e-02,  1.7700e-02,
         -1.4343e-02,  6.0303e-02,  8.1543e-02, -2.9053e-02,  3.1738e-02,
         -7.5195e-02, -8.7891e-02, -6.3965e-02,  2.0386e-02, -1.1230e-02,
         -3.4424e-02, -2.3438e-02,  1.8921e-02, -7.0801e-02,  1.9043e-02,
         -6.3477e-02, -1.9165e-02,  2.3193e-02, -5.5908e-02,  1.0193e-02,
         -5.6885e-02,  7.5195e-02,  5.5664e-02,  3.7598e-02, -1.1658e-02,
          8.2031e-02,  1.3794e-02, -8.3008e-02, -4.0039e-02, -4.5898e-02,
         -6.6895e-02, -2.3804e-03, -8.6426e-02,  1.6113e-02, -5.9326e-02,
          5.6152e-02,  5.1270e-03,  1.2756e-02, -3.9307e-02,  1.6235e-02,
          9.2163e-03,  1.2817e-02,  1.9379e-03, -6.6223e-03, -7.3242e-02,
         -1.9226e-03, -4.2236e-02,  5.8594e-02, -6.5430e-02,  5.1025e-02,
         -6.6406e-02, -6.8848e-02, -2.6733e-02,  8.1543e-02, -4.1809e-03,
          5.1270e-02,  3.7109e-02,  4.8523e-03, -8.3984e-02, -4.0283e-02,
          5.3711e-03, -2.3926e-02, -2.4902e-02,  7.9346e-03,  5.7373e-02,
          6.5918e-02,  6.3477e-02, -6.5918e-02,  3.7842e-02, -7.0801e-03,
         -2.1484e-02,  7.4768e-04, -2.2583e-02, -4.0894e-03,  2.4658e-02,
          3.2959e-02,  7.3730e-02,  4.4189e-02,  3.6133e-02, -7.8125e-02,
          5.3711e-03,  1.0910e-03,  2.8198e-02, -6.2988e-02,  1.5381e-02,
         -5.2734e-02,  7.0801e-02, -2.6855e-02,  8.2031e-02, -4.4434e-02,
         -3.1128e-02,  5.8105e-02, -8.3496e-02,  5.1575e-03, -9.2773e-03,
          5.8838e-02, -1.3062e-02, -2.7222e-02,  2.2705e-02,  2.3315e-02,
          7.7637e-02, -7.0312e-02, -8.0566e-02,  4.4189e-02,  1.4832e-02,
         -4.1504e-02, -4.6387e-02,  8.3008e-02, -7.5378e-03,  7.6172e-02,
         -3.7842e-02,  1.8066e-02, -7.1777e-02, -2.9602e-03, -8.6914e-02,
         -4.6387e-02, -5.8350e-02,  7.4707e-02, -4.3213e-02,  1.6724e-02,
         -6.8848e-02,  1.6724e-02, -1.6098e-03,  2.2705e-02,  8.4473e-02,
         -3.1494e-02, -4.4922e-02,  8.6426e-02,  7.6660e-02, -2.2827e-02,
         -2.9785e-02,  5.2979e-02,  3.0670e-03, -7.3242e-02, -5.6641e-02,
         -6.5918e-02, -4.6631e-02, -7.3242e-02, -6.8359e-02, -8.2031e-02,
         -8.1055e-02,  8.3496e-02,  3.0029e-02, -4.3213e-02, -4.3945e-02,
         -5.6152e-02,  2.2217e-02,  3.6621e-02,  4.8828e-02, -6.4453e-02,
         -6.1523e-02, -5.7129e-02,  8.1055e-02,  6.9336e-02,  2.3804e-02,
         -6.7383e-02,  8.0566e-02,  1.2573e-02,  3.5156e-02, -8.6426e-02,
         -6.1523e-02, -2.0264e-02,  8.9722e-03, -7.3242e-02,  6.7383e-02,
         -3.7109e-02, -8.0078e-02,  2.5635e-02,  2.9663e-02,  7.9102e-02,
          7.8125e-02,  9.4604e-03,  4.6143e-02,  4.0283e-02,  1.9287e-02,
          5.9814e-02,  1.0559e-02,  5.7861e-02, -8.6914e-02,  7.2266e-02,
         -7.7637e-02,  2.4048e-02,  5.8594e-02, -4.4189e-02, -6.3965e-02,
         -5.4932e-02, -3.4424e-02, -4.6631e-02, -8.4473e-02,  1.9043e-02,
          3.2227e-02,  1.4038e-02,  2.6733e-02,  8.5938e-02, -5.6152e-02,
         -8.3496e-02, -1.0742e-02,  8.5449e-02,  2.6855e-03, -1.8799e-02,
          1.4282e-02,  8.6914e-02,  7.3730e-02, -7.9102e-02, -1.9653e-02,
          7.5684e-02, -2.9053e-02,  7.8125e-02, -6.5918e-03,  1.2268e-02,
          3.2471e-02,  3.7598e-02,  3.4027e-03,  5.4443e-02,  3.5889e-02,
          3.9062e-02,  7.8125e-02, -6.6895e-02,  7.9102e-02, -5.9570e-02,
         -1.6602e-02, -4.6387e-02,  1.6724e-02, -4.6631e-02,  7.4219e-02,
         -3.0518e-02, -6.7383e-02, -6.4941e-02, -4.3213e-02,  2.8687e-02,
          3.3203e-02, -3.1250e-02,  1.6846e-02,  4.2725e-02,  8.5449e-02,
          6.1279e-02,  2.3438e-02,  5.7373e-02, -7.2266e-02,  1.4160e-02,
         -6.6895e-02, -8.3008e-02,  6.0791e-02,  3.5889e-02, -2.1729e-02,
         -1.2695e-02,  1.1169e-02,  1.9531e-02, -4.6875e-02,  9.1171e-04,
          6.5918e-02, -2.1118e-02, -7.5684e-02, -2.3926e-02, -8.2520e-02,
          7.3242e-02,  5.7617e-02, -6.9336e-02,  8.5938e-02, -2.7710e-02,
          8.5938e-02,  8.3008e-02, -8.3008e-02, -1.4526e-02, -5.9326e-02,
         -2.7344e-02,  3.4180e-02,  8.3008e-02,  3.4424e-02, -5.7861e-02,
          5.2490e-02,  8.4839e-03,  1.4709e-02,  8.4229e-03,  3.4424e-02,
         -6.0303e-02, -4.9316e-02, -8.4473e-02, -3.3203e-02, -7.3242e-02,
          8.2031e-02, -2.0752e-02,  1.1536e-02, -4.0527e-02,  9.1553e-03,
          7.2754e-02,  7.0801e-02, -5.9570e-02, -6.5430e-02,  3.8574e-02,
          7.7148e-02,  5.9326e-02,  5.2734e-02, -5.2490e-02,  6.1279e-02,
          4.1748e-02, -2.9541e-02, -2.1484e-02,  6.5918e-02, -1.3672e-02,
          6.3477e-03,  4.1992e-02, -6.9824e-02,  5.6641e-02,  5.5420e-02,
          2.5269e-02,  7.2754e-02,  6.5430e-02, -4.9805e-02, -2.2705e-02,
         -5.6641e-02,  2.7588e-02,  6.1035e-02,  6.1523e-02,  8.3984e-02,
         -6.0303e-02, -4.7607e-02, -2.4902e-02, -7.9590e-02, -5.7373e-02,
          2.1484e-02,  8.3008e-02, -4.6875e-02,  8.6426e-02, -8.9722e-03,
         -6.8359e-02, -2.3071e-02,  1.9409e-02,  2.4658e-02,  1.5625e-02,
          2.6001e-02, -6.9824e-02, -6.9336e-02,  5.6641e-02]])[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %77 = "ttnn.add"(%75, %76) : (tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "add_7"])

Node Location: add_7
Found golden for op @ add_7 == add_7.
output intermediate tensor tensor([[-8.7297e-02,  7.6019e-02,  5.6289e-02, -6.8393e-02,  1.5506e-01,
          8.3679e-02,  3.3204e-02,  2.5787e-03,  3.8195e-02, -1.4900e-01,
         -6.3164e-02, -2.7924e-02, -5.9626e-02,  5.1789e-02, -1.5862e-04,
         -1.1002e-01,  4.0856e-02, -8.7894e-02,  2.4470e-02,  9.6796e-02,
         -6.9676e-02,  8.6123e-02, -2.5358e-02,  4.6267e-02,  1.2050e-01,
          1.6614e-02,  3.7651e-02, -8.9260e-02, -9.0188e-02, -9.1153e-02,
          4.2140e-03, -1.6688e-02,  9.3675e-02, -2.8325e-02,  6.1464e-02,
          5.7574e-02,  6.9972e-02, -5.6519e-03,  7.1011e-02, -3.8524e-02,
         -5.2275e-02,  4.9065e-02, -3.0501e-02, -1.1846e-01,  1.9065e-02,
         -3.8920e-02,  9.9439e-02, -7.7114e-02,  1.3388e-01,  1.2377e-01,
          1.0921e-01, -4.4685e-02, -3.7405e-02, -1.0902e-02, -8.3760e-02,
         -9.0581e-03, -7.4779e-02,  5.8647e-02, -5.3070e-02,  9.9300e-02,
         -5.0095e-02, -1.5109e-03, -1.5704e-01,  1.5827e-01,  6.5216e-02,
         -1.5890e-02,  7.0803e-03,  2.7669e-03,  7.7008e-02, -2.1810e-02,
         -1.3599e-01,  4.5678e-02,  3.8982e-02,  2.5336e-02,  6.7455e-02,
          4.5541e-02,  8.8520e-02, -4.9944e-02, -9.4034e-02,  5.3357e-02,
         -1.6304e-01, -6.7497e-03, -7.9003e-04,  5.6874e-02, -4.7805e-02,
         -5.9232e-02,  1.2077e-01,  1.1582e-01, -8.3281e-02, -9.1658e-02,
          1.6083e-01,  4.4913e-02, -7.8580e-02,  4.1618e-02, -7.6746e-02,
          1.8101e-02, -9.1289e-02,  7.2671e-02,  1.1115e-02, -1.8885e-02,
          1.3855e-01, -3.4859e-02, -2.7791e-02,  7.2921e-02, -1.4052e-01,
          1.0970e-01, -7.2436e-02, -1.6548e-02, -6.3881e-02, -6.8902e-02,
          7.0946e-02, -5.0177e-02,  2.0824e-02,  3.7037e-02, -8.6010e-02,
          8.1466e-02,  4.9761e-02,  7.3956e-02, -5.1112e-04, -1.1530e-01,
         -1.0962e-02,  1.3698e-02,  6.8940e-02,  4.2868e-02,  8.6809e-02,
          1.3702e-01,  3.2999e-02, -2.9748e-02, -7.9743e-02, -3.0365e-02,
         -1.4668e-01, -4.3112e-02, -9.2386e-02, -1.5068e-01,  9.4342e-02,
          6.7175e-02, -1.3281e-01,  2.4565e-02,  8.6017e-02, -1.0055e-01,
         -5.3840e-02,  2.6917e-02, -5.7657e-02, -1.6417e-02, -4.5114e-02,
         -1.0740e-02,  2.7143e-02,  1.4357e-02,  4.1170e-02, -1.0925e-01,
         -8.0021e-02,  2.7771e-02,  1.8619e-02, -3.0688e-03, -4.2863e-02,
         -8.9716e-02,  1.3761e-02, -6.0325e-02,  3.6925e-02,  8.0073e-02,
          5.0738e-02, -7.6330e-02, -6.2490e-02,  7.2517e-02,  9.7663e-02,
         -4.5853e-02,  5.9679e-02,  1.2796e-01,  5.5462e-05,  8.1088e-02,
         -3.4797e-02,  2.6148e-02, -6.0452e-02, -1.7143e-02,  1.5922e-01,
          9.6069e-03,  9.6037e-02, -5.6566e-02, -1.5936e-02, -1.2653e-02,
         -2.5522e-03, -1.0745e-01, -2.6756e-02,  1.3914e-02,  9.6324e-02,
         -6.1048e-02, -4.4527e-02, -1.2451e-01, -9.5202e-02, -6.7564e-02,
         -2.3781e-02, -2.1879e-02,  4.0665e-02,  3.4009e-03, -5.6623e-02,
         -2.1803e-03,  2.3116e-02, -8.2934e-03,  3.8269e-02,  9.0860e-02,
         -5.0945e-02, -1.0937e-01, -1.1589e-01,  1.5394e-01, -8.5710e-02,
         -2.3480e-02, -5.1721e-02, -1.9485e-01,  2.7003e-02, -2.5448e-02,
         -3.4860e-02,  2.7483e-02,  2.6877e-02,  1.3447e-02,  5.4079e-02,
          3.8337e-03,  3.9500e-02, -9.2714e-02,  2.7654e-02, -1.7363e-02,
          2.2349e-02, -7.4303e-02,  4.5382e-02,  3.8415e-02, -2.2475e-02,
         -1.3038e-03, -8.2082e-02, -6.2816e-02,  4.6432e-03,  1.6469e-03,
          5.2647e-02,  7.1123e-02, -1.0709e-01,  1.1452e-02,  2.7717e-02,
          8.9292e-02, -6.1353e-02,  5.8554e-02,  8.0057e-02,  1.5603e-04,
         -2.5993e-02, -1.3189e-01,  3.1066e-03,  7.2214e-02,  1.5150e-03,
          8.2885e-02,  7.1435e-02,  5.0943e-02, -1.5306e-02, -4.3258e-02,
          5.7241e-02,  1.0136e-02, -2.0850e-04, -1.7569e-01, -6.9033e-02,
         -1.4585e-02,  1.1562e-01, -2.3901e-02,  3.7990e-02, -7.8429e-02,
          2.0980e-01, -2.9679e-02, -1.2601e-02,  8.6375e-02, -4.4570e-02,
         -6.9833e-02,  8.5413e-02, -1.1044e-02,  2.6074e-02, -5.8314e-02,
          3.1152e-02, -5.7658e-02, -7.7353e-02, -4.8869e-02, -5.8954e-02,
          2.8222e-02,  2.7362e-02, -1.0466e-01,  6.4886e-02,  6.3949e-02,
          1.6819e-02, -6.6951e-02,  9.6050e-02, -8.5706e-03, -2.4917e-03,
         -5.1560e-02,  5.2389e-02,  4.8854e-02, -2.2272e-03, -1.0105e-02,
          3.9090e-02, -2.8702e-02, -9.7630e-02,  2.9602e-02, -9.0256e-03,
          6.9653e-03,  1.0164e-01,  5.0964e-02, -3.2173e-02, -4.6907e-02,
         -7.3567e-02,  3.2194e-02,  7.7021e-02,  1.2677e-02,  2.6591e-02,
         -7.9507e-02,  1.3778e-02,  6.5701e-03,  3.7578e-02, -6.5549e-02,
         -4.1326e-02, -1.3577e-02,  6.8442e-02,  1.1261e-01,  1.3158e-01,
          4.2732e-02,  6.7883e-02,  1.6897e-01, -1.7689e-02, -1.4637e-01,
         -4.3751e-02,  2.9769e-02,  3.4702e-02, -7.4697e-02,  1.1517e-01,
          3.9548e-02,  1.1323e-01,  2.9709e-02,  1.0897e-01,  1.1299e-01,
         -2.2869e-02,  5.3862e-02,  8.7299e-02,  1.0749e-01, -6.1235e-02,
          9.3946e-02, -1.1088e-02, -2.2522e-02,  6.0080e-02,  1.0591e-01,
         -1.6263e-02, -7.9137e-02,  5.8962e-02,  1.0601e-02, -3.2560e-02,
         -3.9657e-02, -1.2248e-02, -8.3493e-03, -1.4248e-02, -1.7923e-02,
          4.7967e-02, -1.2541e-02, -3.4867e-02, -1.4115e-01,  7.1148e-02,
         -5.3232e-02,  1.1780e-01, -2.4674e-02, -5.4916e-02, -7.9406e-02,
          2.9336e-02,  3.7933e-02, -9.0755e-02,  3.8333e-02,  4.9648e-02,
         -3.5534e-02,  8.7739e-02,  1.1033e-01, -4.4214e-02,  7.2166e-02,
          9.7024e-02,  1.9616e-02,  3.5248e-02,  4.8979e-02,  5.4327e-02,
         -5.8783e-02,  4.2827e-02,  1.4059e-02,  9.1627e-02,  4.4281e-02,
          2.8143e-02, -7.9920e-02, -8.6993e-02, -7.3393e-02,  6.8967e-02,
          4.6565e-02, -4.9766e-02,  3.2374e-02, -4.9154e-02,  6.0627e-02,
         -1.6448e-02,  1.0629e-01,  1.3112e-01,  8.8779e-02,  5.8363e-02,
          5.4576e-02, -6.5750e-02, -8.6510e-02,  9.1549e-02,  2.7927e-02,
          4.7203e-02,  1.1419e-01,  3.6783e-02,  3.5602e-02,  8.8565e-02,
         -6.6995e-03,  2.8885e-02,  3.6774e-02, -2.5364e-02, -6.0016e-02,
          5.2420e-02,  2.0056e-02, -3.4723e-02,  5.2933e-02,  6.6125e-02,
         -5.0149e-02,  1.0132e-02,  8.9877e-02, -3.0475e-03,  2.5041e-04,
          5.8542e-02, -3.0719e-02, -2.2432e-02,  8.1333e-02, -9.4479e-02,
          9.5370e-02,  3.5257e-02,  3.5580e-02,  2.6688e-02,  9.3441e-02,
         -3.8040e-02, -2.5071e-02, -7.0349e-02, -1.3110e-01,  1.5255e-02,
          6.4606e-02,  2.3951e-03,  5.4251e-02, -8.7098e-02,  1.5204e-02,
          8.5317e-02,  8.1813e-02, -3.9236e-03, -1.6791e-02, -7.0556e-03,
         -4.9258e-02,  2.2035e-02,  8.3752e-02,  5.7484e-02,  9.6062e-02,
         -2.9346e-02,  1.5941e-01,  5.8130e-02, -9.0451e-02,  9.0224e-02,
         -1.1746e-01, -9.1480e-02,  2.2316e-02, -4.9636e-02, -9.7624e-02,
         -7.2550e-02, -3.5307e-03,  6.3722e-02, -1.1889e-01,  3.9732e-02,
         -1.1409e-01, -5.1462e-02,  8.9500e-03, -1.0884e-01,  2.1425e-02,
         -9.8708e-02,  3.0204e-02,  1.0639e-02,  5.9253e-02, -7.6080e-03,
          1.7348e-01, -8.3261e-02, -8.6699e-02, -6.6101e-02, -8.7828e-02,
          1.8009e-02,  9.0665e-04,  1.6761e-02, -1.3449e-02, -3.8213e-02,
          7.0418e-02,  4.2504e-03,  3.5399e-02, -7.0478e-02, -4.2615e-03,
          3.6277e-02, -1.0500e-02, -9.2410e-02, -7.9177e-02, -6.0578e-02,
          2.2066e-02, -9.6107e-02,  4.6720e-02, -8.7534e-02,  1.3190e-02,
          2.6875e-02, -4.6148e-02,  3.1656e-03,  1.5410e-01, -9.5488e-02,
          1.3233e-02, -1.8516e-02, -2.5381e-02, -1.0533e-01, -8.5013e-02,
         -1.6536e-02,  3.9792e-02,  3.9321e-02, -1.4514e-02,  7.3916e-02,
          2.1537e-01,  1.5344e-02, -9.2798e-02,  5.4958e-02, -2.0225e-03,
         -1.9279e-02, -1.3217e-02,  7.2832e-02, -3.8562e-02,  6.6531e-02,
          3.8803e-03, -2.9838e-02,  5.7876e-02,  2.2705e-02, -8.9587e-02,
         -5.3481e-02, -5.7607e-02, -3.8523e-02,  7.8202e-03, -1.1645e-02,
          3.8616e-04,  1.9313e-02, -4.3290e-02,  1.7201e-01, -7.7369e-02,
          1.2098e-02,  1.0048e-01, -2.3796e-01,  2.5306e-02,  5.8845e-02,
          7.7669e-02, -2.9514e-02, -1.0628e-01,  3.1431e-02, -6.5651e-03,
          1.3554e-01, -7.5560e-02, -4.5688e-02,  1.0262e-01,  5.1020e-02,
         -5.9804e-02, -1.9710e-02,  4.3491e-02, -3.1053e-02,  3.9784e-02,
         -1.0470e-01,  8.7816e-02, -6.4074e-02,  2.7024e-02, -5.1352e-02,
          2.3915e-02,  9.8694e-04,  1.2988e-01, -8.3487e-02,  6.2373e-02,
         -2.8826e-03, -3.0749e-02, -1.3363e-01, -1.5278e-02,  1.0079e-01,
          5.9483e-02, -9.3234e-02,  5.1393e-02,  5.9774e-02, -8.5286e-02,
         -2.9824e-02,  5.4385e-02, -3.8445e-03, -1.5640e-01, -3.4221e-02,
          1.0565e-02, -7.8229e-02, -1.3274e-01, -6.9216e-02, -1.2223e-01,
         -1.2000e-01,  4.1287e-02,  8.9928e-02, -6.8058e-03, -3.2594e-02,
         -2.4734e-02, -1.1853e-01,  1.0313e-01,  8.4362e-02, -2.2536e-02,
         -1.0845e-01, -5.9473e-02,  8.2445e-02,  1.4269e-01,  4.4851e-04,
         -6.8020e-02,  1.6604e-02,  5.1438e-02,  3.3205e-02, -4.3789e-02,
          6.8564e-03, -7.6728e-02, -1.0236e-01, -7.0893e-02,  9.3512e-02,
         -4.1257e-02, -1.4854e-01,  6.4181e-02,  4.6969e-02,  1.1070e-01,
          9.7234e-03,  6.7716e-03,  8.7537e-02,  1.3761e-02,  6.9270e-02,
          7.5468e-02,  2.8784e-02,  3.3246e-02, -2.0398e-01,  7.4372e-02,
         -5.0548e-02,  1.3565e-01, -2.8444e-02, -5.4344e-02, -8.3853e-03,
         -6.0617e-02, -3.3537e-02, -4.9923e-02, -7.6180e-02, -2.5621e-02,
          2.6914e-02, -6.1218e-02,  6.8712e-02,  1.1590e-01, -6.5765e-02,
         -1.6139e-01,  7.9395e-02,  6.6683e-02, -4.4227e-02, -6.4286e-02,
         -5.3118e-02,  8.9170e-02,  1.8123e-01, -1.3277e-01,  6.6408e-02,
          1.4927e-01,  7.6865e-02,  1.6070e-01,  6.2250e-03,  2.6473e-02,
          5.4035e-02,  1.1832e-02, -3.0465e-02,  4.2642e-02,  1.1173e-02,
         -5.6451e-02,  4.7706e-02, -1.2240e-01,  2.8570e-02, -5.0253e-02,
          6.8002e-02, -9.1752e-02,  4.4451e-02, -1.5005e-01,  1.0895e-01,
         -5.0843e-02, -1.1797e-01, -6.8358e-02, -6.2111e-03, -1.6925e-02,
          1.2512e-02,  8.8003e-02, -9.9653e-03, -2.4703e-02,  1.3394e-01,
          5.6590e-02,  8.5231e-03,  1.0411e-01, -5.6194e-02,  4.2239e-02,
         -8.5693e-02, -1.4025e-02,  9.3672e-02,  1.9080e-03, -3.6163e-02,
         -2.7459e-02, -2.4701e-02, -8.9035e-02, -8.4779e-02,  4.4000e-02,
          4.0076e-02,  3.0007e-02, -6.8728e-02, -2.5818e-02, -8.3839e-02,
          7.0450e-02,  1.0068e-01, -1.0806e-02,  1.0070e-01,  4.7825e-03,
          1.7423e-01,  3.7967e-02, -7.3101e-02, -2.4502e-02, -5.7684e-02,
          2.3346e-02, -1.9353e-02,  1.2362e-01, -2.2659e-04, -1.1632e-01,
          9.4609e-02, -1.8259e-02,  5.5189e-02,  4.0834e-02, -1.9583e-02,
         -4.1483e-02, -1.0066e-01, -7.9286e-02, -9.3004e-02, -4.4808e-02,
          4.0946e-02, -2.3224e-02, -3.7867e-02, -7.5229e-02,  1.6441e-02,
          1.0875e-01,  7.6491e-02, -1.0177e-01, -6.1798e-02,  2.6814e-02,
          1.9498e-01,  6.3939e-02,  1.4779e-01,  7.7404e-03,  3.3252e-02,
          7.3344e-02,  6.2407e-02, -2.3877e-02,  7.9137e-02, -4.1002e-02,
          1.9561e-02,  1.0346e-01, -6.4800e-02, -1.5116e-02,  5.5865e-02,
         -2.9341e-02,  7.6999e-02,  7.9298e-02, -7.5155e-02, -6.4282e-02,
         -2.3675e-02,  1.9567e-02,  3.5277e-02,  4.4600e-03,  1.3481e-01,
          2.5997e-02, -1.0276e-01, -5.5259e-02, -5.1217e-02, -5.3641e-03,
          1.6500e-02,  1.0047e-01, -7.3553e-02,  7.3086e-02,  2.4945e-02,
         -6.9308e-02, -2.2515e-02, -2.6096e-02, -5.9206e-02,  9.2785e-02,
         -1.3952e-02, -8.2849e-02, -1.4302e-01,  2.4905e-02]])[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "add_7"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "add_7"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.
[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: %78 = "ttnn.typecast"(%77) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x784xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "convert_element_type_15"])

Node Location: add_7
Found golden for op @ add_7 == add_7.
output intermediate tensor None
Node Location: add_7
Found golden for op @ add_7 == add_7.
output intermediate tensor None
Node Location: convert_element_type_15
Found golden for op @ convert_element_type_15 == convert_element_type_15.
output intermediate tensor tensor([[-8.7402e-02,  7.6172e-02,  5.6396e-02, -6.8359e-02,  1.5527e-01,
          8.3496e-02,  3.3203e-02,  2.5787e-03,  3.8086e-02, -1.4941e-01,
         -6.2988e-02, -2.7954e-02, -5.9570e-02,  5.1758e-02, -1.5831e-04,
         -1.0986e-01,  4.0771e-02, -8.7891e-02,  2.4414e-02,  9.6680e-02,
         -6.9824e-02,  8.5938e-02, -2.5391e-02,  4.6387e-02,  1.2061e-01,
          1.6602e-02,  3.7598e-02, -8.9355e-02, -9.0332e-02, -9.1309e-02,
          4.2114e-03, -1.6724e-02,  9.3750e-02, -2.8320e-02,  6.1523e-02,
          5.7617e-02,  6.9824e-02, -5.6458e-03,  7.0801e-02, -3.8574e-02,
         -5.2246e-02,  4.9072e-02, -3.0518e-02, -1.1865e-01,  1.9043e-02,
         -3.8818e-02,  9.9609e-02, -7.7148e-02,  1.3379e-01,  1.2354e-01,
          1.0938e-01, -4.4678e-02, -3.7354e-02, -1.0925e-02, -8.3984e-02,
         -9.0332e-03, -7.4707e-02,  5.8594e-02, -5.2979e-02,  9.9121e-02,
         -5.0049e-02, -1.5106e-03, -1.5723e-01,  1.5820e-01,  6.5430e-02,
         -1.5869e-02,  7.0801e-03,  2.7618e-03,  7.7148e-02, -2.1851e-02,
         -1.3574e-01,  4.5654e-02,  3.9062e-02,  2.5391e-02,  6.7383e-02,
          4.5654e-02,  8.8379e-02, -5.0049e-02, -9.4238e-02,  5.3467e-02,
         -1.6309e-01, -6.7444e-03, -7.8964e-04,  5.6885e-02, -4.7852e-02,
         -5.9326e-02,  1.2061e-01,  1.1572e-01, -8.3496e-02, -9.1797e-02,
          1.6113e-01,  4.4922e-02, -7.8613e-02,  4.1504e-02, -7.6660e-02,
          1.8066e-02, -9.1309e-02,  7.2754e-02,  1.1108e-02, -1.8921e-02,
          1.3867e-01, -3.4912e-02, -2.7832e-02,  7.2754e-02, -1.4062e-01,
          1.0986e-01, -7.2266e-02, -1.6602e-02, -6.3965e-02, -6.8848e-02,
          7.0801e-02, -5.0293e-02,  2.0874e-02,  3.7109e-02, -8.5938e-02,
          8.1543e-02,  4.9805e-02,  7.3730e-02, -5.1117e-04, -1.1523e-01,
         -1.0986e-02,  1.3672e-02,  6.8848e-02,  4.2969e-02,  8.6914e-02,
          1.3672e-01,  3.2959e-02, -2.9785e-02, -7.9590e-02, -3.0396e-02,
         -1.4648e-01, -4.3213e-02, -9.2285e-02, -1.5039e-01,  9.4238e-02,
          6.7383e-02, -1.3281e-01,  2.4536e-02,  8.5938e-02, -1.0059e-01,
         -5.3955e-02,  2.6978e-02, -5.7617e-02, -1.6357e-02, -4.5166e-02,
         -1.0742e-02,  2.7100e-02,  1.4343e-02,  4.1260e-02, -1.0938e-01,
         -8.0078e-02,  2.7832e-02,  1.8677e-02, -3.0670e-03, -4.2969e-02,
         -8.9844e-02,  1.3733e-02, -6.0303e-02,  3.6865e-02,  8.0078e-02,
          5.0781e-02, -7.6172e-02, -6.2500e-02,  7.2754e-02,  9.7656e-02,
         -4.5898e-02,  5.9570e-02,  1.2793e-01,  5.5552e-05,  8.1055e-02,
         -3.4912e-02,  2.6123e-02, -6.0547e-02, -1.7090e-02,  1.5918e-01,
          9.5825e-03,  9.6191e-02, -5.6641e-02, -1.5991e-02, -1.2634e-02,
         -2.5482e-03, -1.0742e-01, -2.6733e-02,  1.3916e-02,  9.6191e-02,
         -6.1035e-02, -4.4434e-02, -1.2451e-01, -9.5215e-02, -6.7383e-02,
         -2.3804e-02, -2.1851e-02,  4.0771e-02,  3.4027e-03, -5.6641e-02,
         -2.1820e-03,  2.3071e-02, -8.3008e-03,  3.8330e-02,  9.0820e-02,
         -5.1025e-02, -1.0938e-01, -1.1572e-01,  1.5430e-01, -8.5938e-02,
         -2.3438e-02, -5.1758e-02, -1.9531e-01,  2.6978e-02, -2.5391e-02,
         -3.4912e-02,  2.7466e-02,  2.6855e-02,  1.3428e-02,  5.4199e-02,
          3.8300e-03,  3.9551e-02, -9.2773e-02,  2.7710e-02, -1.7334e-02,
          2.2339e-02, -7.4219e-02,  4.5410e-02,  3.8330e-02, -2.2461e-02,
         -1.3046e-03, -8.2031e-02, -6.2988e-02,  4.6387e-03,  1.6479e-03,
          5.2734e-02,  7.1289e-02, -1.0693e-01,  1.1475e-02,  2.7710e-02,
          8.9355e-02, -6.1279e-02,  5.8594e-02,  8.0078e-02,  1.5640e-04,
         -2.6001e-02, -1.3184e-01,  3.1128e-03,  7.2266e-02,  1.5182e-03,
          8.3008e-02,  7.1289e-02,  5.1025e-02, -1.5320e-02, -4.3213e-02,
          5.7129e-02,  1.0132e-02, -2.0885e-04, -1.7578e-01, -6.8848e-02,
         -1.4587e-02,  1.1572e-01, -2.3926e-02,  3.8086e-02, -7.8613e-02,
          2.0996e-01, -2.9663e-02, -1.2573e-02,  8.6426e-02, -4.4678e-02,
         -6.9824e-02,  8.5449e-02, -1.1047e-02,  2.6123e-02, -5.8350e-02,
          3.1128e-02, -5.7617e-02, -7.7148e-02, -4.8828e-02, -5.8838e-02,
          2.8198e-02,  2.7344e-02, -1.0449e-01,  6.4941e-02,  6.3965e-02,
          1.6846e-02, -6.6895e-02,  9.6191e-02, -8.5449e-03, -2.4872e-03,
         -5.1514e-02,  5.2490e-02,  4.8828e-02, -2.2278e-03, -1.0132e-02,
          3.9062e-02, -2.8687e-02, -9.7656e-02,  2.9663e-02, -9.0332e-03,
          6.9580e-03,  1.0156e-01,  5.1025e-02, -3.2227e-02, -4.6875e-02,
         -7.3730e-02,  3.2227e-02,  7.7148e-02,  1.2695e-02,  2.6611e-02,
         -7.9590e-02,  1.3794e-02,  6.5613e-03,  3.7598e-02, -6.5430e-02,
         -4.1260e-02, -1.3550e-02,  6.8359e-02,  1.1279e-01,  1.3184e-01,
          4.2725e-02,  6.7871e-02,  1.6895e-01, -1.7700e-02, -1.4648e-01,
         -4.3701e-02,  2.9785e-02,  3.4668e-02, -7.4707e-02,  1.1523e-01,
          3.9551e-02,  1.1328e-01,  2.9663e-02,  1.0889e-01,  1.1279e-01,
         -2.2827e-02,  5.3955e-02,  8.7402e-02,  1.0742e-01, -6.1279e-02,
          9.3750e-02, -1.1108e-02, -2.2461e-02,  6.0059e-02,  1.0596e-01,
         -1.6235e-02, -7.9102e-02,  5.9082e-02,  1.0620e-02, -3.2471e-02,
         -3.9551e-02, -1.2268e-02, -8.3618e-03, -1.4221e-02, -1.7944e-02,
          4.7852e-02, -1.2512e-02, -3.4912e-02, -1.4160e-01,  7.1289e-02,
         -5.3223e-02,  1.1768e-01, -2.4658e-02, -5.4932e-02, -7.9590e-02,
          2.9297e-02,  3.7842e-02, -9.0820e-02,  3.8330e-02,  4.9561e-02,
         -3.5645e-02,  8.7891e-02,  1.1035e-01, -4.4189e-02,  7.2266e-02,
          9.7168e-02,  1.9653e-02,  3.5156e-02,  4.9072e-02,  5.4443e-02,
         -5.8838e-02,  4.2725e-02,  1.4038e-02,  9.1797e-02,  4.4189e-02,
          2.8198e-02, -8.0078e-02, -8.6914e-02, -7.3242e-02,  6.8848e-02,
          4.6631e-02, -4.9805e-02,  3.2471e-02, -4.9072e-02,  6.0547e-02,
         -1.6479e-02,  1.0645e-01,  1.3086e-01,  8.8867e-02,  5.8350e-02,
          5.4688e-02, -6.5918e-02, -8.6426e-02,  9.1309e-02,  2.7954e-02,
          4.7119e-02,  1.1426e-01,  3.6865e-02,  3.5645e-02,  8.8379e-02,
         -6.7139e-03,  2.8931e-02,  3.6865e-02, -2.5391e-02, -6.0059e-02,
          5.2490e-02,  2.0020e-02, -3.4668e-02,  5.2979e-02,  6.5918e-02,
         -5.0049e-02,  1.0132e-02,  8.9844e-02, -3.0518e-03,  2.4986e-04,
          5.8594e-02, -3.0762e-02, -2.2461e-02,  8.1543e-02, -9.4238e-02,
          9.5215e-02,  3.5156e-02,  3.5645e-02,  2.6733e-02,  9.3262e-02,
         -3.8086e-02, -2.5024e-02, -7.0312e-02, -1.3086e-01,  1.5259e-02,
          6.4453e-02,  2.3956e-03,  5.4199e-02, -8.6914e-02,  1.5198e-02,
          8.5449e-02,  8.2031e-02, -3.9368e-03, -1.6846e-02, -7.0496e-03,
         -4.9316e-02,  2.2095e-02,  8.3984e-02,  5.7373e-02,  9.6191e-02,
         -2.9297e-02,  1.5918e-01,  5.8105e-02, -9.0332e-02,  9.0332e-02,
         -1.1768e-01, -9.1309e-02,  2.2339e-02, -4.9561e-02, -9.7656e-02,
         -7.2754e-02, -3.5248e-03,  6.3965e-02, -1.1865e-01,  3.9795e-02,
         -1.1426e-01, -5.1514e-02,  8.9722e-03, -1.0889e-01,  2.1484e-02,
         -9.8633e-02,  3.0151e-02,  1.0620e-02,  5.9326e-02, -7.5989e-03,
          1.7383e-01, -8.3496e-02, -8.6914e-02, -6.5918e-02, -8.7891e-02,
          1.8066e-02,  9.0790e-04,  1.6724e-02, -1.3428e-02, -3.8330e-02,
          7.0312e-02,  4.2419e-03,  3.5400e-02, -7.0312e-02, -4.2725e-03,
          3.6377e-02, -1.0498e-02, -9.2285e-02, -7.9102e-02, -6.0547e-02,
          2.2095e-02, -9.6191e-02,  4.6631e-02, -8.7402e-02,  1.3184e-02,
          2.6855e-02, -4.6143e-02,  3.1586e-03,  1.5430e-01, -9.5703e-02,
          1.3245e-02, -1.8555e-02, -2.5391e-02, -1.0547e-01, -8.4961e-02,
         -1.6479e-02,  3.9795e-02,  3.9307e-02, -1.4526e-02,  7.3730e-02,
          2.1582e-01,  1.5320e-02, -9.2773e-02,  5.4932e-02, -2.0294e-03,
         -1.9287e-02, -1.3245e-02,  7.2754e-02, -3.8574e-02,  6.6406e-02,
          3.8757e-03, -2.9785e-02,  5.7861e-02,  2.2705e-02, -8.9355e-02,
         -5.3467e-02, -5.7617e-02, -3.8574e-02,  7.8125e-03, -1.1658e-02,
          3.8528e-04,  1.9287e-02, -4.3213e-02,  1.7188e-01, -7.7148e-02,
          1.2085e-02,  1.0059e-01, -2.3828e-01,  2.5269e-02,  5.8838e-02,
          7.7637e-02, -2.9541e-02, -1.0645e-01,  3.1494e-02, -6.5613e-03,
          1.3574e-01, -7.5684e-02, -4.5654e-02,  1.0254e-01,  5.1025e-02,
         -5.9814e-02, -1.9653e-02,  4.3457e-02, -3.1006e-02,  3.9795e-02,
         -1.0449e-01,  8.7891e-02, -6.3965e-02,  2.6978e-02, -5.1270e-02,
          2.3926e-02,  9.8419e-04,  1.2988e-01, -8.3496e-02,  6.2256e-02,
         -2.8839e-03, -3.0762e-02, -1.3379e-01, -1.5259e-02,  1.0059e-01,
          5.9570e-02, -9.3262e-02,  5.1514e-02,  5.9814e-02, -8.5449e-02,
         -2.9785e-02,  5.4443e-02, -3.8452e-03, -1.5625e-01, -3.4180e-02,
          1.0559e-02, -7.8125e-02, -1.3281e-01, -6.9336e-02, -1.2207e-01,
         -1.2012e-01,  4.1260e-02,  8.9844e-02, -6.8054e-03, -3.2715e-02,
         -2.4780e-02, -1.1865e-01,  1.0303e-01,  8.4473e-02, -2.2583e-02,
         -1.0840e-01, -5.9570e-02,  8.2520e-02,  1.4258e-01,  4.4823e-04,
         -6.7871e-02,  1.6602e-02,  5.1514e-02,  3.3203e-02, -4.3701e-02,
          6.8665e-03, -7.6660e-02, -1.0254e-01, -7.0801e-02,  9.3750e-02,
         -4.1260e-02, -1.4844e-01,  6.3965e-02,  4.6875e-02,  1.1084e-01,
          9.7046e-03,  6.7749e-03,  8.7402e-02,  1.3733e-02,  6.9336e-02,
          7.5684e-02,  2.8809e-02,  3.3203e-02, -2.0410e-01,  7.4219e-02,
         -5.0537e-02,  1.3574e-01, -2.8442e-02, -5.4443e-02, -8.3618e-03,
         -6.0547e-02, -3.3447e-02, -4.9805e-02, -7.6172e-02, -2.5635e-02,
          2.6855e-02, -6.1279e-02,  6.8848e-02,  1.1572e-01, -6.5918e-02,
         -1.6113e-01,  7.9590e-02,  6.6895e-02, -4.4189e-02, -6.4453e-02,
         -5.3223e-02,  8.9355e-02,  1.8164e-01, -1.3281e-01,  6.6406e-02,
          1.4941e-01,  7.6660e-02,  1.6113e-01,  6.2256e-03,  2.6489e-02,
          5.3955e-02,  1.1841e-02, -3.0518e-02,  4.2725e-02,  1.1169e-02,
         -5.6396e-02,  4.7607e-02, -1.2256e-01,  2.8564e-02, -5.0293e-02,
          6.7871e-02, -9.1797e-02,  4.4434e-02, -1.5039e-01,  1.0889e-01,
         -5.0781e-02, -1.1816e-01, -6.8359e-02, -6.2256e-03, -1.6968e-02,
          1.2512e-02,  8.7891e-02, -9.9487e-03, -2.4658e-02,  1.3379e-01,
          5.6641e-02,  8.5449e-03,  1.0400e-01, -5.6152e-02,  4.2236e-02,
         -8.5449e-02, -1.4038e-02,  9.3750e-02,  1.9073e-03, -3.6133e-02,
         -2.7466e-02, -2.4658e-02, -8.8867e-02, -8.4961e-02,  4.3945e-02,
          4.0039e-02,  3.0029e-02, -6.8848e-02, -2.5757e-02, -8.3984e-02,
          7.0312e-02,  1.0059e-01, -1.0803e-02,  1.0059e-01,  4.7913e-03,
          1.7383e-01,  3.8086e-02, -7.3242e-02, -2.4536e-02, -5.7617e-02,
          2.3315e-02, -1.9409e-02,  1.2354e-01, -2.2697e-04, -1.1621e-01,
          9.4727e-02, -1.8311e-02,  5.5176e-02,  4.0771e-02, -1.9531e-02,
         -4.1504e-02, -1.0059e-01, -7.9102e-02, -9.2773e-02, -4.4922e-02,
          4.1016e-02, -2.3193e-02, -3.7842e-02, -7.5195e-02,  1.6479e-02,
          1.0889e-01,  7.6660e-02, -1.0156e-01, -6.1768e-02,  2.6855e-02,
          1.9531e-01,  6.3965e-02,  1.4746e-01,  7.7515e-03,  3.3203e-02,
          7.3242e-02,  6.2500e-02, -2.3926e-02,  7.9102e-02, -4.1016e-02,
          1.9531e-02,  1.0352e-01, -6.4941e-02, -1.5137e-02,  5.5908e-02,
         -2.9297e-02,  7.7148e-02,  7.9102e-02, -7.5195e-02, -6.4453e-02,
         -2.3682e-02,  1.9531e-02,  3.5156e-02,  4.4556e-03,  1.3477e-01,
          2.6001e-02, -1.0254e-01, -5.5176e-02, -5.1270e-02, -5.3711e-03,
          1.6479e-02,  1.0059e-01, -7.3730e-02,  7.3242e-02,  2.4902e-02,
         -6.9336e-02, -2.2461e-02, -2.6123e-02, -5.9326e-02,  9.2773e-02,
         -1.3977e-02, -8.3008e-02, -1.4258e-01,  2.4902e-02]],
       dtype=torch.bfloat16)[32m            RuntimeTTNN[0m | [1m[38;5;240m   DEBUG[0m | Executing operation: "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x25x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc(fused["/localdev/jameszianxu/tt-torch/tests/models/autoencoder_linear/test_autoencoder_linear.py":50:0, "convert_element_type_15"])
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | getting output tensor is not supported for DeallocateOp
Warning: getOpOutputTensor returned a null tensor.

Node Location: convert_element_type_15
Found golden for op @ convert_element_type_15 == convert_element_type_15.
output intermediate tensor None
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0000, threshold: 0.01 ✅

Metrics for convert_element_type: pcc [1.0]	atol [0.0]
atol too high for convert_element_type: [0.0]
pcc too low for convert_element_type: [1.0]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0036, threshold: 0.01 ✅

Metrics for mm: pcc [0.9999979398087803]	atol [0.003571152687072754]
atol too high for mm: [0.003571152687072754]
pcc too low for mm: [0.9999979398087803]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0036, threshold: 0.01 ✅

Metrics for mul: pcc [0.9999979398087803]	atol [0.003571152687072754]
atol too high for mul: [0.003571152687072754]
pcc too low for mul: [0.9999979398087803]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0036, threshold: 0.01 ✅

Metrics for add: pcc [0.9999979544847958]	atol [0.003571152687072754]
atol too high for add: [0.003571152687072754]
pcc too low for add: [0.9999979544847958]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0078, threshold: 0.01 ✅

Metrics for convert_element_type_1: pcc [0.9999941646321289]	atol [0.0078125]
atol too high for convert_element_type_1: [0.0078125]
pcc too low for convert_element_type_1: [0.9999941646321289]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0039, threshold: 0.01 ✅

Metrics for relu: pcc [0.9999923319475527]	atol [0.00390625]
atol too high for relu: [0.00390625]
pcc too low for relu: [0.9999923319475527]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0039, threshold: 0.01 ✅

Metrics for convert_element_type_2: pcc [0.9999923319475527]	atol [0.00390625]
atol too high for convert_element_type_2: [0.00390625]
pcc too low for convert_element_type_2: [0.9999923319475527]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0018, threshold: 0.01 ✅

Metrics for mm_1: pcc [0.9999967394462813]	atol [0.0017630159854888916]
atol too high for mm_1: [0.0017630159854888916]
pcc too low for mm_1: [0.9999967394462813]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0018, threshold: 0.01 ✅

Metrics for mul_1: pcc [0.9999967394462813]	atol [0.0017630159854888916]
atol too high for mul_1: [0.0017630159854888916]
pcc too low for mul_1: [0.9999967394462813]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0018, threshold: 0.01 ✅

Metrics for add_1: pcc [0.9999959422808846]	atol [0.0017630159854888916]
atol too high for add_1: [0.0017630159854888916]
pcc too low for add_1: [0.9999959422808846]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0020, threshold: 0.01 ✅

Metrics for convert_element_type_3: pcc [0.9999914907686377]	atol [0.001953125]
atol too high for convert_element_type_3: [0.001953125]
pcc too low for convert_element_type_3: [0.9999914907686377]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0020, threshold: 0.01 ✅

Metrics for relu_1: pcc [0.9999894123748955]	atol [0.001953125]
atol too high for relu_1: [0.001953125]
pcc too low for relu_1: [0.9999894123748955]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0020, threshold: 0.01 ✅

Metrics for convert_element_type_4: pcc [0.9999894123748955]	atol [0.001953125]
atol too high for convert_element_type_4: [0.001953125]
pcc too low for convert_element_type_4: [0.9999894123748955]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0004, threshold: 0.01 ✅

Metrics for mm_2: pcc [0.9999963275982158]	atol [0.00036634504795074463]
atol too high for mm_2: [0.00036634504795074463]
pcc too low for mm_2: [0.9999963275982158]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0004, threshold: 0.01 ✅

Metrics for mul_2: pcc [0.9999963275982158]	atol [0.00036634504795074463]
atol too high for mul_2: [0.00036634504795074463]
pcc too low for mul_2: [0.9999963275982158]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0004, threshold: 0.01 ✅

Metrics for add_2: pcc [0.9999980609118666]	atol [0.00036634504795074463]
atol too high for add_2: [0.00036634504795074463]
pcc too low for add_2: [0.9999980609118666]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0010, threshold: 0.01 ✅

Metrics for convert_element_type_5: pcc [0.9999971768016037]	atol [0.0009765625]
atol too high for convert_element_type_5: [0.0009765625]
pcc too low for convert_element_type_5: [0.9999971768016037]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0010, threshold: 0.01 ✅

Metrics for relu_2: pcc [0.9999934153961271]	atol [0.0009765625]
atol too high for relu_2: [0.0009765625]
pcc too low for relu_2: [0.9999934153961271]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0010, threshold: 0.01 ✅

Metrics for convert_element_type_6: pcc [0.9999934153961271]	atol [0.0009765625]
atol too high for convert_element_type_6: [0.0009765625]
pcc too low for convert_element_type_6: [0.9999934153961271]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0003, threshold: 0.01 ✅

Metrics for mm_3: pcc [0.9999995671478621]	atol [0.00027314573526382446]
atol too high for mm_3: [0.00027314573526382446]
pcc too low for mm_3: [0.9999995671478621]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0003, threshold: 0.01 ✅

Metrics for mul_3: pcc [0.9999995671478621]	atol [0.00027314573526382446]
atol too high for mul_3: [0.00027314573526382446]
pcc too low for mul_3: [0.9999995671478621]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0003, threshold: 0.01 ✅

Metrics for add_3: pcc [0.9999996562560132]	atol [0.00027314573526382446]
atol too high for add_3: [0.00027314573526382446]
pcc too low for add_3: [0.9999996562560132]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0002, threshold: 0.01 ✅

Metrics for convert_element_type_7: pcc [0.9999987893600226]	atol [0.000244140625]
atol too high for convert_element_type_7: [0.000244140625]
pcc too low for convert_element_type_7: [0.9999987893600226]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0002, threshold: 0.01 ✅

Metrics for convert_element_type_8: pcc [0.9999987893600226]	atol [0.000244140625]
atol too high for convert_element_type_8: [0.000244140625]
pcc too low for convert_element_type_8: [0.9999987893600226]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0002, threshold: 0.01 ✅

Metrics for mm_4: pcc [0.9999859947041945]	atol [0.00023221969604492188]
atol too high for mm_4: [0.00023221969604492188]
pcc too low for mm_4: [0.9999859947041945]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0002, threshold: 0.01 ✅

Metrics for mul_4: pcc [0.9999859947041945]	atol [0.00023221969604492188]
atol too high for mul_4: [0.00023221969604492188]
pcc too low for mul_4: [0.9999859947041945]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0002, threshold: 0.01 ✅

Metrics for add_4: pcc [0.9999999152322885]	atol [0.00023221969604492188]
atol too high for add_4: [0.00023221969604492188]
pcc too low for add_4: [0.9999999152322885]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0005, threshold: 0.01 ✅

Metrics for convert_element_type_9: pcc [0.999999745130747]	atol [0.00048828125]
atol too high for convert_element_type_9: [0.00048828125]
pcc too low for convert_element_type_9: [0.999999745130747]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0005, threshold: 0.01 ✅

Metrics for relu_3: pcc [0.9999996113089956]	atol [0.00048828125]
atol too high for relu_3: [0.00048828125]
pcc too low for relu_3: [0.9999996113089956]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0005, threshold: 0.01 ✅

Metrics for convert_element_type_10: pcc [0.9999996113089956]	atol [0.00048828125]
atol too high for convert_element_type_10: [0.00048828125]
pcc too low for convert_element_type_10: [0.9999996113089956]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0002, threshold: 0.01 ✅

Metrics for mm_5: pcc [0.9999996367019034]	atol [0.00016164779663085938]
atol too high for mm_5: [0.00016164779663085938]
pcc too low for mm_5: [0.9999996367019034]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0002, threshold: 0.01 ✅

Metrics for mul_5: pcc [0.9999996367019034]	atol [0.00016164779663085938]
atol too high for mul_5: [0.00016164779663085938]
pcc too low for mul_5: [0.9999996367019034]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0002, threshold: 0.01 ✅

Metrics for add_5: pcc [0.9999999090263197]	atol [0.00016164779663085938]
atol too high for add_5: [0.00016164779663085938]
pcc too low for add_5: [0.9999999090263197]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0020, threshold: 0.01 ✅

Metrics for convert_element_type_11: pcc [0.9999988707874748]	atol [0.001953125]
atol too high for convert_element_type_11: [0.001953125]
pcc too low for convert_element_type_11: [0.9999988707874748]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0020, threshold: 0.01 ✅

Metrics for relu_4: pcc [0.999997952292292]	atol [0.001953125]
atol too high for relu_4: [0.001953125]
pcc too low for relu_4: [0.999997952292292]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0020, threshold: 0.01 ✅

Metrics for convert_element_type_12: pcc [0.999997952292292]	atol [0.001953125]
atol too high for convert_element_type_12: [0.001953125]
pcc too low for convert_element_type_12: [0.999997952292292]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0003, threshold: 0.01 ✅

Metrics for mm_6: pcc [0.9999985874910261]	atol [0.00029087066650390625]
atol too high for mm_6: [0.00029087066650390625]
pcc too low for mm_6: [0.9999985874910261]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0003, threshold: 0.01 ✅

Metrics for mul_6: pcc [0.9999985874910261]	atol [0.00029087066650390625]
atol too high for mul_6: [0.00029087066650390625]
pcc too low for mul_6: [0.9999985874910261]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0003, threshold: 0.01 ✅

Metrics for add_6: pcc [0.999999174893843]	atol [0.00029087066650390625]
atol too high for add_6: [0.00029087066650390625]
pcc too low for add_6: [0.999999174893843]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0010, threshold: 0.01 ✅

Metrics for convert_element_type_13: pcc [0.9999973409938592]	atol [0.0009765625]
atol too high for convert_element_type_13: [0.0009765625]
pcc too low for convert_element_type_13: [0.9999973409938592]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0010, threshold: 0.01 ✅

Metrics for relu_5: pcc [0.9999960732038726]	atol [0.0009765625]
atol too high for relu_5: [0.0009765625]
pcc too low for relu_5: [0.9999960732038726]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0010, threshold: 0.01 ✅

Metrics for convert_element_type_14: pcc [0.9999960732038726]	atol [0.0009765625]
atol too high for convert_element_type_14: [0.0009765625]
pcc too low for convert_element_type_14: [0.9999960732038726]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0004, threshold: 0.01 ✅

Metrics for mm_7: pcc [0.9999977984889451]	atol [0.0003651641309261322]
atol too high for mm_7: [0.0003651641309261322]
pcc too low for mm_7: [0.9999977984889451]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0004, threshold: 0.01 ✅

Metrics for mul_7: pcc [0.9999977984889451]	atol [0.0003651641309261322]
atol too high for mul_7: [0.0003651641309261322]
pcc too low for mul_7: [0.9999977984889451]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0004, threshold: 0.01 ✅

Metrics for add_7: pcc [0.9999989012722933]	atol [0.0003651641309261322]
atol too high for add_7: [0.0003651641309261322]
pcc too low for add_7: [0.9999989012722933]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0010, threshold: 0.01 ✅

Metrics for convert_element_type_15: pcc [0.9999970561969524]	atol [0.0009765625]
atol too high for convert_element_type_15: [0.0009765625]
pcc too low for convert_element_type_15: [0.9999970561969524]
Results for output 0:
  PCC: 1.0000, threshold: 0.99 ✅
  ATOL: 0.0010, threshold: 0.01 ✅

Output:  tensor([[-8.7402e-02,  7.6172e-02,  5.6396e-02, -6.8359e-02,  1.5527e-01,
          8.3496e-02,  3.3203e-02,  2.5787e-03,  3.8086e-02, -1.4941e-01,
         -6.2988e-02, -2.7954e-02, -5.9570e-02,  5.1758e-02, -1.5831e-04,
         -1.0986e-01,  4.0771e-02, -8.7891e-02,  2.4414e-02,  9.6680e-02,
         -6.9824e-02,  8.5938e-02, -2.5391e-02,  4.6387e-02,  1.2061e-01,
          1.6602e-02,  3.7598e-02, -8.9355e-02, -9.0332e-02, -9.1309e-02,
          4.2114e-03, -1.6724e-02,  9.3750e-02, -2.8320e-02,  6.1523e-02,
          5.7617e-02,  6.9824e-02, -5.6458e-03,  7.0801e-02, -3.8574e-02,
         -5.2246e-02,  4.9072e-02, -3.0518e-02, -1.1865e-01,  1.9043e-02,
         -3.8818e-02,  9.9609e-02, -7.7148e-02,  1.3379e-01,  1.2354e-01,
          1.0938e-01, -4.4678e-02, -3.7354e-02, -1.0925e-02, -8.3984e-02,
         -9.0332e-03, -7.4707e-02,  5.8594e-02, -5.2979e-02,  9.9121e-02,
         -5.0049e-02, -1.5106e-03, -1.5723e-01,  1.5820e-01,  6.5430e-02,
         -1.5869e-02,  7.0801e-03,  2.7618e-03,  7.7148e-02, -2.1851e-02,
         -1.3574e-01,  4.5654e-02,  3.9062e-02,  2.5391e-02,  6.7383e-02,
          4.5654e-02,  8.8379e-02, -5.0049e-02, -9.4238e-02,  5.3467e-02,
         -1.6309e-01, -6.7444e-03, -7.8964e-04,  5.6885e-02, -4.7852e-02,
         -5.9326e-02,  1.2061e-01,  1.1572e-01, -8.3496e-02, -9.1797e-02,
          1.6113e-01,  4.4922e-02, -7.8613e-02,  4.1504e-02, -7.6660e-02,
          1.8066e-02, -9.1309e-02,  7.2754e-02,  1.1108e-02, -1.8921e-02,
          1.3867e-01, -3.4912e-02, -2.7832e-02,  7.2754e-02, -1.4062e-01,
          1.0986e-01, -7.2266e-02, -1.6602e-02, -6.3965e-02, -6.8848e-02,
          7.0801e-02, -5.0293e-02,  2.0874e-02,  3.7109e-02, -8.5938e-02,
          8.1543e-02,  4.9805e-02,  7.3730e-02, -5.1117e-04, -1.1523e-01,
         -1.0986e-02,  1.3672e-02,  6.8848e-02,  4.2969e-02,  8.6914e-02,
          1.3672e-01,  3.2959e-02, -2.9785e-02, -7.9590e-02, -3.0396e-02,
         -1.4648e-01, -4.3213e-02, -9.2285e-02, -1.5039e-01,  9.4238e-02,
          6.7383e-02, -1.3281e-01,  2.4536e-02,  8.5938e-02, -1.0059e-01,
         -5.3955e-02,  2.6978e-02, -5.7617e-02, -1.6357e-02, -4.5166e-02,
         -1.0742e-02,  2.7100e-02,  1.4343e-02,  4.1260e-02, -1.0938e-01,
         -8.0078e-02,  2.7832e-02,  1.8677e-02, -3.0670e-03, -4.2969e-02,
         -8.9844e-02,  1.3733e-02, -6.0303e-02,  3.6865e-02,  8.0078e-02,
          5.0781e-02, -7.6172e-02, -6.2500e-02,  7.2754e-02,  9.7656e-02,
         -4.5898e-02,  5.9570e-02,  1.2793e-01,  5.5552e-05,  8.1055e-02,
         -3.4912e-02,  2.6123e-02, -6.0547e-02, -1.7090e-02,  1.5918e-01,
          9.5825e-03,  9.6191e-02, -5.6641e-02, -1.5991e-02, -1.2634e-02,
         -2.5482e-03, -1.0742e-01, -2.6733e-02,  1.3916e-02,  9.6191e-02,
         -6.1035e-02, -4.4434e-02, -1.2451e-01, -9.5215e-02, -6.7383e-02,
         -2.3804e-02, -2.1851e-02,  4.0771e-02,  3.4027e-03, -5.6641e-02,
         -2.1820e-03,  2.3071e-02, -8.3008e-03,  3.8330e-02,  9.0820e-02,
         -5.1025e-02, -1.0938e-01, -1.1572e-01,  1.5430e-01, -8.5938e-02,
         -2.3438e-02, -5.1758e-02, -1.9531e-01,  2.6978e-02, -2.5391e-02,
         -3.4912e-02,  2.7466e-02,  2.6855e-02,  1.3428e-02,  5.4199e-02,
          3.8300e-03,  3.9551e-02, -9.2773e-02,  2.7710e-02, -1.7334e-02,
          2.2339e-02, -7.4219e-02,  4.5410e-02,  3.8330e-02, -2.2461e-02,
         -1.3046e-03, -8.2031e-02, -6.2988e-02,  4.6387e-03,  1.6479e-03,
          5.2734e-02,  7.1289e-02, -1.0693e-01,  1.1475e-02,  2.7710e-02,
          8.9355e-02, -6.1279e-02,  5.8594e-02,  8.0078e-02,  1.5640e-04,
         -2.6001e-02, -1.3184e-01,  3.1128e-03,  7.2266e-02,  1.5182e-03,
          8.3008e-02,  7.1289e-02,  5.1025e-02, -1.5320e-02, -4.3213e-02,
          5.7129e-02,  1.0132e-02, -2.0885e-04, -1.7578e-01, -6.8848e-02,
         -1.4587e-02,  1.1572e-01, -2.3926e-02,  3.8086e-02, -7.8613e-02,
          2.0996e-01, -2.9663e-02, -1.2573e-02,  8.6426e-02, -4.4678e-02,
         -6.9824e-02,  8.5449e-02, -1.1047e-02,  2.6123e-02, -5.8350e-02,
          3.1128e-02, -5.7617e-02, -7.7148e-02, -4.8828e-02, -5.8838e-02,
          2.8198e-02,  2.7344e-02, -1.0449e-01,  6.4941e-02,  6.3965e-02,
          1.6846e-02, -6.6895e-02,  9.6191e-02, -8.5449e-03, -2.4872e-03,
         -5.1514e-02,  5.2490e-02,  4.8828e-02, -2.2278e-03, -1.0132e-02,
          3.9062e-02, -2.8687e-02, -9.7656e-02,  2.9663e-02, -9.0332e-03,
          6.9580e-03,  1.0156e-01,  5.1025e-02, -3.2227e-02, -4.6875e-02,
         -7.3730e-02,  3.2227e-02,  7.7148e-02,  1.2695e-02,  2.6611e-02,
         -7.9590e-02,  1.3794e-02,  6.5613e-03,  3.7598e-02, -6.5430e-02,
         -4.1260e-02, -1.3550e-02,  6.8359e-02,  1.1279e-01,  1.3184e-01,
          4.2725e-02,  6.7871e-02,  1.6895e-01, -1.7700e-02, -1.4648e-01,
         -4.3701e-02,  2.9785e-02,  3.4668e-02, -7.4707e-02,  1.1523e-01,
          3.9551e-02,  1.1328e-01,  2.9663e-02,  1.0889e-01,  1.1279e-01,
         -2.2827e-02,  5.3955e-02,  8.7402e-02,  1.0742e-01, -6.1279e-02,
          9.3750e-02, -1.1108e-02, -2.2461e-02,  6.0059e-02,  1.0596e-01,
         -1.6235e-02, -7.9102e-02,  5.9082e-02,  1.0620e-02, -3.2471e-02,
         -3.9551e-02, -1.2268e-02, -8.3618e-03, -1.4221e-02, -1.7944e-02,
          4.7852e-02, -1.2512e-02, -3.4912e-02, -1.4160e-01,  7.1289e-02,
         -5.3223e-02,  1.1768e-01, -2.4658e-02, -5.4932e-02, -7.9590e-02,
          2.9297e-02,  3.7842e-02, -9.0820e-02,  3.8330e-02,  4.9561e-02,
         -3.5645e-02,  8.7891e-02,  1.1035e-01, -4.4189e-02,  7.2266e-02,
          9.7168e-02,  1.9653e-02,  3.5156e-02,  4.9072e-02,  5.4443e-02,
         -5.8838e-02,  4.2725e-02,  1.4038e-02,  9.1797e-02,  4.4189e-02,
          2.8198e-02, -8.0078e-02, -8.6914e-02, -7.3242e-02,  6.8848e-02,
          4.6631e-02, -4.9805e-02,  3.2471e-02, -4.9072e-02,  6.0547e-02,
         -1.6479e-02,  1.0645e-01,  1.3086e-01,  8.8867e-02,  5.8350e-02,
          5.4688e-02, -6.5918e-02, -8.6426e-02,  9.1309e-02,  2.7954e-02,
          4.7119e-02,  1.1426e-01,  3.6865e-02,  3.5645e-02,  8.8379e-02,
         -6.7139e-03,  2.8931e-02,  3.6865e-02, -2.5391e-02, -6.0059e-02,
          5.2490e-02,  2.0020e-02, -3.4668e-02,  5.2979e-02,  6.5918e-02,
         -5.0049e-02,  1.0132e-02,  8.9844e-02, -3.0518e-03,  2.4986e-04,
          5.8594e-02, -3.0762e-02, -2.2461e-02,  8.1543e-02, -9.4238e-02,
          9.5215e-02,  3.5156e-02,  3.5645e-02,  2.6733e-02,  9.3262e-02,
         -3.8086e-02, -2.5024e-02, -7.0312e-02, -1.3086e-01,  1.5259e-02,
          6.4453e-02,  2.3956e-03,  5.4199e-02, -8.6914e-02,  1.5198e-02,
          8.5449e-02,  8.2031e-02, -3.9368e-03, -1.6846e-02, -7.0496e-03,
         -4.9316e-02,  2.2095e-02,  8.3984e-02,  5.7373e-02,  9.6191e-02,
         -2.9297e-02,  1.5918e-01,  5.8105e-02, -9.0332e-02,  9.0332e-02,
         -1.1768e-01, -9.1309e-02,  2.2339e-02, -4.9561e-02, -9.7656e-02,
         -7.2754e-02, -3.5248e-03,  6.3965e-02, -1.1865e-01,  3.9795e-02,
         -1.1426e-01, -5.1514e-02,  8.9722e-03, -1.0889e-01,  2.1484e-02,
         -9.8633e-02,  3.0151e-02,  1.0620e-02,  5.9326e-02, -7.5989e-03,
          1.7383e-01, -8.3496e-02, -8.6914e-02, -6.5918e-02, -8.7891e-02,
          1.8066e-02,  9.0790e-04,  1.6724e-02, -1.3428e-02, -3.8330e-02,
          7.0312e-02,  4.2419e-03,  3.5400e-02, -7.0312e-02, -4.2725e-03,
          3.6377e-02, -1.0498e-02, -9.2285e-02, -7.9102e-02, -6.0547e-02,
          2.2095e-02, -9.6191e-02,  4.6631e-02, -8.7402e-02,  1.3184e-02,
          2.6855e-02, -4.6143e-02,  3.1586e-03,  1.5430e-01, -9.5703e-02,
          1.3245e-02, -1.8555e-02, -2.5391e-02, -1.0547e-01, -8.4961e-02,
         -1.6479e-02,  3.9795e-02,  3.9307e-02, -1.4526e-02,  7.3730e-02,
          2.1582e-01,  1.5320e-02, -9.2773e-02,  5.4932e-02, -2.0294e-03,
         -1.9287e-02, -1.3245e-02,  7.2754e-02, -3.8574e-02,  6.6406e-02,
          3.8757e-03, -2.9785e-02,  5.7861e-02,  2.2705e-02, -8.9355e-02,
         -5.3467e-02, -5.7617e-02, -3.8574e-02,  7.8125e-03, -1.1658e-02,
          3.8528e-04,  1.9287e-02, -4.3213e-02,  1.7188e-01, -7.7148e-02,
          1.2085e-02,  1.0059e-01, -2.3828e-01,  2.5269e-02,  5.8838e-02,
          7.7637e-02, -2.9541e-02, -1.0645e-01,  3.1494e-02, -6.5613e-03,
          1.3574e-01, -7.5684e-02, -4.5654e-02,  1.0254e-01,  5.1025e-02,
         -5.9814e-02, -1.9653e-02,  4.3457e-02, -3.1006e-02,  3.9795e-02,
         -1.0449e-01,  8.7891e-02, -6.3965e-02,  2.6978e-02, -5.1270e-02,
          2.3926e-02,  9.8419e-04,  1.2988e-01, -8.3496e-02,  6.2256e-02,
         -2.8839e-03, -3.0762e-02, -1.3379e-01, -1.5259e-02,  1.0059e-01,
          5.9570e-02, -9.3262e-02,  5.1514e-02,  5.9814e-02, -8.5449e-02,
         -2.9785e-02,  5.4443e-02, -3.8452e-03, -1.5625e-01, -3.4180e-02,
          1.0559e-02, -7.8125e-02, -1.3281e-01, -6.9336e-02, -1.2207e-01,
         -1.2012e-01,  4.1260e-02,  8.9844e-02, -6.8054e-03, -3.2715e-02,
         -2.4780e-02, -1.1865e-01,  1.0303e-01,  8.4473e-02, -2.2583e-02,
         -1.0840e-01, -5.9570e-02,  8.2520e-02,  1.4258e-01,  4.4823e-04,
         -6.7871e-02,  1.6602e-02,  5.1514e-02,  3.3203e-02, -4.3701e-02,
          6.8665e-03, -7.6660e-02, -1.0254e-01, -7.0801e-02,  9.3750e-02,
         -4.1260e-02, -1.4844e-01,  6.3965e-02,  4.6875e-02,  1.1084e-01,
          9.7046e-03,  6.7749e-03,  8.7402e-02,  1.3733e-02,  6.9336e-02,
          7.5684e-02,  2.8809e-02,  3.3203e-02, -2.0410e-01,  7.4219e-02,
         -5.0537e-02,  1.3574e-01, -2.8442e-02, -5.4443e-02, -8.3618e-03,
         -6.0547e-02, -3.3447e-02, -4.9805e-02, -7.6172e-02, -2.5635e-02,
          2.6855e-02, -6.1279e-02,  6.8848e-02,  1.1572e-01, -6.5918e-02,
         -1.6113e-01,  7.9590e-02,  6.6895e-02, -4.4189e-02, -6.4453e-02,
         -5.3223e-02,  8.9355e-02,  1.8164e-01, -1.3281e-01,  6.6406e-02,
          1.4941e-01,  7.6660e-02,  1.6113e-01,  6.2256e-03,  2.6489e-02,
          5.3955e-02,  1.1841e-02, -3.0518e-02,  4.2725e-02,  1.1169e-02,
         -5.6396e-02,  4.7607e-02, -1.2256e-01,  2.8564e-02, -5.0293e-02,
          6.7871e-02, -9.1797e-02,  4.4434e-02, -1.5039e-01,  1.0889e-01,
         -5.0781e-02, -1.1816e-01, -6.8359e-02, -6.2256e-03, -1.6968e-02,
          1.2512e-02,  8.7891e-02, -9.9487e-03, -2.4658e-02,  1.3379e-01,
          5.6641e-02,  8.5449e-03,  1.0400e-01, -5.6152e-02,  4.2236e-02,
         -8.5449e-02, -1.4038e-02,  9.3750e-02,  1.9073e-03, -3.6133e-02,
         -2.7466e-02, -2.4658e-02, -8.8867e-02, -8.4961e-02,  4.3945e-02,
          4.0039e-02,  3.0029e-02, -6.8848e-02, -2.5757e-02, -8.3984e-02,
          7.0312e-02,  1.0059e-01, -1.0803e-02,  1.0059e-01,  4.7913e-03,
          1.7383e-01,  3.8086e-02, -7.3242e-02, -2.4536e-02, -5.7617e-02,
          2.3315e-02, -1.9409e-02,  1.2354e-01, -2.2697e-04, -1.1621e-01,
          9.4727e-02, -1.8311e-02,  5.5176e-02,  4.0771e-02, -1.9531e-02,
         -4.1504e-02, -1.0059e-01, -7.9102e-02, -9.2773e-02, -4.4922e-02,
          4.1016e-02, -2.3193e-02, -3.7842e-02, -7.5195e-02,  1.6479e-02,
          1.0889e-01,  7.6660e-02, -1.0156e-01, -6.1768e-02,  2.6855e-02,
          1.9531e-01,  6.3965e-02,  1.4746e-01,  7.7515e-03,  3.3203e-02,
          7.3242e-02,  6.2500e-02, -2.3926e-02,  7.9102e-02, -4.1016e-02,
          1.9531e-02,  1.0352e-01, -6.4941e-02, -1.5137e-02,  5.5908e-02,
         -2.9297e-02,  7.7148e-02,  7.9102e-02, -7.5195e-02, -6.4453e-02,
         -2.3682e-02,  1.9531e-02,  3.5156e-02,  4.4556e-03,  1.3477e-01,
          2.6001e-02, -1.0254e-01, -5.5176e-02, -5.1270e-02, -5.3711e-03,
          1.6479e-02,  1.0059e-01, -7.3730e-02,  7.3242e-02,  2.4902e-02,
         -6.9336e-02, -2.2461e-02, -2.6123e-02, -5.9326e-02,  9.2773e-02,
         -1.3977e-02, -8.3008e-02, -1.4258e-01,  2.4902e-02]],
       dtype=torch.bfloat16)
PASSED

=============================== warnings summary ===============================
tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
  /localdev/jameszianxu/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1338: UserWarning: Failed to fetch module L__self___encoder_lin1!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
  /localdev/jameszianxu/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/experimental/const_fold.py:264: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer
    new_node = root_const_gm.graph.get_attr(in_node.target)

tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
  /localdev/jameszianxu/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1338: UserWarning: Failed to fetch module L__self___encoder_lin2!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
  /localdev/jameszianxu/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1338: UserWarning: Failed to fetch module L__self___encoder_lin3!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
  /localdev/jameszianxu/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1338: UserWarning: Failed to fetch module L__self___encoder_lin4!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
  /localdev/jameszianxu/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1338: UserWarning: Failed to fetch module L__self___decoder_lin1!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
  /localdev/jameszianxu/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1338: UserWarning: Failed to fetch module L__self___decoder_lin2!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
  /localdev/jameszianxu/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1338: UserWarning: Failed to fetch module L__self___decoder_lin3!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
tests/models/autoencoder_linear/test_autoencoder_linear.py::test_autoencoder_linear[full-eval]
  /localdev/jameszianxu/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1338: UserWarning: Failed to fetch module L__self___decoder_lin4!
    warnings.warn(f"Failed to fetch module {module_path}!")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================== 1 passed, 18 warnings in 67.47s (0:01:07) ===================
