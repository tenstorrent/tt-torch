{
    "test_params": {
        "gpt_neo/pytorch": {
            "pcc": 0.98
        },
        "vovnet/pytorch": {
            "pcc": false
        },
        "hardnet/pytorch": {
            "pcc": 0.98
        },
        "qwen/casual_lm/pytorch": {
            "pcc": false
        },
        "clip/pytorch": {
            "pcc": false
        },
        "yolo_x/pytorch": {
            "pcc": false
        },
        "wide_resnet/pytorch": {
            "pcc": 0.96
        },
        "efficientnet/pytorch": {
            "pcc": false
        },
        "xglm/pytorch": {
            "pcc": false
        },
        "resnet/pytorch": {
            "pcc": 0.97
        },
        "mamba/pytorch": {
            "pcc": 0.95
        },
        "openpose/v2/pytorch": {
            "pcc": false
        },
        "albert/masked_lm/pytorch-albert-xxlarge-v2": {
            "pcc": 0.98
        },
        "deit/pytorch": {
            "pcc": 0.97,
            "relative_atol": 0.015
        },
        "yolov3/pytorch-base": {
            "pcc": 0.97
        },
        "yolov4/pytorch-base": {
            "pcc": 0.98
        },
        "flan_t5/pytorch": {
            "pcc": false
        },
        "musicgen_small/pytorch": {
            "pcc": false
        },
        "falcon/pytorch": {
            "pcc": false
        }
    },
    "skip_models": {
        "yolov10/pytorch": {
            "skip": "TorchMlirCompilerError: Lowering Torch Backend IR -> StableHLO Backend IR failed"
        },
        "qwen/token_classification/pytorch": {
            "skip": "Out of Memory: Not enough space to allocate 135790592 B DRAM buffer across 12 banks, where each bank needs to store 11317248 B"
        },
        "t5/pytorch": {
            "skip": "ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds"
        },
        "vgg19_unet/pytorch": {
            "skip":  "Out of Memory: Not enough space to allocate 84213760 B L1 buffer across 64 banks, where each bank needs to store 1315840 B - https://github.com/tenstorrent/tt-torch/issues/729"
        },
        "yolov9/pytorch": {
            "skip":  "RuntimeError: TT_FATAL @ Inputs must be of bfloat16 or bfloat8_b type"
        },
        "detr/pytorch": {
            "skip":  "Out of Memory: Not enough space to allocate 4294967296 B DRAM buffer across 12 banks"
        },
        "monodepth2/pytorch": {
            "skip":  "tt-forge-models needs to be updated to use get_file() api for loading .pth files"
        },
        "glpn_kitti/pytorch": {
            "skip":  "RuntimeError: Input type (c10::BFloat16) and bias type (float) should be the same"
        },
        "oft/pytorch": {
            "skip":  "Out of Memory: Not enough space to allocate 2902982656 B DRAM buffer across 12 banks - https://github.com/tenstorrent/tt-torch/issues/727"
        },
        "yolov8/pytorch": {
            "skip":  "RuntimeError: TT_FATAL @ Inputs must be of bfloat16 or bfloat8_b type"
        },
        "vilt/pytorch": {
            "skip":  "RuntimeError: cannot sample n_sample <= 0 samples"
        },
        "whisper/pytorch": {
            "skip":  "ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")"
        },
        "yolo_v6/pytorch": {
            "skip":  "Needs fix import library in tt-forge-models"
        },
        "gliner_model/pytorch": {
            "skip":  "AttributeError: 'function' object has no attribute 'parameters'"
        },
        "deepseek/pytorch": {
            "skip": "Fix KILLED"
        },
        "deepseek/qwen/pytorch": {
            "skip": "Fix KILLED"
        },
        "mistral/pixtral/pytorch": {
            "skip": "Fix KILLED"
        }
    }
}
