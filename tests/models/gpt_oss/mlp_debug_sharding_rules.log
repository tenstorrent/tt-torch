WARNING:root:Defaulting to PJRT_DEVICE=CPU
2025-09-22 14:17:51.562067: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<32x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg6: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %3 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %5 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<128x!vhlo.i64_v1>
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %7 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>
    %8 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,32]{0,1}">} : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>
    %9 = "vhlo.dot_general_v2"(%7, %8) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %11 = "vhlo.add_v1"(%9, %10) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %12 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %13 = "vhlo.broadcast_in_dim_v1"(%12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.i32_v1>
    %14:2 = "vhlo.sort_v1"(%11, %13) <{dimension = #vhlo.integer_v1<1 : i64>, is_stable = #vhlo.bool_v1<false>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg14: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %66 = "vhlo.compare_v1"(%arg11, %arg12) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>)
    %15 = "vhlo.slice_v1"(%14#1) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i32_v1>
    %16 = "vhlo.convert_v1"(%15) : (!vhlo.tensor_v1<128x4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i64_v1>
    %17 = "vhlo.reshape_v1"(%16) : (!vhlo.tensor_v1<128x4x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %18 = "vhlo.concatenate_v1"(%6, %17) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>
    %19 = "vhlo.slice_v1"(%14#0) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %20 = "vhlo.reduce_v1"(%19, %1) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.maximum_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %22 = "vhlo.subtract_v1"(%19, %21) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %23 = "vhlo.exponential_v2"(%22) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %24 = "vhlo.reduce_v1"(%23, %0) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %25 = "vhlo.broadcast_in_dim_v1"(%24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %26 = "vhlo.divide_v1"(%23, %25) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %27 = "vhlo.scatter_v2"(%4, %18, %26) <{index_vector_dim = #vhlo.integer_v1<2 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %28 = "vhlo.broadcast_in_dim_v1"(%arg10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %29 = "vhlo.concatenate_v1"(%7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7) <{dimension = #vhlo.integer_v1<0 : i64>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %31 = "vhlo.dot_general_v2"(%30, %arg9) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %32 = "vhlo.broadcast_in_dim_v1"(%arg8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %33 = "vhlo.add_v1"(%31, %32) : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %34 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 1]> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %35 = "vhlo.broadcast_in_dim_v1"(%arg6) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %36 = "vhlo.clamp_v1"(%28, %34, %35) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %37 = "vhlo.add_v1"(%36, %3) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %38 = "vhlo.convert_v1"(%37) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %40 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %41 = "vhlo.clamp_v1"(%39, %40, %35) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %42 = "vhlo.convert_v1"(%41) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %43 = "vhlo.broadcast_in_dim_v1"(%arg5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %44 = "vhlo.multiply_v1"(%42, %43) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %46 = "vhlo.logistic_v2"(%45) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %47 = "vhlo.convert_v1"(%46) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %48 = "vhlo.multiply_v1"(%42, %47) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %50 = "vhlo.convert_v1"(%49) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %51 = "vhlo.multiply_v1"(%38, %50) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %52 = "vhlo.convert_v1"(%51) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %53 = "vhlo.dot_general_v2"(%52, %arg4) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %54 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %55 = "vhlo.add_v1"(%53, %54) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %58 = "vhlo.transpose_v1"(%27) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[32,128]{0,1}">} : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>
    %59 = "vhlo.reshape_v1"(%58) : (!vhlo.tensor_v1<32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>
    %60 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>
    %61 = "vhlo.reshape_v1"(%60) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>
    %62 = "vhlo.broadcast_in_dim_v1"(%61) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %63 = "vhlo.multiply_v1"(%57, %62) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %64 = "vhlo.convert_v1"(%63) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %65 = "vhlo.reduce_v1"(%64, %0) <{dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>
    "vhlo.return_v1"(%4, %27, %27, %65, %65) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
// -----// IR Dump Before VhloToVersionPass (vhlo-to-version) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<32x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg6: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %3 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %5 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<128x!vhlo.i64_v1>
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %7 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>
    %8 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,32]{0,1}">} : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>
    %9 = "vhlo.dot_general_v2"(%7, %8) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %11 = "vhlo.add_v1"(%9, %10) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %12 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %13 = "vhlo.broadcast_in_dim_v1"(%12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.i32_v1>
    %14:2 = "vhlo.sort_v1"(%11, %13) <{dimension = #vhlo.integer_v1<1 : i64>, is_stable = #vhlo.bool_v1<false>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg14: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %66 = "vhlo.compare_v1"(%arg11, %arg12) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>)
    %15 = "vhlo.slice_v1"(%14#1) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i32_v1>
    %16 = "vhlo.convert_v1"(%15) : (!vhlo.tensor_v1<128x4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i64_v1>
    %17 = "vhlo.reshape_v1"(%16) : (!vhlo.tensor_v1<128x4x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %18 = "vhlo.concatenate_v1"(%6, %17) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>
    %19 = "vhlo.slice_v1"(%14#0) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %20 = "vhlo.reduce_v1"(%19, %1) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.maximum_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %22 = "vhlo.subtract_v1"(%19, %21) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %23 = "vhlo.exponential_v2"(%22) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %24 = "vhlo.reduce_v1"(%23, %0) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %25 = "vhlo.broadcast_in_dim_v1"(%24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %26 = "vhlo.divide_v1"(%23, %25) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %27 = "vhlo.scatter_v2"(%4, %18, %26) <{index_vector_dim = #vhlo.integer_v1<2 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %28 = "vhlo.broadcast_in_dim_v1"(%arg10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %29 = "vhlo.concatenate_v1"(%7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7) <{dimension = #vhlo.integer_v1<0 : i64>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %31 = "vhlo.dot_general_v2"(%30, %arg9) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %32 = "vhlo.broadcast_in_dim_v1"(%arg8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %33 = "vhlo.add_v1"(%31, %32) : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %34 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 1]> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %35 = "vhlo.broadcast_in_dim_v1"(%arg6) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %36 = "vhlo.clamp_v1"(%28, %34, %35) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %37 = "vhlo.add_v1"(%36, %3) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %38 = "vhlo.convert_v1"(%37) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %40 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %41 = "vhlo.clamp_v1"(%39, %40, %35) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %42 = "vhlo.convert_v1"(%41) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %43 = "vhlo.broadcast_in_dim_v1"(%arg5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %44 = "vhlo.multiply_v1"(%42, %43) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %46 = "vhlo.logistic_v2"(%45) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %47 = "vhlo.convert_v1"(%46) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %48 = "vhlo.multiply_v1"(%42, %47) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %50 = "vhlo.convert_v1"(%49) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %51 = "vhlo.multiply_v1"(%38, %50) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %52 = "vhlo.convert_v1"(%51) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %53 = "vhlo.dot_general_v2"(%52, %arg4) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %54 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %55 = "vhlo.add_v1"(%53, %54) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %58 = "vhlo.transpose_v1"(%27) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[32,128]{0,1}">} : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>
    %59 = "vhlo.reshape_v1"(%58) : (!vhlo.tensor_v1<32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>
    %60 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>
    %61 = "vhlo.reshape_v1"(%60) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>
    %62 = "vhlo.broadcast_in_dim_v1"(%61) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %63 = "vhlo.multiply_v1"(%57, %62) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %64 = "vhlo.convert_v1"(%63) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %65 = "vhlo.reduce_v1"(%64, %0) <{dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>
    "vhlo.return_v1"(%4, %27, %27, %65, %65) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump Before VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<32x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg6: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %3 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %5 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<128x!vhlo.i64_v1>
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %7 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>
    %8 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,32]{0,1}">} : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>
    %9 = "vhlo.dot_general_v2"(%7, %8) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %11 = "vhlo.add_v1"(%9, %10) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %12 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %13 = "vhlo.broadcast_in_dim_v1"(%12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.i32_v1>
    %14:2 = "vhlo.sort_v1"(%11, %13) <{dimension = #vhlo.integer_v1<1 : i64>, is_stable = #vhlo.bool_v1<false>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg14: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %66 = "vhlo.compare_v1"(%arg11, %arg12) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>)
    %15 = "vhlo.slice_v1"(%14#1) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i32_v1>
    %16 = "vhlo.convert_v1"(%15) : (!vhlo.tensor_v1<128x4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i64_v1>
    %17 = "vhlo.reshape_v1"(%16) : (!vhlo.tensor_v1<128x4x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %18 = "vhlo.concatenate_v1"(%6, %17) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>
    %19 = "vhlo.slice_v1"(%14#0) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %20 = "vhlo.reduce_v1"(%19, %1) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.maximum_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %22 = "vhlo.subtract_v1"(%19, %21) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %23 = "vhlo.exponential_v2"(%22) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %24 = "vhlo.reduce_v1"(%23, %0) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %25 = "vhlo.broadcast_in_dim_v1"(%24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %26 = "vhlo.divide_v1"(%23, %25) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %27 = "vhlo.scatter_v2"(%4, %18, %26) <{index_vector_dim = #vhlo.integer_v1<2 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %28 = "vhlo.broadcast_in_dim_v1"(%arg10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %29 = "vhlo.concatenate_v1"(%7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7) <{dimension = #vhlo.integer_v1<0 : i64>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %31 = "vhlo.dot_general_v2"(%30, %arg9) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %32 = "vhlo.broadcast_in_dim_v1"(%arg8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %33 = "vhlo.add_v1"(%31, %32) : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %34 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 1]> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %35 = "vhlo.broadcast_in_dim_v1"(%arg6) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %36 = "vhlo.clamp_v1"(%28, %34, %35) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %37 = "vhlo.add_v1"(%36, %3) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %38 = "vhlo.convert_v1"(%37) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %40 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %41 = "vhlo.clamp_v1"(%39, %40, %35) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %42 = "vhlo.convert_v1"(%41) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %43 = "vhlo.broadcast_in_dim_v1"(%arg5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %44 = "vhlo.multiply_v1"(%42, %43) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %46 = "vhlo.logistic_v2"(%45) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %47 = "vhlo.convert_v1"(%46) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %48 = "vhlo.multiply_v1"(%42, %47) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %50 = "vhlo.convert_v1"(%49) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %51 = "vhlo.multiply_v1"(%38, %50) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %52 = "vhlo.convert_v1"(%51) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %53 = "vhlo.dot_general_v2"(%52, %arg4) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %54 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %55 = "vhlo.add_v1"(%53, %54) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %58 = "vhlo.transpose_v1"(%27) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[32,128]{0,1}">} : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>
    %59 = "vhlo.reshape_v1"(%58) : (!vhlo.tensor_v1<32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>
    %60 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>
    %61 = "vhlo.reshape_v1"(%60) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>
    %62 = "vhlo.broadcast_in_dim_v1"(%61) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %63 = "vhlo.multiply_v1"(%57, %62) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %64 = "vhlo.convert_v1"(%63) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %65 = "vhlo.reduce_v1"(%64, %0) <{dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>
    "vhlo.return_v1"(%4, %27, %27, %65, %65) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump After VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j]) {i=128, j=32}>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([j, k]) {i=1, j=128, k=2880}>} : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=2880, j=32}>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, k], [k, j])->([i, j]) {i=128, j=32, k=2880} reduction={k}>} : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=32}>} : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER {sdy.sharding_rule = #sdy.op_sharding_rule<([], [])->([])>} : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j], [i, j]) {i=128, j=32} need_replication={j}>} : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, l])->([i, j, m]) {i=128, j=4, k=1, l=1, m=1}>} : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([k, l], [i, j, m], [i, j])->([k, l]) {i=128, j=4, k=128, l=32, m=2} reduction={i, j} need_replication={m}>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i], [k, i], [l, i], [m, i], [n, i], [o, i], [p, i], [q, i], [r, i], [s, i], [t, i], [u, i], [v, i], [w, i], [x, i], [y, i], [z, i], [z_1, i], [z_2, i], [z_3, i], [z_4, i], [z_5, i], [z_6, i], [z_7, i], [z_8, i], [z_9, i], [z_10, i], [z_11, i], [z_12, i], [z_13, i], [z_14, i], [z_15, i])->([z_16, i]) {i=2880, j=1, k=1, l=1, m=1, n=1, o=1, p=1, q=1, r=1, s=1, t=1, u=1, v=1, w=1, x=1, y=1, z=1, z_1=1, z_2=1, z_3=1, z_4=1, z_5=1, z_6=1, z_7=1, z_8=1, z_9=1, z_10=1, z_11=1, z_12=1, z_13=1, z_14=1, z_15=1, z_16=1}>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([ij, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=5760, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=5760}>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=5760}>} : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=2880, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=32, j=128}>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l, m])->([i, k, l]) {i=32, j=1, k=1, l=128, m=1}>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [])->([j, k, l]) {i=32, j=1, k=128, l=2880} reduction={i}>} : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j]) {i=128, j=32}>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([j, k]) {i=1, j=128, k=2880}>} : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=2880, j=32}>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, k], [k, j])->([i, j]) {i=128, j=32, k=2880} reduction={k}>} : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=32}>} : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER {sdy.sharding_rule = #sdy.op_sharding_rule<([], [])->([])>} : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j], [i, j]) {i=128, j=32} need_replication={j}>} : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, l])->([i, j, m]) {i=128, j=4, k=1, l=1, m=1}>} : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([k, l], [i, j, m], [i, j])->([k, l]) {i=128, j=4, k=128, l=32, m=2} reduction={i, j} need_replication={m}>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i], [k, i], [l, i], [m, i], [n, i], [o, i], [p, i], [q, i], [r, i], [s, i], [t, i], [u, i], [v, i], [w, i], [x, i], [y, i], [z, i], [z_1, i], [z_2, i], [z_3, i], [z_4, i], [z_5, i], [z_6, i], [z_7, i], [z_8, i], [z_9, i], [z_10, i], [z_11, i], [z_12, i], [z_13, i], [z_14, i], [z_15, i])->([z_16, i]) {i=2880, j=1, k=1, l=1, m=1, n=1, o=1, p=1, q=1, r=1, s=1, t=1, u=1, v=1, w=1, x=1, y=1, z=1, z_1=1, z_2=1, z_3=1, z_4=1, z_5=1, z_6=1, z_7=1, z_8=1, z_9=1, z_10=1, z_11=1, z_12=1, z_13=1, z_14=1, z_15=1, z_16=1}>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([ij, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=5760, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=5760}>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=5760}>} : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=2880, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=32, j=128}>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l, m])->([i, k, l]) {i=32, j=1, k=1, l=128, m=1}>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [])->([j, k, l]) {i=32, j=1, k=128, l=2880} reduction={i}>} : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j]) {i=128, j=32}>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([j, k]) {i=1, j=128, k=2880}>} : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=2880, j=32}>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, k], [k, j])->([i, j]) {i=128, j=32, k=2880} reduction={k}>} : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=32}>} : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER {sdy.sharding_rule = #sdy.op_sharding_rule<([], [])->([])>} : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j], [i, j]) {i=128, j=32} need_replication={j}>} : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, l])->([i, j, m]) {i=128, j=4, k=1, l=1, m=1}>} : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([k, l], [i, j, m], [i, j])->([k, l]) {i=128, j=4, k=128, l=32, m=2} reduction={i, j} need_replication={m}>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i], [k, i], [l, i], [m, i], [n, i], [o, i], [p, i], [q, i], [r, i], [s, i], [t, i], [u, i], [v, i], [w, i], [x, i], [y, i], [z, i], [z_1, i], [z_2, i], [z_3, i], [z_4, i], [z_5, i], [z_6, i], [z_7, i], [z_8, i], [z_9, i], [z_10, i], [z_11, i], [z_12, i], [z_13, i], [z_14, i], [z_15, i])->([z_16, i]) {i=2880, j=1, k=1, l=1, m=1, n=1, o=1, p=1, q=1, r=1, s=1, t=1, u=1, v=1, w=1, x=1, y=1, z=1, z_1=1, z_2=1, z_3=1, z_4=1, z_5=1, z_6=1, z_7=1, z_8=1, z_9=1, z_10=1, z_11=1, z_12=1, z_13=1, z_14=1, z_15=1, z_16=1}>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([ij, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=5760, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=5760}>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=5760}>} : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=2880, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=32, j=128}>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l, m])->([i, k, l]) {i=32, j=1, k=1, l=128, m=1}>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [])->([j, k, l]) {i=32, j=1, k=128, l=2880} reduction={i}>} : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j]) {i=128, j=32}>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([j, k]) {i=1, j=128, k=2880}>} : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=2880, j=32}>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, k], [k, j])->([i, j]) {i=128, j=32, k=2880} reduction={k}>} : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=32}>} : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER {sdy.sharding_rule = #sdy.op_sharding_rule<([], [])->([])>} : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j], [i, j]) {i=128, j=32} need_replication={j}>} : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, l])->([i, j, m]) {i=128, j=4, k=1, l=1, m=1}>} : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([k, l], [i, j, m], [i, j])->([k, l]) {i=128, j=4, k=128, l=32, m=2} reduction={i, j} need_replication={m}>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i], [k, i], [l, i], [m, i], [n, i], [o, i], [p, i], [q, i], [r, i], [s, i], [t, i], [u, i], [v, i], [w, i], [x, i], [y, i], [z, i], [z_1, i], [z_2, i], [z_3, i], [z_4, i], [z_5, i], [z_6, i], [z_7, i], [z_8, i], [z_9, i], [z_10, i], [z_11, i], [z_12, i], [z_13, i], [z_14, i], [z_15, i])->([z_16, i]) {i=2880, j=1, k=1, l=1, m=1, n=1, o=1, p=1, q=1, r=1, s=1, t=1, u=1, v=1, w=1, x=1, y=1, z=1, z_1=1, z_2=1, z_3=1, z_4=1, z_5=1, z_6=1, z_7=1, z_8=1, z_9=1, z_10=1, z_11=1, z_12=1, z_13=1, z_14=1, z_15=1, z_16=1}>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([ij, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=5760, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=5760}>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=5760}>} : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=2880, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=32, j=128}>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l, m])->([i, k, l]) {i=32, j=1, k=1, l=128, m=1}>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [])->([j, k, l]) {i=32, j=1, k=128, l=2880} reduction={i}>} : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    %63 = sdy.all_reduce {"_axis_0"} %62 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
    return %1, %24, %24, %63, %63 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j]) {i=128, j=32}>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([j, k]) {i=1, j=128, k=2880}>} : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=2880, j=32}>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, k], [k, j])->([i, j]) {i=128, j=32, k=2880} reduction={k}>} : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=32}>} : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER {sdy.sharding_rule = #sdy.op_sharding_rule<([], [])->([])>} : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j], [i, j]) {i=128, j=32} need_replication={j}>} : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, l])->([i, j, m]) {i=128, j=4, k=1, l=1, m=1}>} : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([k, l], [i, j, m], [i, j])->([k, l]) {i=128, j=4, k=128, l=32, m=2} reduction={i, j} need_replication={m}>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i], [k, i], [l, i], [m, i], [n, i], [o, i], [p, i], [q, i], [r, i], [s, i], [t, i], [u, i], [v, i], [w, i], [x, i], [y, i], [z, i], [z_1, i], [z_2, i], [z_3, i], [z_4, i], [z_5, i], [z_6, i], [z_7, i], [z_8, i], [z_9, i], [z_10, i], [z_11, i], [z_12, i], [z_13, i], [z_14, i], [z_15, i])->([z_16, i]) {i=2880, j=1, k=1, l=1, m=1, n=1, o=1, p=1, q=1, r=1, s=1, t=1, u=1, v=1, w=1, x=1, y=1, z=1, z_1=1, z_2=1, z_3=1, z_4=1, z_5=1, z_6=1, z_7=1, z_8=1, z_9=1, z_10=1, z_11=1, z_12=1, z_13=1, z_14=1, z_15=1, z_16=1}>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([ij, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=5760, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=5760}>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=5760}>} : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=2880, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=32, j=128}>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l, m])->([i, k, l]) {i=32, j=1, k=1, l=128, m=1}>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [])->([j, k, l]) {i=32, j=1, k=128, l=2880} reduction={i}>} : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    %63 = sdy.all_reduce {"_axis_0"} %62 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
    return %1, %24, %24, %63, %63 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<32x2880xbf16>, %arg15: tensor<32x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<32x5760xbf16>, %arg20: tensor<32x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j]) {i=128, j=32}>} : (tensor<bf16>) -> tensor<128x32xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([j, k]) {i=1, j=128, k=2880}>} : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=2880, j=32}>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, k], [k, j])->([i, j]) {i=128, j=32, k=2880} reduction={k}>} : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=32}>} : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %65 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER {sdy.sharding_rule = #sdy.op_sharding_rule<([], [])->([])>} : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %65 : tensor<i1>
      }) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j], [i, j]) {i=128, j=32} need_replication={j}>} : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, l])->([i, j, m]) {i=128, j=4, k=1, l=1, m=1}>} : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([k, l], [i, j, m], [i, j])->([k, l]) {i=128, j=4, k=128, l=32, m=2} reduction={i, j} need_replication={m}>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %27 = stablehlo.concatenate %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i], [k, i], [l, i], [m, i], [n, i], [o, i], [p, i], [q, i], [r, i], [s, i], [t, i], [u, i], [v, i], [w, i], [x, i], [y, i], [z, i], [z_1, i], [z_2, i], [z_3, i], [z_4, i], [z_5, i], [z_6, i], [z_7, i], [z_8, i], [z_9, i], [z_10, i], [z_11, i], [z_12, i], [z_13, i], [z_14, i], [z_15, i])->([z_16, i]) {i=2880, j=1, k=1, l=1, m=1, n=1, o=1, p=1, q=1, r=1, s=1, t=1, u=1, v=1, w=1, x=1, y=1, z=1, z_1=1, z_2=1, z_3=1, z_4=1, z_5=1, z_6=1, z_7=1, z_8=1, z_9=1, z_10=1, z_11=1, z_12=1, z_13=1, z_14=1, z_15=1, z_16=1}>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
      %28 = stablehlo.reshape %27 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([ij, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
      %29 = stablehlo.dot_general %28, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=5760, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
      %30 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=5760}>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
      %31 = stablehlo.add %29, %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=5760}>} : tensor<32x128x5760xbf16>
      %32 = stablehlo.slice %31 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %33 = stablehlo.broadcast_in_dim %arg17, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %34 = stablehlo.clamp %26, %32, %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %35 = stablehlo.add %34, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %36 = stablehlo.convert %35 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %37 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %38 = stablehlo.slice %31 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %39 = stablehlo.clamp %37, %38, %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %40 = stablehlo.convert %39 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %41 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<f32>) -> tensor<32x128x2880xf32>
      %42 = stablehlo.multiply %40, %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
      %43 = stablehlo.convert %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %44 = stablehlo.logistic %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %45 = stablehlo.convert %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %46 = stablehlo.multiply %40, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
      %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %48 = stablehlo.convert %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %49 = stablehlo.multiply %36, %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
      %50 = stablehlo.convert %49 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %51 = stablehlo.dot_general %50, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=2880, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
      %52 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
      %53 = stablehlo.add %51, %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %54 = stablehlo.reshape %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
      %55 = stablehlo.convert %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
      %56 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=32, j=128}>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
      %57 = stablehlo.reshape %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
      %58 = stablehlo.convert %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
      %59 = stablehlo.reshape %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l, m])->([i, k, l]) {i=32, j=1, k=1, l=128, m=1}>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
      %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
      %61 = stablehlo.multiply %55, %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : tensor<32x1x128x2880xf32>
      %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
      %63 = stablehlo.reduce(%62 init: %cst) applies stablehlo.add across dimensions = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [])->([j, k, l]) {i=32, j=1, k=128, l=2880} reduction={i}>} : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %64 = sdy.all_reduce {"_axis_0"} %63 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %64, %64 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<32x2880xbf16>, %arg15: tensor<32x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<32x5760xbf16>, %arg20: tensor<32x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j]) {i=128, j=32}>} : (tensor<bf16>) -> tensor<128x32xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([j, k]) {i=1, j=128, k=2880}>} : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=2880, j=32}>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, k], [k, j])->([i, j]) {i=128, j=32, k=2880} reduction={k}>} : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=32}>} : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %65 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER {sdy.sharding_rule = #sdy.op_sharding_rule<([], [])->([])>} : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %65 : tensor<i1>
      }) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j], [i, j]) {i=128, j=32} need_replication={j}>} : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, l])->([i, j, m]) {i=128, j=4, k=1, l=1, m=1}>} : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([k, l], [i, j, m], [i, j])->([k, l]) {i=128, j=4, k=128, l=32, m=2} reduction={i, j} need_replication={m}>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %27 = stablehlo.concatenate %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i], [k, i], [l, i], [m, i], [n, i], [o, i], [p, i], [q, i], [r, i], [s, i], [t, i], [u, i], [v, i], [w, i], [x, i], [y, i], [z, i], [z_1, i], [z_2, i], [z_3, i], [z_4, i], [z_5, i], [z_6, i], [z_7, i], [z_8, i], [z_9, i], [z_10, i], [z_11, i], [z_12, i], [z_13, i], [z_14, i], [z_15, i])->([z_16, i]) {i=2880, j=1, k=1, l=1, m=1, n=1, o=1, p=1, q=1, r=1, s=1, t=1, u=1, v=1, w=1, x=1, y=1, z=1, z_1=1, z_2=1, z_3=1, z_4=1, z_5=1, z_6=1, z_7=1, z_8=1, z_9=1, z_10=1, z_11=1, z_12=1, z_13=1, z_14=1, z_15=1, z_16=1}>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
      %28 = stablehlo.reshape %27 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([ij, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
      %29 = stablehlo.dot_general %28, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=5760, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
      %30 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=5760}>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
      %31 = stablehlo.add %29, %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=5760}>} : tensor<32x128x5760xbf16>
      %32 = stablehlo.slice %31 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %33 = stablehlo.broadcast_in_dim %arg17, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %34 = stablehlo.clamp %26, %32, %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %35 = stablehlo.add %34, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %36 = stablehlo.convert %35 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %37 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %38 = stablehlo.slice %31 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %39 = stablehlo.clamp %37, %38, %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %40 = stablehlo.convert %39 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %41 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<f32>) -> tensor<32x128x2880xf32>
      %42 = stablehlo.multiply %40, %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
      %43 = stablehlo.convert %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %44 = stablehlo.logistic %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %45 = stablehlo.convert %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %46 = stablehlo.multiply %40, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
      %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %48 = stablehlo.convert %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %49 = stablehlo.multiply %36, %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
      %50 = stablehlo.convert %49 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %51 = stablehlo.dot_general %50, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=2880, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
      %52 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
      %53 = stablehlo.add %51, %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %54 = stablehlo.reshape %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
      %55 = stablehlo.convert %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
      %56 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=32, j=128}>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
      %57 = stablehlo.reshape %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
      %58 = stablehlo.convert %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
      %59 = stablehlo.reshape %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l, m])->([i, k, l]) {i=32, j=1, k=1, l=128, m=1}>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
      %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
      %61 = stablehlo.multiply %55, %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : tensor<32x1x128x2880xf32>
      %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
      %63 = stablehlo.reduce(%62 init: %cst) applies stablehlo.add across dimensions = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [])->([j, k, l]) {i=32, j=1, k=128, l=2880} reduction={i}>} : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %64 = sdy.all_reduce {"_axis_0"} %63 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %64, %64 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<32x2880xbf16>, %arg15: tensor<32x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<32x5760xbf16>, %arg20: tensor<32x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j]) {i=128, j=32}>} : (tensor<bf16>) -> tensor<128x32xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([j, k]) {i=1, j=128, k=2880}>} : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=2880, j=32}>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, k], [k, j])->([i, j]) {i=128, j=32, k=2880} reduction={k}>} : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=32}>} : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %65 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER {sdy.sharding_rule = #sdy.op_sharding_rule<([], [])->([])>} : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %65 : tensor<i1>
      }) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j], [i, j]) {i=128, j=32} need_replication={j}>} : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, l])->([i, j, m]) {i=128, j=4, k=1, l=1, m=1}>} : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([k, l], [i, j, m], [i, j])->([k, l]) {i=128, j=4, k=128, l=32, m=2} reduction={i, j} need_replication={m}>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %27 = stablehlo.concatenate %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i], [k, i], [l, i], [m, i], [n, i], [o, i], [p, i], [q, i], [r, i], [s, i], [t, i], [u, i], [v, i], [w, i], [x, i], [y, i], [z, i], [z_1, i], [z_2, i], [z_3, i], [z_4, i], [z_5, i], [z_6, i], [z_7, i], [z_8, i], [z_9, i], [z_10, i], [z_11, i], [z_12, i], [z_13, i], [z_14, i], [z_15, i])->([z_16, i]) {i=2880, j=1, k=1, l=1, m=1, n=1, o=1, p=1, q=1, r=1, s=1, t=1, u=1, v=1, w=1, x=1, y=1, z=1, z_1=1, z_2=1, z_3=1, z_4=1, z_5=1, z_6=1, z_7=1, z_8=1, z_9=1, z_10=1, z_11=1, z_12=1, z_13=1, z_14=1, z_15=1, z_16=1}>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
      %28 = stablehlo.reshape %27 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([ij, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
      %29 = stablehlo.dot_general %28, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=5760, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
      %30 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=5760}>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
      %31 = stablehlo.add %29, %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=5760}>} : tensor<32x128x5760xbf16>
      %32 = stablehlo.slice %31 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %33 = stablehlo.broadcast_in_dim %arg17, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %34 = stablehlo.clamp %26, %32, %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %35 = stablehlo.add %34, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %36 = stablehlo.convert %35 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %37 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %38 = stablehlo.slice %31 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %39 = stablehlo.clamp %37, %38, %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %40 = stablehlo.convert %39 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %41 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<f32>) -> tensor<32x128x2880xf32>
      %42 = stablehlo.multiply %40, %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
      %43 = stablehlo.convert %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %44 = stablehlo.logistic %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %45 = stablehlo.convert %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %46 = stablehlo.multiply %40, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
      %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %48 = stablehlo.convert %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %49 = stablehlo.multiply %36, %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xf32>
      %50 = stablehlo.convert %49 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %51 = stablehlo.dot_general %50, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=2880, l=2880} reduction={l}>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
      %52 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
      %53 = stablehlo.add %51, %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : tensor<32x128x2880xbf16>
      %54 = stablehlo.reshape %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
      %55 = stablehlo.convert %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
      %56 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=32, j=128}>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
      %57 = stablehlo.reshape %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
      %58 = stablehlo.convert %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
      %59 = stablehlo.reshape %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l, m])->([i, k, l]) {i=32, j=1, k=1, l=128, m=1}>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
      %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
      %61 = stablehlo.multiply %55, %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : tensor<32x1x128x2880xf32>
      %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
      %63 = stablehlo.reduce(%62 init: %cst) applies stablehlo.add across dimensions = [0] {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [])->([j, k, l]) {i=32, j=1, k=128, l=2880} reduction={i}>} : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %64 = sdy.all_reduce {"_axis_0"} %63 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %64, %64 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


#sdy<dimension_sharding{?}>
#sdy<dimension_sharding{"_axis_0", ?}>
loc("concatenate.78"): error: 'stablehlo.concatenate' op inferred type(s) 'tensor<4096x2880xbf16>' are incompatible with return type(s) of operation 'tensor<512x2880xbf16>'
loc("concatenate.78"): error: 'stablehlo.concatenate' op failed to infer returned types
// -----// IR Dump After UpdateGlobalToLocalShapesPass Failed (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
"builtin.module"() <{sym_name = "SyncTensorsGraph.134"}> ({
  "sdy.mesh"() <{mesh = #sdy.mesh<["_axis_0_updated"=1, "_axis_0"=8]>, sym_name = "mesh"}> : () -> ()
  "func.func"() <{arg_attrs = [{ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}], function_type = (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>), res_attrs = [{ttcore.shard_status = #ttcore.shard_status<unsharded>}, {ttcore.shard_status = #ttcore.shard_status<unsharded>}, {ttcore.shard_status = #ttcore.shard_status<unsharded>}, {ttcore.shard_status = #ttcore.shard_status<unsharded>}, {ttcore.shard_status = #ttcore.shard_status<unsharded>}], sym_name = "main"}> ({
  ^bb0(%arg0: tensor<32xbf16>, %arg1: tensor<32x2880xbf16>, %arg2: tensor<1x128x2880xbf16>, %arg3: tensor<32x2880xbf16>, %arg4: tensor<32x2880x2880xbf16>, %arg5: tensor<f32>, %arg6: tensor<bf16>, %arg7: tensor<bf16>, %arg8: tensor<32x5760xbf16>, %arg9: tensor<32x2880x5760xbf16>, %arg10: tensor<bf16>):
    %0:5 = "sdy.manual_computation"(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) <{in_shardings = #sdy.sharding_per_value<[<@mesh, [{?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>]>, manual_axes = #sdy<manual_axes{"_axis_0_updated", "_axis_0"}>, out_shardings = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>]>}> ({
    ^bb0(%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<4x2880xbf16>, %arg15: tensor<4x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<4x5760xbf16>, %arg20: tensor<4x2880x5760xbf16>, %arg21: tensor<bf16>):
      %1 = "stablehlo.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
      %2 = "stablehlo.constant"() <{value = dense<0xFF80> : tensor<bf16>}> : () -> tensor<bf16>
      %3 = "stablehlo.constant"() <{value = dense<1.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
      %4 = "stablehlo.broadcast_in_dim"(%3) <{broadcast_dimensions = array<i64>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %5 = "stablehlo.broadcast_in_dim"(%1) <{broadcast_dimensions = array<i64>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j]) {i=128, j=32}>} : (tensor<bf16>) -> tensor<128x4xbf16>
      %6 = "stablehlo.iota"() <{iota_dimension = 0 : i64}> : () -> tensor<128xi64>
      %7 = "stablehlo.broadcast_in_dim"(%6) <{broadcast_dimensions = array<i64: 0>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %8 = "stablehlo.reshape"(%arg13) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([j, k]) {i=1, j=128, k=2880}>} : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %9 = "stablehlo.transpose"(%arg12) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=2880, j=32}>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %10 = "stablehlo.dot_general"(%8, %9) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i, k], [k, j])->([i, j]) {i=128, j=32, k=2880} reduction={k}>} : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %11 = "stablehlo.broadcast_in_dim"(%arg11) <{broadcast_dimensions = array<i64: 1>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %12 = "stablehlo.add"(%10, %11) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=32}>} : (tensor<128x32xbf16>, tensor<128x32xbf16>) -> tensor<128x32xbf16>
      %13 = "stablehlo.iota"() <{iota_dimension = 0 : i64}> : () -> tensor<32xi32>
      %14 = "stablehlo.broadcast_in_dim"(%13) <{broadcast_dimensions = array<i64: 1>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([j])->([i, j]) {i=128, j=32}>} : (tensor<32xi32>) -> tensor<128x32xi32>
      %15:2 = "stablehlo.sort"(%12, %14) <{dimension = 1 : i64}> ({
      ^bb0(%arg32: tensor<bf16>, %arg33: tensor<bf16>, %arg34: tensor<i32>, %arg35: tensor<i32>):
        %72 = "stablehlo.compare"(%arg32, %arg33) <{compare_type = #stablehlo<comparison_type TOTALORDER>, comparison_direction = #stablehlo<comparison_direction GT>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([], [])->([])>} : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        "stablehlo.return"(%72) : (tensor<i1>) -> ()
      }) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j], [i, j]) {i=128, j=32} need_replication={j}>} : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %16 = "stablehlo.slice"(%15#1) <{limit_indices = array<i64: 128, 4>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %17 = "stablehlo.convert"(%16) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %18 = "stablehlo.reshape"(%17) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j, k]) {i=128, j=4, k=1}>} : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %19 = "stablehlo.concatenate"(%7, %18) <{dimension = 2 : i64}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, l])->([i, j, m]) {i=128, j=4, k=1, l=1, m=1}>} : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %20 = "stablehlo.slice"(%15#0) <{limit_indices = array<i64: 128, 4>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=32} permutation={j} blocked_propagation={j}>} : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %21 = "stablehlo.reduce"(%20, %2) <{dimensions = array<i64: 1>}> ({
      ^bb0(%arg30: tensor<bf16>, %arg31: tensor<bf16>):
        %71 = "stablehlo.maximum"(%arg30, %arg31) {sdy.sharding_rule = #sdy.op_sharding_rule<([], [])->([])>} : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%71) : (tensor<bf16>) -> ()
      }) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %22 = "stablehlo.broadcast_in_dim"(%21) <{broadcast_dimensions = array<i64: 0>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %23 = "stablehlo.subtract"(%20, %22) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : (tensor<128x4xbf16>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %24 = "stablehlo.exponential"(%23) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j])->([i, j]) {i=128, j=4}>} : (tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %25 = "stablehlo.reduce"(%24, %1) <{dimensions = array<i64: 1>}> ({
      ^bb0(%arg28: tensor<bf16>, %arg29: tensor<bf16>):
        %70 = "stablehlo.add"(%arg28, %arg29) {sdy.sharding_rule = #sdy.op_sharding_rule<([], [])->([])>} : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%70) : (tensor<bf16>) -> ()
      }) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [])->([i]) {i=128, j=4} reduction={j}>} : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %26 = "stablehlo.broadcast_in_dim"(%25) <{broadcast_dimensions = array<i64: 0>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i])->([i, j]) {i=128, j=4}>} : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %27 = "stablehlo.divide"(%24, %26) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j], [i, j])->([i, j]) {i=128, j=4}>} : (tensor<128x4xbf16>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %28 = "stablehlo.scatter"(%5, %19, %27) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg26: tensor<bf16>, %arg27: tensor<bf16>):
        "stablehlo.return"(%arg27) : (tensor<bf16>) -> ()
      }) {sdy.sharding_rule = #sdy.op_sharding_rule<([k, l], [i, j, m], [i, j])->([k, l]) {i=128, j=4, k=128, l=32, m=2} reduction={i, j} need_replication={m}>} : (tensor<128x4xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %29 = "stablehlo.broadcast_in_dim"(%arg21) <{broadcast_dimensions = array<i64>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %30 = "stablehlo.concatenate"(%8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8, %8) <{dimension = 0 : i64}> {sdy.sharding_rule = #sdy.op_sharding_rule<([j, i], [k, i], [l, i], [m, i], [n, i], [o, i], [p, i], [q, i], [r, i], [s, i], [t, i], [u, i], [v, i], [w, i], [x, i], [y, i], [z, i], [z_1, i], [z_2, i], [z_3, i], [z_4, i], [z_5, i], [z_6, i], [z_7, i], [z_8, i], [z_9, i], [z_10, i], [z_11, i], [z_12, i], [z_13, i], [z_14, i], [z_15, i])->([z_16, i]) {i=2880, j=1, k=1, l=1, m=1, n=1, o=1, p=1, q=1, r=1, s=1, t=1, u=1, v=1, w=1, x=1, y=1, z=1, z_1=1, z_2=1, z_3=1, z_4=1, z_5=1, z_6=1, z_7=1, z_8=1, z_9=1, z_10=1, z_11=1, z_12=1, z_13=1, z_14=1, z_15=1, z_16=1}>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<512x2880xbf16>
      %31 = "stablehlo.reshape"(%30) {sdy.sharding_rule = #sdy.op_sharding_rule<([ij, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<512x2880xbf16>) -> tensor<4x128x2880xbf16>
      %32 = "stablehlo.dot_general"(%31, %arg20) <{dot_dimension_numbers = #stablehlo.dot<lhs_batching_dimensions = [0], rhs_batching_dimensions = [0], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1]>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=5760, l=2880} reduction={l}>} : (tensor<4x128x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x128x5760xbf16>
      %33 = "stablehlo.broadcast_in_dim"(%arg19) <{broadcast_dimensions = array<i64: 0, 2>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=5760}>} : (tensor<4x5760xbf16>) -> tensor<4x128x5760xbf16>
      %34 = "stablehlo.add"(%32, %33) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=5760}>} : (tensor<4x128x5760xbf16>, tensor<4x128x5760xbf16>) -> tensor<4x128x5760xbf16>
      %35 = "stablehlo.slice"(%34) <{limit_indices = array<i64: 4, 128, 5760>, start_indices = array<i64: 0, 0, 1>, strides = array<i64: 1, 1, 2>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %36 = "stablehlo.broadcast_in_dim"(%arg17) <{broadcast_dimensions = array<i64>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %37 = "stablehlo.clamp"(%29, %35, %36) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>) -> tensor<4x128x2880xbf16>
      %38 = "stablehlo.add"(%37, %4) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>) -> tensor<4x128x2880xbf16>
      %39 = "stablehlo.convert"(%38) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %40 = "stablehlo.broadcast_in_dim"(%arg18) <{broadcast_dimensions = array<i64>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %41 = "stablehlo.slice"(%34) <{limit_indices = array<i64: 4, 128, 5760>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 2>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=5760} permutation={k} blocked_propagation={k}>} : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %42 = "stablehlo.clamp"(%40, %41, %36) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>) -> tensor<4x128x2880xbf16>
      %43 = "stablehlo.convert"(%42) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %44 = "stablehlo.broadcast_in_dim"(%arg16) <{broadcast_dimensions = array<i64>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<f32>) -> tensor<4x128x2880xf32>
      %45 = "stablehlo.multiply"(%43, %44) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xf32>, tensor<4x128x2880xf32>) -> tensor<4x128x2880xf32>
      %46 = "stablehlo.convert"(%45) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %47 = "stablehlo.logistic"(%46) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xbf16>
      %48 = "stablehlo.convert"(%47) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %49 = "stablehlo.multiply"(%43, %48) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xf32>, tensor<4x128x2880xf32>) -> tensor<4x128x2880xf32>
      %50 = "stablehlo.convert"(%49) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %51 = "stablehlo.convert"(%50) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %52 = "stablehlo.multiply"(%39, %51) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xf32>, tensor<4x128x2880xf32>) -> tensor<4x128x2880xf32>
      %53 = "stablehlo.convert"(%52) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %54 = "stablehlo.dot_general"(%53, %arg15) <{dot_dimension_numbers = #stablehlo.dot<lhs_batching_dimensions = [0], rhs_batching_dimensions = [0], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1]>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k])->([i, j, k]) {i=32, j=128, k=2880, l=2880} reduction={l}>} : (tensor<4x128x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x128x2880xbf16>
      %55 = "stablehlo.broadcast_in_dim"(%arg14) <{broadcast_dimensions = array<i64: 0, 2>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x2880xbf16>) -> tensor<4x128x2880xbf16>
      %56 = "stablehlo.add"(%54, %55) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k], [i, j, k])->([i, j, k]) {i=32, j=128, k=2880}>} : (tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>) -> tensor<4x128x2880xbf16>
      %57 = "stablehlo.reshape"(%56) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<4x128x2880xbf16>) -> tensor<4x1x128x2880xbf16>
      %58 = "stablehlo.convert"(%57) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<4x1x128x2880xbf16>) -> tensor<4x1x128x2880xf32>
      %59 = "stablehlo.transpose"(%28) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding_rule = #sdy.op_sharding_rule<([j, i])->([i, j]) {i=32, j=128}>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x4xbf16>) -> tensor<4x128xbf16>
      %60 = "stablehlo.reshape"(%59) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, k])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<4x128xbf16>) -> tensor<4x1x128x1xbf16>
      %61 = "stablehlo.convert"(%60) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=1}>} : (tensor<4x1x128x1xbf16>) -> tensor<4x1x128x1xf32>
      %62 = "stablehlo.reshape"(%61) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l, m])->([i, k, l]) {i=32, j=1, k=1, l=128, m=1}>} : (tensor<4x1x128x1xf32>) -> tensor<4x1x128xf32>
      %63 = "stablehlo.broadcast_in_dim"(%62) <{broadcast_dimensions = array<i64: 0, 1, 2>}> {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<4x1x128xf32>) -> tensor<4x1x128x2880xf32>
      %64 = "stablehlo.multiply"(%58, %63) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<4x1x128x2880xf32>, tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xf32>
      %65 = "stablehlo.convert"(%64) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l])->([i, j, k, l]) {i=32, j=1, k=128, l=2880}>} : (tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xbf16>
      %66 = "stablehlo.reduce"(%65, %1) <{dimensions = array<i64: 0>}> ({
      ^bb0(%arg24: tensor<bf16>, %arg25: tensor<bf16>):
        %69 = "stablehlo.add"(%arg24, %arg25) {sdy.sharding_rule = #sdy.op_sharding_rule<([], [])->([])>} : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%69) : (tensor<bf16>) -> ()
      }) {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, k, l], [])->([j, k, l]) {i=32, j=1, k=128, l=2880} reduction={i}>} : (tensor<4x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %67 = "stablehlo.all_reduce"(%66) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        %68 = "stablehlo.add"(%arg22, %arg23) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%68) : (tensor<bf16>) -> ()
      }) : (tensor<1x128x2880xbf16>) -> tensor<1x128x2880xbf16>
      "sdy.return"(%5, %28, %28, %67, %67) : (tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) -> ()
    }) : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    "func.return"(%0#0, %0#1, %0#2, %0#3, %0#4) : (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) -> ()
  }) : () -> ()
}) {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} : () -> ()


Traceback (most recent call last):
  File "/localdev/hshah/tt-torch/tests/models/gpt_oss/test_gpt_oss_mlp_tensor_parallel.py", line 291, in run_mlp_test
    torch_xla.sync()
  File "/localdev/hshah/tt-torch/env/venv/lib/python3.11/site-packages/torch_xla/torch_xla.py", line 87, in sync
    torch_xla._XLAC._xla_step_marker(
ValueError: Error code: 13
Traceback (most recent call last):
  File "/localdev/hshah/tt-torch/tests/models/gpt_oss/test_gpt_oss_mlp_tensor_parallel.py", line 326, in main
    run_mlp_test()
  File "/localdev/hshah/tt-torch/tests/models/gpt_oss/test_gpt_oss_mlp_tensor_parallel.py", line 291, in run_mlp_test
    torch_xla.sync()
  File "/localdev/hshah/tt-torch/env/venv/lib/python3.11/site-packages/torch_xla/torch_xla.py", line 87, in sync
    torch_xla._XLAC._xla_step_marker(
ValueError: Error code: 13
GPT-OSS MLP Layer Test with Torch-XLA SPMD Tensor Parallelism
============================================================
Setting up GPT-OSS Multichip MLP Layer Test...
Setting up XLA environment...
XLA environment configured.
Created device mesh: (1, 8) with 8 devices
Config: hidden_size=2880, intermediate_size=2880, num_experts=32, top_k=4
Creating MLP layer...
Setting up tensor parallelism...
Creating normalized synthetic inputs...
Input shapes (after RMSNorm): hidden_states=torch.Size([1, 128, 2880]) (torch.bfloat16)
Moving inputs to XLA device...
After moving to XLA - hidden_states dtype: torch.bfloat16
Running MLP Layer Forward Pass with Tensor Parallelism...
Hidden states shape before repeat: torch.Size([128, 2880])
Hidden states shape after repeat: torch.Size([4096, 2880])
Hidden states shape after view: torch.Size([32, 128, 2880])
MLP forward pass completed!
Output shapes: mlp_output=torch.Size([1, 128, 2880]), router_scores=torch.Size([128, 32])
Synchronizing XLA device...
Error during MLP forward pass: Error code: 13
Error type: ValueError
Test failed with error: Error code: 13
[HET DEBUG][SCATTER] attrDict elements start:
  scatter_dimension_numbers: #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>
  sdy.sharding: #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>
  sdy.sharding_rule: #sdy.op_sharding_rule<([k, l], [i, j, m], [i, j])->([k, l]) {i=128, j=4, k=128, l=32, m=2} reduction={i, j} need_replication={m}>
[HET DEBUG][SCATTER] attrDict elements end
[HET DEBUG][SCATTER] Operand dim shardings start:


[HET DEBUG][SCATTER] Operand dim shardings end
