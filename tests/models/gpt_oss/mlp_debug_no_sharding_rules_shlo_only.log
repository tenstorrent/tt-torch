WARNING:root:Defaulting to PJRT_DEVICE=CPU
2025-09-22 16:05:02.799627: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<32x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg6: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %3 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %5 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<128x!vhlo.i64_v1>
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %7 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>
    %8 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,32]{0,1}">} : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>
    %9 = "vhlo.dot_general_v2"(%7, %8) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %11 = "vhlo.add_v1"(%9, %10) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %12 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %13 = "vhlo.broadcast_in_dim_v1"(%12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.i32_v1>
    %14:2 = "vhlo.sort_v1"(%11, %13) <{dimension = #vhlo.integer_v1<1 : i64>, is_stable = #vhlo.bool_v1<false>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg14: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %66 = "vhlo.compare_v1"(%arg11, %arg12) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>)
    %15 = "vhlo.slice_v1"(%14#1) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i32_v1>
    %16 = "vhlo.convert_v1"(%15) : (!vhlo.tensor_v1<128x4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i64_v1>
    %17 = "vhlo.reshape_v1"(%16) : (!vhlo.tensor_v1<128x4x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %18 = "vhlo.concatenate_v1"(%6, %17) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>
    %19 = "vhlo.slice_v1"(%14#0) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %20 = "vhlo.reduce_v1"(%19, %1) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.maximum_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %22 = "vhlo.subtract_v1"(%19, %21) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %23 = "vhlo.exponential_v2"(%22) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %24 = "vhlo.reduce_v1"(%23, %0) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %25 = "vhlo.broadcast_in_dim_v1"(%24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %26 = "vhlo.divide_v1"(%23, %25) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %27 = "vhlo.scatter_v2"(%4, %18, %26) <{index_vector_dim = #vhlo.integer_v1<2 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %28 = "vhlo.broadcast_in_dim_v1"(%arg10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %29 = "vhlo.concatenate_v1"(%7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7) <{dimension = #vhlo.integer_v1<0 : i64>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %31 = "vhlo.dot_general_v2"(%30, %arg9) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %32 = "vhlo.broadcast_in_dim_v1"(%arg8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %33 = "vhlo.add_v1"(%31, %32) : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %34 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 1]> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %35 = "vhlo.broadcast_in_dim_v1"(%arg6) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %36 = "vhlo.clamp_v1"(%28, %34, %35) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %37 = "vhlo.add_v1"(%36, %3) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %38 = "vhlo.convert_v1"(%37) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %40 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %41 = "vhlo.clamp_v1"(%39, %40, %35) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %42 = "vhlo.convert_v1"(%41) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %43 = "vhlo.broadcast_in_dim_v1"(%arg5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %44 = "vhlo.multiply_v1"(%42, %43) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %46 = "vhlo.logistic_v2"(%45) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %47 = "vhlo.convert_v1"(%46) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %48 = "vhlo.multiply_v1"(%42, %47) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %50 = "vhlo.convert_v1"(%49) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %51 = "vhlo.multiply_v1"(%38, %50) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %52 = "vhlo.convert_v1"(%51) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %53 = "vhlo.dot_general_v2"(%52, %arg4) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %54 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %55 = "vhlo.add_v1"(%53, %54) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %58 = "vhlo.transpose_v1"(%27) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[32,128]{0,1}">} : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>
    %59 = "vhlo.reshape_v1"(%58) : (!vhlo.tensor_v1<32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>
    %60 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>
    %61 = "vhlo.reshape_v1"(%60) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>
    %62 = "vhlo.broadcast_in_dim_v1"(%61) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %63 = "vhlo.multiply_v1"(%57, %62) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %64 = "vhlo.convert_v1"(%63) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %65 = "vhlo.reduce_v1"(%64, %0) <{dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>
    "vhlo.return_v1"(%4, %27, %27, %65, %65) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
// -----// IR Dump Before VhloToVersionPass (vhlo-to-version) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<32x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg6: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %3 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %5 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<128x!vhlo.i64_v1>
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %7 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>
    %8 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,32]{0,1}">} : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>
    %9 = "vhlo.dot_general_v2"(%7, %8) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %11 = "vhlo.add_v1"(%9, %10) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %12 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %13 = "vhlo.broadcast_in_dim_v1"(%12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.i32_v1>
    %14:2 = "vhlo.sort_v1"(%11, %13) <{dimension = #vhlo.integer_v1<1 : i64>, is_stable = #vhlo.bool_v1<false>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg14: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %66 = "vhlo.compare_v1"(%arg11, %arg12) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>)
    %15 = "vhlo.slice_v1"(%14#1) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i32_v1>
    %16 = "vhlo.convert_v1"(%15) : (!vhlo.tensor_v1<128x4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i64_v1>
    %17 = "vhlo.reshape_v1"(%16) : (!vhlo.tensor_v1<128x4x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %18 = "vhlo.concatenate_v1"(%6, %17) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>
    %19 = "vhlo.slice_v1"(%14#0) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %20 = "vhlo.reduce_v1"(%19, %1) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.maximum_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %22 = "vhlo.subtract_v1"(%19, %21) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %23 = "vhlo.exponential_v2"(%22) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %24 = "vhlo.reduce_v1"(%23, %0) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %25 = "vhlo.broadcast_in_dim_v1"(%24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %26 = "vhlo.divide_v1"(%23, %25) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %27 = "vhlo.scatter_v2"(%4, %18, %26) <{index_vector_dim = #vhlo.integer_v1<2 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %28 = "vhlo.broadcast_in_dim_v1"(%arg10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %29 = "vhlo.concatenate_v1"(%7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7) <{dimension = #vhlo.integer_v1<0 : i64>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %31 = "vhlo.dot_general_v2"(%30, %arg9) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %32 = "vhlo.broadcast_in_dim_v1"(%arg8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %33 = "vhlo.add_v1"(%31, %32) : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %34 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 1]> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %35 = "vhlo.broadcast_in_dim_v1"(%arg6) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %36 = "vhlo.clamp_v1"(%28, %34, %35) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %37 = "vhlo.add_v1"(%36, %3) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %38 = "vhlo.convert_v1"(%37) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %40 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %41 = "vhlo.clamp_v1"(%39, %40, %35) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %42 = "vhlo.convert_v1"(%41) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %43 = "vhlo.broadcast_in_dim_v1"(%arg5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %44 = "vhlo.multiply_v1"(%42, %43) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %46 = "vhlo.logistic_v2"(%45) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %47 = "vhlo.convert_v1"(%46) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %48 = "vhlo.multiply_v1"(%42, %47) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %50 = "vhlo.convert_v1"(%49) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %51 = "vhlo.multiply_v1"(%38, %50) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %52 = "vhlo.convert_v1"(%51) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %53 = "vhlo.dot_general_v2"(%52, %arg4) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %54 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %55 = "vhlo.add_v1"(%53, %54) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %58 = "vhlo.transpose_v1"(%27) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[32,128]{0,1}">} : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>
    %59 = "vhlo.reshape_v1"(%58) : (!vhlo.tensor_v1<32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>
    %60 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>
    %61 = "vhlo.reshape_v1"(%60) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>
    %62 = "vhlo.broadcast_in_dim_v1"(%61) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %63 = "vhlo.multiply_v1"(%57, %62) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %64 = "vhlo.convert_v1"(%63) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %65 = "vhlo.reduce_v1"(%64, %0) <{dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>
    "vhlo.return_v1"(%4, %27, %27, %65, %65) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump Before VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<32x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg6: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %3 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %5 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<128x!vhlo.i64_v1>
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %7 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>
    %8 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,32]{0,1}">} : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>
    %9 = "vhlo.dot_general_v2"(%7, %8) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %11 = "vhlo.add_v1"(%9, %10) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %12 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %13 = "vhlo.broadcast_in_dim_v1"(%12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.i32_v1>
    %14:2 = "vhlo.sort_v1"(%11, %13) <{dimension = #vhlo.integer_v1<1 : i64>, is_stable = #vhlo.bool_v1<false>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg14: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %66 = "vhlo.compare_v1"(%arg11, %arg12) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>)
    %15 = "vhlo.slice_v1"(%14#1) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i32_v1>
    %16 = "vhlo.convert_v1"(%15) : (!vhlo.tensor_v1<128x4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i64_v1>
    %17 = "vhlo.reshape_v1"(%16) : (!vhlo.tensor_v1<128x4x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %18 = "vhlo.concatenate_v1"(%6, %17) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>
    %19 = "vhlo.slice_v1"(%14#0) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %20 = "vhlo.reduce_v1"(%19, %1) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.maximum_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %22 = "vhlo.subtract_v1"(%19, %21) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %23 = "vhlo.exponential_v2"(%22) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %24 = "vhlo.reduce_v1"(%23, %0) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %25 = "vhlo.broadcast_in_dim_v1"(%24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %26 = "vhlo.divide_v1"(%23, %25) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %27 = "vhlo.scatter_v2"(%4, %18, %26) <{index_vector_dim = #vhlo.integer_v1<2 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %28 = "vhlo.broadcast_in_dim_v1"(%arg10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %29 = "vhlo.concatenate_v1"(%7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7, %7) <{dimension = #vhlo.integer_v1<0 : i64>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %31 = "vhlo.dot_general_v2"(%30, %arg9) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %32 = "vhlo.broadcast_in_dim_v1"(%arg8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %33 = "vhlo.add_v1"(%31, %32) : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %34 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 1]> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %35 = "vhlo.broadcast_in_dim_v1"(%arg6) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %36 = "vhlo.clamp_v1"(%28, %34, %35) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %37 = "vhlo.add_v1"(%36, %3) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %38 = "vhlo.convert_v1"(%37) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %40 = "vhlo.slice_v1"(%33) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %41 = "vhlo.clamp_v1"(%39, %40, %35) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %42 = "vhlo.convert_v1"(%41) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %43 = "vhlo.broadcast_in_dim_v1"(%arg5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %44 = "vhlo.multiply_v1"(%42, %43) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %46 = "vhlo.logistic_v2"(%45) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %47 = "vhlo.convert_v1"(%46) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %48 = "vhlo.multiply_v1"(%42, %47) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %50 = "vhlo.convert_v1"(%49) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %51 = "vhlo.multiply_v1"(%38, %50) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %52 = "vhlo.convert_v1"(%51) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %53 = "vhlo.dot_general_v2"(%52, %arg4) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %54 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %55 = "vhlo.add_v1"(%53, %54) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %58 = "vhlo.transpose_v1"(%27) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[32,128]{0,1}">} : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>
    %59 = "vhlo.reshape_v1"(%58) : (!vhlo.tensor_v1<32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>
    %60 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>
    %61 = "vhlo.reshape_v1"(%60) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>
    %62 = "vhlo.broadcast_in_dim_v1"(%61) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %63 = "vhlo.multiply_v1"(%57, %62) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %64 = "vhlo.convert_v1"(%63) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %65 = "vhlo.reduce_v1"(%64, %0) <{dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %66 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%66) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>
    "vhlo.return_v1"(%4, %27, %27, %65, %65) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump After VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %63 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %63 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = stablehlo.concatenate %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, %4, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %27 = stablehlo.reshape %26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %28 = stablehlo.dot_general %27, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %30 = stablehlo.add %28, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
    %31 = stablehlo.slice %30 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.clamp %25, %31, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %34 = stablehlo.add %33, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %37 = stablehlo.slice %30 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %38 = stablehlo.clamp %36, %37, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %39 = stablehlo.convert %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %40 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %39, %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.logistic %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %44 = stablehlo.convert %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %45 = stablehlo.multiply %39, %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %46 = stablehlo.convert %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %35, %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %55 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %56 = stablehlo.reshape %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %58 = stablehlo.reshape %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %59 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %60 = stablehlo.multiply %54, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
    %61 = stablehlo.convert %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %62 = stablehlo.reduce(%61 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %1, %24, %24, %62, %62 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %96 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %96 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %27 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %28 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %29 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %30 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %31 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %32 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %33 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %34 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %35 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %36 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %37 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %38 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %39 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %40 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %41 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %42 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %43 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %44 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %45 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %46 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %47 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %48 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %49 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %50 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %51 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %52 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %53 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %54 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %55 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %56 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %57 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %58 = stablehlo.concatenate %26, %27, %28, %29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, %42, %43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, %56, %57, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %59 = stablehlo.reshape %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %60 = stablehlo.dot_general %59, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %61 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %62 = stablehlo.add %60, %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
    %63 = stablehlo.slice %62 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %64 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %65 = stablehlo.clamp %25, %63, %64 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %66 = stablehlo.add %65, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %67 = stablehlo.convert %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %68 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %69 = stablehlo.slice %62 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %70 = stablehlo.clamp %68, %69, %64 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %71 = stablehlo.convert %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %72 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %73 = stablehlo.multiply %71, %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %74 = stablehlo.convert %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %75 = stablehlo.logistic %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %76 = stablehlo.convert %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %77 = stablehlo.multiply %71, %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %78 = stablehlo.convert %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %80 = stablehlo.multiply %67, %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %81 = stablehlo.convert %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %82 = stablehlo.dot_general %81, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %83 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %84 = stablehlo.add %82, %83 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %85 = stablehlo.reshape %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %86 = stablehlo.convert %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %87 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %88 = stablehlo.reshape %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %89 = stablehlo.convert %88 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %92 = stablehlo.multiply %86, %91 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
    %93 = stablehlo.convert %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %94 = stablehlo.reduce(%93 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    %95 = sdy.all_reduce {"_axis_0"} %94 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
    return %1, %24, %24, %95, %95 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.iota dim = 0 : tensor<128xi64>
    %3 = stablehlo.broadcast_in_dim %2, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %4 = stablehlo.reshape %arg2 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %5 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %6 = stablehlo.dot_general %4, %5, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %7 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %8 = stablehlo.add %6, %7 : tensor<128x32xbf16>
    %9 = stablehlo.iota dim = 0 : tensor<32xi32>
    %10 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %11:2 = "stablehlo.sort"(%8, %10) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %96 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %96 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %12 = stablehlo.slice %11#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %13 = stablehlo.convert %12 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %14 = stablehlo.reshape %13 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %15 = stablehlo.concatenate %3, %14, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %16 = stablehlo.slice %11#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.subtract %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.exponential %19 : tensor<128x4xbf16>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %23 = stablehlo.divide %20, %22 : tensor<128x4xbf16>
    %24 = "stablehlo.scatter"(%1, %15, %23) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.broadcast_in_dim %arg10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %26 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %27 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %28 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %29 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %30 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %31 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %32 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %33 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %34 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %35 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %36 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %37 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %38 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %39 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %40 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %41 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %42 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %43 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %44 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %45 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %46 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %47 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %48 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %49 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %50 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %51 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %52 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %53 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %54 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %55 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %56 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %57 = sdy.reshard %4 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %58 = stablehlo.concatenate %26, %27, %28, %29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, %42, %43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, %56, %57, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %59 = stablehlo.reshape %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %60 = stablehlo.dot_general %59, %arg9, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %61 = stablehlo.broadcast_in_dim %arg8, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %62 = stablehlo.add %60, %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
    %63 = stablehlo.slice %62 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %64 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %65 = stablehlo.clamp %25, %63, %64 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %66 = stablehlo.add %65, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %67 = stablehlo.convert %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %68 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %69 = stablehlo.slice %62 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %70 = stablehlo.clamp %68, %69, %64 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %71 = stablehlo.convert %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %72 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %73 = stablehlo.multiply %71, %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %74 = stablehlo.convert %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %75 = stablehlo.logistic %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %76 = stablehlo.convert %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %77 = stablehlo.multiply %71, %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %78 = stablehlo.convert %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %80 = stablehlo.multiply %67, %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %81 = stablehlo.convert %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %82 = stablehlo.dot_general %81, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %83 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %84 = stablehlo.add %82, %83 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %85 = stablehlo.reshape %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %86 = stablehlo.convert %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %87 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %88 = stablehlo.reshape %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %89 = stablehlo.convert %88 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %92 = stablehlo.multiply %86, %91 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
    %93 = stablehlo.convert %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %94 = stablehlo.reduce(%93 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    %95 = sdy.all_reduce {"_axis_0"} %94 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
    return %1, %24, %24, %95, %95 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<32x2880xbf16>, %arg15: tensor<32x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<32x5760xbf16>, %arg20: tensor<32x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %97 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %97 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %27 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %28 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %29 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %30 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %31 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %32 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %33 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %34 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %35 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %36 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %37 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %38 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %39 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %40 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %41 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %42 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %43 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %44 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %45 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %46 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %47 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %48 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %49 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %50 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %51 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %52 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %53 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %54 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %55 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %56 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %57 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %58 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %59 = stablehlo.concatenate %27, %28, %29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, %42, %43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, %56, %57, %58, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
      %60 = stablehlo.reshape %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
      %61 = stablehlo.dot_general %60, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
      %62 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
      %63 = stablehlo.add %61, %62 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
      %64 = stablehlo.slice %63 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %65 = stablehlo.broadcast_in_dim %arg17, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %66 = stablehlo.clamp %26, %64, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %67 = stablehlo.add %66, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %68 = stablehlo.convert %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %69 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %70 = stablehlo.slice %63 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %71 = stablehlo.clamp %69, %70, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %72 = stablehlo.convert %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %73 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
      %74 = stablehlo.multiply %72, %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %75 = stablehlo.convert %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %76 = stablehlo.logistic %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %77 = stablehlo.convert %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %78 = stablehlo.multiply %72, %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %80 = stablehlo.convert %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %81 = stablehlo.multiply %68, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %83 = stablehlo.dot_general %82, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
      %84 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
      %85 = stablehlo.add %83, %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %86 = stablehlo.reshape %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
      %87 = stablehlo.convert %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
      %88 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
      %89 = stablehlo.reshape %88 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
      %90 = stablehlo.convert %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
      %91 = stablehlo.reshape %90 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
      %92 = stablehlo.broadcast_in_dim %91, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
      %93 = stablehlo.multiply %87, %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
      %94 = stablehlo.convert %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
      %95 = stablehlo.reduce(%94 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %96 = sdy.all_reduce {"_axis_0"} %95 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %96, %96 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<32x2880xbf16>, %arg15: tensor<32x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<32x5760xbf16>, %arg20: tensor<32x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %97 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %97 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %27 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %28 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %29 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %30 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %31 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %32 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %33 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %34 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %35 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %36 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %37 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %38 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %39 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %40 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %41 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %42 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %43 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %44 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %45 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %46 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %47 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %48 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %49 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %50 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %51 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %52 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %53 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %54 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %55 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %56 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %57 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %58 = sdy.reshard %5 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %59 = stablehlo.concatenate %27, %28, %29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, %42, %43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, %56, %57, %58, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
      %60 = stablehlo.reshape %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
      %61 = stablehlo.dot_general %60, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
      %62 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
      %63 = stablehlo.add %61, %62 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
      %64 = stablehlo.slice %63 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %65 = stablehlo.broadcast_in_dim %arg17, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %66 = stablehlo.clamp %26, %64, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %67 = stablehlo.add %66, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %68 = stablehlo.convert %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %69 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %70 = stablehlo.slice %63 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %71 = stablehlo.clamp %69, %70, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %72 = stablehlo.convert %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %73 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
      %74 = stablehlo.multiply %72, %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %75 = stablehlo.convert %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %76 = stablehlo.logistic %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %77 = stablehlo.convert %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %78 = stablehlo.multiply %72, %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %80 = stablehlo.convert %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %81 = stablehlo.multiply %68, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %83 = stablehlo.dot_general %82, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
      %84 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
      %85 = stablehlo.add %83, %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %86 = stablehlo.reshape %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
      %87 = stablehlo.convert %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
      %88 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
      %89 = stablehlo.reshape %88 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
      %90 = stablehlo.convert %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
      %91 = stablehlo.reshape %90 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
      %92 = stablehlo.broadcast_in_dim %91, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
      %93 = stablehlo.multiply %87, %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
      %94 = stablehlo.convert %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
      %95 = stablehlo.reduce(%94 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %96 = sdy.all_reduce {"_axis_0"} %95 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %96, %96 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<32x2880xbf16>, %arg15: tensor<32x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<32x5760xbf16>, %arg20: tensor<32x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %97 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %97 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %27 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %28 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %29 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %30 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %31 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %32 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %33 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %34 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %35 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %36 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %37 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %38 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %39 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %40 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %41 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %42 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %43 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %44 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %45 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %46 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %47 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %48 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %49 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %50 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %51 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %52 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %53 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %54 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %55 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %56 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %57 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %58 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %59 = stablehlo.concatenate %27, %28, %29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, %42, %43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, %56, %57, %58, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
      %60 = stablehlo.reshape %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
      %61 = stablehlo.dot_general %60, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
      %62 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
      %63 = stablehlo.add %61, %62 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
      %64 = stablehlo.slice %63 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %65 = stablehlo.broadcast_in_dim %arg17, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %66 = stablehlo.clamp %26, %64, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %67 = stablehlo.add %66, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %68 = stablehlo.convert %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %69 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %70 = stablehlo.slice %63 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %71 = stablehlo.clamp %69, %70, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %72 = stablehlo.convert %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %73 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
      %74 = stablehlo.multiply %72, %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %75 = stablehlo.convert %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %76 = stablehlo.logistic %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %77 = stablehlo.convert %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %78 = stablehlo.multiply %72, %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %80 = stablehlo.convert %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %81 = stablehlo.multiply %68, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %83 = stablehlo.dot_general %82, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
      %84 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
      %85 = stablehlo.add %83, %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %86 = stablehlo.reshape %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
      %87 = stablehlo.convert %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
      %88 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
      %89 = stablehlo.reshape %88 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
      %90 = stablehlo.convert %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
      %91 = stablehlo.reshape %90 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
      %92 = stablehlo.broadcast_in_dim %91, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
      %93 = stablehlo.multiply %87, %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
      %94 = stablehlo.convert %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
      %95 = stablehlo.reduce(%94 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %96 = sdy.all_reduce {"_axis_0"} %95 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %96, %96 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<32x2880xbf16>, %arg15: tensor<32x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<32x5760xbf16>, %arg20: tensor<32x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %97 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %97 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %27 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %28 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %29 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %30 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %31 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %32 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %33 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %34 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %35 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %36 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %37 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %38 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %39 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %40 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %41 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %42 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %43 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %44 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %45 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %46 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %47 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %48 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %49 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %50 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %51 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %52 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %53 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %54 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %55 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %56 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %57 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %58 = sdy.all_slice [{"_axis_0"}, {}] %5 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %59 = stablehlo.concatenate %27, %28, %29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, %42, %43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, %56, %57, %58, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
      %60 = stablehlo.reshape %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
      %61 = stablehlo.dot_general %60, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
      %62 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
      %63 = stablehlo.add %61, %62 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
      %64 = stablehlo.slice %63 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %65 = stablehlo.broadcast_in_dim %arg17, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %66 = stablehlo.clamp %26, %64, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %67 = stablehlo.add %66, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %68 = stablehlo.convert %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %69 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %70 = stablehlo.slice %63 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %71 = stablehlo.clamp %69, %70, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %72 = stablehlo.convert %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %73 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
      %74 = stablehlo.multiply %72, %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %75 = stablehlo.convert %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %76 = stablehlo.logistic %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %77 = stablehlo.convert %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %78 = stablehlo.multiply %72, %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %80 = stablehlo.convert %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %81 = stablehlo.multiply %68, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %83 = stablehlo.dot_general %82, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
      %84 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
      %85 = stablehlo.add %83, %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %86 = stablehlo.reshape %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
      %87 = stablehlo.convert %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
      %88 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
      %89 = stablehlo.reshape %88 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
      %90 = stablehlo.convert %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
      %91 = stablehlo.reshape %90 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
      %92 = stablehlo.broadcast_in_dim %91, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
      %93 = stablehlo.multiply %87, %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
      %94 = stablehlo.convert %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
      %95 = stablehlo.reduce(%94 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %96 = sdy.all_reduce {"_axis_0"} %95 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %96, %96 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


#sdy<dimension_sharding{?}>
#sdy<dimension_sharding{"_axis_0", ?}>
// -----// IR Dump After UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<4x2880xbf16>, %arg15: tensor<4x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<4x5760xbf16>, %arg20: tensor<4x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x4xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %193 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %193 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) : (tensor<128x4xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %27 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %28 = "stablehlo.all_to_all"(%27) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %29 = stablehlo.slice %28 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %31 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %32 = "stablehlo.all_to_all"(%31) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %33 = stablehlo.slice %32 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %34 = stablehlo.reshape %33 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %35 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %36 = "stablehlo.all_to_all"(%35) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %37 = stablehlo.slice %36 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %38 = stablehlo.reshape %37 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %39 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %40 = "stablehlo.all_to_all"(%39) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %41 = stablehlo.slice %40 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %42 = stablehlo.reshape %41 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %43 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %44 = "stablehlo.all_to_all"(%43) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %45 = stablehlo.slice %44 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %46 = stablehlo.reshape %45 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %47 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %48 = "stablehlo.all_to_all"(%47) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %49 = stablehlo.slice %48 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %50 = stablehlo.reshape %49 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %51 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %52 = "stablehlo.all_to_all"(%51) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %53 = stablehlo.slice %52 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %54 = stablehlo.reshape %53 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %55 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %56 = "stablehlo.all_to_all"(%55) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %57 = stablehlo.slice %56 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %58 = stablehlo.reshape %57 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %59 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %60 = "stablehlo.all_to_all"(%59) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %61 = stablehlo.slice %60 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %62 = stablehlo.reshape %61 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %63 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %64 = "stablehlo.all_to_all"(%63) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %65 = stablehlo.slice %64 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %66 = stablehlo.reshape %65 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %67 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %68 = "stablehlo.all_to_all"(%67) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %69 = stablehlo.slice %68 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %70 = stablehlo.reshape %69 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %71 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %72 = "stablehlo.all_to_all"(%71) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %74 = stablehlo.reshape %73 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %75 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %76 = "stablehlo.all_to_all"(%75) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %77 = stablehlo.slice %76 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %78 = stablehlo.reshape %77 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %79 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %80 = "stablehlo.all_to_all"(%79) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %81 = stablehlo.slice %80 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %82 = stablehlo.reshape %81 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %83 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %84 = "stablehlo.all_to_all"(%83) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %85 = stablehlo.slice %84 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %86 = stablehlo.reshape %85 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %87 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %88 = "stablehlo.all_to_all"(%87) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %89 = stablehlo.slice %88 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %90 = stablehlo.reshape %89 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %91 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %92 = "stablehlo.all_to_all"(%91) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %93 = stablehlo.slice %92 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %94 = stablehlo.reshape %93 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %95 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %96 = "stablehlo.all_to_all"(%95) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %97 = stablehlo.slice %96 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %98 = stablehlo.reshape %97 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %99 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %100 = "stablehlo.all_to_all"(%99) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %101 = stablehlo.slice %100 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %102 = stablehlo.reshape %101 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %103 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %104 = "stablehlo.all_to_all"(%103) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %105 = stablehlo.slice %104 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %106 = stablehlo.reshape %105 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %107 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %108 = "stablehlo.all_to_all"(%107) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %109 = stablehlo.slice %108 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %110 = stablehlo.reshape %109 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %111 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %112 = "stablehlo.all_to_all"(%111) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %113 = stablehlo.slice %112 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %114 = stablehlo.reshape %113 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %115 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %116 = "stablehlo.all_to_all"(%115) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %117 = stablehlo.slice %116 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %119 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %120 = "stablehlo.all_to_all"(%119) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %121 = stablehlo.slice %120 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %122 = stablehlo.reshape %121 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %123 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %124 = "stablehlo.all_to_all"(%123) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %125 = stablehlo.slice %124 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %126 = stablehlo.reshape %125 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %127 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %128 = "stablehlo.all_to_all"(%127) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %129 = stablehlo.slice %128 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %131 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %132 = "stablehlo.all_to_all"(%131) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %133 = stablehlo.slice %132 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %134 = stablehlo.reshape %133 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %135 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %136 = "stablehlo.all_to_all"(%135) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %137 = stablehlo.slice %136 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %138 = stablehlo.reshape %137 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %139 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %140 = "stablehlo.all_to_all"(%139) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %141 = stablehlo.slice %140 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %142 = stablehlo.reshape %141 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %143 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %144 = "stablehlo.all_to_all"(%143) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %145 = stablehlo.slice %144 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %146 = stablehlo.reshape %145 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %147 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %148 = "stablehlo.all_to_all"(%147) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %149 = stablehlo.slice %148 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %150 = stablehlo.reshape %149 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %151 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %152 = "stablehlo.all_to_all"(%151) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %153 = stablehlo.slice %152 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %154 = stablehlo.reshape %153 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %155 = stablehlo.concatenate %30, %34, %38, %42, %46, %50, %54, %58, %62, %66, %70, %74, %78, %82, %86, %90, %94, %98, %102, %106, %110, %114, %118, %122, %126, %130, %134, %138, %142, %146, %150, %154, dim = 0 : (tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>) -> tensor<512x2880xbf16>
      %156 = stablehlo.reshape %155 : (tensor<512x2880xbf16>) -> tensor<4x128x2880xbf16>
      %157 = stablehlo.dot_general %156, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x128x5760xbf16>
      %158 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] : (tensor<4x5760xbf16>) -> tensor<4x128x5760xbf16>
      %159 = stablehlo.add %157, %158 : tensor<4x128x5760xbf16>
      %160 = stablehlo.slice %159 [0:4, 0:128, 1:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %161 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %162 = stablehlo.clamp %26, %160, %161 : tensor<4x128x2880xbf16>
      %163 = stablehlo.add %162, %1 : tensor<4x128x2880xbf16>
      %164 = stablehlo.convert %163 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %165 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %166 = stablehlo.slice %159 [0:4, 0:128, 0:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %167 = stablehlo.clamp %165, %166, %161 : tensor<4x128x2880xbf16>
      %168 = stablehlo.convert %167 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %169 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<4x128x2880xf32>
      %170 = stablehlo.multiply %168, %169 : tensor<4x128x2880xf32>
      %171 = stablehlo.convert %170 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %172 = stablehlo.logistic %171 : tensor<4x128x2880xbf16>
      %173 = stablehlo.convert %172 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %174 = stablehlo.multiply %168, %173 : tensor<4x128x2880xf32>
      %175 = stablehlo.convert %174 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %176 = stablehlo.convert %175 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %177 = stablehlo.multiply %164, %176 : tensor<4x128x2880xf32>
      %178 = stablehlo.convert %177 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %179 = stablehlo.dot_general %178, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x128x2880xbf16>
      %180 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] : (tensor<4x2880xbf16>) -> tensor<4x128x2880xbf16>
      %181 = stablehlo.add %179, %180 : tensor<4x128x2880xbf16>
      %182 = stablehlo.reshape %181 : (tensor<4x128x2880xbf16>) -> tensor<4x1x128x2880xbf16>
      %183 = stablehlo.convert %182 : (tensor<4x1x128x2880xbf16>) -> tensor<4x1x128x2880xf32>
      %184 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x4xbf16>) -> tensor<4x128xbf16>
      %185 = stablehlo.reshape %184 : (tensor<4x128xbf16>) -> tensor<4x1x128x1xbf16>
      %186 = stablehlo.convert %185 : (tensor<4x1x128x1xbf16>) -> tensor<4x1x128x1xf32>
      %187 = stablehlo.reshape %186 : (tensor<4x1x128x1xf32>) -> tensor<4x1x128xf32>
      %188 = stablehlo.broadcast_in_dim %187, dims = [0, 1, 2] : (tensor<4x1x128xf32>) -> tensor<4x1x128x2880xf32>
      %189 = stablehlo.multiply %183, %188 : tensor<4x1x128x2880xf32>
      %190 = stablehlo.convert %189 : (tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xbf16>
      %191 = stablehlo.reduce(%190 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %192 = "stablehlo.all_reduce"(%191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        %193 = stablehlo.add %arg22, %arg23 : tensor<bf16>
        stablehlo.return %193 : tensor<bf16>
      }) : (tensor<1x128x2880xbf16>) -> tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %192, %192 : tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<4x2880xbf16>, %arg15: tensor<4x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<4x5760xbf16>, %arg20: tensor<4x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x4xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %193 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %193 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) : (tensor<128x4xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %27 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %28 = "stablehlo.all_to_all"(%27) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %29 = stablehlo.slice %28 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %31 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %32 = "stablehlo.all_to_all"(%31) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %33 = stablehlo.slice %32 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %34 = stablehlo.reshape %33 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %35 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %36 = "stablehlo.all_to_all"(%35) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %37 = stablehlo.slice %36 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %38 = stablehlo.reshape %37 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %39 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %40 = "stablehlo.all_to_all"(%39) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %41 = stablehlo.slice %40 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %42 = stablehlo.reshape %41 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %43 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %44 = "stablehlo.all_to_all"(%43) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %45 = stablehlo.slice %44 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %46 = stablehlo.reshape %45 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %47 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %48 = "stablehlo.all_to_all"(%47) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %49 = stablehlo.slice %48 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %50 = stablehlo.reshape %49 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %51 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %52 = "stablehlo.all_to_all"(%51) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %53 = stablehlo.slice %52 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %54 = stablehlo.reshape %53 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %55 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %56 = "stablehlo.all_to_all"(%55) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %57 = stablehlo.slice %56 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %58 = stablehlo.reshape %57 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %59 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %60 = "stablehlo.all_to_all"(%59) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %61 = stablehlo.slice %60 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %62 = stablehlo.reshape %61 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %63 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %64 = "stablehlo.all_to_all"(%63) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %65 = stablehlo.slice %64 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %66 = stablehlo.reshape %65 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %67 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %68 = "stablehlo.all_to_all"(%67) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %69 = stablehlo.slice %68 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %70 = stablehlo.reshape %69 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %71 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %72 = "stablehlo.all_to_all"(%71) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %74 = stablehlo.reshape %73 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %75 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %76 = "stablehlo.all_to_all"(%75) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %77 = stablehlo.slice %76 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %78 = stablehlo.reshape %77 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %79 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %80 = "stablehlo.all_to_all"(%79) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %81 = stablehlo.slice %80 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %82 = stablehlo.reshape %81 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %83 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %84 = "stablehlo.all_to_all"(%83) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %85 = stablehlo.slice %84 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %86 = stablehlo.reshape %85 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %87 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %88 = "stablehlo.all_to_all"(%87) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %89 = stablehlo.slice %88 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %90 = stablehlo.reshape %89 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %91 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %92 = "stablehlo.all_to_all"(%91) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %93 = stablehlo.slice %92 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %94 = stablehlo.reshape %93 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %95 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %96 = "stablehlo.all_to_all"(%95) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %97 = stablehlo.slice %96 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %98 = stablehlo.reshape %97 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %99 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %100 = "stablehlo.all_to_all"(%99) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %101 = stablehlo.slice %100 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %102 = stablehlo.reshape %101 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %103 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %104 = "stablehlo.all_to_all"(%103) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %105 = stablehlo.slice %104 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %106 = stablehlo.reshape %105 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %107 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %108 = "stablehlo.all_to_all"(%107) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %109 = stablehlo.slice %108 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %110 = stablehlo.reshape %109 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %111 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %112 = "stablehlo.all_to_all"(%111) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %113 = stablehlo.slice %112 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %114 = stablehlo.reshape %113 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %115 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %116 = "stablehlo.all_to_all"(%115) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %117 = stablehlo.slice %116 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %119 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %120 = "stablehlo.all_to_all"(%119) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %121 = stablehlo.slice %120 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %122 = stablehlo.reshape %121 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %123 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %124 = "stablehlo.all_to_all"(%123) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %125 = stablehlo.slice %124 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %126 = stablehlo.reshape %125 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %127 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %128 = "stablehlo.all_to_all"(%127) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %129 = stablehlo.slice %128 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %131 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %132 = "stablehlo.all_to_all"(%131) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %133 = stablehlo.slice %132 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %134 = stablehlo.reshape %133 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %135 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %136 = "stablehlo.all_to_all"(%135) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %137 = stablehlo.slice %136 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %138 = stablehlo.reshape %137 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %139 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %140 = "stablehlo.all_to_all"(%139) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %141 = stablehlo.slice %140 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %142 = stablehlo.reshape %141 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %143 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %144 = "stablehlo.all_to_all"(%143) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %145 = stablehlo.slice %144 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %146 = stablehlo.reshape %145 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %147 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %148 = "stablehlo.all_to_all"(%147) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %149 = stablehlo.slice %148 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %150 = stablehlo.reshape %149 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %151 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %152 = "stablehlo.all_to_all"(%151) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %153 = stablehlo.slice %152 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %154 = stablehlo.reshape %153 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %155 = stablehlo.concatenate %30, %34, %38, %42, %46, %50, %54, %58, %62, %66, %70, %74, %78, %82, %86, %90, %94, %98, %102, %106, %110, %114, %118, %122, %126, %130, %134, %138, %142, %146, %150, %154, dim = 0 : (tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>) -> tensor<512x2880xbf16>
      %156 = stablehlo.reshape %155 : (tensor<512x2880xbf16>) -> tensor<4x128x2880xbf16>
      %157 = stablehlo.dot_general %156, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x128x5760xbf16>
      %158 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] : (tensor<4x5760xbf16>) -> tensor<4x128x5760xbf16>
      %159 = stablehlo.add %157, %158 : tensor<4x128x5760xbf16>
      %160 = stablehlo.slice %159 [0:4, 0:128, 1:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %161 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %162 = stablehlo.clamp %26, %160, %161 : tensor<4x128x2880xbf16>
      %163 = stablehlo.add %162, %1 : tensor<4x128x2880xbf16>
      %164 = stablehlo.convert %163 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %165 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %166 = stablehlo.slice %159 [0:4, 0:128, 0:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %167 = stablehlo.clamp %165, %166, %161 : tensor<4x128x2880xbf16>
      %168 = stablehlo.convert %167 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %169 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<4x128x2880xf32>
      %170 = stablehlo.multiply %168, %169 : tensor<4x128x2880xf32>
      %171 = stablehlo.convert %170 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %172 = stablehlo.logistic %171 : tensor<4x128x2880xbf16>
      %173 = stablehlo.convert %172 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %174 = stablehlo.multiply %168, %173 : tensor<4x128x2880xf32>
      %175 = stablehlo.convert %174 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %176 = stablehlo.convert %175 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %177 = stablehlo.multiply %164, %176 : tensor<4x128x2880xf32>
      %178 = stablehlo.convert %177 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %179 = stablehlo.dot_general %178, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x128x2880xbf16>
      %180 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] : (tensor<4x2880xbf16>) -> tensor<4x128x2880xbf16>
      %181 = stablehlo.add %179, %180 : tensor<4x128x2880xbf16>
      %182 = stablehlo.reshape %181 : (tensor<4x128x2880xbf16>) -> tensor<4x1x128x2880xbf16>
      %183 = stablehlo.convert %182 : (tensor<4x1x128x2880xbf16>) -> tensor<4x1x128x2880xf32>
      %184 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x4xbf16>) -> tensor<4x128xbf16>
      %185 = stablehlo.reshape %184 : (tensor<4x128xbf16>) -> tensor<4x1x128x1xbf16>
      %186 = stablehlo.convert %185 : (tensor<4x1x128x1xbf16>) -> tensor<4x1x128x1xf32>
      %187 = stablehlo.reshape %186 : (tensor<4x1x128x1xf32>) -> tensor<4x1x128xf32>
      %188 = stablehlo.broadcast_in_dim %187, dims = [0, 1, 2] : (tensor<4x1x128xf32>) -> tensor<4x1x128x2880xf32>
      %189 = stablehlo.multiply %183, %188 : tensor<4x1x128x2880xf32>
      %190 = stablehlo.convert %189 : (tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xbf16>
      %191 = stablehlo.reduce(%190 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %192 = "stablehlo.all_reduce"(%191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        %193 = stablehlo.add %arg22, %arg23 : tensor<bf16>
        stablehlo.return %193 : tensor<bf16>
      }) : (tensor<1x128x2880xbf16>) -> tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %192, %192 : tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<4x2880xbf16>, %arg15: tensor<4x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<4x5760xbf16>, %arg20: tensor<4x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x4xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %193 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %193 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) : (tensor<128x4xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %27 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %28 = "stablehlo.all_to_all"(%27) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %29 = stablehlo.slice %28 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %31 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %32 = "stablehlo.all_to_all"(%31) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %33 = stablehlo.slice %32 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %34 = stablehlo.reshape %33 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %35 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %36 = "stablehlo.all_to_all"(%35) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %37 = stablehlo.slice %36 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %38 = stablehlo.reshape %37 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %39 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %40 = "stablehlo.all_to_all"(%39) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %41 = stablehlo.slice %40 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %42 = stablehlo.reshape %41 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %43 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %44 = "stablehlo.all_to_all"(%43) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %45 = stablehlo.slice %44 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %46 = stablehlo.reshape %45 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %47 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %48 = "stablehlo.all_to_all"(%47) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %49 = stablehlo.slice %48 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %50 = stablehlo.reshape %49 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %51 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %52 = "stablehlo.all_to_all"(%51) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %53 = stablehlo.slice %52 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %54 = stablehlo.reshape %53 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %55 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %56 = "stablehlo.all_to_all"(%55) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %57 = stablehlo.slice %56 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %58 = stablehlo.reshape %57 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %59 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %60 = "stablehlo.all_to_all"(%59) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %61 = stablehlo.slice %60 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %62 = stablehlo.reshape %61 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %63 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %64 = "stablehlo.all_to_all"(%63) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %65 = stablehlo.slice %64 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %66 = stablehlo.reshape %65 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %67 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %68 = "stablehlo.all_to_all"(%67) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %69 = stablehlo.slice %68 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %70 = stablehlo.reshape %69 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %71 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %72 = "stablehlo.all_to_all"(%71) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %74 = stablehlo.reshape %73 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %75 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %76 = "stablehlo.all_to_all"(%75) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %77 = stablehlo.slice %76 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %78 = stablehlo.reshape %77 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %79 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %80 = "stablehlo.all_to_all"(%79) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %81 = stablehlo.slice %80 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %82 = stablehlo.reshape %81 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %83 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %84 = "stablehlo.all_to_all"(%83) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %85 = stablehlo.slice %84 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %86 = stablehlo.reshape %85 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %87 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %88 = "stablehlo.all_to_all"(%87) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %89 = stablehlo.slice %88 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %90 = stablehlo.reshape %89 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %91 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %92 = "stablehlo.all_to_all"(%91) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %93 = stablehlo.slice %92 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %94 = stablehlo.reshape %93 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %95 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %96 = "stablehlo.all_to_all"(%95) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %97 = stablehlo.slice %96 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %98 = stablehlo.reshape %97 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %99 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %100 = "stablehlo.all_to_all"(%99) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %101 = stablehlo.slice %100 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %102 = stablehlo.reshape %101 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %103 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %104 = "stablehlo.all_to_all"(%103) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %105 = stablehlo.slice %104 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %106 = stablehlo.reshape %105 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %107 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %108 = "stablehlo.all_to_all"(%107) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %109 = stablehlo.slice %108 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %110 = stablehlo.reshape %109 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %111 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %112 = "stablehlo.all_to_all"(%111) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %113 = stablehlo.slice %112 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %114 = stablehlo.reshape %113 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %115 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %116 = "stablehlo.all_to_all"(%115) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %117 = stablehlo.slice %116 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %119 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %120 = "stablehlo.all_to_all"(%119) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %121 = stablehlo.slice %120 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %122 = stablehlo.reshape %121 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %123 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %124 = "stablehlo.all_to_all"(%123) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %125 = stablehlo.slice %124 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %126 = stablehlo.reshape %125 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %127 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %128 = "stablehlo.all_to_all"(%127) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %129 = stablehlo.slice %128 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %131 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %132 = "stablehlo.all_to_all"(%131) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %133 = stablehlo.slice %132 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %134 = stablehlo.reshape %133 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %135 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %136 = "stablehlo.all_to_all"(%135) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %137 = stablehlo.slice %136 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %138 = stablehlo.reshape %137 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %139 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %140 = "stablehlo.all_to_all"(%139) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %141 = stablehlo.slice %140 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %142 = stablehlo.reshape %141 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %143 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %144 = "stablehlo.all_to_all"(%143) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %145 = stablehlo.slice %144 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %146 = stablehlo.reshape %145 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %147 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %148 = "stablehlo.all_to_all"(%147) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %149 = stablehlo.slice %148 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %150 = stablehlo.reshape %149 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %151 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %152 = "stablehlo.all_to_all"(%151) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %153 = stablehlo.slice %152 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %154 = stablehlo.reshape %153 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %155 = stablehlo.concatenate %30, %34, %38, %42, %46, %50, %54, %58, %62, %66, %70, %74, %78, %82, %86, %90, %94, %98, %102, %106, %110, %114, %118, %122, %126, %130, %134, %138, %142, %146, %150, %154, dim = 0 : (tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>) -> tensor<512x2880xbf16>
      %156 = stablehlo.reshape %155 : (tensor<512x2880xbf16>) -> tensor<4x128x2880xbf16>
      %157 = stablehlo.dot_general %156, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x128x5760xbf16>
      %158 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] : (tensor<4x5760xbf16>) -> tensor<4x128x5760xbf16>
      %159 = stablehlo.add %157, %158 : tensor<4x128x5760xbf16>
      %160 = stablehlo.slice %159 [0:4, 0:128, 1:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %161 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %162 = stablehlo.clamp %26, %160, %161 : tensor<4x128x2880xbf16>
      %163 = stablehlo.add %162, %1 : tensor<4x128x2880xbf16>
      %164 = stablehlo.convert %163 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %165 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %166 = stablehlo.slice %159 [0:4, 0:128, 0:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %167 = stablehlo.clamp %165, %166, %161 : tensor<4x128x2880xbf16>
      %168 = stablehlo.convert %167 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %169 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<4x128x2880xf32>
      %170 = stablehlo.multiply %168, %169 : tensor<4x128x2880xf32>
      %171 = stablehlo.convert %170 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %172 = stablehlo.logistic %171 : tensor<4x128x2880xbf16>
      %173 = stablehlo.convert %172 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %174 = stablehlo.multiply %168, %173 : tensor<4x128x2880xf32>
      %175 = stablehlo.convert %174 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %176 = stablehlo.convert %175 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %177 = stablehlo.multiply %164, %176 : tensor<4x128x2880xf32>
      %178 = stablehlo.convert %177 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %179 = stablehlo.dot_general %178, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x128x2880xbf16>
      %180 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] : (tensor<4x2880xbf16>) -> tensor<4x128x2880xbf16>
      %181 = stablehlo.add %179, %180 : tensor<4x128x2880xbf16>
      %182 = stablehlo.reshape %181 : (tensor<4x128x2880xbf16>) -> tensor<4x1x128x2880xbf16>
      %183 = stablehlo.convert %182 : (tensor<4x1x128x2880xbf16>) -> tensor<4x1x128x2880xf32>
      %184 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x4xbf16>) -> tensor<4x128xbf16>
      %185 = stablehlo.reshape %184 : (tensor<4x128xbf16>) -> tensor<4x1x128x1xbf16>
      %186 = stablehlo.convert %185 : (tensor<4x1x128x1xbf16>) -> tensor<4x1x128x1xf32>
      %187 = stablehlo.reshape %186 : (tensor<4x1x128x1xf32>) -> tensor<4x1x128xf32>
      %188 = stablehlo.broadcast_in_dim %187, dims = [0, 1, 2] : (tensor<4x1x128xf32>) -> tensor<4x1x128x2880xf32>
      %189 = stablehlo.multiply %183, %188 : tensor<4x1x128x2880xf32>
      %190 = stablehlo.convert %189 : (tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xbf16>
      %191 = stablehlo.reduce(%190 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %192 = "stablehlo.all_reduce"(%191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        %193 = stablehlo.add %arg22, %arg23 : tensor<bf16>
        stablehlo.return %193 : tensor<bf16>
      }) : (tensor<1x128x2880xbf16>) -> tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %192, %192 : tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<4x2880xbf16>, %arg15: tensor<4x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<4x5760xbf16>, %arg20: tensor<4x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x4xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %193 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %193 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) : (tensor<128x4xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %27 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %28 = "stablehlo.all_to_all"(%27) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %29 = stablehlo.slice %28 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %31 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %32 = "stablehlo.all_to_all"(%31) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %33 = stablehlo.slice %32 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %34 = stablehlo.reshape %33 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %35 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %36 = "stablehlo.all_to_all"(%35) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %37 = stablehlo.slice %36 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %38 = stablehlo.reshape %37 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %39 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %40 = "stablehlo.all_to_all"(%39) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %41 = stablehlo.slice %40 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %42 = stablehlo.reshape %41 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %43 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %44 = "stablehlo.all_to_all"(%43) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %45 = stablehlo.slice %44 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %46 = stablehlo.reshape %45 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %47 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %48 = "stablehlo.all_to_all"(%47) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %49 = stablehlo.slice %48 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %50 = stablehlo.reshape %49 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %51 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %52 = "stablehlo.all_to_all"(%51) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %53 = stablehlo.slice %52 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %54 = stablehlo.reshape %53 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %55 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %56 = "stablehlo.all_to_all"(%55) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %57 = stablehlo.slice %56 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %58 = stablehlo.reshape %57 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %59 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %60 = "stablehlo.all_to_all"(%59) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %61 = stablehlo.slice %60 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %62 = stablehlo.reshape %61 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %63 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %64 = "stablehlo.all_to_all"(%63) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %65 = stablehlo.slice %64 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %66 = stablehlo.reshape %65 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %67 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %68 = "stablehlo.all_to_all"(%67) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %69 = stablehlo.slice %68 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %70 = stablehlo.reshape %69 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %71 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %72 = "stablehlo.all_to_all"(%71) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %74 = stablehlo.reshape %73 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %75 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %76 = "stablehlo.all_to_all"(%75) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %77 = stablehlo.slice %76 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %78 = stablehlo.reshape %77 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %79 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %80 = "stablehlo.all_to_all"(%79) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %81 = stablehlo.slice %80 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %82 = stablehlo.reshape %81 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %83 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %84 = "stablehlo.all_to_all"(%83) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %85 = stablehlo.slice %84 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %86 = stablehlo.reshape %85 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %87 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %88 = "stablehlo.all_to_all"(%87) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %89 = stablehlo.slice %88 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %90 = stablehlo.reshape %89 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %91 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %92 = "stablehlo.all_to_all"(%91) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %93 = stablehlo.slice %92 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %94 = stablehlo.reshape %93 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %95 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %96 = "stablehlo.all_to_all"(%95) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %97 = stablehlo.slice %96 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %98 = stablehlo.reshape %97 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %99 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %100 = "stablehlo.all_to_all"(%99) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %101 = stablehlo.slice %100 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %102 = stablehlo.reshape %101 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %103 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %104 = "stablehlo.all_to_all"(%103) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %105 = stablehlo.slice %104 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %106 = stablehlo.reshape %105 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %107 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %108 = "stablehlo.all_to_all"(%107) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %109 = stablehlo.slice %108 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %110 = stablehlo.reshape %109 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %111 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %112 = "stablehlo.all_to_all"(%111) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %113 = stablehlo.slice %112 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %114 = stablehlo.reshape %113 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %115 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %116 = "stablehlo.all_to_all"(%115) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %117 = stablehlo.slice %116 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %119 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %120 = "stablehlo.all_to_all"(%119) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %121 = stablehlo.slice %120 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %122 = stablehlo.reshape %121 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %123 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %124 = "stablehlo.all_to_all"(%123) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %125 = stablehlo.slice %124 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %126 = stablehlo.reshape %125 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %127 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %128 = "stablehlo.all_to_all"(%127) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %129 = stablehlo.slice %128 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %131 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %132 = "stablehlo.all_to_all"(%131) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %133 = stablehlo.slice %132 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %134 = stablehlo.reshape %133 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %135 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %136 = "stablehlo.all_to_all"(%135) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %137 = stablehlo.slice %136 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %138 = stablehlo.reshape %137 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %139 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %140 = "stablehlo.all_to_all"(%139) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %141 = stablehlo.slice %140 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %142 = stablehlo.reshape %141 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %143 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %144 = "stablehlo.all_to_all"(%143) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %145 = stablehlo.slice %144 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %146 = stablehlo.reshape %145 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %147 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %148 = "stablehlo.all_to_all"(%147) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %149 = stablehlo.slice %148 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %150 = stablehlo.reshape %149 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %151 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %152 = "stablehlo.all_to_all"(%151) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %153 = stablehlo.slice %152 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %154 = stablehlo.reshape %153 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %155 = stablehlo.concatenate %30, %34, %38, %42, %46, %50, %54, %58, %62, %66, %70, %74, %78, %82, %86, %90, %94, %98, %102, %106, %110, %114, %118, %122, %126, %130, %134, %138, %142, %146, %150, %154, dim = 0 : (tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>) -> tensor<512x2880xbf16>
      %156 = stablehlo.reshape %155 : (tensor<512x2880xbf16>) -> tensor<4x128x2880xbf16>
      %157 = stablehlo.dot_general %156, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x128x5760xbf16>
      %158 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] : (tensor<4x5760xbf16>) -> tensor<4x128x5760xbf16>
      %159 = stablehlo.add %157, %158 : tensor<4x128x5760xbf16>
      %160 = stablehlo.slice %159 [0:4, 0:128, 1:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %161 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %162 = stablehlo.clamp %26, %160, %161 : tensor<4x128x2880xbf16>
      %163 = stablehlo.add %162, %1 : tensor<4x128x2880xbf16>
      %164 = stablehlo.convert %163 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %165 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %166 = stablehlo.slice %159 [0:4, 0:128, 0:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %167 = stablehlo.clamp %165, %166, %161 : tensor<4x128x2880xbf16>
      %168 = stablehlo.convert %167 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %169 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<4x128x2880xf32>
      %170 = stablehlo.multiply %168, %169 : tensor<4x128x2880xf32>
      %171 = stablehlo.convert %170 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %172 = stablehlo.logistic %171 : tensor<4x128x2880xbf16>
      %173 = stablehlo.convert %172 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %174 = stablehlo.multiply %168, %173 : tensor<4x128x2880xf32>
      %175 = stablehlo.convert %174 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %176 = stablehlo.convert %175 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %177 = stablehlo.multiply %164, %176 : tensor<4x128x2880xf32>
      %178 = stablehlo.convert %177 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %179 = stablehlo.dot_general %178, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x128x2880xbf16>
      %180 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] : (tensor<4x2880xbf16>) -> tensor<4x128x2880xbf16>
      %181 = stablehlo.add %179, %180 : tensor<4x128x2880xbf16>
      %182 = stablehlo.reshape %181 : (tensor<4x128x2880xbf16>) -> tensor<4x1x128x2880xbf16>
      %183 = stablehlo.convert %182 : (tensor<4x1x128x2880xbf16>) -> tensor<4x1x128x2880xf32>
      %184 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x4xbf16>) -> tensor<4x128xbf16>
      %185 = stablehlo.reshape %184 : (tensor<4x128xbf16>) -> tensor<4x1x128x1xbf16>
      %186 = stablehlo.convert %185 : (tensor<4x1x128x1xbf16>) -> tensor<4x1x128x1xf32>
      %187 = stablehlo.reshape %186 : (tensor<4x1x128x1xf32>) -> tensor<4x1x128xf32>
      %188 = stablehlo.broadcast_in_dim %187, dims = [0, 1, 2] : (tensor<4x1x128xf32>) -> tensor<4x1x128x2880xf32>
      %189 = stablehlo.multiply %183, %188 : tensor<4x1x128x2880xf32>
      %190 = stablehlo.convert %189 : (tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xbf16>
      %191 = stablehlo.reduce(%190 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %192 = "stablehlo.all_reduce"(%191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        %193 = stablehlo.add %arg22, %arg23 : tensor<bf16>
        stablehlo.return %193 : tensor<bf16>
      }) : (tensor<1x128x2880xbf16>) -> tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %192, %192 : tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<4x2880xbf16>, %arg15: tensor<4x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<4x5760xbf16>, %arg20: tensor<4x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x4xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %193 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %193 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) : (tensor<128x4xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %27 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %28 = "stablehlo.all_to_all"(%27) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %29 = stablehlo.slice %28 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %31 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %32 = "stablehlo.all_to_all"(%31) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %33 = stablehlo.slice %32 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %34 = stablehlo.reshape %33 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %35 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %36 = "stablehlo.all_to_all"(%35) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %37 = stablehlo.slice %36 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %38 = stablehlo.reshape %37 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %39 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %40 = "stablehlo.all_to_all"(%39) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %41 = stablehlo.slice %40 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %42 = stablehlo.reshape %41 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %43 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %44 = "stablehlo.all_to_all"(%43) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %45 = stablehlo.slice %44 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %46 = stablehlo.reshape %45 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %47 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %48 = "stablehlo.all_to_all"(%47) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %49 = stablehlo.slice %48 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %50 = stablehlo.reshape %49 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %51 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %52 = "stablehlo.all_to_all"(%51) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %53 = stablehlo.slice %52 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %54 = stablehlo.reshape %53 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %55 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %56 = "stablehlo.all_to_all"(%55) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %57 = stablehlo.slice %56 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %58 = stablehlo.reshape %57 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %59 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %60 = "stablehlo.all_to_all"(%59) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %61 = stablehlo.slice %60 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %62 = stablehlo.reshape %61 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %63 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %64 = "stablehlo.all_to_all"(%63) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %65 = stablehlo.slice %64 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %66 = stablehlo.reshape %65 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %67 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %68 = "stablehlo.all_to_all"(%67) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %69 = stablehlo.slice %68 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %70 = stablehlo.reshape %69 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %71 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %72 = "stablehlo.all_to_all"(%71) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %74 = stablehlo.reshape %73 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %75 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %76 = "stablehlo.all_to_all"(%75) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %77 = stablehlo.slice %76 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %78 = stablehlo.reshape %77 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %79 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %80 = "stablehlo.all_to_all"(%79) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %81 = stablehlo.slice %80 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %82 = stablehlo.reshape %81 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %83 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %84 = "stablehlo.all_to_all"(%83) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %85 = stablehlo.slice %84 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %86 = stablehlo.reshape %85 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %87 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %88 = "stablehlo.all_to_all"(%87) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %89 = stablehlo.slice %88 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %90 = stablehlo.reshape %89 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %91 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %92 = "stablehlo.all_to_all"(%91) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %93 = stablehlo.slice %92 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %94 = stablehlo.reshape %93 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %95 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %96 = "stablehlo.all_to_all"(%95) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %97 = stablehlo.slice %96 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %98 = stablehlo.reshape %97 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %99 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %100 = "stablehlo.all_to_all"(%99) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %101 = stablehlo.slice %100 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %102 = stablehlo.reshape %101 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %103 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %104 = "stablehlo.all_to_all"(%103) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %105 = stablehlo.slice %104 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %106 = stablehlo.reshape %105 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %107 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %108 = "stablehlo.all_to_all"(%107) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %109 = stablehlo.slice %108 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %110 = stablehlo.reshape %109 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %111 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %112 = "stablehlo.all_to_all"(%111) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %113 = stablehlo.slice %112 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %114 = stablehlo.reshape %113 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %115 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %116 = "stablehlo.all_to_all"(%115) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %117 = stablehlo.slice %116 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %119 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %120 = "stablehlo.all_to_all"(%119) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %121 = stablehlo.slice %120 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %122 = stablehlo.reshape %121 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %123 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %124 = "stablehlo.all_to_all"(%123) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %125 = stablehlo.slice %124 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %126 = stablehlo.reshape %125 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %127 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %128 = "stablehlo.all_to_all"(%127) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %129 = stablehlo.slice %128 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %131 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %132 = "stablehlo.all_to_all"(%131) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %133 = stablehlo.slice %132 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %134 = stablehlo.reshape %133 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %135 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %136 = "stablehlo.all_to_all"(%135) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %137 = stablehlo.slice %136 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %138 = stablehlo.reshape %137 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %139 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %140 = "stablehlo.all_to_all"(%139) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %141 = stablehlo.slice %140 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %142 = stablehlo.reshape %141 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %143 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %144 = "stablehlo.all_to_all"(%143) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %145 = stablehlo.slice %144 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %146 = stablehlo.reshape %145 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %147 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %148 = "stablehlo.all_to_all"(%147) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %149 = stablehlo.slice %148 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %150 = stablehlo.reshape %149 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %151 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %152 = "stablehlo.all_to_all"(%151) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %153 = stablehlo.slice %152 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %154 = stablehlo.reshape %153 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %155 = stablehlo.concatenate %30, %34, %38, %42, %46, %50, %54, %58, %62, %66, %70, %74, %78, %82, %86, %90, %94, %98, %102, %106, %110, %114, %118, %122, %126, %130, %134, %138, %142, %146, %150, %154, dim = 0 : (tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>) -> tensor<512x2880xbf16>
      %156 = stablehlo.reshape %155 : (tensor<512x2880xbf16>) -> tensor<4x128x2880xbf16>
      %157 = stablehlo.dot_general %156, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x128x5760xbf16>
      %158 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] : (tensor<4x5760xbf16>) -> tensor<4x128x5760xbf16>
      %159 = stablehlo.add %157, %158 : tensor<4x128x5760xbf16>
      %160 = stablehlo.slice %159 [0:4, 0:128, 1:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %161 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %162 = stablehlo.clamp %26, %160, %161 : tensor<4x128x2880xbf16>
      %163 = stablehlo.add %162, %1 : tensor<4x128x2880xbf16>
      %164 = stablehlo.convert %163 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %165 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %166 = stablehlo.slice %159 [0:4, 0:128, 0:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %167 = stablehlo.clamp %165, %166, %161 : tensor<4x128x2880xbf16>
      %168 = stablehlo.convert %167 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %169 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<4x128x2880xf32>
      %170 = stablehlo.multiply %168, %169 : tensor<4x128x2880xf32>
      %171 = stablehlo.convert %170 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %172 = stablehlo.logistic %171 : tensor<4x128x2880xbf16>
      %173 = stablehlo.convert %172 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %174 = stablehlo.multiply %168, %173 : tensor<4x128x2880xf32>
      %175 = stablehlo.convert %174 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %176 = stablehlo.convert %175 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %177 = stablehlo.multiply %164, %176 : tensor<4x128x2880xf32>
      %178 = stablehlo.convert %177 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %179 = stablehlo.dot_general %178, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x128x2880xbf16>
      %180 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] : (tensor<4x2880xbf16>) -> tensor<4x128x2880xbf16>
      %181 = stablehlo.add %179, %180 : tensor<4x128x2880xbf16>
      %182 = stablehlo.reshape %181 : (tensor<4x128x2880xbf16>) -> tensor<4x1x128x2880xbf16>
      %183 = stablehlo.convert %182 : (tensor<4x1x128x2880xbf16>) -> tensor<4x1x128x2880xf32>
      %184 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x4xbf16>) -> tensor<4x128xbf16>
      %185 = stablehlo.reshape %184 : (tensor<4x128xbf16>) -> tensor<4x1x128x1xbf16>
      %186 = stablehlo.convert %185 : (tensor<4x1x128x1xbf16>) -> tensor<4x1x128x1xf32>
      %187 = stablehlo.reshape %186 : (tensor<4x1x128x1xf32>) -> tensor<4x1x128xf32>
      %188 = stablehlo.broadcast_in_dim %187, dims = [0, 1, 2] : (tensor<4x1x128xf32>) -> tensor<4x1x128x2880xf32>
      %189 = stablehlo.multiply %183, %188 : tensor<4x1x128x2880xf32>
      %190 = stablehlo.convert %189 : (tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xbf16>
      %191 = stablehlo.reduce(%190 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %192 = "stablehlo.all_reduce"(%191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        %193 = stablehlo.add %arg22, %arg23 : tensor<bf16>
        stablehlo.return %193 : tensor<bf16>
      }) : (tensor<1x128x2880xbf16>) -> tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %192, %192 : tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}
// -----// IR Dump Before ConvertArithToStableHLO (convert-arith-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<4x2880xbf16>, %arg15: tensor<4x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<4x5760xbf16>, %arg20: tensor<4x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x4xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %193 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %193 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) : (tensor<128x4xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %27 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %28 = "stablehlo.all_to_all"(%27) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %29 = stablehlo.slice %28 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %31 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %32 = "stablehlo.all_to_all"(%31) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %33 = stablehlo.slice %32 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %34 = stablehlo.reshape %33 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %35 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %36 = "stablehlo.all_to_all"(%35) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %37 = stablehlo.slice %36 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %38 = stablehlo.reshape %37 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %39 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %40 = "stablehlo.all_to_all"(%39) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %41 = stablehlo.slice %40 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %42 = stablehlo.reshape %41 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %43 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %44 = "stablehlo.all_to_all"(%43) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %45 = stablehlo.slice %44 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %46 = stablehlo.reshape %45 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %47 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %48 = "stablehlo.all_to_all"(%47) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %49 = stablehlo.slice %48 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %50 = stablehlo.reshape %49 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %51 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %52 = "stablehlo.all_to_all"(%51) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %53 = stablehlo.slice %52 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %54 = stablehlo.reshape %53 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %55 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %56 = "stablehlo.all_to_all"(%55) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %57 = stablehlo.slice %56 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %58 = stablehlo.reshape %57 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %59 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %60 = "stablehlo.all_to_all"(%59) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %61 = stablehlo.slice %60 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %62 = stablehlo.reshape %61 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %63 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %64 = "stablehlo.all_to_all"(%63) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %65 = stablehlo.slice %64 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %66 = stablehlo.reshape %65 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %67 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %68 = "stablehlo.all_to_all"(%67) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %69 = stablehlo.slice %68 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %70 = stablehlo.reshape %69 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %71 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %72 = "stablehlo.all_to_all"(%71) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %74 = stablehlo.reshape %73 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %75 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %76 = "stablehlo.all_to_all"(%75) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %77 = stablehlo.slice %76 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %78 = stablehlo.reshape %77 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %79 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %80 = "stablehlo.all_to_all"(%79) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %81 = stablehlo.slice %80 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %82 = stablehlo.reshape %81 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %83 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %84 = "stablehlo.all_to_all"(%83) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %85 = stablehlo.slice %84 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %86 = stablehlo.reshape %85 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %87 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %88 = "stablehlo.all_to_all"(%87) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %89 = stablehlo.slice %88 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %90 = stablehlo.reshape %89 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %91 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %92 = "stablehlo.all_to_all"(%91) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %93 = stablehlo.slice %92 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %94 = stablehlo.reshape %93 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %95 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %96 = "stablehlo.all_to_all"(%95) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %97 = stablehlo.slice %96 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %98 = stablehlo.reshape %97 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %99 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %100 = "stablehlo.all_to_all"(%99) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %101 = stablehlo.slice %100 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %102 = stablehlo.reshape %101 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %103 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %104 = "stablehlo.all_to_all"(%103) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %105 = stablehlo.slice %104 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %106 = stablehlo.reshape %105 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %107 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %108 = "stablehlo.all_to_all"(%107) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %109 = stablehlo.slice %108 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %110 = stablehlo.reshape %109 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %111 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %112 = "stablehlo.all_to_all"(%111) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %113 = stablehlo.slice %112 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %114 = stablehlo.reshape %113 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %115 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %116 = "stablehlo.all_to_all"(%115) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %117 = stablehlo.slice %116 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %119 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %120 = "stablehlo.all_to_all"(%119) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %121 = stablehlo.slice %120 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %122 = stablehlo.reshape %121 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %123 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %124 = "stablehlo.all_to_all"(%123) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %125 = stablehlo.slice %124 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %126 = stablehlo.reshape %125 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %127 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %128 = "stablehlo.all_to_all"(%127) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %129 = stablehlo.slice %128 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %131 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %132 = "stablehlo.all_to_all"(%131) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %133 = stablehlo.slice %132 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %134 = stablehlo.reshape %133 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %135 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %136 = "stablehlo.all_to_all"(%135) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %137 = stablehlo.slice %136 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %138 = stablehlo.reshape %137 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %139 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %140 = "stablehlo.all_to_all"(%139) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %141 = stablehlo.slice %140 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %142 = stablehlo.reshape %141 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %143 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %144 = "stablehlo.all_to_all"(%143) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %145 = stablehlo.slice %144 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %146 = stablehlo.reshape %145 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %147 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %148 = "stablehlo.all_to_all"(%147) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %149 = stablehlo.slice %148 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %150 = stablehlo.reshape %149 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %151 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %152 = "stablehlo.all_to_all"(%151) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %153 = stablehlo.slice %152 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %154 = stablehlo.reshape %153 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %155 = stablehlo.concatenate %30, %34, %38, %42, %46, %50, %54, %58, %62, %66, %70, %74, %78, %82, %86, %90, %94, %98, %102, %106, %110, %114, %118, %122, %126, %130, %134, %138, %142, %146, %150, %154, dim = 0 : (tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>) -> tensor<512x2880xbf16>
      %156 = stablehlo.reshape %155 : (tensor<512x2880xbf16>) -> tensor<4x128x2880xbf16>
      %157 = stablehlo.dot_general %156, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x128x5760xbf16>
      %158 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] : (tensor<4x5760xbf16>) -> tensor<4x128x5760xbf16>
      %159 = stablehlo.add %157, %158 : tensor<4x128x5760xbf16>
      %160 = stablehlo.slice %159 [0:4, 0:128, 1:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %161 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %162 = stablehlo.clamp %26, %160, %161 : tensor<4x128x2880xbf16>
      %163 = stablehlo.add %162, %1 : tensor<4x128x2880xbf16>
      %164 = stablehlo.convert %163 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %165 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %166 = stablehlo.slice %159 [0:4, 0:128, 0:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %167 = stablehlo.clamp %165, %166, %161 : tensor<4x128x2880xbf16>
      %168 = stablehlo.convert %167 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %169 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<4x128x2880xf32>
      %170 = stablehlo.multiply %168, %169 : tensor<4x128x2880xf32>
      %171 = stablehlo.convert %170 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %172 = stablehlo.logistic %171 : tensor<4x128x2880xbf16>
      %173 = stablehlo.convert %172 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %174 = stablehlo.multiply %168, %173 : tensor<4x128x2880xf32>
      %175 = stablehlo.convert %174 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %176 = stablehlo.convert %175 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %177 = stablehlo.multiply %164, %176 : tensor<4x128x2880xf32>
      %178 = stablehlo.convert %177 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %179 = stablehlo.dot_general %178, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x128x2880xbf16>
      %180 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] : (tensor<4x2880xbf16>) -> tensor<4x128x2880xbf16>
      %181 = stablehlo.add %179, %180 : tensor<4x128x2880xbf16>
      %182 = stablehlo.reshape %181 : (tensor<4x128x2880xbf16>) -> tensor<4x1x128x2880xbf16>
      %183 = stablehlo.convert %182 : (tensor<4x1x128x2880xbf16>) -> tensor<4x1x128x2880xf32>
      %184 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x4xbf16>) -> tensor<4x128xbf16>
      %185 = stablehlo.reshape %184 : (tensor<4x128xbf16>) -> tensor<4x1x128x1xbf16>
      %186 = stablehlo.convert %185 : (tensor<4x1x128x1xbf16>) -> tensor<4x1x128x1xf32>
      %187 = stablehlo.reshape %186 : (tensor<4x1x128x1xf32>) -> tensor<4x1x128xf32>
      %188 = stablehlo.broadcast_in_dim %187, dims = [0, 1, 2] : (tensor<4x1x128xf32>) -> tensor<4x1x128x2880xf32>
      %189 = stablehlo.multiply %183, %188 : tensor<4x1x128x2880xf32>
      %190 = stablehlo.convert %189 : (tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xbf16>
      %191 = stablehlo.reduce(%190 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %192 = "stablehlo.all_reduce"(%191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        %193 = stablehlo.add %arg22, %arg23 : tensor<bf16>
        stablehlo.return %193 : tensor<bf16>
      }) : (tensor<1x128x2880xbf16>) -> tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %192, %192 : tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before LegalizeStableHLOCompositeToTTIR (legalize-stablehlo-composite-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<4x2880xbf16>, %arg15: tensor<4x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<4x5760xbf16>, %arg20: tensor<4x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x4xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %193 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %193 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) : (tensor<128x4xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %27 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %28 = "stablehlo.all_to_all"(%27) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %29 = stablehlo.slice %28 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %31 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %32 = "stablehlo.all_to_all"(%31) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %33 = stablehlo.slice %32 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %34 = stablehlo.reshape %33 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %35 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %36 = "stablehlo.all_to_all"(%35) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %37 = stablehlo.slice %36 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %38 = stablehlo.reshape %37 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %39 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %40 = "stablehlo.all_to_all"(%39) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %41 = stablehlo.slice %40 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %42 = stablehlo.reshape %41 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %43 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %44 = "stablehlo.all_to_all"(%43) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %45 = stablehlo.slice %44 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %46 = stablehlo.reshape %45 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %47 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %48 = "stablehlo.all_to_all"(%47) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %49 = stablehlo.slice %48 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %50 = stablehlo.reshape %49 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %51 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %52 = "stablehlo.all_to_all"(%51) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %53 = stablehlo.slice %52 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %54 = stablehlo.reshape %53 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %55 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %56 = "stablehlo.all_to_all"(%55) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %57 = stablehlo.slice %56 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %58 = stablehlo.reshape %57 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %59 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %60 = "stablehlo.all_to_all"(%59) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %61 = stablehlo.slice %60 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %62 = stablehlo.reshape %61 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %63 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %64 = "stablehlo.all_to_all"(%63) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %65 = stablehlo.slice %64 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %66 = stablehlo.reshape %65 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %67 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %68 = "stablehlo.all_to_all"(%67) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %69 = stablehlo.slice %68 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %70 = stablehlo.reshape %69 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %71 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %72 = "stablehlo.all_to_all"(%71) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %74 = stablehlo.reshape %73 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %75 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %76 = "stablehlo.all_to_all"(%75) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %77 = stablehlo.slice %76 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %78 = stablehlo.reshape %77 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %79 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %80 = "stablehlo.all_to_all"(%79) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %81 = stablehlo.slice %80 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %82 = stablehlo.reshape %81 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %83 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %84 = "stablehlo.all_to_all"(%83) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %85 = stablehlo.slice %84 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %86 = stablehlo.reshape %85 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %87 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %88 = "stablehlo.all_to_all"(%87) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %89 = stablehlo.slice %88 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %90 = stablehlo.reshape %89 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %91 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %92 = "stablehlo.all_to_all"(%91) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %93 = stablehlo.slice %92 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %94 = stablehlo.reshape %93 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %95 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %96 = "stablehlo.all_to_all"(%95) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %97 = stablehlo.slice %96 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %98 = stablehlo.reshape %97 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %99 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %100 = "stablehlo.all_to_all"(%99) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %101 = stablehlo.slice %100 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %102 = stablehlo.reshape %101 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %103 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %104 = "stablehlo.all_to_all"(%103) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %105 = stablehlo.slice %104 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %106 = stablehlo.reshape %105 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %107 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %108 = "stablehlo.all_to_all"(%107) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %109 = stablehlo.slice %108 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %110 = stablehlo.reshape %109 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %111 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %112 = "stablehlo.all_to_all"(%111) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %113 = stablehlo.slice %112 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %114 = stablehlo.reshape %113 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %115 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %116 = "stablehlo.all_to_all"(%115) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %117 = stablehlo.slice %116 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %119 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %120 = "stablehlo.all_to_all"(%119) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %121 = stablehlo.slice %120 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %122 = stablehlo.reshape %121 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %123 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %124 = "stablehlo.all_to_all"(%123) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %125 = stablehlo.slice %124 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %126 = stablehlo.reshape %125 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %127 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %128 = "stablehlo.all_to_all"(%127) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %129 = stablehlo.slice %128 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %131 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %132 = "stablehlo.all_to_all"(%131) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %133 = stablehlo.slice %132 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %134 = stablehlo.reshape %133 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %135 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %136 = "stablehlo.all_to_all"(%135) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %137 = stablehlo.slice %136 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %138 = stablehlo.reshape %137 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %139 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %140 = "stablehlo.all_to_all"(%139) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %141 = stablehlo.slice %140 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %142 = stablehlo.reshape %141 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %143 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %144 = "stablehlo.all_to_all"(%143) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %145 = stablehlo.slice %144 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %146 = stablehlo.reshape %145 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %147 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %148 = "stablehlo.all_to_all"(%147) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %149 = stablehlo.slice %148 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %150 = stablehlo.reshape %149 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %151 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %152 = "stablehlo.all_to_all"(%151) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %153 = stablehlo.slice %152 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %154 = stablehlo.reshape %153 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %155 = stablehlo.concatenate %30, %34, %38, %42, %46, %50, %54, %58, %62, %66, %70, %74, %78, %82, %86, %90, %94, %98, %102, %106, %110, %114, %118, %122, %126, %130, %134, %138, %142, %146, %150, %154, dim = 0 : (tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>) -> tensor<512x2880xbf16>
      %156 = stablehlo.reshape %155 : (tensor<512x2880xbf16>) -> tensor<4x128x2880xbf16>
      %157 = stablehlo.dot_general %156, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x128x5760xbf16>
      %158 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] : (tensor<4x5760xbf16>) -> tensor<4x128x5760xbf16>
      %159 = stablehlo.add %157, %158 : tensor<4x128x5760xbf16>
      %160 = stablehlo.slice %159 [0:4, 0:128, 1:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %161 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %162 = stablehlo.clamp %26, %160, %161 : tensor<4x128x2880xbf16>
      %163 = stablehlo.add %162, %1 : tensor<4x128x2880xbf16>
      %164 = stablehlo.convert %163 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %165 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %166 = stablehlo.slice %159 [0:4, 0:128, 0:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %167 = stablehlo.clamp %165, %166, %161 : tensor<4x128x2880xbf16>
      %168 = stablehlo.convert %167 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %169 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<4x128x2880xf32>
      %170 = stablehlo.multiply %168, %169 : tensor<4x128x2880xf32>
      %171 = stablehlo.convert %170 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %172 = stablehlo.logistic %171 : tensor<4x128x2880xbf16>
      %173 = stablehlo.convert %172 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %174 = stablehlo.multiply %168, %173 : tensor<4x128x2880xf32>
      %175 = stablehlo.convert %174 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %176 = stablehlo.convert %175 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %177 = stablehlo.multiply %164, %176 : tensor<4x128x2880xf32>
      %178 = stablehlo.convert %177 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %179 = stablehlo.dot_general %178, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x128x2880xbf16>
      %180 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] : (tensor<4x2880xbf16>) -> tensor<4x128x2880xbf16>
      %181 = stablehlo.add %179, %180 : tensor<4x128x2880xbf16>
      %182 = stablehlo.reshape %181 : (tensor<4x128x2880xbf16>) -> tensor<4x1x128x2880xbf16>
      %183 = stablehlo.convert %182 : (tensor<4x1x128x2880xbf16>) -> tensor<4x1x128x2880xf32>
      %184 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x4xbf16>) -> tensor<4x128xbf16>
      %185 = stablehlo.reshape %184 : (tensor<4x128xbf16>) -> tensor<4x1x128x1xbf16>
      %186 = stablehlo.convert %185 : (tensor<4x1x128x1xbf16>) -> tensor<4x1x128x1xf32>
      %187 = stablehlo.reshape %186 : (tensor<4x1x128x1xf32>) -> tensor<4x1x128xf32>
      %188 = stablehlo.broadcast_in_dim %187, dims = [0, 1, 2] : (tensor<4x1x128xf32>) -> tensor<4x1x128x2880xf32>
      %189 = stablehlo.multiply %183, %188 : tensor<4x1x128x2880xf32>
      %190 = stablehlo.convert %189 : (tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xbf16>
      %191 = stablehlo.reduce(%190 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %192 = "stablehlo.all_reduce"(%191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        %193 = stablehlo.add %arg22, %arg23 : tensor<bf16>
        stablehlo.return %193 : tensor<bf16>
      }) : (tensor<1x128x2880xbf16>) -> tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %192, %192 : tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before StablehloLegalizeCompositeToCallPass (stablehlo-legalize-composite-to-call) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<4x2880xbf16>, %arg15: tensor<4x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<4x5760xbf16>, %arg20: tensor<4x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x4xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %193 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %193 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) : (tensor<128x4xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %27 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %28 = "stablehlo.all_to_all"(%27) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %29 = stablehlo.slice %28 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %31 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %32 = "stablehlo.all_to_all"(%31) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %33 = stablehlo.slice %32 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %34 = stablehlo.reshape %33 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %35 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %36 = "stablehlo.all_to_all"(%35) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %37 = stablehlo.slice %36 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %38 = stablehlo.reshape %37 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %39 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %40 = "stablehlo.all_to_all"(%39) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %41 = stablehlo.slice %40 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %42 = stablehlo.reshape %41 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %43 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %44 = "stablehlo.all_to_all"(%43) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %45 = stablehlo.slice %44 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %46 = stablehlo.reshape %45 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %47 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %48 = "stablehlo.all_to_all"(%47) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %49 = stablehlo.slice %48 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %50 = stablehlo.reshape %49 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %51 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %52 = "stablehlo.all_to_all"(%51) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %53 = stablehlo.slice %52 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %54 = stablehlo.reshape %53 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %55 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %56 = "stablehlo.all_to_all"(%55) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %57 = stablehlo.slice %56 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %58 = stablehlo.reshape %57 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %59 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %60 = "stablehlo.all_to_all"(%59) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %61 = stablehlo.slice %60 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %62 = stablehlo.reshape %61 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %63 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %64 = "stablehlo.all_to_all"(%63) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %65 = stablehlo.slice %64 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %66 = stablehlo.reshape %65 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %67 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %68 = "stablehlo.all_to_all"(%67) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %69 = stablehlo.slice %68 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %70 = stablehlo.reshape %69 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %71 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %72 = "stablehlo.all_to_all"(%71) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %74 = stablehlo.reshape %73 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %75 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %76 = "stablehlo.all_to_all"(%75) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %77 = stablehlo.slice %76 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %78 = stablehlo.reshape %77 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %79 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %80 = "stablehlo.all_to_all"(%79) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %81 = stablehlo.slice %80 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %82 = stablehlo.reshape %81 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %83 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %84 = "stablehlo.all_to_all"(%83) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %85 = stablehlo.slice %84 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %86 = stablehlo.reshape %85 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %87 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %88 = "stablehlo.all_to_all"(%87) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %89 = stablehlo.slice %88 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %90 = stablehlo.reshape %89 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %91 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %92 = "stablehlo.all_to_all"(%91) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %93 = stablehlo.slice %92 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %94 = stablehlo.reshape %93 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %95 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %96 = "stablehlo.all_to_all"(%95) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %97 = stablehlo.slice %96 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %98 = stablehlo.reshape %97 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %99 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %100 = "stablehlo.all_to_all"(%99) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %101 = stablehlo.slice %100 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %102 = stablehlo.reshape %101 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %103 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %104 = "stablehlo.all_to_all"(%103) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %105 = stablehlo.slice %104 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %106 = stablehlo.reshape %105 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %107 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %108 = "stablehlo.all_to_all"(%107) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %109 = stablehlo.slice %108 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %110 = stablehlo.reshape %109 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %111 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %112 = "stablehlo.all_to_all"(%111) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %113 = stablehlo.slice %112 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %114 = stablehlo.reshape %113 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %115 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %116 = "stablehlo.all_to_all"(%115) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %117 = stablehlo.slice %116 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %119 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %120 = "stablehlo.all_to_all"(%119) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %121 = stablehlo.slice %120 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %122 = stablehlo.reshape %121 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %123 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %124 = "stablehlo.all_to_all"(%123) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %125 = stablehlo.slice %124 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %126 = stablehlo.reshape %125 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %127 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %128 = "stablehlo.all_to_all"(%127) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %129 = stablehlo.slice %128 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %131 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %132 = "stablehlo.all_to_all"(%131) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %133 = stablehlo.slice %132 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %134 = stablehlo.reshape %133 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %135 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %136 = "stablehlo.all_to_all"(%135) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %137 = stablehlo.slice %136 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %138 = stablehlo.reshape %137 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %139 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %140 = "stablehlo.all_to_all"(%139) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %141 = stablehlo.slice %140 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %142 = stablehlo.reshape %141 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %143 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %144 = "stablehlo.all_to_all"(%143) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %145 = stablehlo.slice %144 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %146 = stablehlo.reshape %145 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %147 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %148 = "stablehlo.all_to_all"(%147) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %149 = stablehlo.slice %148 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %150 = stablehlo.reshape %149 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %151 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %152 = "stablehlo.all_to_all"(%151) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %153 = stablehlo.slice %152 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %154 = stablehlo.reshape %153 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %155 = stablehlo.concatenate %30, %34, %38, %42, %46, %50, %54, %58, %62, %66, %70, %74, %78, %82, %86, %90, %94, %98, %102, %106, %110, %114, %118, %122, %126, %130, %134, %138, %142, %146, %150, %154, dim = 0 : (tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>) -> tensor<512x2880xbf16>
      %156 = stablehlo.reshape %155 : (tensor<512x2880xbf16>) -> tensor<4x128x2880xbf16>
      %157 = stablehlo.dot_general %156, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x128x5760xbf16>
      %158 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] : (tensor<4x5760xbf16>) -> tensor<4x128x5760xbf16>
      %159 = stablehlo.add %157, %158 : tensor<4x128x5760xbf16>
      %160 = stablehlo.slice %159 [0:4, 0:128, 1:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %161 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %162 = stablehlo.clamp %26, %160, %161 : tensor<4x128x2880xbf16>
      %163 = stablehlo.add %162, %1 : tensor<4x128x2880xbf16>
      %164 = stablehlo.convert %163 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %165 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %166 = stablehlo.slice %159 [0:4, 0:128, 0:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %167 = stablehlo.clamp %165, %166, %161 : tensor<4x128x2880xbf16>
      %168 = stablehlo.convert %167 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %169 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<4x128x2880xf32>
      %170 = stablehlo.multiply %168, %169 : tensor<4x128x2880xf32>
      %171 = stablehlo.convert %170 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %172 = stablehlo.logistic %171 : tensor<4x128x2880xbf16>
      %173 = stablehlo.convert %172 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %174 = stablehlo.multiply %168, %173 : tensor<4x128x2880xf32>
      %175 = stablehlo.convert %174 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %176 = stablehlo.convert %175 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %177 = stablehlo.multiply %164, %176 : tensor<4x128x2880xf32>
      %178 = stablehlo.convert %177 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %179 = stablehlo.dot_general %178, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x128x2880xbf16>
      %180 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] : (tensor<4x2880xbf16>) -> tensor<4x128x2880xbf16>
      %181 = stablehlo.add %179, %180 : tensor<4x128x2880xbf16>
      %182 = stablehlo.reshape %181 : (tensor<4x128x2880xbf16>) -> tensor<4x1x128x2880xbf16>
      %183 = stablehlo.convert %182 : (tensor<4x1x128x2880xbf16>) -> tensor<4x1x128x2880xf32>
      %184 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x4xbf16>) -> tensor<4x128xbf16>
      %185 = stablehlo.reshape %184 : (tensor<4x128xbf16>) -> tensor<4x1x128x1xbf16>
      %186 = stablehlo.convert %185 : (tensor<4x1x128x1xbf16>) -> tensor<4x1x128x1xf32>
      %187 = stablehlo.reshape %186 : (tensor<4x1x128x1xf32>) -> tensor<4x1x128xf32>
      %188 = stablehlo.broadcast_in_dim %187, dims = [0, 1, 2] : (tensor<4x1x128xf32>) -> tensor<4x1x128x2880xf32>
      %189 = stablehlo.multiply %183, %188 : tensor<4x1x128x2880xf32>
      %190 = stablehlo.convert %189 : (tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xbf16>
      %191 = stablehlo.reduce(%190 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %192 = "stablehlo.all_reduce"(%191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        %193 = stablehlo.add %arg22, %arg23 : tensor<bf16>
        stablehlo.return %193 : tensor<bf16>
      }) : (tensor<1x128x2880xbf16>) -> tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %192, %192 : tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<4x2880xbf16>, %arg15: tensor<4x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<4x5760xbf16>, %arg20: tensor<4x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x4xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %193 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %193 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) : (tensor<128x4xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %27 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %28 = "stablehlo.all_to_all"(%27) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %29 = stablehlo.slice %28 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %31 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %32 = "stablehlo.all_to_all"(%31) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %33 = stablehlo.slice %32 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %34 = stablehlo.reshape %33 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %35 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %36 = "stablehlo.all_to_all"(%35) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %37 = stablehlo.slice %36 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %38 = stablehlo.reshape %37 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %39 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %40 = "stablehlo.all_to_all"(%39) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %41 = stablehlo.slice %40 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %42 = stablehlo.reshape %41 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %43 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %44 = "stablehlo.all_to_all"(%43) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %45 = stablehlo.slice %44 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %46 = stablehlo.reshape %45 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %47 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %48 = "stablehlo.all_to_all"(%47) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %49 = stablehlo.slice %48 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %50 = stablehlo.reshape %49 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %51 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %52 = "stablehlo.all_to_all"(%51) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %53 = stablehlo.slice %52 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %54 = stablehlo.reshape %53 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %55 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %56 = "stablehlo.all_to_all"(%55) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %57 = stablehlo.slice %56 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %58 = stablehlo.reshape %57 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %59 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %60 = "stablehlo.all_to_all"(%59) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %61 = stablehlo.slice %60 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %62 = stablehlo.reshape %61 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %63 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %64 = "stablehlo.all_to_all"(%63) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %65 = stablehlo.slice %64 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %66 = stablehlo.reshape %65 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %67 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %68 = "stablehlo.all_to_all"(%67) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %69 = stablehlo.slice %68 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %70 = stablehlo.reshape %69 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %71 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %72 = "stablehlo.all_to_all"(%71) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %74 = stablehlo.reshape %73 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %75 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %76 = "stablehlo.all_to_all"(%75) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %77 = stablehlo.slice %76 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %78 = stablehlo.reshape %77 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %79 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %80 = "stablehlo.all_to_all"(%79) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %81 = stablehlo.slice %80 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %82 = stablehlo.reshape %81 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %83 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %84 = "stablehlo.all_to_all"(%83) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %85 = stablehlo.slice %84 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %86 = stablehlo.reshape %85 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %87 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %88 = "stablehlo.all_to_all"(%87) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %89 = stablehlo.slice %88 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %90 = stablehlo.reshape %89 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %91 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %92 = "stablehlo.all_to_all"(%91) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %93 = stablehlo.slice %92 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %94 = stablehlo.reshape %93 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %95 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %96 = "stablehlo.all_to_all"(%95) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %97 = stablehlo.slice %96 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %98 = stablehlo.reshape %97 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %99 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %100 = "stablehlo.all_to_all"(%99) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %101 = stablehlo.slice %100 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %102 = stablehlo.reshape %101 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %103 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %104 = "stablehlo.all_to_all"(%103) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %105 = stablehlo.slice %104 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %106 = stablehlo.reshape %105 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %107 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %108 = "stablehlo.all_to_all"(%107) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %109 = stablehlo.slice %108 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %110 = stablehlo.reshape %109 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %111 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %112 = "stablehlo.all_to_all"(%111) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %113 = stablehlo.slice %112 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %114 = stablehlo.reshape %113 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %115 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %116 = "stablehlo.all_to_all"(%115) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %117 = stablehlo.slice %116 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %119 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %120 = "stablehlo.all_to_all"(%119) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %121 = stablehlo.slice %120 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %122 = stablehlo.reshape %121 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %123 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %124 = "stablehlo.all_to_all"(%123) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %125 = stablehlo.slice %124 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %126 = stablehlo.reshape %125 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %127 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %128 = "stablehlo.all_to_all"(%127) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %129 = stablehlo.slice %128 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %131 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %132 = "stablehlo.all_to_all"(%131) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %133 = stablehlo.slice %132 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %134 = stablehlo.reshape %133 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %135 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %136 = "stablehlo.all_to_all"(%135) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %137 = stablehlo.slice %136 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %138 = stablehlo.reshape %137 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %139 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %140 = "stablehlo.all_to_all"(%139) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %141 = stablehlo.slice %140 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %142 = stablehlo.reshape %141 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %143 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %144 = "stablehlo.all_to_all"(%143) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %145 = stablehlo.slice %144 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %146 = stablehlo.reshape %145 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %147 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %148 = "stablehlo.all_to_all"(%147) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %149 = stablehlo.slice %148 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %150 = stablehlo.reshape %149 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %151 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %152 = "stablehlo.all_to_all"(%151) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %153 = stablehlo.slice %152 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %154 = stablehlo.reshape %153 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %155 = stablehlo.concatenate %30, %34, %38, %42, %46, %50, %54, %58, %62, %66, %70, %74, %78, %82, %86, %90, %94, %98, %102, %106, %110, %114, %118, %122, %126, %130, %134, %138, %142, %146, %150, %154, dim = 0 : (tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>) -> tensor<512x2880xbf16>
      %156 = stablehlo.reshape %155 : (tensor<512x2880xbf16>) -> tensor<4x128x2880xbf16>
      %157 = stablehlo.dot_general %156, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x128x5760xbf16>
      %158 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] : (tensor<4x5760xbf16>) -> tensor<4x128x5760xbf16>
      %159 = stablehlo.add %157, %158 : tensor<4x128x5760xbf16>
      %160 = stablehlo.slice %159 [0:4, 0:128, 1:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %161 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %162 = stablehlo.clamp %26, %160, %161 : tensor<4x128x2880xbf16>
      %163 = stablehlo.add %162, %1 : tensor<4x128x2880xbf16>
      %164 = stablehlo.convert %163 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %165 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %166 = stablehlo.slice %159 [0:4, 0:128, 0:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %167 = stablehlo.clamp %165, %166, %161 : tensor<4x128x2880xbf16>
      %168 = stablehlo.convert %167 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %169 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<4x128x2880xf32>
      %170 = stablehlo.multiply %168, %169 : tensor<4x128x2880xf32>
      %171 = stablehlo.convert %170 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %172 = stablehlo.logistic %171 : tensor<4x128x2880xbf16>
      %173 = stablehlo.convert %172 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %174 = stablehlo.multiply %168, %173 : tensor<4x128x2880xf32>
      %175 = stablehlo.convert %174 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %176 = stablehlo.convert %175 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %177 = stablehlo.multiply %164, %176 : tensor<4x128x2880xf32>
      %178 = stablehlo.convert %177 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %179 = stablehlo.dot_general %178, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x128x2880xbf16>
      %180 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] : (tensor<4x2880xbf16>) -> tensor<4x128x2880xbf16>
      %181 = stablehlo.add %179, %180 : tensor<4x128x2880xbf16>
      %182 = stablehlo.reshape %181 : (tensor<4x128x2880xbf16>) -> tensor<4x1x128x2880xbf16>
      %183 = stablehlo.convert %182 : (tensor<4x1x128x2880xbf16>) -> tensor<4x1x128x2880xf32>
      %184 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x4xbf16>) -> tensor<4x128xbf16>
      %185 = stablehlo.reshape %184 : (tensor<4x128xbf16>) -> tensor<4x1x128x1xbf16>
      %186 = stablehlo.convert %185 : (tensor<4x1x128x1xbf16>) -> tensor<4x1x128x1xf32>
      %187 = stablehlo.reshape %186 : (tensor<4x1x128x1xf32>) -> tensor<4x1x128xf32>
      %188 = stablehlo.broadcast_in_dim %187, dims = [0, 1, 2] : (tensor<4x1x128xf32>) -> tensor<4x1x128x2880xf32>
      %189 = stablehlo.multiply %183, %188 : tensor<4x1x128x2880xf32>
      %190 = stablehlo.convert %189 : (tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xbf16>
      %191 = stablehlo.reduce(%190 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %192 = "stablehlo.all_reduce"(%191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        %193 = stablehlo.add %arg22, %arg23 : tensor<bf16>
        stablehlo.return %193 : tensor<bf16>
      }) : (tensor<1x128x2880xbf16>) -> tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %192, %192 : tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<4x2880xbf16>, %arg15: tensor<4x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<4x5760xbf16>, %arg20: tensor<4x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x4xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %193 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %193 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) : (tensor<128x4xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %27 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %28 = "stablehlo.all_to_all"(%27) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %29 = stablehlo.slice %28 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %31 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %32 = "stablehlo.all_to_all"(%31) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %33 = stablehlo.slice %32 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %34 = stablehlo.reshape %33 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %35 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %36 = "stablehlo.all_to_all"(%35) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %37 = stablehlo.slice %36 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %38 = stablehlo.reshape %37 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %39 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %40 = "stablehlo.all_to_all"(%39) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %41 = stablehlo.slice %40 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %42 = stablehlo.reshape %41 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %43 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %44 = "stablehlo.all_to_all"(%43) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %45 = stablehlo.slice %44 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %46 = stablehlo.reshape %45 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %47 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %48 = "stablehlo.all_to_all"(%47) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %49 = stablehlo.slice %48 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %50 = stablehlo.reshape %49 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %51 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %52 = "stablehlo.all_to_all"(%51) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %53 = stablehlo.slice %52 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %54 = stablehlo.reshape %53 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %55 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %56 = "stablehlo.all_to_all"(%55) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %57 = stablehlo.slice %56 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %58 = stablehlo.reshape %57 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %59 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %60 = "stablehlo.all_to_all"(%59) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %61 = stablehlo.slice %60 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %62 = stablehlo.reshape %61 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %63 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %64 = "stablehlo.all_to_all"(%63) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %65 = stablehlo.slice %64 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %66 = stablehlo.reshape %65 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %67 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %68 = "stablehlo.all_to_all"(%67) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %69 = stablehlo.slice %68 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %70 = stablehlo.reshape %69 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %71 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %72 = "stablehlo.all_to_all"(%71) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %74 = stablehlo.reshape %73 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %75 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %76 = "stablehlo.all_to_all"(%75) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %77 = stablehlo.slice %76 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %78 = stablehlo.reshape %77 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %79 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %80 = "stablehlo.all_to_all"(%79) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %81 = stablehlo.slice %80 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %82 = stablehlo.reshape %81 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %83 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %84 = "stablehlo.all_to_all"(%83) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %85 = stablehlo.slice %84 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %86 = stablehlo.reshape %85 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %87 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %88 = "stablehlo.all_to_all"(%87) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %89 = stablehlo.slice %88 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %90 = stablehlo.reshape %89 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %91 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %92 = "stablehlo.all_to_all"(%91) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %93 = stablehlo.slice %92 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %94 = stablehlo.reshape %93 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %95 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %96 = "stablehlo.all_to_all"(%95) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %97 = stablehlo.slice %96 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %98 = stablehlo.reshape %97 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %99 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %100 = "stablehlo.all_to_all"(%99) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %101 = stablehlo.slice %100 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %102 = stablehlo.reshape %101 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %103 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %104 = "stablehlo.all_to_all"(%103) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %105 = stablehlo.slice %104 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %106 = stablehlo.reshape %105 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %107 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %108 = "stablehlo.all_to_all"(%107) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %109 = stablehlo.slice %108 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %110 = stablehlo.reshape %109 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %111 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %112 = "stablehlo.all_to_all"(%111) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %113 = stablehlo.slice %112 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %114 = stablehlo.reshape %113 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %115 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %116 = "stablehlo.all_to_all"(%115) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %117 = stablehlo.slice %116 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %119 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %120 = "stablehlo.all_to_all"(%119) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %121 = stablehlo.slice %120 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %122 = stablehlo.reshape %121 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %123 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %124 = "stablehlo.all_to_all"(%123) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %125 = stablehlo.slice %124 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %126 = stablehlo.reshape %125 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %127 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %128 = "stablehlo.all_to_all"(%127) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %129 = stablehlo.slice %128 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %131 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %132 = "stablehlo.all_to_all"(%131) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %133 = stablehlo.slice %132 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %134 = stablehlo.reshape %133 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %135 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %136 = "stablehlo.all_to_all"(%135) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %137 = stablehlo.slice %136 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %138 = stablehlo.reshape %137 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %139 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %140 = "stablehlo.all_to_all"(%139) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %141 = stablehlo.slice %140 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %142 = stablehlo.reshape %141 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %143 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %144 = "stablehlo.all_to_all"(%143) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %145 = stablehlo.slice %144 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %146 = stablehlo.reshape %145 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %147 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %148 = "stablehlo.all_to_all"(%147) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %149 = stablehlo.slice %148 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %150 = stablehlo.reshape %149 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %151 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %152 = "stablehlo.all_to_all"(%151) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %153 = stablehlo.slice %152 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %154 = stablehlo.reshape %153 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %155 = stablehlo.concatenate %30, %34, %38, %42, %46, %50, %54, %58, %62, %66, %70, %74, %78, %82, %86, %90, %94, %98, %102, %106, %110, %114, %118, %122, %126, %130, %134, %138, %142, %146, %150, %154, dim = 0 : (tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>) -> tensor<512x2880xbf16>
      %156 = stablehlo.reshape %155 : (tensor<512x2880xbf16>) -> tensor<4x128x2880xbf16>
      %157 = stablehlo.dot_general %156, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x128x5760xbf16>
      %158 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] : (tensor<4x5760xbf16>) -> tensor<4x128x5760xbf16>
      %159 = stablehlo.add %157, %158 : tensor<4x128x5760xbf16>
      %160 = stablehlo.slice %159 [0:4, 0:128, 1:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %161 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %162 = stablehlo.clamp %26, %160, %161 : tensor<4x128x2880xbf16>
      %163 = stablehlo.add %162, %1 : tensor<4x128x2880xbf16>
      %164 = stablehlo.convert %163 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %165 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %166 = stablehlo.slice %159 [0:4, 0:128, 0:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %167 = stablehlo.clamp %165, %166, %161 : tensor<4x128x2880xbf16>
      %168 = stablehlo.convert %167 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %169 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<4x128x2880xf32>
      %170 = stablehlo.multiply %168, %169 : tensor<4x128x2880xf32>
      %171 = stablehlo.convert %170 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %172 = stablehlo.logistic %171 : tensor<4x128x2880xbf16>
      %173 = stablehlo.convert %172 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %174 = stablehlo.multiply %168, %173 : tensor<4x128x2880xf32>
      %175 = stablehlo.convert %174 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %176 = stablehlo.convert %175 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %177 = stablehlo.multiply %164, %176 : tensor<4x128x2880xf32>
      %178 = stablehlo.convert %177 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %179 = stablehlo.dot_general %178, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x128x2880xbf16>
      %180 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] : (tensor<4x2880xbf16>) -> tensor<4x128x2880xbf16>
      %181 = stablehlo.add %179, %180 : tensor<4x128x2880xbf16>
      %182 = stablehlo.reshape %181 : (tensor<4x128x2880xbf16>) -> tensor<4x1x128x2880xbf16>
      %183 = stablehlo.convert %182 : (tensor<4x1x128x2880xbf16>) -> tensor<4x1x128x2880xf32>
      %184 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x4xbf16>) -> tensor<4x128xbf16>
      %185 = stablehlo.reshape %184 : (tensor<4x128xbf16>) -> tensor<4x1x128x1xbf16>
      %186 = stablehlo.convert %185 : (tensor<4x1x128x1xbf16>) -> tensor<4x1x128x1xf32>
      %187 = stablehlo.reshape %186 : (tensor<4x1x128x1xf32>) -> tensor<4x1x128xf32>
      %188 = stablehlo.broadcast_in_dim %187, dims = [0, 1, 2] : (tensor<4x1x128xf32>) -> tensor<4x1x128x2880xf32>
      %189 = stablehlo.multiply %183, %188 : tensor<4x1x128x2880xf32>
      %190 = stablehlo.convert %189 : (tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xbf16>
      %191 = stablehlo.reduce(%190 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %192 = "stablehlo.all_reduce"(%191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        %193 = stablehlo.add %arg22, %arg23 : tensor<bf16>
        stablehlo.return %193 : tensor<bf16>
      }) : (tensor<1x128x2880xbf16>) -> tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %192, %192 : tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.134) //----- //
module @SyncTensorsGraph.134 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<32xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<32x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:5 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>] out_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<1x128x2880xbf16>, %arg14: tensor<4x2880xbf16>, %arg15: tensor<4x2880x2880xbf16>, %arg16: tensor<f32>, %arg17: tensor<bf16>, %arg18: tensor<bf16>, %arg19: tensor<4x5760xbf16>, %arg20: tensor<4x2880x5760xbf16>, %arg21: tensor<bf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x4xbf16>
      %3 = stablehlo.iota dim = 0 : tensor<128xi64>
      %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %5 = stablehlo.reshape %arg13 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %6 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %8 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %9 = stablehlo.add %7, %8 : tensor<128x32xbf16>
      %10 = stablehlo.iota dim = 0 : tensor<32xi32>
      %11 = stablehlo.broadcast_in_dim %10, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %12:2 = "stablehlo.sort"(%9, %11) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %193 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %193 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %13 = stablehlo.slice %12#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %14 = stablehlo.convert %13 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %15 = stablehlo.reshape %14 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %16 = stablehlo.concatenate %4, %15, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %17 = stablehlo.slice %12#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.subtract %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.exponential %20 : tensor<128x4xbf16>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %23 = stablehlo.broadcast_in_dim %22, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %24 = stablehlo.divide %21, %23 : tensor<128x4xbf16>
      %25 = "stablehlo.scatter"(%2, %16, %24) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) : (tensor<128x4xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %26 = stablehlo.broadcast_in_dim %arg21, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %27 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %28 = "stablehlo.all_to_all"(%27) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %29 = stablehlo.slice %28 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %31 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %32 = "stablehlo.all_to_all"(%31) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %33 = stablehlo.slice %32 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %34 = stablehlo.reshape %33 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %35 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %36 = "stablehlo.all_to_all"(%35) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %37 = stablehlo.slice %36 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %38 = stablehlo.reshape %37 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %39 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %40 = "stablehlo.all_to_all"(%39) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %41 = stablehlo.slice %40 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %42 = stablehlo.reshape %41 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %43 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %44 = "stablehlo.all_to_all"(%43) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %45 = stablehlo.slice %44 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %46 = stablehlo.reshape %45 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %47 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %48 = "stablehlo.all_to_all"(%47) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %49 = stablehlo.slice %48 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %50 = stablehlo.reshape %49 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %51 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %52 = "stablehlo.all_to_all"(%51) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %53 = stablehlo.slice %52 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %54 = stablehlo.reshape %53 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %55 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %56 = "stablehlo.all_to_all"(%55) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %57 = stablehlo.slice %56 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %58 = stablehlo.reshape %57 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %59 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %60 = "stablehlo.all_to_all"(%59) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %61 = stablehlo.slice %60 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %62 = stablehlo.reshape %61 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %63 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %64 = "stablehlo.all_to_all"(%63) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %65 = stablehlo.slice %64 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %66 = stablehlo.reshape %65 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %67 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %68 = "stablehlo.all_to_all"(%67) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %69 = stablehlo.slice %68 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %70 = stablehlo.reshape %69 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %71 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %72 = "stablehlo.all_to_all"(%71) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %74 = stablehlo.reshape %73 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %75 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %76 = "stablehlo.all_to_all"(%75) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %77 = stablehlo.slice %76 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %78 = stablehlo.reshape %77 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %79 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %80 = "stablehlo.all_to_all"(%79) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %81 = stablehlo.slice %80 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %82 = stablehlo.reshape %81 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %83 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %84 = "stablehlo.all_to_all"(%83) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %85 = stablehlo.slice %84 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %86 = stablehlo.reshape %85 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %87 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %88 = "stablehlo.all_to_all"(%87) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %89 = stablehlo.slice %88 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %90 = stablehlo.reshape %89 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %91 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %92 = "stablehlo.all_to_all"(%91) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %93 = stablehlo.slice %92 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %94 = stablehlo.reshape %93 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %95 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %96 = "stablehlo.all_to_all"(%95) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %97 = stablehlo.slice %96 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %98 = stablehlo.reshape %97 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %99 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %100 = "stablehlo.all_to_all"(%99) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %101 = stablehlo.slice %100 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %102 = stablehlo.reshape %101 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %103 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %104 = "stablehlo.all_to_all"(%103) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %105 = stablehlo.slice %104 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %106 = stablehlo.reshape %105 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %107 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %108 = "stablehlo.all_to_all"(%107) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %109 = stablehlo.slice %108 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %110 = stablehlo.reshape %109 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %111 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %112 = "stablehlo.all_to_all"(%111) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %113 = stablehlo.slice %112 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %114 = stablehlo.reshape %113 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %115 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %116 = "stablehlo.all_to_all"(%115) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %117 = stablehlo.slice %116 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %119 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %120 = "stablehlo.all_to_all"(%119) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %121 = stablehlo.slice %120 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %122 = stablehlo.reshape %121 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %123 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %124 = "stablehlo.all_to_all"(%123) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %125 = stablehlo.slice %124 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %126 = stablehlo.reshape %125 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %127 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %128 = "stablehlo.all_to_all"(%127) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %129 = stablehlo.slice %128 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %131 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %132 = "stablehlo.all_to_all"(%131) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %133 = stablehlo.slice %132 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %134 = stablehlo.reshape %133 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %135 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %136 = "stablehlo.all_to_all"(%135) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %137 = stablehlo.slice %136 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %138 = stablehlo.reshape %137 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %139 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %140 = "stablehlo.all_to_all"(%139) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %141 = stablehlo.slice %140 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %142 = stablehlo.reshape %141 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %143 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %144 = "stablehlo.all_to_all"(%143) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %145 = stablehlo.slice %144 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %146 = stablehlo.reshape %145 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %147 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %148 = "stablehlo.all_to_all"(%147) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %149 = stablehlo.slice %148 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %150 = stablehlo.reshape %149 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %151 = stablehlo.reshape %5 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %152 = "stablehlo.all_to_all"(%151) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %153 = stablehlo.slice %152 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %154 = stablehlo.reshape %153 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %155 = stablehlo.concatenate %30, %34, %38, %42, %46, %50, %54, %58, %62, %66, %70, %74, %78, %82, %86, %90, %94, %98, %102, %106, %110, %114, %118, %122, %126, %130, %134, %138, %142, %146, %150, %154, dim = 0 : (tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>) -> tensor<512x2880xbf16>
      %156 = stablehlo.reshape %155 : (tensor<512x2880xbf16>) -> tensor<4x128x2880xbf16>
      %157 = stablehlo.dot_general %156, %arg20, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x128x5760xbf16>
      %158 = stablehlo.broadcast_in_dim %arg19, dims = [0, 2] : (tensor<4x5760xbf16>) -> tensor<4x128x5760xbf16>
      %159 = stablehlo.add %157, %158 : tensor<4x128x5760xbf16>
      %160 = stablehlo.slice %159 [0:4, 0:128, 1:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %161 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %162 = stablehlo.clamp %26, %160, %161 : tensor<4x128x2880xbf16>
      %163 = stablehlo.add %162, %1 : tensor<4x128x2880xbf16>
      %164 = stablehlo.convert %163 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %165 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %166 = stablehlo.slice %159 [0:4, 0:128, 0:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %167 = stablehlo.clamp %165, %166, %161 : tensor<4x128x2880xbf16>
      %168 = stablehlo.convert %167 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %169 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<f32>) -> tensor<4x128x2880xf32>
      %170 = stablehlo.multiply %168, %169 : tensor<4x128x2880xf32>
      %171 = stablehlo.convert %170 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %172 = stablehlo.logistic %171 : tensor<4x128x2880xbf16>
      %173 = stablehlo.convert %172 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %174 = stablehlo.multiply %168, %173 : tensor<4x128x2880xf32>
      %175 = stablehlo.convert %174 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %176 = stablehlo.convert %175 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %177 = stablehlo.multiply %164, %176 : tensor<4x128x2880xf32>
      %178 = stablehlo.convert %177 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %179 = stablehlo.dot_general %178, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x128x2880xbf16>
      %180 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] : (tensor<4x2880xbf16>) -> tensor<4x128x2880xbf16>
      %181 = stablehlo.add %179, %180 : tensor<4x128x2880xbf16>
      %182 = stablehlo.reshape %181 : (tensor<4x128x2880xbf16>) -> tensor<4x1x128x2880xbf16>
      %183 = stablehlo.convert %182 : (tensor<4x1x128x2880xbf16>) -> tensor<4x1x128x2880xf32>
      %184 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x4xbf16>) -> tensor<4x128xbf16>
      %185 = stablehlo.reshape %184 : (tensor<4x128xbf16>) -> tensor<4x1x128x1xbf16>
      %186 = stablehlo.convert %185 : (tensor<4x1x128x1xbf16>) -> tensor<4x1x128x1xf32>
      %187 = stablehlo.reshape %186 : (tensor<4x1x128x1xf32>) -> tensor<4x1x128xf32>
      %188 = stablehlo.broadcast_in_dim %187, dims = [0, 1, 2] : (tensor<4x1x128xf32>) -> tensor<4x1x128x2880xf32>
      %189 = stablehlo.multiply %183, %188 : tensor<4x1x128x2880xf32>
      %190 = stablehlo.convert %189 : (tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xbf16>
      %191 = stablehlo.reduce(%190 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %192 = "stablehlo.all_reduce"(%191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        %193 = stablehlo.add %arg22, %arg23 : tensor<bf16>
        stablehlo.return %193 : tensor<bf16>
      }) : (tensor<1x128x2880xbf16>) -> tensor<1x128x2880xbf16>
      sdy.return %2, %25, %25, %192, %192 : tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<32xbf16>, tensor<32x2880xbf16>, tensor<1x128x2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>) -> (tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4 : tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}

