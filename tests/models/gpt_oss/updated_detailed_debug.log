WARNING:root:Defaulting to PJRT_DEVICE=CPU
2025-09-08 16:44:17.181631: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
Torch-XLA SPMD Tensor Parallelism for GPT-OSS 20B Model
==================================================
Setting up XLA environment...
XLA environment configured.
Created device mesh: (1, 8) with 8 devices
Loading model...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 228.12it/s]
Some weights of the model checkpoint at openai/gpt-oss-20b were not used when initializing GptOssForCausalLM: ['model.layers.0.mlp.experts.down_proj_blocks', 'model.layers.0.mlp.experts.down_proj_scales', 'model.layers.0.mlp.experts.gate_up_proj_blocks', 'model.layers.0.mlp.experts.gate_up_proj_scales', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.experts.down_proj_bias', 'model.layers.1.mlp.experts.down_proj_blocks', 'model.layers.1.mlp.experts.down_proj_scales', 'model.layers.1.mlp.experts.gate_up_proj_bias', 'model.layers.1.mlp.experts.gate_up_proj_blocks', 'model.layers.1.mlp.experts.gate_up_proj_scales', 'model.layers.1.mlp.router.bias', 'model.layers.1.mlp.router.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.sinks', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.experts.down_proj_bias', 'model.layers.10.mlp.experts.down_proj_blocks', 'model.layers.10.mlp.experts.down_proj_scales', 'model.layers.10.mlp.experts.gate_up_proj_bias', 'model.layers.10.mlp.experts.gate_up_proj_blocks', 'model.layers.10.mlp.experts.gate_up_proj_scales', 'model.layers.10.mlp.router.bias', 'model.layers.10.mlp.router.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.sinks', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.experts.down_proj_bias', 'model.layers.11.mlp.experts.down_proj_blocks', 'model.layers.11.mlp.experts.down_proj_scales', 'model.layers.11.mlp.experts.gate_up_proj_bias', 'model.layers.11.mlp.experts.gate_up_proj_blocks', 'model.layers.11.mlp.experts.gate_up_proj_scales', 'model.layers.11.mlp.router.bias', 'model.layers.11.mlp.router.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.sinks', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.experts.down_proj_bias', 'model.layers.12.mlp.experts.down_proj_blocks', 'model.layers.12.mlp.experts.down_proj_scales', 'model.layers.12.mlp.experts.gate_up_proj_bias', 'model.layers.12.mlp.experts.gate_up_proj_blocks', 'model.layers.12.mlp.experts.gate_up_proj_scales', 'model.layers.12.mlp.router.bias', 'model.layers.12.mlp.router.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.sinks', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.experts.down_proj_bias', 'model.layers.13.mlp.experts.down_proj_blocks', 'model.layers.13.mlp.experts.down_proj_scales', 'model.layers.13.mlp.experts.gate_up_proj_bias', 'model.layers.13.mlp.experts.gate_up_proj_blocks', 'model.layers.13.mlp.experts.gate_up_proj_scales', 'model.layers.13.mlp.router.bias', 'model.layers.13.mlp.router.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.sinks', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.experts.down_proj_bias', 'model.layers.14.mlp.experts.down_proj_blocks', 'model.layers.14.mlp.experts.down_proj_scales', 'model.layers.14.mlp.experts.gate_up_proj_bias', 'model.layers.14.mlp.experts.gate_up_proj_blocks', 'model.layers.14.mlp.experts.gate_up_proj_scales', 'model.layers.14.mlp.router.bias', 'model.layers.14.mlp.router.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.sinks', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.experts.down_proj_bias', 'model.layers.15.mlp.experts.down_proj_blocks', 'model.layers.15.mlp.experts.down_proj_scales', 'model.layers.15.mlp.experts.gate_up_proj_bias', 'model.layers.15.mlp.experts.gate_up_proj_blocks', 'model.layers.15.mlp.experts.gate_up_proj_scales', 'model.layers.15.mlp.router.bias', 'model.layers.15.mlp.router.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.sinks', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.experts.down_proj_bias', 'model.layers.16.mlp.experts.down_proj_blocks', 'model.layers.16.mlp.experts.down_proj_scales', 'model.layers.16.mlp.experts.gate_up_proj_bias', 'model.layers.16.mlp.experts.gate_up_proj_blocks', 'model.layers.16.mlp.experts.gate_up_proj_scales', 'model.layers.16.mlp.router.bias', 'model.layers.16.mlp.router.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.sinks', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.experts.down_proj_bias', 'model.layers.17.mlp.experts.down_proj_blocks', 'model.layers.17.mlp.experts.down_proj_scales', 'model.layers.17.mlp.experts.gate_up_proj_bias', 'model.layers.17.mlp.experts.gate_up_proj_blocks', 'model.layers.17.mlp.experts.gate_up_proj_scales', 'model.layers.17.mlp.router.bias', 'model.layers.17.mlp.router.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.sinks', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.mlp.experts.down_proj_bias', 'model.layers.18.mlp.experts.down_proj_blocks', 'model.layers.18.mlp.experts.down_proj_scales', 'model.layers.18.mlp.experts.gate_up_proj_bias', 'model.layers.18.mlp.experts.gate_up_proj_blocks', 'model.layers.18.mlp.experts.gate_up_proj_scales', 'model.layers.18.mlp.router.bias', 'model.layers.18.mlp.router.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.sinks', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.mlp.experts.down_proj_bias', 'model.layers.19.mlp.experts.down_proj_blocks', 'model.layers.19.mlp.experts.down_proj_scales', 'model.layers.19.mlp.experts.gate_up_proj_bias', 'model.layers.19.mlp.experts.gate_up_proj_blocks', 'model.layers.19.mlp.experts.gate_up_proj_scales', 'model.layers.19.mlp.router.bias', 'model.layers.19.mlp.router.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.sinks', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.experts.down_proj_bias', 'model.layers.2.mlp.experts.down_proj_blocks', 'model.layers.2.mlp.experts.down_proj_scales', 'model.layers.2.mlp.experts.gate_up_proj_bias', 'model.layers.2.mlp.experts.gate_up_proj_blocks', 'model.layers.2.mlp.experts.gate_up_proj_scales', 'model.layers.2.mlp.router.bias', 'model.layers.2.mlp.router.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.sinks', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.mlp.experts.down_proj_bias', 'model.layers.20.mlp.experts.down_proj_blocks', 'model.layers.20.mlp.experts.down_proj_scales', 'model.layers.20.mlp.experts.gate_up_proj_bias', 'model.layers.20.mlp.experts.gate_up_proj_blocks', 'model.layers.20.mlp.experts.gate_up_proj_scales', 'model.layers.20.mlp.router.bias', 'model.layers.20.mlp.router.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.sinks', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.mlp.experts.down_proj_bias', 'model.layers.21.mlp.experts.down_proj_blocks', 'model.layers.21.mlp.experts.down_proj_scales', 'model.layers.21.mlp.experts.gate_up_proj_bias', 'model.layers.21.mlp.experts.gate_up_proj_blocks', 'model.layers.21.mlp.experts.gate_up_proj_scales', 'model.layers.21.mlp.router.bias', 'model.layers.21.mlp.router.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.sinks', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.mlp.experts.down_proj_bias', 'model.layers.22.mlp.experts.down_proj_blocks', 'model.layers.22.mlp.experts.down_proj_scales', 'model.layers.22.mlp.experts.gate_up_proj_bias', 'model.layers.22.mlp.experts.gate_up_proj_blocks', 'model.layers.22.mlp.experts.gate_up_proj_scales', 'model.layers.22.mlp.router.bias', 'model.layers.22.mlp.router.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.sinks', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.mlp.experts.down_proj_bias', 'model.layers.23.mlp.experts.down_proj_blocks', 'model.layers.23.mlp.experts.down_proj_scales', 'model.layers.23.mlp.experts.gate_up_proj_bias', 'model.layers.23.mlp.experts.gate_up_proj_blocks', 'model.layers.23.mlp.experts.gate_up_proj_scales', 'model.layers.23.mlp.router.bias', 'model.layers.23.mlp.router.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.sinks', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.experts.down_proj_bias', 'model.layers.3.mlp.experts.down_proj_blocks', 'model.layers.3.mlp.experts.down_proj_scales', 'model.layers.3.mlp.experts.gate_up_proj_bias', 'model.layers.3.mlp.experts.gate_up_proj_blocks', 'model.layers.3.mlp.experts.gate_up_proj_scales', 'model.layers.3.mlp.router.bias', 'model.layers.3.mlp.router.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.sinks', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.experts.down_proj_bias', 'model.layers.4.mlp.experts.down_proj_blocks', 'model.layers.4.mlp.experts.down_proj_scales', 'model.layers.4.mlp.experts.gate_up_proj_bias', 'model.layers.4.mlp.experts.gate_up_proj_blocks', 'model.layers.4.mlp.experts.gate_up_proj_scales', 'model.layers.4.mlp.router.bias', 'model.layers.4.mlp.router.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.sinks', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.experts.down_proj_bias', 'model.layers.5.mlp.experts.down_proj_blocks', 'model.layers.5.mlp.experts.down_proj_scales', 'model.layers.5.mlp.experts.gate_up_proj_bias', 'model.layers.5.mlp.experts.gate_up_proj_blocks', 'model.layers.5.mlp.experts.gate_up_proj_scales', 'model.layers.5.mlp.router.bias', 'model.layers.5.mlp.router.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.sinks', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.experts.down_proj_bias', 'model.layers.6.mlp.experts.down_proj_blocks', 'model.layers.6.mlp.experts.down_proj_scales', 'model.layers.6.mlp.experts.gate_up_proj_bias', 'model.layers.6.mlp.experts.gate_up_proj_blocks', 'model.layers.6.mlp.experts.gate_up_proj_scales', 'model.layers.6.mlp.router.bias', 'model.layers.6.mlp.router.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.sinks', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.experts.down_proj_bias', 'model.layers.7.mlp.experts.down_proj_blocks', 'model.layers.7.mlp.experts.down_proj_scales', 'model.layers.7.mlp.experts.gate_up_proj_bias', 'model.layers.7.mlp.experts.gate_up_proj_blocks', 'model.layers.7.mlp.experts.gate_up_proj_scales', 'model.layers.7.mlp.router.bias', 'model.layers.7.mlp.router.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.sinks', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.experts.down_proj_bias', 'model.layers.8.mlp.experts.down_proj_blocks', 'model.layers.8.mlp.experts.down_proj_scales', 'model.layers.8.mlp.experts.gate_up_proj_bias', 'model.layers.8.mlp.experts.gate_up_proj_blocks', 'model.layers.8.mlp.experts.gate_up_proj_scales', 'model.layers.8.mlp.router.bias', 'model.layers.8.mlp.router.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.sinks', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.experts.down_proj_bias', 'model.layers.9.mlp.experts.down_proj_blocks', 'model.layers.9.mlp.experts.down_proj_scales', 'model.layers.9.mlp.experts.gate_up_proj_bias', 'model.layers.9.mlp.experts.gate_up_proj_blocks', 'model.layers.9.mlp.experts.gate_up_proj_scales', 'model.layers.9.mlp.router.bias', 'model.layers.9.mlp.router.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.sinks', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing GptOssForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GptOssForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GptOssForCausalLM were not initialized from the model checkpoint at openai/gpt-oss-20b and are newly initialized: ['model.layers.0.mlp.experts.down_proj', 'model.layers.0.mlp.experts.gate_up_proj']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<512x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg3: !vhlo.tensor_v1<1x13x!vhlo.i64_v1>, %arg4: !vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg7: !vhlo.tensor_v1<32x!vhlo.f32_v1>, %arg8: !vhlo.tensor_v1<512x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<32x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>, %arg15: !vhlo.tensor_v1<64x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg17: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg18: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg19: !vhlo.tensor_v1<4096x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>, %arg21: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg23: !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg25: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg26: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg27: !vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>, %arg28: !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>, %arg29: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg30: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x201088x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>>}> : () -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<ui8>>}> : () -> !vhlo.tensor_v1<!vhlo.ui8_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.47222231E-4> : tensor<1x13xf32>>}> : () -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>>}> : () -> !vhlo.tensor_v1<1x1x13x!vhlo.f32_v1>
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<ui8>>}> : () -> !vhlo.tensor_v1<!vhlo.ui8_v1>
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %9 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %12 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>
    %13 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %14 = "vhlo.broadcast_in_dim_v1"(%4) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %15 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %16 = "vhlo.convert_v1"(%arg5) : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x!vhlo.f32_v1>
    %17 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %18 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<1x13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %19 = "vhlo.convert_v1"(%18) : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.ui32_v1>
    %20 = "vhlo.gather_v2"(%arg4, %19) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 2880]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %21 = "vhlo.reshape_v1"(%20) : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %22 = "vhlo.convert_v1"(%21) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %23 = "vhlo.power_v1"(%22, %14) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %24 = "vhlo.reduce_v1"(%23, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg32: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %25 = "vhlo.multiply_v1"(%24, %5) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %26 = "vhlo.reshape_v1"(%25) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %27 = "vhlo.broadcast_in_dim_v1"(%arg2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %28 = "vhlo.add_v1"(%26, %27) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %29 = "vhlo.rsqrt_v2"(%28) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %31 = "vhlo.broadcast_in_dim_v1"(%30) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %32 = "vhlo.multiply_v1"(%22, %31) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %33 = "vhlo.multiply_v1"(%17, %32) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %34 = "vhlo.convert_v1"(%33) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %36 = "vhlo.transpose_v1"(%arg20) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,4096]{0,1}">} : (!vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>
    %37 = "vhlo.dot_general_v2"(%35, %36) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>
    %38 = "vhlo.reshape_v1"(%37) : (!vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%arg19) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>
    %40 = "vhlo.add_v1"(%38, %39) : (!vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>
    %41 = "vhlo.reshape_v1"(%40) : (!vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>
    %42 = "vhlo.transpose_v1"(%41) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,13,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %43 = "vhlo.slice_v1"(%42) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 13, 32]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %44 = "vhlo.convert_v1"(%43) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %45 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x1x!vhlo.f32_v1>
    %46 = "vhlo.dot_general_v2"(%45, %6) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x32x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x13x!vhlo.f32_v1>
    %47 = "vhlo.transpose_v1"(%46) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,13,32]{1,2,0}">} : (!vhlo.tensor_v1<1x32x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %48 = "vhlo.cosine_v2"(%47) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,13,32]{1,2,0}">} : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %49 = "vhlo.broadcast_in_dim_v1"(%arg6) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %50 = "vhlo.multiply_v1"(%48, %49) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %51 = "vhlo.convert_v1"(%50) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>
    %52 = "vhlo.reshape_v1"(%51) : (!vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>
    %53 = "vhlo.convert_v1"(%52) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>
    %54 = "vhlo.reshape_v1"(%53) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %55 = "vhlo.broadcast_in_dim_v1"(%54) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %56 = "vhlo.multiply_v1"(%44, %55) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %58 = "vhlo.slice_v1"(%42) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 13, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 32]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %59 = "vhlo.convert_v1"(%58) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %60 = "vhlo.sine_v2"(%47) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,13,32]{1,2,0}">} : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %61 = "vhlo.multiply_v1"(%60, %49) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %62 = "vhlo.convert_v1"(%61) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>
    %63 = "vhlo.reshape_v1"(%62) : (!vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>
    %64 = "vhlo.convert_v1"(%63) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>
    %65 = "vhlo.reshape_v1"(%64) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %66 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %67 = "vhlo.multiply_v1"(%59, %66) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %68 = "vhlo.convert_v1"(%67) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %69 = "vhlo.subtract_v1"(%57, %68) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %70 = "vhlo.multiply_v1"(%59, %55) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %71 = "vhlo.convert_v1"(%70) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %72 = "vhlo.multiply_v1"(%44, %66) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %73 = "vhlo.convert_v1"(%72) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %74 = "vhlo.add_v1"(%71, %73) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %75 = "vhlo.concatenate_v1"(%69, %74) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %76 = "vhlo.reshape_v1"(%75) : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>
    %77 = "vhlo.transpose_v1"(%arg9) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,512]{0,1}">} : (!vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>
    %78 = "vhlo.dot_general_v2"(%35, %77) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x512x!vhlo.bf16_v1>
    %79 = "vhlo.reshape_v1"(%78) : (!vhlo.tensor_v1<13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %80 = "vhlo.broadcast_in_dim_v1"(%arg8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %81 = "vhlo.add_v1"(%79, %80) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %82 = "vhlo.reshape_v1"(%81) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>
    %83 = "vhlo.transpose_v1"(%82) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,13,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>
    %84 = "vhlo.slice_v1"(%83) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 13, 32]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %85 = "vhlo.convert_v1"(%84) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %86 = "vhlo.broadcast_in_dim_v1"(%54) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %87 = "vhlo.multiply_v1"(%85, %86) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %88 = "vhlo.convert_v1"(%87) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %89 = "vhlo.slice_v1"(%83) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 13, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 32]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %90 = "vhlo.convert_v1"(%89) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %91 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %92 = "vhlo.multiply_v1"(%90, %91) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %93 = "vhlo.convert_v1"(%92) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %94 = "vhlo.subtract_v1"(%88, %93) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %95 = "vhlo.multiply_v1"(%90, %86) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %96 = "vhlo.convert_v1"(%95) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %97 = "vhlo.multiply_v1"(%85, %91) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %98 = "vhlo.convert_v1"(%97) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %99 = "vhlo.add_v1"(%96, %98) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %100 = "vhlo.concatenate_v1"(%94, %99) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>
    %101 = "vhlo.broadcast_in_dim_v1"(%100) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>
    %102 = "vhlo.reshape_v1"(%101) : (!vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %103 = "vhlo.transpose_v1"(%102) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,64,13]{2,3,1,0}">} : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x64x13x!vhlo.bf16_v1>
    %104 = "vhlo.reshape_v1"(%103) : (!vhlo.tensor_v1<1x64x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x64x13x!vhlo.bf16_v1>
    %105 = "vhlo.dot_general_v2"(%76, %104) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<64x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>
    %106 = "vhlo.reshape_v1"(%105) : (!vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %107 = "vhlo.convert_v1"(%106) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>
    %108 = "vhlo.broadcast_in_dim_v1"(%arg18) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>
    %109 = "vhlo.multiply_v1"(%107, %108) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>
    %110 = "vhlo.convert_v1"(%109) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %111 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.i64_v1>
    %112 = "vhlo.broadcast_in_dim_v1"(%arg17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %113 = "vhlo.subtract_v1"(%1, %112) : (!vhlo.tensor_v1<13x!vhlo.i64_v1>, !vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %114 = "vhlo.broadcast_in_dim_v1"(%113) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.i64_v1>
    %115 = "vhlo.compare_v1"(%111, %114) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<13x13x!vhlo.i64_v1>, !vhlo.tensor_v1<13x13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %116 = "vhlo.convert_v1"(%115) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %117 = "vhlo.and_v1"(%116, %13) : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %118 = "vhlo.compare_v1"(%117, %15) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 NE>}> : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %119 = "vhlo.convert_v1"(%118) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %120 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.i64_v1>
    %121 = "vhlo.compare_v1"(%111, %120) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LE>}> : (!vhlo.tensor_v1<13x13x!vhlo.i64_v1>, !vhlo.tensor_v1<13x13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %122 = "vhlo.convert_v1"(%121) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %123 = "vhlo.and_v1"(%119, %122) : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %124 = "vhlo.compare_v1"(%123, %15) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 NE>}> : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %125 = "vhlo.reshape_v1"(%124) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bool_v1>
    %126 = "vhlo.reshape_v1"(%arg16) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.bf16_v1>
    %127 = "vhlo.broadcast_in_dim_v1"(%126) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>
    %128 = "vhlo.select_v1"(%125, %12, %127) : (!vhlo.tensor_v1<1x1x13x13x!vhlo.bool_v1>, !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>
    %129 = "vhlo.reshape_v1"(%128) : (!vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x13x!vhlo.bf16_v1>
    %130 = "vhlo.broadcast_in_dim_v1"(%129) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %131 = "vhlo.add_v1"(%110, %130) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %132 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.bf16_v1>
    %133 = "vhlo.broadcast_in_dim_v1"(%132) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x1x!vhlo.bf16_v1>
    %134 = "vhlo.concatenate_v1"(%131, %133) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %135 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,512]{0,1}">} : (!vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>
    %136 = "vhlo.dot_general_v2"(%35, %135) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x512x!vhlo.bf16_v1>
    %137 = "vhlo.reshape_v1"(%136) : (!vhlo.tensor_v1<13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %138 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %139 = "vhlo.add_v1"(%137, %138) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %140 = "vhlo.reshape_v1"(%139) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>
    %141 = "vhlo.transpose_v1"(%140) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,13,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>
    %142 = "vhlo.convert_v1"(%arg30) : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x!vhlo.f32_v1>
    %143 = "vhlo.broadcast_in_dim_v1"(%142) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %144 = "vhlo.reduce_v1"(%134, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.maximum_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>
    %145 = "vhlo.broadcast_in_dim_v1"(%144) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %146 = "vhlo.subtract_v1"(%134, %145) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %147 = "vhlo.reduce_v1"(%146, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.maximum_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>
    %148 = "vhlo.broadcast_in_dim_v1"(%147) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %149 = "vhlo.subtract_v1"(%146, %148) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %150 = "vhlo.exponential_v2"(%149) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %151 = "vhlo.reduce_v1"(%150, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>
    %152 = "vhlo.broadcast_in_dim_v1"(%151) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %153 = "vhlo.divide_v1"(%150, %152) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %154 = "vhlo.slice_v1"(%153) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 13, 13]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %155 = "vhlo.reshape_v1"(%154) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>
    %156 = "vhlo.broadcast_in_dim_v1"(%141) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>
    %157 = "vhlo.reshape_v1"(%156) : (!vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>
    %158 = "vhlo.dot_general_v2"(%155, %157) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>
    %159 = "vhlo.reshape_v1"(%158) : (!vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %160 = "vhlo.transpose_v1"(%159) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,13,64,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>
    %161 = "vhlo.reshape_v1"(%160) : (!vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>
    %162 = "vhlo.transpose_v1"(%arg14) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[4096,2880]{0,1}">} : (!vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>
    %163 = "vhlo.dot_general_v2"(%161, %162) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>, !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %164 = "vhlo.reshape_v1"(%163) : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %165 = "vhlo.broadcast_in_dim_v1"(%arg13) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %166 = "vhlo.add_v1"(%164, %165) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %167 = "vhlo.add_v1"(%21, %166) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %168 = "vhlo.broadcast_in_dim_v1"(%arg29) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %169 = "vhlo.convert_v1"(%arg21) : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x!vhlo.f32_v1>
    %170 = "vhlo.broadcast_in_dim_v1"(%169) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %171 = "vhlo.convert_v1"(%167) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %172 = "vhlo.power_v1"(%171, %14) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %173 = "vhlo.reduce_v1"(%172, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg32: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %174 = "vhlo.multiply_v1"(%173, %5) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %175 = "vhlo.reshape_v1"(%174) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %176 = "vhlo.add_v1"(%175, %27) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %177 = "vhlo.rsqrt_v2"(%176) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %178 = "vhlo.reshape_v1"(%177) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %179 = "vhlo.broadcast_in_dim_v1"(%178) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %180 = "vhlo.multiply_v1"(%171, %179) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %181 = "vhlo.multiply_v1"(%170, %180) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %182 = "vhlo.convert_v1"(%181) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %183 = "vhlo.reshape_v1"(%182) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %184 = "vhlo.concatenate_v1"(%183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183) <{dimension = #vhlo.integer_v1<0 : i64>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<416x2880x!vhlo.bf16_v1>
    %185 = "vhlo.reshape_v1"(%184) : (!vhlo.tensor_v1<416x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %186 = "vhlo.dot_general_v2"(%185, %arg28) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>
    %187 = "vhlo.broadcast_in_dim_v1"(%arg27) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>
    %188 = "vhlo.add_v1"(%186, %187) : (!vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>
    %189 = "vhlo.slice_v1"(%188) <{limit_indices = #vhlo.tensor_v1<dense<[32, 13, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 1]> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %190 = "vhlo.broadcast_in_dim_v1"(%arg25) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %191 = "vhlo.clamp_v1"(%168, %189, %190) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %192 = "vhlo.add_v1"(%191, %11) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %193 = "vhlo.convert_v1"(%192) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %194 = "vhlo.broadcast_in_dim_v1"(%arg26) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %195 = "vhlo.slice_v1"(%188) <{limit_indices = #vhlo.tensor_v1<dense<[32, 13, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %196 = "vhlo.clamp_v1"(%194, %195, %190) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %197 = "vhlo.convert_v1"(%196) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %198 = "vhlo.broadcast_in_dim_v1"(%arg24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %199 = "vhlo.multiply_v1"(%197, %198) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %200 = "vhlo.convert_v1"(%199) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %201 = "vhlo.logistic_v2"(%200) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %202 = "vhlo.convert_v1"(%201) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %203 = "vhlo.multiply_v1"(%197, %202) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %204 = "vhlo.convert_v1"(%203) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %205 = "vhlo.convert_v1"(%204) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %206 = "vhlo.multiply_v1"(%193, %205) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %207 = "vhlo.convert_v1"(%206) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %208 = "vhlo.dot_general_v2"(%207, %arg23) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %209 = "vhlo.broadcast_in_dim_v1"(%arg22) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %210 = "vhlo.add_v1"(%208, %209) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %211 = "vhlo.reshape_v1"(%210) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>
    %212 = "vhlo.convert_v1"(%211) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>
    %213 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %214 = "vhlo.broadcast_in_dim_v1"(%213) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>
    %215 = "vhlo.transpose_v1"(%arg12) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,32]{0,1}">} : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>
    %216 = "vhlo.dot_general_v2"(%183, %215) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %217 = "vhlo.broadcast_in_dim_v1"(%arg11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %218 = "vhlo.add_v1"(%216, %217) : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %219 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %220 = "vhlo.broadcast_in_dim_v1"(%219) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.i32_v1>
    %221:2 = "vhlo.sort_v1"(%218, %220) <{dimension = #vhlo.integer_v1<1 : i64>, is_stable = #vhlo.bool_v1<false>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg33: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg34: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %260 = "vhlo.compare_v1"(%arg31, %arg32) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x32x!vhlo.i32_v1>) -> (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x32x!vhlo.i32_v1>)
    %222 = "vhlo.slice_v1"(%221#1) <{limit_indices = #vhlo.tensor_v1<dense<[13, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<13x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.i32_v1>
    %223 = "vhlo.convert_v1"(%222) : (!vhlo.tensor_v1<13x4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.i64_v1>
    %224 = "vhlo.reshape_v1"(%223) : (!vhlo.tensor_v1<13x4x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>
    %225 = "vhlo.concatenate_v1"(%214, %224) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>, !vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x4x2x!vhlo.i64_v1>
    %226 = "vhlo.slice_v1"(%221#0) <{limit_indices = #vhlo.tensor_v1<dense<[13, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %227 = "vhlo.reduce_v1"(%226, %3) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.maximum_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x!vhlo.bf16_v1>
    %228 = "vhlo.broadcast_in_dim_v1"(%227) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %229 = "vhlo.subtract_v1"(%226, %228) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %230 = "vhlo.exponential_v2"(%229) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %231 = "vhlo.reduce_v1"(%230, %9) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x!vhlo.bf16_v1>
    %232 = "vhlo.broadcast_in_dim_v1"(%231) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %233 = "vhlo.divide_v1"(%230, %232) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %234 = "vhlo.scatter_v2"(%10, %225, %233) <{index_vector_dim = #vhlo.integer_v1<2 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x4x2x!vhlo.i64_v1>, !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %235 = "vhlo.transpose_v1"(%234) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[32,13]{0,1}">} : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x!vhlo.bf16_v1>
    %236 = "vhlo.reshape_v1"(%235) : (!vhlo.tensor_v1<32x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x1x!vhlo.bf16_v1>
    %237 = "vhlo.convert_v1"(%236) : (!vhlo.tensor_v1<32x1x13x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x1x!vhlo.f32_v1>
    %238 = "vhlo.reshape_v1"(%237) : (!vhlo.tensor_v1<32x1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x!vhlo.f32_v1>
    %239 = "vhlo.broadcast_in_dim_v1"(%238) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>
    %240 = "vhlo.multiply_v1"(%212, %239) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>
    %241 = "vhlo.convert_v1"(%240) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>
    %242 = "vhlo.reduce_v1"(%241, %9) <{dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %243 = "vhlo.add_v1"(%167, %242) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %244 = "vhlo.convert_v1"(%243) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %245 = "vhlo.power_v1"(%244, %14) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %246 = "vhlo.reduce_v1"(%245, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg32: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %247 = "vhlo.multiply_v1"(%246, %5) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %248 = "vhlo.reshape_v1"(%247) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %249 = "vhlo.add_v1"(%248, %27) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %250 = "vhlo.rsqrt_v2"(%249) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %251 = "vhlo.reshape_v1"(%250) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %252 = "vhlo.broadcast_in_dim_v1"(%251) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %253 = "vhlo.multiply_v1"(%244, %252) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %254 = "vhlo.multiply_v1"(%143, %253) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %255 = "vhlo.convert_v1"(%254) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %256 = "vhlo.reshape_v1"(%255) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %257 = "vhlo.transpose_v1"(%arg10) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,201088]{0,1}">} : (!vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x201088x!vhlo.bf16_v1>
    %258 = "vhlo.dot_general_v2"(%256, %257) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x201088x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>
    %259 = "vhlo.reshape_v1"(%258) : (!vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x201088x!vhlo.bf16_v1>
    "vhlo.return_v1"(%139, %140, %141, %100, %258, %259) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x201088x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
// -----// IR Dump Before VhloToVersionPass (vhlo-to-version) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<512x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg3: !vhlo.tensor_v1<1x13x!vhlo.i64_v1>, %arg4: !vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg7: !vhlo.tensor_v1<32x!vhlo.f32_v1>, %arg8: !vhlo.tensor_v1<512x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<32x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>, %arg15: !vhlo.tensor_v1<64x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg17: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg18: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg19: !vhlo.tensor_v1<4096x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>, %arg21: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg23: !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg25: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg26: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg27: !vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>, %arg28: !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>, %arg29: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg30: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x201088x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>>}> : () -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<ui8>>}> : () -> !vhlo.tensor_v1<!vhlo.ui8_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.47222231E-4> : tensor<1x13xf32>>}> : () -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>>}> : () -> !vhlo.tensor_v1<1x1x13x!vhlo.f32_v1>
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<ui8>>}> : () -> !vhlo.tensor_v1<!vhlo.ui8_v1>
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %9 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %12 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>
    %13 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %14 = "vhlo.broadcast_in_dim_v1"(%4) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %15 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %16 = "vhlo.convert_v1"(%arg5) : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x!vhlo.f32_v1>
    %17 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %18 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<1x13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %19 = "vhlo.convert_v1"(%18) : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.ui32_v1>
    %20 = "vhlo.gather_v2"(%arg4, %19) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 2880]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %21 = "vhlo.reshape_v1"(%20) : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %22 = "vhlo.convert_v1"(%21) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %23 = "vhlo.power_v1"(%22, %14) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %24 = "vhlo.reduce_v1"(%23, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg32: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %25 = "vhlo.multiply_v1"(%24, %5) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %26 = "vhlo.reshape_v1"(%25) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %27 = "vhlo.broadcast_in_dim_v1"(%arg2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %28 = "vhlo.add_v1"(%26, %27) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %29 = "vhlo.rsqrt_v2"(%28) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %31 = "vhlo.broadcast_in_dim_v1"(%30) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %32 = "vhlo.multiply_v1"(%22, %31) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %33 = "vhlo.multiply_v1"(%17, %32) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %34 = "vhlo.convert_v1"(%33) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %36 = "vhlo.transpose_v1"(%arg20) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,4096]{0,1}">} : (!vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>
    %37 = "vhlo.dot_general_v2"(%35, %36) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>
    %38 = "vhlo.reshape_v1"(%37) : (!vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%arg19) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>
    %40 = "vhlo.add_v1"(%38, %39) : (!vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>
    %41 = "vhlo.reshape_v1"(%40) : (!vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>
    %42 = "vhlo.transpose_v1"(%41) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,13,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %43 = "vhlo.slice_v1"(%42) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 13, 32]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %44 = "vhlo.convert_v1"(%43) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %45 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x1x!vhlo.f32_v1>
    %46 = "vhlo.dot_general_v2"(%45, %6) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x32x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x13x!vhlo.f32_v1>
    %47 = "vhlo.transpose_v1"(%46) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,13,32]{1,2,0}">} : (!vhlo.tensor_v1<1x32x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %48 = "vhlo.cosine_v2"(%47) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,13,32]{1,2,0}">} : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %49 = "vhlo.broadcast_in_dim_v1"(%arg6) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %50 = "vhlo.multiply_v1"(%48, %49) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %51 = "vhlo.convert_v1"(%50) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>
    %52 = "vhlo.reshape_v1"(%51) : (!vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>
    %53 = "vhlo.convert_v1"(%52) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>
    %54 = "vhlo.reshape_v1"(%53) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %55 = "vhlo.broadcast_in_dim_v1"(%54) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %56 = "vhlo.multiply_v1"(%44, %55) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %58 = "vhlo.slice_v1"(%42) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 13, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 32]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %59 = "vhlo.convert_v1"(%58) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %60 = "vhlo.sine_v2"(%47) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,13,32]{1,2,0}">} : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %61 = "vhlo.multiply_v1"(%60, %49) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %62 = "vhlo.convert_v1"(%61) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>
    %63 = "vhlo.reshape_v1"(%62) : (!vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>
    %64 = "vhlo.convert_v1"(%63) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>
    %65 = "vhlo.reshape_v1"(%64) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %66 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %67 = "vhlo.multiply_v1"(%59, %66) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %68 = "vhlo.convert_v1"(%67) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %69 = "vhlo.subtract_v1"(%57, %68) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %70 = "vhlo.multiply_v1"(%59, %55) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %71 = "vhlo.convert_v1"(%70) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %72 = "vhlo.multiply_v1"(%44, %66) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %73 = "vhlo.convert_v1"(%72) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %74 = "vhlo.add_v1"(%71, %73) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %75 = "vhlo.concatenate_v1"(%69, %74) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %76 = "vhlo.reshape_v1"(%75) : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>
    %77 = "vhlo.transpose_v1"(%arg9) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,512]{0,1}">} : (!vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>
    %78 = "vhlo.dot_general_v2"(%35, %77) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x512x!vhlo.bf16_v1>
    %79 = "vhlo.reshape_v1"(%78) : (!vhlo.tensor_v1<13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %80 = "vhlo.broadcast_in_dim_v1"(%arg8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %81 = "vhlo.add_v1"(%79, %80) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %82 = "vhlo.reshape_v1"(%81) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>
    %83 = "vhlo.transpose_v1"(%82) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,13,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>
    %84 = "vhlo.slice_v1"(%83) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 13, 32]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %85 = "vhlo.convert_v1"(%84) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %86 = "vhlo.broadcast_in_dim_v1"(%54) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %87 = "vhlo.multiply_v1"(%85, %86) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %88 = "vhlo.convert_v1"(%87) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %89 = "vhlo.slice_v1"(%83) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 13, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 32]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %90 = "vhlo.convert_v1"(%89) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %91 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %92 = "vhlo.multiply_v1"(%90, %91) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %93 = "vhlo.convert_v1"(%92) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %94 = "vhlo.subtract_v1"(%88, %93) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %95 = "vhlo.multiply_v1"(%90, %86) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %96 = "vhlo.convert_v1"(%95) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %97 = "vhlo.multiply_v1"(%85, %91) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %98 = "vhlo.convert_v1"(%97) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %99 = "vhlo.add_v1"(%96, %98) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %100 = "vhlo.concatenate_v1"(%94, %99) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>
    %101 = "vhlo.broadcast_in_dim_v1"(%100) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>
    %102 = "vhlo.reshape_v1"(%101) : (!vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %103 = "vhlo.transpose_v1"(%102) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,64,13]{2,3,1,0}">} : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x64x13x!vhlo.bf16_v1>
    %104 = "vhlo.reshape_v1"(%103) : (!vhlo.tensor_v1<1x64x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x64x13x!vhlo.bf16_v1>
    %105 = "vhlo.dot_general_v2"(%76, %104) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<64x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>
    %106 = "vhlo.reshape_v1"(%105) : (!vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %107 = "vhlo.convert_v1"(%106) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>
    %108 = "vhlo.broadcast_in_dim_v1"(%arg18) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>
    %109 = "vhlo.multiply_v1"(%107, %108) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>
    %110 = "vhlo.convert_v1"(%109) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %111 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.i64_v1>
    %112 = "vhlo.broadcast_in_dim_v1"(%arg17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %113 = "vhlo.subtract_v1"(%1, %112) : (!vhlo.tensor_v1<13x!vhlo.i64_v1>, !vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %114 = "vhlo.broadcast_in_dim_v1"(%113) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.i64_v1>
    %115 = "vhlo.compare_v1"(%111, %114) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<13x13x!vhlo.i64_v1>, !vhlo.tensor_v1<13x13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %116 = "vhlo.convert_v1"(%115) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %117 = "vhlo.and_v1"(%116, %13) : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %118 = "vhlo.compare_v1"(%117, %15) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 NE>}> : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %119 = "vhlo.convert_v1"(%118) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %120 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.i64_v1>
    %121 = "vhlo.compare_v1"(%111, %120) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LE>}> : (!vhlo.tensor_v1<13x13x!vhlo.i64_v1>, !vhlo.tensor_v1<13x13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %122 = "vhlo.convert_v1"(%121) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %123 = "vhlo.and_v1"(%119, %122) : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %124 = "vhlo.compare_v1"(%123, %15) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 NE>}> : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %125 = "vhlo.reshape_v1"(%124) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bool_v1>
    %126 = "vhlo.reshape_v1"(%arg16) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.bf16_v1>
    %127 = "vhlo.broadcast_in_dim_v1"(%126) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>
    %128 = "vhlo.select_v1"(%125, %12, %127) : (!vhlo.tensor_v1<1x1x13x13x!vhlo.bool_v1>, !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>
    %129 = "vhlo.reshape_v1"(%128) : (!vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x13x!vhlo.bf16_v1>
    %130 = "vhlo.broadcast_in_dim_v1"(%129) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %131 = "vhlo.add_v1"(%110, %130) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %132 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.bf16_v1>
    %133 = "vhlo.broadcast_in_dim_v1"(%132) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x1x!vhlo.bf16_v1>
    %134 = "vhlo.concatenate_v1"(%131, %133) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %135 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,512]{0,1}">} : (!vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>
    %136 = "vhlo.dot_general_v2"(%35, %135) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x512x!vhlo.bf16_v1>
    %137 = "vhlo.reshape_v1"(%136) : (!vhlo.tensor_v1<13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %138 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %139 = "vhlo.add_v1"(%137, %138) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %140 = "vhlo.reshape_v1"(%139) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>
    %141 = "vhlo.transpose_v1"(%140) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,13,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>
    %142 = "vhlo.convert_v1"(%arg30) : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x!vhlo.f32_v1>
    %143 = "vhlo.broadcast_in_dim_v1"(%142) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %144 = "vhlo.reduce_v1"(%134, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.maximum_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>
    %145 = "vhlo.broadcast_in_dim_v1"(%144) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %146 = "vhlo.subtract_v1"(%134, %145) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %147 = "vhlo.reduce_v1"(%146, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.maximum_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>
    %148 = "vhlo.broadcast_in_dim_v1"(%147) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %149 = "vhlo.subtract_v1"(%146, %148) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %150 = "vhlo.exponential_v2"(%149) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %151 = "vhlo.reduce_v1"(%150, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>
    %152 = "vhlo.broadcast_in_dim_v1"(%151) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %153 = "vhlo.divide_v1"(%150, %152) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %154 = "vhlo.slice_v1"(%153) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 13, 13]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %155 = "vhlo.reshape_v1"(%154) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>
    %156 = "vhlo.broadcast_in_dim_v1"(%141) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>
    %157 = "vhlo.reshape_v1"(%156) : (!vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>
    %158 = "vhlo.dot_general_v2"(%155, %157) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>
    %159 = "vhlo.reshape_v1"(%158) : (!vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %160 = "vhlo.transpose_v1"(%159) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,13,64,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>
    %161 = "vhlo.reshape_v1"(%160) : (!vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>
    %162 = "vhlo.transpose_v1"(%arg14) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[4096,2880]{0,1}">} : (!vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>
    %163 = "vhlo.dot_general_v2"(%161, %162) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>, !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %164 = "vhlo.reshape_v1"(%163) : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %165 = "vhlo.broadcast_in_dim_v1"(%arg13) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %166 = "vhlo.add_v1"(%164, %165) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %167 = "vhlo.add_v1"(%21, %166) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %168 = "vhlo.broadcast_in_dim_v1"(%arg29) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %169 = "vhlo.convert_v1"(%arg21) : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x!vhlo.f32_v1>
    %170 = "vhlo.broadcast_in_dim_v1"(%169) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %171 = "vhlo.convert_v1"(%167) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %172 = "vhlo.power_v1"(%171, %14) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %173 = "vhlo.reduce_v1"(%172, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg32: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %174 = "vhlo.multiply_v1"(%173, %5) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %175 = "vhlo.reshape_v1"(%174) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %176 = "vhlo.add_v1"(%175, %27) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %177 = "vhlo.rsqrt_v2"(%176) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %178 = "vhlo.reshape_v1"(%177) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %179 = "vhlo.broadcast_in_dim_v1"(%178) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %180 = "vhlo.multiply_v1"(%171, %179) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %181 = "vhlo.multiply_v1"(%170, %180) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %182 = "vhlo.convert_v1"(%181) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %183 = "vhlo.reshape_v1"(%182) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %184 = "vhlo.concatenate_v1"(%183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183) <{dimension = #vhlo.integer_v1<0 : i64>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<416x2880x!vhlo.bf16_v1>
    %185 = "vhlo.reshape_v1"(%184) : (!vhlo.tensor_v1<416x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %186 = "vhlo.dot_general_v2"(%185, %arg28) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>
    %187 = "vhlo.broadcast_in_dim_v1"(%arg27) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>
    %188 = "vhlo.add_v1"(%186, %187) : (!vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>
    %189 = "vhlo.slice_v1"(%188) <{limit_indices = #vhlo.tensor_v1<dense<[32, 13, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 1]> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %190 = "vhlo.broadcast_in_dim_v1"(%arg25) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %191 = "vhlo.clamp_v1"(%168, %189, %190) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %192 = "vhlo.add_v1"(%191, %11) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %193 = "vhlo.convert_v1"(%192) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %194 = "vhlo.broadcast_in_dim_v1"(%arg26) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %195 = "vhlo.slice_v1"(%188) <{limit_indices = #vhlo.tensor_v1<dense<[32, 13, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %196 = "vhlo.clamp_v1"(%194, %195, %190) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %197 = "vhlo.convert_v1"(%196) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %198 = "vhlo.broadcast_in_dim_v1"(%arg24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %199 = "vhlo.multiply_v1"(%197, %198) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %200 = "vhlo.convert_v1"(%199) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %201 = "vhlo.logistic_v2"(%200) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %202 = "vhlo.convert_v1"(%201) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %203 = "vhlo.multiply_v1"(%197, %202) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %204 = "vhlo.convert_v1"(%203) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %205 = "vhlo.convert_v1"(%204) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %206 = "vhlo.multiply_v1"(%193, %205) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %207 = "vhlo.convert_v1"(%206) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %208 = "vhlo.dot_general_v2"(%207, %arg23) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %209 = "vhlo.broadcast_in_dim_v1"(%arg22) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %210 = "vhlo.add_v1"(%208, %209) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %211 = "vhlo.reshape_v1"(%210) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>
    %212 = "vhlo.convert_v1"(%211) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>
    %213 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %214 = "vhlo.broadcast_in_dim_v1"(%213) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>
    %215 = "vhlo.transpose_v1"(%arg12) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,32]{0,1}">} : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>
    %216 = "vhlo.dot_general_v2"(%183, %215) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %217 = "vhlo.broadcast_in_dim_v1"(%arg11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %218 = "vhlo.add_v1"(%216, %217) : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %219 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %220 = "vhlo.broadcast_in_dim_v1"(%219) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.i32_v1>
    %221:2 = "vhlo.sort_v1"(%218, %220) <{dimension = #vhlo.integer_v1<1 : i64>, is_stable = #vhlo.bool_v1<false>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg33: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg34: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %260 = "vhlo.compare_v1"(%arg31, %arg32) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x32x!vhlo.i32_v1>) -> (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x32x!vhlo.i32_v1>)
    %222 = "vhlo.slice_v1"(%221#1) <{limit_indices = #vhlo.tensor_v1<dense<[13, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<13x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.i32_v1>
    %223 = "vhlo.convert_v1"(%222) : (!vhlo.tensor_v1<13x4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.i64_v1>
    %224 = "vhlo.reshape_v1"(%223) : (!vhlo.tensor_v1<13x4x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>
    %225 = "vhlo.concatenate_v1"(%214, %224) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>, !vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x4x2x!vhlo.i64_v1>
    %226 = "vhlo.slice_v1"(%221#0) <{limit_indices = #vhlo.tensor_v1<dense<[13, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %227 = "vhlo.reduce_v1"(%226, %3) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.maximum_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x!vhlo.bf16_v1>
    %228 = "vhlo.broadcast_in_dim_v1"(%227) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %229 = "vhlo.subtract_v1"(%226, %228) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %230 = "vhlo.exponential_v2"(%229) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %231 = "vhlo.reduce_v1"(%230, %9) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x!vhlo.bf16_v1>
    %232 = "vhlo.broadcast_in_dim_v1"(%231) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %233 = "vhlo.divide_v1"(%230, %232) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %234 = "vhlo.scatter_v2"(%10, %225, %233) <{index_vector_dim = #vhlo.integer_v1<2 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x4x2x!vhlo.i64_v1>, !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %235 = "vhlo.transpose_v1"(%234) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[32,13]{0,1}">} : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x!vhlo.bf16_v1>
    %236 = "vhlo.reshape_v1"(%235) : (!vhlo.tensor_v1<32x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x1x!vhlo.bf16_v1>
    %237 = "vhlo.convert_v1"(%236) : (!vhlo.tensor_v1<32x1x13x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x1x!vhlo.f32_v1>
    %238 = "vhlo.reshape_v1"(%237) : (!vhlo.tensor_v1<32x1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x!vhlo.f32_v1>
    %239 = "vhlo.broadcast_in_dim_v1"(%238) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>
    %240 = "vhlo.multiply_v1"(%212, %239) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>
    %241 = "vhlo.convert_v1"(%240) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>
    %242 = "vhlo.reduce_v1"(%241, %9) <{dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %243 = "vhlo.add_v1"(%167, %242) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %244 = "vhlo.convert_v1"(%243) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %245 = "vhlo.power_v1"(%244, %14) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %246 = "vhlo.reduce_v1"(%245, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg32: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %247 = "vhlo.multiply_v1"(%246, %5) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %248 = "vhlo.reshape_v1"(%247) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %249 = "vhlo.add_v1"(%248, %27) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %250 = "vhlo.rsqrt_v2"(%249) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %251 = "vhlo.reshape_v1"(%250) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %252 = "vhlo.broadcast_in_dim_v1"(%251) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %253 = "vhlo.multiply_v1"(%244, %252) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %254 = "vhlo.multiply_v1"(%143, %253) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %255 = "vhlo.convert_v1"(%254) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %256 = "vhlo.reshape_v1"(%255) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %257 = "vhlo.transpose_v1"(%arg10) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,201088]{0,1}">} : (!vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x201088x!vhlo.bf16_v1>
    %258 = "vhlo.dot_general_v2"(%256, %257) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x201088x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>
    %259 = "vhlo.reshape_v1"(%258) : (!vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x201088x!vhlo.bf16_v1>
    "vhlo.return_v1"(%139, %140, %141, %100, %258, %259) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x201088x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump Before VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<512x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg3: !vhlo.tensor_v1<1x13x!vhlo.i64_v1>, %arg4: !vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg7: !vhlo.tensor_v1<32x!vhlo.f32_v1>, %arg8: !vhlo.tensor_v1<512x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<32x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>, %arg15: !vhlo.tensor_v1<64x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg17: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg18: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg19: !vhlo.tensor_v1<4096x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>, %arg21: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg23: !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg25: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg26: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg27: !vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>, %arg28: !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>, %arg29: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg30: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x201088x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>>}> : () -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<ui8>>}> : () -> !vhlo.tensor_v1<!vhlo.ui8_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.47222231E-4> : tensor<1x13xf32>>}> : () -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>>}> : () -> !vhlo.tensor_v1<1x1x13x!vhlo.f32_v1>
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<ui8>>}> : () -> !vhlo.tensor_v1<!vhlo.ui8_v1>
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %9 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %12 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>
    %13 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %14 = "vhlo.broadcast_in_dim_v1"(%4) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %15 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %16 = "vhlo.convert_v1"(%arg5) : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x!vhlo.f32_v1>
    %17 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %18 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<1x13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %19 = "vhlo.convert_v1"(%18) : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.ui32_v1>
    %20 = "vhlo.gather_v2"(%arg4, %19) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 2880]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %21 = "vhlo.reshape_v1"(%20) : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %22 = "vhlo.convert_v1"(%21) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %23 = "vhlo.power_v1"(%22, %14) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %24 = "vhlo.reduce_v1"(%23, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg32: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %25 = "vhlo.multiply_v1"(%24, %5) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %26 = "vhlo.reshape_v1"(%25) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %27 = "vhlo.broadcast_in_dim_v1"(%arg2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %28 = "vhlo.add_v1"(%26, %27) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %29 = "vhlo.rsqrt_v2"(%28) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %31 = "vhlo.broadcast_in_dim_v1"(%30) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %32 = "vhlo.multiply_v1"(%22, %31) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %33 = "vhlo.multiply_v1"(%17, %32) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %34 = "vhlo.convert_v1"(%33) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %36 = "vhlo.transpose_v1"(%arg20) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,4096]{0,1}">} : (!vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>
    %37 = "vhlo.dot_general_v2"(%35, %36) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>
    %38 = "vhlo.reshape_v1"(%37) : (!vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%arg19) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>
    %40 = "vhlo.add_v1"(%38, %39) : (!vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>
    %41 = "vhlo.reshape_v1"(%40) : (!vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>
    %42 = "vhlo.transpose_v1"(%41) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,13,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %43 = "vhlo.slice_v1"(%42) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 13, 32]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %44 = "vhlo.convert_v1"(%43) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %45 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x1x!vhlo.f32_v1>
    %46 = "vhlo.dot_general_v2"(%45, %6) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x32x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x13x!vhlo.f32_v1>
    %47 = "vhlo.transpose_v1"(%46) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,13,32]{1,2,0}">} : (!vhlo.tensor_v1<1x32x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %48 = "vhlo.cosine_v2"(%47) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,13,32]{1,2,0}">} : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %49 = "vhlo.broadcast_in_dim_v1"(%arg6) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %50 = "vhlo.multiply_v1"(%48, %49) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %51 = "vhlo.convert_v1"(%50) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>
    %52 = "vhlo.reshape_v1"(%51) : (!vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>
    %53 = "vhlo.convert_v1"(%52) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>
    %54 = "vhlo.reshape_v1"(%53) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %55 = "vhlo.broadcast_in_dim_v1"(%54) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %56 = "vhlo.multiply_v1"(%44, %55) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %58 = "vhlo.slice_v1"(%42) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 13, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 32]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %59 = "vhlo.convert_v1"(%58) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %60 = "vhlo.sine_v2"(%47) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,13,32]{1,2,0}">} : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %61 = "vhlo.multiply_v1"(%60, %49) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %62 = "vhlo.convert_v1"(%61) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>
    %63 = "vhlo.reshape_v1"(%62) : (!vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>
    %64 = "vhlo.convert_v1"(%63) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>
    %65 = "vhlo.reshape_v1"(%64) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %66 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %67 = "vhlo.multiply_v1"(%59, %66) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %68 = "vhlo.convert_v1"(%67) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %69 = "vhlo.subtract_v1"(%57, %68) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %70 = "vhlo.multiply_v1"(%59, %55) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %71 = "vhlo.convert_v1"(%70) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %72 = "vhlo.multiply_v1"(%44, %66) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %73 = "vhlo.convert_v1"(%72) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %74 = "vhlo.add_v1"(%71, %73) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %75 = "vhlo.concatenate_v1"(%69, %74) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %76 = "vhlo.reshape_v1"(%75) : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>
    %77 = "vhlo.transpose_v1"(%arg9) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,512]{0,1}">} : (!vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>
    %78 = "vhlo.dot_general_v2"(%35, %77) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x512x!vhlo.bf16_v1>
    %79 = "vhlo.reshape_v1"(%78) : (!vhlo.tensor_v1<13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %80 = "vhlo.broadcast_in_dim_v1"(%arg8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %81 = "vhlo.add_v1"(%79, %80) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %82 = "vhlo.reshape_v1"(%81) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>
    %83 = "vhlo.transpose_v1"(%82) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,13,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>
    %84 = "vhlo.slice_v1"(%83) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 13, 32]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %85 = "vhlo.convert_v1"(%84) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %86 = "vhlo.broadcast_in_dim_v1"(%54) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %87 = "vhlo.multiply_v1"(%85, %86) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %88 = "vhlo.convert_v1"(%87) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %89 = "vhlo.slice_v1"(%83) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 13, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 32]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %90 = "vhlo.convert_v1"(%89) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %91 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %92 = "vhlo.multiply_v1"(%90, %91) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %93 = "vhlo.convert_v1"(%92) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %94 = "vhlo.subtract_v1"(%88, %93) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %95 = "vhlo.multiply_v1"(%90, %86) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %96 = "vhlo.convert_v1"(%95) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %97 = "vhlo.multiply_v1"(%85, %91) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %98 = "vhlo.convert_v1"(%97) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %99 = "vhlo.add_v1"(%96, %98) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %100 = "vhlo.concatenate_v1"(%94, %99) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>
    %101 = "vhlo.broadcast_in_dim_v1"(%100) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>
    %102 = "vhlo.reshape_v1"(%101) : (!vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %103 = "vhlo.transpose_v1"(%102) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,64,13]{2,3,1,0}">} : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x64x13x!vhlo.bf16_v1>
    %104 = "vhlo.reshape_v1"(%103) : (!vhlo.tensor_v1<1x64x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x64x13x!vhlo.bf16_v1>
    %105 = "vhlo.dot_general_v2"(%76, %104) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<64x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>
    %106 = "vhlo.reshape_v1"(%105) : (!vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %107 = "vhlo.convert_v1"(%106) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>
    %108 = "vhlo.broadcast_in_dim_v1"(%arg18) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>
    %109 = "vhlo.multiply_v1"(%107, %108) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>
    %110 = "vhlo.convert_v1"(%109) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %111 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.i64_v1>
    %112 = "vhlo.broadcast_in_dim_v1"(%arg17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %113 = "vhlo.subtract_v1"(%1, %112) : (!vhlo.tensor_v1<13x!vhlo.i64_v1>, !vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %114 = "vhlo.broadcast_in_dim_v1"(%113) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.i64_v1>
    %115 = "vhlo.compare_v1"(%111, %114) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<13x13x!vhlo.i64_v1>, !vhlo.tensor_v1<13x13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %116 = "vhlo.convert_v1"(%115) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %117 = "vhlo.and_v1"(%116, %13) : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %118 = "vhlo.compare_v1"(%117, %15) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 NE>}> : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %119 = "vhlo.convert_v1"(%118) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %120 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.i64_v1>
    %121 = "vhlo.compare_v1"(%111, %120) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LE>}> : (!vhlo.tensor_v1<13x13x!vhlo.i64_v1>, !vhlo.tensor_v1<13x13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %122 = "vhlo.convert_v1"(%121) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %123 = "vhlo.and_v1"(%119, %122) : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %124 = "vhlo.compare_v1"(%123, %15) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 NE>}> : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %125 = "vhlo.reshape_v1"(%124) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bool_v1>
    %126 = "vhlo.reshape_v1"(%arg16) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.bf16_v1>
    %127 = "vhlo.broadcast_in_dim_v1"(%126) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>
    %128 = "vhlo.select_v1"(%125, %12, %127) : (!vhlo.tensor_v1<1x1x13x13x!vhlo.bool_v1>, !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>
    %129 = "vhlo.reshape_v1"(%128) : (!vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x13x!vhlo.bf16_v1>
    %130 = "vhlo.broadcast_in_dim_v1"(%129) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %131 = "vhlo.add_v1"(%110, %130) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %132 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.bf16_v1>
    %133 = "vhlo.broadcast_in_dim_v1"(%132) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x1x!vhlo.bf16_v1>
    %134 = "vhlo.concatenate_v1"(%131, %133) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %135 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,512]{0,1}">} : (!vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>
    %136 = "vhlo.dot_general_v2"(%35, %135) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x512x!vhlo.bf16_v1>
    %137 = "vhlo.reshape_v1"(%136) : (!vhlo.tensor_v1<13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %138 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %139 = "vhlo.add_v1"(%137, %138) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %140 = "vhlo.reshape_v1"(%139) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>
    %141 = "vhlo.transpose_v1"(%140) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,13,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>
    %142 = "vhlo.convert_v1"(%arg30) : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x!vhlo.f32_v1>
    %143 = "vhlo.broadcast_in_dim_v1"(%142) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %144 = "vhlo.reduce_v1"(%134, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.maximum_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>
    %145 = "vhlo.broadcast_in_dim_v1"(%144) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %146 = "vhlo.subtract_v1"(%134, %145) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %147 = "vhlo.reduce_v1"(%146, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.maximum_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>
    %148 = "vhlo.broadcast_in_dim_v1"(%147) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %149 = "vhlo.subtract_v1"(%146, %148) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %150 = "vhlo.exponential_v2"(%149) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %151 = "vhlo.reduce_v1"(%150, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>
    %152 = "vhlo.broadcast_in_dim_v1"(%151) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %153 = "vhlo.divide_v1"(%150, %152) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %154 = "vhlo.slice_v1"(%153) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 13, 13]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %155 = "vhlo.reshape_v1"(%154) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>
    %156 = "vhlo.broadcast_in_dim_v1"(%141) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>
    %157 = "vhlo.reshape_v1"(%156) : (!vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>
    %158 = "vhlo.dot_general_v2"(%155, %157) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>
    %159 = "vhlo.reshape_v1"(%158) : (!vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %160 = "vhlo.transpose_v1"(%159) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,13,64,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>
    %161 = "vhlo.reshape_v1"(%160) : (!vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>
    %162 = "vhlo.transpose_v1"(%arg14) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[4096,2880]{0,1}">} : (!vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>
    %163 = "vhlo.dot_general_v2"(%161, %162) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>, !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %164 = "vhlo.reshape_v1"(%163) : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %165 = "vhlo.broadcast_in_dim_v1"(%arg13) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %166 = "vhlo.add_v1"(%164, %165) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %167 = "vhlo.add_v1"(%21, %166) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %168 = "vhlo.broadcast_in_dim_v1"(%arg29) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %169 = "vhlo.convert_v1"(%arg21) : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x!vhlo.f32_v1>
    %170 = "vhlo.broadcast_in_dim_v1"(%169) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %171 = "vhlo.convert_v1"(%167) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %172 = "vhlo.power_v1"(%171, %14) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %173 = "vhlo.reduce_v1"(%172, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg32: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %174 = "vhlo.multiply_v1"(%173, %5) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %175 = "vhlo.reshape_v1"(%174) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %176 = "vhlo.add_v1"(%175, %27) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %177 = "vhlo.rsqrt_v2"(%176) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %178 = "vhlo.reshape_v1"(%177) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %179 = "vhlo.broadcast_in_dim_v1"(%178) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %180 = "vhlo.multiply_v1"(%171, %179) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %181 = "vhlo.multiply_v1"(%170, %180) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %182 = "vhlo.convert_v1"(%181) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %183 = "vhlo.reshape_v1"(%182) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %184 = "vhlo.concatenate_v1"(%183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183) <{dimension = #vhlo.integer_v1<0 : i64>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<416x2880x!vhlo.bf16_v1>
    %185 = "vhlo.reshape_v1"(%184) : (!vhlo.tensor_v1<416x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %186 = "vhlo.dot_general_v2"(%185, %arg28) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>
    %187 = "vhlo.broadcast_in_dim_v1"(%arg27) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>
    %188 = "vhlo.add_v1"(%186, %187) : (!vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>
    %189 = "vhlo.slice_v1"(%188) <{limit_indices = #vhlo.tensor_v1<dense<[32, 13, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 1]> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %190 = "vhlo.broadcast_in_dim_v1"(%arg25) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %191 = "vhlo.clamp_v1"(%168, %189, %190) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %192 = "vhlo.add_v1"(%191, %11) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %193 = "vhlo.convert_v1"(%192) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %194 = "vhlo.broadcast_in_dim_v1"(%arg26) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %195 = "vhlo.slice_v1"(%188) <{limit_indices = #vhlo.tensor_v1<dense<[32, 13, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %196 = "vhlo.clamp_v1"(%194, %195, %190) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %197 = "vhlo.convert_v1"(%196) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %198 = "vhlo.broadcast_in_dim_v1"(%arg24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %199 = "vhlo.multiply_v1"(%197, %198) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %200 = "vhlo.convert_v1"(%199) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %201 = "vhlo.logistic_v2"(%200) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %202 = "vhlo.convert_v1"(%201) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %203 = "vhlo.multiply_v1"(%197, %202) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %204 = "vhlo.convert_v1"(%203) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %205 = "vhlo.convert_v1"(%204) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %206 = "vhlo.multiply_v1"(%193, %205) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %207 = "vhlo.convert_v1"(%206) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %208 = "vhlo.dot_general_v2"(%207, %arg23) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %209 = "vhlo.broadcast_in_dim_v1"(%arg22) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %210 = "vhlo.add_v1"(%208, %209) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %211 = "vhlo.reshape_v1"(%210) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>
    %212 = "vhlo.convert_v1"(%211) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>
    %213 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %214 = "vhlo.broadcast_in_dim_v1"(%213) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>
    %215 = "vhlo.transpose_v1"(%arg12) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,32]{0,1}">} : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>
    %216 = "vhlo.dot_general_v2"(%183, %215) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %217 = "vhlo.broadcast_in_dim_v1"(%arg11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %218 = "vhlo.add_v1"(%216, %217) : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %219 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %220 = "vhlo.broadcast_in_dim_v1"(%219) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.i32_v1>
    %221:2 = "vhlo.sort_v1"(%218, %220) <{dimension = #vhlo.integer_v1<1 : i64>, is_stable = #vhlo.bool_v1<false>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg33: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg34: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %260 = "vhlo.compare_v1"(%arg31, %arg32) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x32x!vhlo.i32_v1>) -> (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x32x!vhlo.i32_v1>)
    %222 = "vhlo.slice_v1"(%221#1) <{limit_indices = #vhlo.tensor_v1<dense<[13, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<13x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.i32_v1>
    %223 = "vhlo.convert_v1"(%222) : (!vhlo.tensor_v1<13x4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.i64_v1>
    %224 = "vhlo.reshape_v1"(%223) : (!vhlo.tensor_v1<13x4x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>
    %225 = "vhlo.concatenate_v1"(%214, %224) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>, !vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x4x2x!vhlo.i64_v1>
    %226 = "vhlo.slice_v1"(%221#0) <{limit_indices = #vhlo.tensor_v1<dense<[13, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %227 = "vhlo.reduce_v1"(%226, %3) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.maximum_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x!vhlo.bf16_v1>
    %228 = "vhlo.broadcast_in_dim_v1"(%227) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %229 = "vhlo.subtract_v1"(%226, %228) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %230 = "vhlo.exponential_v2"(%229) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %231 = "vhlo.reduce_v1"(%230, %9) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x!vhlo.bf16_v1>
    %232 = "vhlo.broadcast_in_dim_v1"(%231) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %233 = "vhlo.divide_v1"(%230, %232) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %234 = "vhlo.scatter_v2"(%10, %225, %233) <{index_vector_dim = #vhlo.integer_v1<2 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x4x2x!vhlo.i64_v1>, !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %235 = "vhlo.transpose_v1"(%234) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[32,13]{0,1}">} : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x!vhlo.bf16_v1>
    %236 = "vhlo.reshape_v1"(%235) : (!vhlo.tensor_v1<32x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x1x!vhlo.bf16_v1>
    %237 = "vhlo.convert_v1"(%236) : (!vhlo.tensor_v1<32x1x13x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x1x!vhlo.f32_v1>
    %238 = "vhlo.reshape_v1"(%237) : (!vhlo.tensor_v1<32x1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x!vhlo.f32_v1>
    %239 = "vhlo.broadcast_in_dim_v1"(%238) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>
    %240 = "vhlo.multiply_v1"(%212, %239) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>
    %241 = "vhlo.convert_v1"(%240) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>
    %242 = "vhlo.reduce_v1"(%241, %9) <{dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %243 = "vhlo.add_v1"(%167, %242) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %244 = "vhlo.convert_v1"(%243) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %245 = "vhlo.power_v1"(%244, %14) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %246 = "vhlo.reduce_v1"(%245, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg32: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %247 = "vhlo.multiply_v1"(%246, %5) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %248 = "vhlo.reshape_v1"(%247) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %249 = "vhlo.add_v1"(%248, %27) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %250 = "vhlo.rsqrt_v2"(%249) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %251 = "vhlo.reshape_v1"(%250) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %252 = "vhlo.broadcast_in_dim_v1"(%251) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %253 = "vhlo.multiply_v1"(%244, %252) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %254 = "vhlo.multiply_v1"(%143, %253) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %255 = "vhlo.convert_v1"(%254) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %256 = "vhlo.reshape_v1"(%255) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %257 = "vhlo.transpose_v1"(%arg10) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,201088]{0,1}">} : (!vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x201088x!vhlo.bf16_v1>
    %258 = "vhlo.dot_general_v2"(%256, %257) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x201088x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>
    %259 = "vhlo.reshape_v1"(%258) : (!vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x201088x!vhlo.bf16_v1>
    "vhlo.return_v1"(%139, %140, %141, %100, %258, %259) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x201088x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump After VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}
// -----// IR Dump Before ConvertArithToStableHLO (convert-arith-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before LegalizeStableHLOCompositeToTTIR (legalize-stablehlo-composite-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before StablehloLegalizeCompositeToCallPass (stablehlo-legalize-composite-to-call) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=8]>
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>}> : () -> tensor<13xi64>
    %2 = "ttir.constant"() <{value = dense<0> : tensor<ui8>}> : () -> tensor<ui8>
    %3 = "ttir.constant"() <{value = dense<0xFF80> : tensor<bf16>}> : () -> tensor<bf16>
    %4 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %5 = "ttir.constant"() <{value = dense<3.47222231E-4> : tensor<1x13xf32>}> : () -> tensor<1x13xf32>
    %6 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
    %7 = "ttir.constant"() <{value = dense<1> : tensor<ui8>}> : () -> tensor<ui8>
    %8 = "ttir.constant"() <{value = dense<1.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %9 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %10 = ttir.empty() : tensor<1x1xbf16>
    %11 = "ttir.reshape"(%9, %10) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %12 = ttir.empty() : tensor<13x32xbf16>
    %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %14 = ttir.empty() : tensor<1x1x1xbf16>
    %15 = "ttir.reshape"(%8, %14) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %16 = ttir.empty() : tensor<32x13x2880xbf16>
    %17 = "ttir.broadcast"(%15, %16) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %18 = ttir.empty() : tensor<1x1x1x1xbf16>
    %19 = "ttir.reshape"(%9, %18) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %20 = ttir.empty() : tensor<1x1x13x13xbf16>
    %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %22 = ttir.empty() : tensor<1x1xui8>
    %23 = "ttir.reshape"(%7, %22) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %24 = ttir.empty() : tensor<13x13xui8>
    %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %26 = ttir.empty() : tensor<1x1x1xf32>
    %27 = "ttir.reshape"(%4, %26) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %28 = ttir.empty() : tensor<1x13x2880xf32>
    %29 = "ttir.broadcast"(%27, %28) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %30 = ttir.empty() : tensor<1x1xui8>
    %31 = "ttir.reshape"(%2, %30) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %32 = ttir.empty() : tensor<13x13xui8>
    %33 = "ttir.broadcast"(%31, %32) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %34 = ttir.empty() : tensor<2880xf32>
    %35 = "ttir.typecast"(%arg5, %34) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %36 = ttir.empty() : tensor<1x1x2880xf32>
    %37 = "ttir.reshape"(%35, %36) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %38 = ttir.empty() : tensor<1x13x2880xf32>
    %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %40 = ttir.empty() : tensor<13xi64>
    %41 = "ttir.reshape"(%arg3, %40) <{shape = [13 : i32]}> : (tensor<1x13xi64>, tensor<13xi64>) -> tensor<13xi64>
    %42 = ttir.empty() : tensor<13xui32>
    %43 = "ttir.typecast"(%41, %42) <{conservative_folding = false}> : (tensor<13xi64>, tensor<13xui32>) -> tensor<13xui32>
    %44 = ttir.empty() : tensor<13x2880xbf16>
    %45 = "ttir.gather"(%arg4, %43, %44) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %46 = ttir.empty() : tensor<1x13x2880xbf16>
    %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %48 = ttir.empty() : tensor<1x13x2880xf32>
    %49 = "ttir.typecast"(%47, %48) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %50 = ttir.empty() : tensor<1x13x2880xf32>
    %51 = "ttir.pow"(%49, %29, %50) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %52 = ttir.empty() : tensor<1x13xf32>
    %53 = "ttir.sum"(%51, %52) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %54 = ttir.empty() : tensor<1x13xf32>
    %55 = "ttir.multiply"(%53, %5, %54) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %56 = ttir.empty() : tensor<1x13x1xf32>
    %57 = "ttir.reshape"(%55, %56) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %58 = ttir.empty() : tensor<1x1x1xf32>
    %59 = "ttir.reshape"(%arg2, %58) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %60 = ttir.empty() : tensor<1x13x1xf32>
    %61 = "ttir.broadcast"(%59, %60) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %62 = ttir.empty() : tensor<1x13x1xf32>
    %63 = "ttir.add"(%57, %61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %64 = ttir.empty() : tensor<1x13x1xf32>
    %65 = "ttir.rsqrt"(%63, %64) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %66 = ttir.empty() : tensor<1x13xf32>
    %67 = "ttir.reshape"(%65, %66) <{shape = [1 : i32, 13 : i32]}> : (tensor<1x13x1xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %68 = ttir.empty() : tensor<1x13x1xf32>
    %69 = "ttir.reshape"(%67, %68) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %70 = ttir.empty() : tensor<1x13x2880xf32>
    %71 = "ttir.broadcast"(%69, %70) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %72 = ttir.empty() : tensor<1x13x2880xf32>
    %73 = "ttir.multiply"(%49, %71, %72) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %74 = ttir.empty() : tensor<1x13x2880xf32>
    %75 = "ttir.multiply"(%39, %73, %74) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %76 = ttir.empty() : tensor<1x13x2880xbf16>
    %77 = "ttir.typecast"(%75, %76) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %78 = ttir.empty() : tensor<13x2880xbf16>
    %79 = "ttir.reshape"(%77, %78) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %80 = ttir.empty() : tensor<2880x4096xbf16>
    %81 = "ttir.permute"(%arg20, %80) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
    %82 = "ttir.dot_general"(%79, %81) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %83 = ttir.empty() : tensor<1x13x4096xbf16>
    %84 = "ttir.reshape"(%82, %83) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %85 = ttir.empty() : tensor<1x1x4096xbf16>
    %86 = "ttir.reshape"(%arg19, %85) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %87 = ttir.empty() : tensor<1x13x4096xbf16>
    %88 = "ttir.broadcast"(%86, %87) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %89 = ttir.empty() : tensor<1x13x4096xbf16>
    %90 = "ttir.add"(%84, %88, %89) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %91 = ttir.empty() : tensor<1x13x64x64xbf16>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %93 = ttir.empty() : tensor<1x64x13x64xbf16>
    %94 = "ttir.permute"(%92, %93) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %95 = ttir.empty() : tensor<1x64x13x32xbf16>
    %96 = "ttir.slice_static"(%94, %95) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %97 = ttir.empty() : tensor<1x64x13x32xf32>
    %98 = "ttir.typecast"(%96, %97) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %99 = ttir.empty() : tensor<1x32x1xf32>
    %100 = "ttir.reshape"(%arg7, %99) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
    %101 = "ttir.dot_general"(%100, %6) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %102 = ttir.empty() : tensor<1x13x32xf32>
    %103 = "ttir.permute"(%101, %102) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %104 = ttir.empty() : tensor<1x13x32xf32>
    %105 = "ttir.cos"(%103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %106 = ttir.empty() : tensor<1x1x1xf32>
    %107 = "ttir.reshape"(%arg6, %106) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %108 = ttir.empty() : tensor<1x13x32xf32>
    %109 = "ttir.broadcast"(%107, %108) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %110 = ttir.empty() : tensor<1x13x32xf32>
    %111 = "ttir.multiply"(%105, %109, %110) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %112 = ttir.empty() : tensor<1x13x32xbf16>
    %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %114 = ttir.empty() : tensor<1x1x13x32xbf16>
    %115 = "ttir.reshape"(%113, %114) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %116 = ttir.empty() : tensor<1x1x13x32xf32>
    %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %118 = ttir.empty() : tensor<1x13x32xf32>
    %119 = "ttir.reshape"(%117, %118) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %120 = ttir.empty() : tensor<1x1x13x32xf32>
    %121 = "ttir.reshape"(%119, %120) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %122 = ttir.empty() : tensor<1x64x13x32xf32>
    %123 = "ttir.broadcast"(%121, %122) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %124 = ttir.empty() : tensor<1x64x13x32xf32>
    %125 = "ttir.multiply"(%98, %123, %124) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %126 = ttir.empty() : tensor<1x64x13x32xbf16>
    %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %128 = ttir.empty() : tensor<1x64x13x32xbf16>
    %129 = "ttir.slice_static"(%94, %128) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %130 = ttir.empty() : tensor<1x64x13x32xf32>
    %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %132 = ttir.empty() : tensor<1x13x32xf32>
    %133 = "ttir.sin"(%103, %132) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %134 = ttir.empty() : tensor<1x13x32xf32>
    %135 = "ttir.multiply"(%133, %109, %134) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %136 = ttir.empty() : tensor<1x13x32xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %138 = ttir.empty() : tensor<1x1x13x32xbf16>
    %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %140 = ttir.empty() : tensor<1x1x13x32xf32>
    %141 = "ttir.typecast"(%139, %140) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %142 = ttir.empty() : tensor<1x13x32xf32>
    %143 = "ttir.reshape"(%141, %142) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %144 = ttir.empty() : tensor<1x1x13x32xf32>
    %145 = "ttir.reshape"(%143, %144) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %146 = ttir.empty() : tensor<1x64x13x32xf32>
    %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %148 = ttir.empty() : tensor<1x64x13x32xf32>
    %149 = "ttir.multiply"(%131, %147, %148) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %150 = ttir.empty() : tensor<1x64x13x32xbf16>
    %151 = "ttir.typecast"(%149, %150) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %152 = ttir.empty() : tensor<1x64x13x32xbf16>
    %153 = "ttir.subtract"(%127, %151, %152) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %154 = ttir.empty() : tensor<1x64x13x32xf32>
    %155 = "ttir.multiply"(%131, %123, %154) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %156 = ttir.empty() : tensor<1x64x13x32xbf16>
    %157 = "ttir.typecast"(%155, %156) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %158 = ttir.empty() : tensor<1x64x13x32xf32>
    %159 = "ttir.multiply"(%98, %147, %158) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %160 = ttir.empty() : tensor<1x64x13x32xbf16>
    %161 = "ttir.typecast"(%159, %160) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %162 = ttir.empty() : tensor<1x64x13x32xbf16>
    %163 = "ttir.add"(%157, %161, %162) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %164 = ttir.empty() : tensor<1x64x13x64xbf16>
    %165 = "ttir.concat"(%153, %163, %164) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %166 = ttir.empty() : tensor<64x13x64xbf16>
    %167 = "ttir.reshape"(%165, %166) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %168 = ttir.empty() : tensor<2880x512xbf16>
    %169 = "ttir.permute"(%arg9, %168) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %170 = "ttir.dot_general"(%79, %169) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %171 = ttir.empty() : tensor<1x13x512xbf16>
    %172 = "ttir.reshape"(%170, %171) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %173 = ttir.empty() : tensor<1x1x512xbf16>
    %174 = "ttir.reshape"(%arg8, %173) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %175 = ttir.empty() : tensor<1x13x512xbf16>
    %176 = "ttir.broadcast"(%174, %175) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %177 = ttir.empty() : tensor<1x13x512xbf16>
    %178 = "ttir.add"(%172, %176, %177) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %179 = ttir.empty() : tensor<1x13x8x64xbf16>
    %180 = "ttir.reshape"(%178, %179) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %181 = ttir.empty() : tensor<1x8x13x64xbf16>
    %182 = "ttir.permute"(%180, %181) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %183 = ttir.empty() : tensor<1x8x13x32xbf16>
    %184 = "ttir.slice_static"(%182, %183) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %185 = ttir.empty() : tensor<1x8x13x32xf32>
    %186 = "ttir.typecast"(%184, %185) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %187 = ttir.empty() : tensor<1x1x13x32xf32>
    %188 = "ttir.reshape"(%119, %187) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %189 = ttir.empty() : tensor<1x8x13x32xf32>
    %190 = "ttir.broadcast"(%188, %189) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %191 = ttir.empty() : tensor<1x8x13x32xf32>
    %192 = "ttir.multiply"(%186, %190, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %193 = ttir.empty() : tensor<1x8x13x32xbf16>
    %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %195 = ttir.empty() : tensor<1x8x13x32xbf16>
    %196 = "ttir.slice_static"(%182, %195) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %197 = ttir.empty() : tensor<1x8x13x32xf32>
    %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %199 = ttir.empty() : tensor<1x1x13x32xf32>
    %200 = "ttir.reshape"(%143, %199) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %201 = ttir.empty() : tensor<1x8x13x32xf32>
    %202 = "ttir.broadcast"(%200, %201) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %203 = ttir.empty() : tensor<1x8x13x32xf32>
    %204 = "ttir.multiply"(%198, %202, %203) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %205 = ttir.empty() : tensor<1x8x13x32xbf16>
    %206 = "ttir.typecast"(%204, %205) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %207 = ttir.empty() : tensor<1x8x13x32xbf16>
    %208 = "ttir.subtract"(%194, %206, %207) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %209 = ttir.empty() : tensor<1x8x13x32xf32>
    %210 = "ttir.multiply"(%198, %190, %209) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %211 = ttir.empty() : tensor<1x8x13x32xbf16>
    %212 = "ttir.typecast"(%210, %211) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %213 = ttir.empty() : tensor<1x8x13x32xf32>
    %214 = "ttir.multiply"(%186, %202, %213) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %215 = ttir.empty() : tensor<1x8x13x32xbf16>
    %216 = "ttir.typecast"(%214, %215) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %217 = ttir.empty() : tensor<1x8x13x32xbf16>
    %218 = "ttir.add"(%212, %216, %217) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %219 = ttir.empty() : tensor<1x8x13x64xbf16>
    %220 = "ttir.concat"(%208, %218, %219) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %221 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %222 = "ttir.reshape"(%220, %221) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %223 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %224 = "ttir.broadcast"(%222, %223) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %225 = ttir.empty() : tensor<1x64x13x64xbf16>
    %226 = "ttir.reshape"(%224, %225) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %227 = ttir.empty() : tensor<1x64x64x13xbf16>
    %228 = "ttir.permute"(%226, %227) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
    %229 = ttir.empty() : tensor<64x64x13xbf16>
    %230 = "ttir.reshape"(%228, %229) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %231 = "ttir.dot_general"(%167, %230) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %232 = ttir.empty() : tensor<1x64x13x13xbf16>
    %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %234 = ttir.empty() : tensor<1x64x13x13xf32>
    %235 = "ttir.typecast"(%233, %234) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %236 = ttir.empty() : tensor<1x1x1x1xf32>
    %237 = "ttir.reshape"(%arg18, %236) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %238 = ttir.empty() : tensor<1x64x13x13xf32>
    %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %240 = ttir.empty() : tensor<1x64x13x13xf32>
    %241 = "ttir.multiply"(%235, %239, %240) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %242 = ttir.empty() : tensor<1x64x13x13xbf16>
    %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %244 = ttir.empty() : tensor<1x13xi64>
    %245 = "ttir.reshape"(%1, %244) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xi64>, tensor<1x13xi64>) -> tensor<1x13xi64>
    %246 = ttir.empty() : tensor<13x13xi64>
    %247 = "ttir.broadcast"(%245, %246) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %248 = ttir.empty() : tensor<1xi64>
    %249 = "ttir.reshape"(%arg17, %248) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %250 = ttir.empty() : tensor<13xi64>
    %251 = "ttir.broadcast"(%249, %250) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xi64>, tensor<13xi64>) -> tensor<13xi64>
    %252 = ttir.empty() : tensor<13xi64>
    %253 = "ttir.subtract"(%1, %251, %252) : (tensor<13xi64>, tensor<13xi64>, tensor<13xi64>) -> tensor<13xi64>
    %254 = ttir.empty() : tensor<13x1xi64>
    %255 = "ttir.reshape"(%253, %254) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1xi64>) -> tensor<13x1xi64>
    %256 = ttir.empty() : tensor<13x13xi64>
    %257 = "ttir.broadcast"(%255, %256) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %258 = ttir.empty() : tensor<13x13xi1>
    %259 = "ttir.gt"(%247, %257, %258) : (tensor<13x13xi64>, tensor<13x13xi64>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %260 = ttir.empty() : tensor<13x13xui8>
    %261 = "ttir.typecast"(%259, %260) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %262 = ttir.empty() : tensor<13x13xui8>
    %263 = "ttir.bitwise_and"(%261, %25, %262) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %264 = ttir.empty() : tensor<13x13xi1>
    %265 = "ttir.ne"(%263, %33, %264) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %266 = ttir.empty() : tensor<13x13xui8>
    %267 = "ttir.typecast"(%265, %266) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %268 = ttir.empty() : tensor<13x1xi64>
    %269 = "ttir.reshape"(%1, %268) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1xi64>) -> tensor<13x1xi64>
    %270 = ttir.empty() : tensor<13x13xi64>
    %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %272 = ttir.empty() : tensor<13x13xi1>
    %273 = "ttir.le"(%247, %271, %272) : (tensor<13x13xi64>, tensor<13x13xi64>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %274 = ttir.empty() : tensor<13x13xui8>
    %275 = "ttir.typecast"(%273, %274) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %276 = ttir.empty() : tensor<13x13xui8>
    %277 = "ttir.bitwise_and"(%267, %275, %276) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %278 = ttir.empty() : tensor<13x13xi1>
    %279 = "ttir.ne"(%277, %33, %278) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %280 = ttir.empty() : tensor<1x1x13x13xi1>
    %281 = "ttir.reshape"(%279, %280) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xi1>, tensor<1x1x13x13xi1>) -> tensor<1x1x13x13xi1>
    %282 = ttir.empty() : tensor<1x1xbf16>
    %283 = "ttir.reshape"(%arg16, %282) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %284 = ttir.empty() : tensor<1x1x1x1xbf16>
    %285 = "ttir.reshape"(%283, %284) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xbf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %286 = ttir.empty() : tensor<1x1x13x13xbf16>
    %287 = "ttir.broadcast"(%285, %286) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %288 = ttir.empty() : tensor<1x1x13x13xbf16>
    %289 = "ttir.where"(%281, %21, %287, %288) : (tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %290 = ttir.empty() : tensor<1x13x13xbf16>
    %291 = "ttir.reshape"(%289, %290) <{shape = [1 : i32, 13 : i32, 13 : i32]}> : (tensor<1x1x13x13xbf16>, tensor<1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %292 = ttir.empty() : tensor<1x1x13x13xbf16>
    %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %294 = ttir.empty() : tensor<1x64x13x13xbf16>
    %295 = "ttir.broadcast"(%293, %294) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %296 = ttir.empty() : tensor<1x64x13x13xbf16>
    %297 = "ttir.add"(%243, %295, %296) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %298 = ttir.empty() : tensor<1x64x1xbf16>
    %299 = "ttir.reshape"(%arg15, %298) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1xbf16>) -> tensor<1x64x1xbf16>
    %300 = ttir.empty() : tensor<1x64x1x1xbf16>
    %301 = "ttir.reshape"(%299, %300) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x1xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %302 = ttir.empty() : tensor<1x64x13x1xbf16>
    %303 = "ttir.broadcast"(%301, %302) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %304 = ttir.empty() : tensor<1x64x13x14xbf16>
    %305 = "ttir.concat"(%297, %303, %304) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %306 = ttir.empty() : tensor<2880x512xbf16>
    %307 = "ttir.permute"(%arg1, %306) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %308 = "ttir.dot_general"(%79, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %309 = ttir.empty() : tensor<1x13x512xbf16>
    %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %311 = ttir.empty() : tensor<1x1x512xbf16>
    %312 = "ttir.reshape"(%arg0, %311) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %313 = ttir.empty() : tensor<1x13x512xbf16>
    %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %315 = ttir.empty() : tensor<1x13x512xbf16>
    %316 = "ttir.add"(%310, %314, %315) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %317 = ttir.empty() : tensor<1x13x8x64xbf16>
    %318 = "ttir.reshape"(%316, %317) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %319 = ttir.empty() : tensor<1x8x13x64xbf16>
    %320 = "ttir.permute"(%318, %319) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %321 = ttir.empty() : tensor<2880xf32>
    %322 = "ttir.typecast"(%arg30, %321) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %323 = ttir.empty() : tensor<1x1x2880xf32>
    %324 = "ttir.reshape"(%322, %323) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %325 = ttir.empty() : tensor<1x13x2880xf32>
    %326 = "ttir.broadcast"(%324, %325) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %327 = ttir.empty() : tensor<1x64x13xbf16>
    %328 = "ttir.max"(%305, %327) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %329 = ttir.empty() : tensor<1x64x13x1xbf16>
    %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %331 = ttir.empty() : tensor<1x64x13x14xbf16>
    %332 = "ttir.broadcast"(%330, %331) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %333 = ttir.empty() : tensor<1x64x13x14xbf16>
    %334 = "ttir.subtract"(%305, %332, %333) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %335 = ttir.empty() : tensor<1x64x13xbf16>
    %336 = "ttir.max"(%334, %335) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %337 = ttir.empty() : tensor<1x64x13x1xbf16>
    %338 = "ttir.reshape"(%336, %337) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %339 = ttir.empty() : tensor<1x64x13x14xbf16>
    %340 = "ttir.broadcast"(%338, %339) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %341 = ttir.empty() : tensor<1x64x13x14xbf16>
    %342 = "ttir.subtract"(%334, %340, %341) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %343 = ttir.empty() : tensor<1x64x13x14xbf16>
    %344 = "ttir.exp"(%342, %343) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %345 = ttir.empty() : tensor<1x64x13xbf16>
    %346 = "ttir.sum"(%344, %345) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %347 = ttir.empty() : tensor<1x64x13x1xbf16>
    %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %349 = ttir.empty() : tensor<1x64x13x14xbf16>
    %350 = "ttir.broadcast"(%348, %349) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %351 = ttir.empty() : tensor<1x64x13x14xbf16>
    %352 = "ttir.div"(%344, %350, %351) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %353 = ttir.empty() : tensor<1x64x13x13xbf16>
    %354 = "ttir.slice_static"(%352, %353) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %355 = ttir.empty() : tensor<64x13x13xbf16>
    %356 = "ttir.reshape"(%354, %355) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %357 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %358 = "ttir.reshape"(%320, %357) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %359 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %361 = ttir.empty() : tensor<64x13x64xbf16>
    %362 = "ttir.reshape"(%360, %361) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %363 = "ttir.dot_general"(%356, %362) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %364 = ttir.empty() : tensor<1x64x13x64xbf16>
    %365 = "ttir.reshape"(%363, %364) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %366 = ttir.empty() : tensor<1x13x64x64xbf16>
    %367 = "ttir.permute"(%365, %366) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %368 = ttir.empty() : tensor<13x4096xbf16>
    %369 = "ttir.reshape"(%367, %368) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
    %370 = ttir.empty() : tensor<4096x2880xbf16>
    %371 = "ttir.permute"(%arg14, %370) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
    %372 = "ttir.dot_general"(%369, %371) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %373 = ttir.empty() : tensor<1x13x2880xbf16>
    %374 = "ttir.reshape"(%372, %373) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %375 = ttir.empty() : tensor<1x1x2880xbf16>
    %376 = "ttir.reshape"(%arg13, %375) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
    %377 = ttir.empty() : tensor<1x13x2880xbf16>
    %378 = "ttir.broadcast"(%376, %377) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %379 = ttir.empty() : tensor<1x13x2880xbf16>
    %380 = "ttir.add"(%374, %378, %379) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %381 = ttir.empty() : tensor<1x13x2880xbf16>
    %382 = "ttir.add"(%47, %380, %381) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %383 = ttir.empty() : tensor<1x1x1xbf16>
    %384 = "ttir.reshape"(%arg29, %383) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %385 = ttir.empty() : tensor<32x13x2880xbf16>
    %386 = "ttir.broadcast"(%384, %385) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %387 = ttir.empty() : tensor<2880xf32>
    %388 = "ttir.typecast"(%arg21, %387) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %389 = ttir.empty() : tensor<1x1x2880xf32>
    %390 = "ttir.reshape"(%388, %389) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %391 = ttir.empty() : tensor<1x13x2880xf32>
    %392 = "ttir.broadcast"(%390, %391) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %393 = ttir.empty() : tensor<1x13x2880xf32>
    %394 = "ttir.typecast"(%382, %393) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %395 = ttir.empty() : tensor<1x13x2880xf32>
    %396 = "ttir.pow"(%394, %29, %395) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %397 = ttir.empty() : tensor<1x13xf32>
    %398 = "ttir.sum"(%396, %397) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %399 = ttir.empty() : tensor<1x13xf32>
    %400 = "ttir.multiply"(%398, %5, %399) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %401 = ttir.empty() : tensor<1x13x1xf32>
    %402 = "ttir.reshape"(%400, %401) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %403 = ttir.empty() : tensor<1x13x1xf32>
    %404 = "ttir.add"(%402, %61, %403) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %405 = ttir.empty() : tensor<1x13x1xf32>
    %406 = "ttir.rsqrt"(%404, %405) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %407 = ttir.empty() : tensor<1x13xf32>
    %408 = "ttir.reshape"(%406, %407) <{shape = [1 : i32, 13 : i32]}> : (tensor<1x13x1xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %409 = ttir.empty() : tensor<1x13x1xf32>
    %410 = "ttir.reshape"(%408, %409) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %411 = ttir.empty() : tensor<1x13x2880xf32>
    %412 = "ttir.broadcast"(%410, %411) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %413 = ttir.empty() : tensor<1x13x2880xf32>
    %414 = "ttir.multiply"(%394, %412, %413) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %415 = ttir.empty() : tensor<1x13x2880xf32>
    %416 = "ttir.multiply"(%392, %414, %415) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %417 = ttir.empty() : tensor<1x13x2880xbf16>
    %418 = "ttir.typecast"(%416, %417) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %419 = ttir.empty() : tensor<13x2880xbf16>
    %420 = "ttir.reshape"(%418, %419) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %421 = ttir.empty() : tensor<416x2880xbf16>
    %422 = "ttir.concat"(%420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %421) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
    %423 = ttir.empty() : tensor<32x13x2880xbf16>
    %424 = "ttir.reshape"(%422, %423) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %425 = "ttir.dot_general"(%424, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %426 = ttir.empty() : tensor<32x1x5760xbf16>
    %427 = "ttir.reshape"(%arg27, %426) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
    %428 = ttir.empty() : tensor<32x13x5760xbf16>
    %429 = "ttir.broadcast"(%427, %428) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %430 = ttir.empty() : tensor<32x13x5760xbf16>
    %431 = "ttir.add"(%425, %429, %430) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %432 = ttir.empty() : tensor<32x13x2880xbf16>
    %433 = "ttir.slice_static"(%431, %432) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %434 = ttir.empty() : tensor<1x1x1xbf16>
    %435 = "ttir.reshape"(%arg25, %434) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %436 = ttir.empty() : tensor<32x13x2880xbf16>
    %437 = "ttir.broadcast"(%435, %436) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %438 = ttir.empty() : tensor<32x13x2880xbf16>
    %439 = "ttir.clamp_tensor"(%433, %386, %437, %438) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %440 = ttir.empty() : tensor<32x13x2880xbf16>
    %441 = "ttir.add"(%439, %17, %440) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %442 = ttir.empty() : tensor<32x13x2880xf32>
    %443 = "ttir.typecast"(%441, %442) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %444 = ttir.empty() : tensor<1x1x1xbf16>
    %445 = "ttir.reshape"(%arg26, %444) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %446 = ttir.empty() : tensor<32x13x2880xbf16>
    %447 = "ttir.broadcast"(%445, %446) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %448 = ttir.empty() : tensor<32x13x2880xbf16>
    %449 = "ttir.slice_static"(%431, %448) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %450 = ttir.empty() : tensor<32x13x2880xbf16>
    %451 = "ttir.clamp_tensor"(%449, %447, %437, %450) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %452 = ttir.empty() : tensor<32x13x2880xf32>
    %453 = "ttir.typecast"(%451, %452) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %454 = ttir.empty() : tensor<1x1x1xf32>
    %455 = "ttir.reshape"(%arg24, %454) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %456 = ttir.empty() : tensor<32x13x2880xf32>
    %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %458 = ttir.empty() : tensor<32x13x2880xf32>
    %459 = "ttir.multiply"(%453, %457, %458) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %460 = ttir.empty() : tensor<32x13x2880xbf16>
    %461 = "ttir.typecast"(%459, %460) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %462 = ttir.empty() : tensor<32x13x2880xbf16>
    %463 = "ttir.sigmoid"(%461, %462) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %464 = ttir.empty() : tensor<32x13x2880xf32>
    %465 = "ttir.typecast"(%463, %464) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %466 = ttir.empty() : tensor<32x13x2880xf32>
    %467 = "ttir.multiply"(%453, %465, %466) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %468 = ttir.empty() : tensor<32x13x2880xbf16>
    %469 = "ttir.typecast"(%467, %468) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %470 = ttir.empty() : tensor<32x13x2880xf32>
    %471 = "ttir.typecast"(%469, %470) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %472 = ttir.empty() : tensor<32x13x2880xf32>
    %473 = "ttir.multiply"(%443, %471, %472) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %474 = ttir.empty() : tensor<32x13x2880xbf16>
    %475 = "ttir.typecast"(%473, %474) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %476 = "ttir.dot_general"(%475, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %477 = ttir.empty() : tensor<32x1x2880xbf16>
    %478 = "ttir.reshape"(%arg22, %477) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
    %479 = ttir.empty() : tensor<32x13x2880xbf16>
    %480 = "ttir.broadcast"(%478, %479) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %481 = ttir.empty() : tensor<32x13x2880xbf16>
    %482 = "ttir.add"(%476, %480, %481) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %483 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %484 = "ttir.reshape"(%482, %483) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %485 = ttir.empty() : tensor<32x1x13x2880xf32>
    %486 = "ttir.typecast"(%484, %485) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %487 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xi64>
    %488 = ttir.empty() : tensor<13x1x1xi64>
    %489 = "ttir.reshape"(%487, %488) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1x1xi64>) -> tensor<13x1x1xi64>
    %490 = ttir.empty() : tensor<13x4x1xi64>
    %491 = "ttir.broadcast"(%489, %490) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x1xi64>
    %492 = ttir.empty() : tensor<2880x32xbf16>
    %493 = "ttir.permute"(%arg12, %492) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
    %494 = "ttir.dot_general"(%420, %493) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %495 = ttir.empty() : tensor<1x32xbf16>
    %496 = "ttir.reshape"(%arg11, %495) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
    %497 = ttir.empty() : tensor<13x32xbf16>
    %498 = "ttir.broadcast"(%496, %497) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %499 = ttir.empty() : tensor<13x32xbf16>
    %500 = "ttir.add"(%494, %498, %499) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %501 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 32 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<32xi32>
    %502 = ttir.empty() : tensor<1x32xi32>
    %503 = "ttir.reshape"(%501, %502) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xi32>, tensor<1x32xi32>) -> tensor<1x32xi32>
    %504 = ttir.empty() : tensor<13x32xi32>
    %505 = "ttir.broadcast"(%503, %504) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xi32>, tensor<13x32xi32>) -> tensor<13x32xi32>
    %506 = ttir.empty() : tensor<13x32xbf16>
    %507 = ttir.empty() : tensor<13x32xi32>
    %values, %indices = "ttir.sort"(%500, %506, %507) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %508 = ttir.empty() : tensor<13x4xi32>
    %509 = "ttir.slice_static"(%indices, %508) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xi32>, tensor<13x4xi32>) -> tensor<13x4xi32>
    %510 = ttir.empty() : tensor<13x4xi64>
    %511 = "ttir.typecast"(%509, %510) <{conservative_folding = false}> : (tensor<13x4xi32>, tensor<13x4xi64>) -> tensor<13x4xi64>
    %512 = ttir.empty() : tensor<13x4x1xi64>
    %513 = "ttir.reshape"(%511, %512) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xi64>, tensor<13x4x1xi64>) -> tensor<13x4x1xi64>
    %514 = ttir.empty() : tensor<13x4x2xi64>
    %515 = "ttir.concat"(%491, %513, %514) <{dim = 2 : si32}> : (tensor<13x4x1xi64>, tensor<13x4x1xi64>, tensor<13x4x2xi64>) -> tensor<13x4x2xi64>
    %516 = ttir.empty() : tensor<13x4xbf16>
    %517 = "ttir.slice_static"(%values, %516) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %518 = ttir.empty() : tensor<13xbf16>
    %519 = "ttir.max"(%517, %518) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %520 = ttir.empty() : tensor<13x1xbf16>
    %521 = "ttir.reshape"(%519, %520) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %522 = ttir.empty() : tensor<13x4xbf16>
    %523 = "ttir.broadcast"(%521, %522) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %524 = ttir.empty() : tensor<13x4xbf16>
    %525 = "ttir.subtract"(%517, %523, %524) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %526 = ttir.empty() : tensor<13x4xbf16>
    %527 = "ttir.exp"(%525, %526) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %528 = ttir.empty() : tensor<13xbf16>
    %529 = "ttir.sum"(%527, %528) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %530 = ttir.empty() : tensor<13x1xbf16>
    %531 = "ttir.reshape"(%529, %530) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %532 = ttir.empty() : tensor<13x4xbf16>
    %533 = "ttir.broadcast"(%531, %532) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %534 = ttir.empty() : tensor<13x4xbf16>
    %535 = "ttir.div"(%527, %533, %534) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %536 = ttir.empty() : tensor<13x32xbf16>
    %537 = "ttir.scatter"(%13, %515, %535, %536) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %538 = ttir.empty() : tensor<32x13xbf16>
    %539 = "ttir.permute"(%537, %538) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
    %540 = ttir.empty() : tensor<32x1x13x1xbf16>
    %541 = "ttir.reshape"(%539, %540) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
    %542 = ttir.empty() : tensor<32x1x13x1xf32>
    %543 = "ttir.typecast"(%541, %542) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
    %544 = ttir.empty() : tensor<32x1x13xf32>
    %545 = "ttir.reshape"(%543, %544) <{shape = [32 : i32, 1 : i32, 13 : i32]}> : (tensor<32x1x13x1xf32>, tensor<32x1x13xf32>) -> tensor<32x1x13xf32>
    %546 = ttir.empty() : tensor<32x1x13x1xf32>
    %547 = "ttir.reshape"(%545, %546) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x1x13xf32>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
    %548 = ttir.empty() : tensor<32x1x13x2880xf32>
    %549 = "ttir.broadcast"(%547, %548) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %550 = ttir.empty() : tensor<32x1x13x2880xf32>
    %551 = "ttir.multiply"(%486, %549, %550) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %552 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %553 = "ttir.typecast"(%551, %552) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %554 = ttir.empty() : tensor<1x13x2880xbf16>
    %555 = "ttir.sum"(%553, %554) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %556 = ttir.empty() : tensor<1x13x2880xbf16>
    %557 = "ttir.add"(%382, %555, %556) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %558 = ttir.empty() : tensor<1x13x2880xf32>
    %559 = "ttir.typecast"(%557, %558) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %560 = ttir.empty() : tensor<1x13x2880xf32>
    %561 = "ttir.pow"(%559, %29, %560) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %562 = ttir.empty() : tensor<1x13xf32>
    %563 = "ttir.sum"(%561, %562) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %564 = ttir.empty() : tensor<1x13xf32>
    %565 = "ttir.multiply"(%563, %5, %564) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %566 = ttir.empty() : tensor<1x13x1xf32>
    %567 = "ttir.reshape"(%565, %566) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %568 = ttir.empty() : tensor<1x13x1xf32>
    %569 = "ttir.add"(%567, %61, %568) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %570 = ttir.empty() : tensor<1x13x1xf32>
    %571 = "ttir.rsqrt"(%569, %570) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %572 = ttir.empty() : tensor<1x13xf32>
    %573 = "ttir.reshape"(%571, %572) <{shape = [1 : i32, 13 : i32]}> : (tensor<1x13x1xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %574 = ttir.empty() : tensor<1x13x1xf32>
    %575 = "ttir.reshape"(%573, %574) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %576 = ttir.empty() : tensor<1x13x2880xf32>
    %577 = "ttir.broadcast"(%575, %576) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %578 = ttir.empty() : tensor<1x13x2880xf32>
    %579 = "ttir.multiply"(%559, %577, %578) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %580 = ttir.empty() : tensor<1x13x2880xf32>
    %581 = "ttir.multiply"(%326, %579, %580) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %582 = ttir.empty() : tensor<1x13x2880xbf16>
    %583 = "ttir.typecast"(%581, %582) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %584 = ttir.empty() : tensor<13x2880xbf16>
    %585 = "ttir.reshape"(%583, %584) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %586 = ttir.empty() : tensor<2880x201088xbf16>
    %587 = "ttir.permute"(%arg10, %586) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
    %588 = "ttir.dot_general"(%585, %587) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %589 = ttir.empty() : tensor<1x13x201088xbf16>
    %590 = "ttir.reshape"(%588, %589) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %316, %318, %320, %220, %588, %590 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>}> : () -> tensor<13xi64>
    %2 = "ttir.constant"() <{value = dense<0> : tensor<ui8>}> : () -> tensor<ui8>
    %3 = "ttir.constant"() <{value = dense<0xFF80> : tensor<bf16>}> : () -> tensor<bf16>
    %4 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %5 = "ttir.constant"() <{value = dense<3.47222231E-4> : tensor<1x13xf32>}> : () -> tensor<1x13xf32>
    %6 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
    %7 = "ttir.constant"() <{value = dense<1> : tensor<ui8>}> : () -> tensor<ui8>
    %8 = "ttir.constant"() <{value = dense<1.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %9 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %10 = ttir.empty() : tensor<1x1xbf16>
    %11 = "ttir.reshape"(%9, %10) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %12 = ttir.empty() : tensor<13x32xbf16>
    %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %14 = ttir.empty() : tensor<1x1x1xbf16>
    %15 = "ttir.reshape"(%8, %14) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %16 = ttir.empty() : tensor<32x13x2880xbf16>
    %17 = "ttir.broadcast"(%15, %16) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %18 = ttir.empty() : tensor<1x1x1x1xbf16>
    %19 = "ttir.reshape"(%9, %18) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %20 = ttir.empty() : tensor<1x1x13x13xbf16>
    %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %22 = ttir.empty() : tensor<1x1xui8>
    %23 = "ttir.reshape"(%7, %22) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %24 = ttir.empty() : tensor<13x13xui8>
    %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %26 = ttir.empty() : tensor<1x1x1xf32>
    %27 = "ttir.reshape"(%4, %26) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %28 = ttir.empty() : tensor<1x13x2880xf32>
    %29 = "ttir.broadcast"(%27, %28) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %30 = ttir.empty() : tensor<1x1xui8>
    %31 = "ttir.reshape"(%2, %30) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %32 = ttir.empty() : tensor<13x13xui8>
    %33 = "ttir.broadcast"(%31, %32) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %34 = ttir.empty() : tensor<2880xf32>
    %35 = "ttir.typecast"(%arg5, %34) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %36 = ttir.empty() : tensor<1x1x2880xf32>
    %37 = "ttir.reshape"(%35, %36) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %38 = ttir.empty() : tensor<1x13x2880xf32>
    %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %40 = ttir.empty() : tensor<13xi64>
    %41 = "ttir.reshape"(%arg3, %40) <{shape = [13 : i32]}> : (tensor<1x13xi64>, tensor<13xi64>) -> tensor<13xi64>
    %42 = ttir.empty() : tensor<13xui32>
    %43 = "ttir.typecast"(%41, %42) <{conservative_folding = false}> : (tensor<13xi64>, tensor<13xui32>) -> tensor<13xui32>
    %44 = ttir.empty() : tensor<13x2880xbf16>
    %45 = "ttir.gather"(%arg4, %43, %44) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %46 = ttir.empty() : tensor<1x13x2880xbf16>
    %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %48 = ttir.empty() : tensor<1x13x2880xf32>
    %49 = "ttir.typecast"(%47, %48) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %50 = ttir.empty() : tensor<1x13x2880xf32>
    %51 = "ttir.pow"(%49, %29, %50) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %52 = ttir.empty() : tensor<1x13xf32>
    %53 = "ttir.sum"(%51, %52) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %54 = ttir.empty() : tensor<1x13xf32>
    %55 = "ttir.multiply"(%53, %5, %54) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %56 = ttir.empty() : tensor<1x13x1xf32>
    %57 = "ttir.reshape"(%55, %56) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %58 = ttir.empty() : tensor<1x1x1xf32>
    %59 = "ttir.reshape"(%arg2, %58) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %60 = ttir.empty() : tensor<1x13x1xf32>
    %61 = "ttir.broadcast"(%59, %60) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %62 = ttir.empty() : tensor<1x13x1xf32>
    %63 = "ttir.add"(%57, %61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %64 = ttir.empty() : tensor<1x13x1xf32>
    %65 = "ttir.rsqrt"(%63, %64) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %66 = ttir.empty() : tensor<1x13xf32>
    %67 = "ttir.reshape"(%65, %66) <{shape = [1 : i32, 13 : i32]}> : (tensor<1x13x1xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %68 = ttir.empty() : tensor<1x13x1xf32>
    %69 = "ttir.reshape"(%67, %68) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %70 = ttir.empty() : tensor<1x13x2880xf32>
    %71 = "ttir.broadcast"(%69, %70) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %72 = ttir.empty() : tensor<1x13x2880xf32>
    %73 = "ttir.multiply"(%49, %71, %72) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %74 = ttir.empty() : tensor<1x13x2880xf32>
    %75 = "ttir.multiply"(%39, %73, %74) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %76 = ttir.empty() : tensor<1x13x2880xbf16>
    %77 = "ttir.typecast"(%75, %76) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %78 = ttir.empty() : tensor<13x2880xbf16>
    %79 = "ttir.reshape"(%77, %78) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %80 = ttir.empty() : tensor<2880x4096xbf16>
    %81 = "ttir.permute"(%arg20, %80) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
    %82 = "ttir.dot_general"(%79, %81) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %83 = ttir.empty() : tensor<1x13x4096xbf16>
    %84 = "ttir.reshape"(%82, %83) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %85 = ttir.empty() : tensor<1x1x4096xbf16>
    %86 = "ttir.reshape"(%arg19, %85) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %87 = ttir.empty() : tensor<1x13x4096xbf16>
    %88 = "ttir.broadcast"(%86, %87) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %89 = ttir.empty() : tensor<1x13x4096xbf16>
    %90 = "ttir.add"(%84, %88, %89) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %91 = ttir.empty() : tensor<1x13x64x64xbf16>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %93 = ttir.empty() : tensor<1x64x13x64xbf16>
    %94 = "ttir.permute"(%92, %93) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %95 = ttir.empty() : tensor<1x64x13x32xbf16>
    %96 = "ttir.slice_static"(%94, %95) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %97 = ttir.empty() : tensor<1x64x13x32xf32>
    %98 = "ttir.typecast"(%96, %97) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %99 = ttir.empty() : tensor<1x32x1xf32>
    %100 = "ttir.reshape"(%arg7, %99) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
    %101 = "ttir.dot_general"(%100, %6) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %102 = ttir.empty() : tensor<1x13x32xf32>
    %103 = "ttir.permute"(%101, %102) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %104 = ttir.empty() : tensor<1x13x32xf32>
    %105 = "ttir.cos"(%103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %106 = ttir.empty() : tensor<1x1x1xf32>
    %107 = "ttir.reshape"(%arg6, %106) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %108 = ttir.empty() : tensor<1x13x32xf32>
    %109 = "ttir.broadcast"(%107, %108) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %110 = ttir.empty() : tensor<1x13x32xf32>
    %111 = "ttir.multiply"(%105, %109, %110) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %112 = ttir.empty() : tensor<1x13x32xbf16>
    %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %114 = ttir.empty() : tensor<1x1x13x32xbf16>
    %115 = "ttir.reshape"(%113, %114) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %116 = ttir.empty() : tensor<1x1x13x32xf32>
    %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %118 = ttir.empty() : tensor<1x13x32xf32>
    %119 = "ttir.reshape"(%117, %118) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %120 = ttir.empty() : tensor<1x1x13x32xf32>
    %121 = "ttir.reshape"(%119, %120) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %122 = ttir.empty() : tensor<1x64x13x32xf32>
    %123 = "ttir.broadcast"(%121, %122) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %124 = ttir.empty() : tensor<1x64x13x32xf32>
    %125 = "ttir.multiply"(%98, %123, %124) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %126 = ttir.empty() : tensor<1x64x13x32xbf16>
    %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %128 = ttir.empty() : tensor<1x64x13x32xbf16>
    %129 = "ttir.slice_static"(%94, %128) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %130 = ttir.empty() : tensor<1x64x13x32xf32>
    %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %132 = ttir.empty() : tensor<1x13x32xf32>
    %133 = "ttir.sin"(%103, %132) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %134 = ttir.empty() : tensor<1x13x32xf32>
    %135 = "ttir.multiply"(%133, %109, %134) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %136 = ttir.empty() : tensor<1x13x32xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %138 = ttir.empty() : tensor<1x1x13x32xbf16>
    %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %140 = ttir.empty() : tensor<1x1x13x32xf32>
    %141 = "ttir.typecast"(%139, %140) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %142 = ttir.empty() : tensor<1x13x32xf32>
    %143 = "ttir.reshape"(%141, %142) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %144 = ttir.empty() : tensor<1x1x13x32xf32>
    %145 = "ttir.reshape"(%143, %144) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %146 = ttir.empty() : tensor<1x64x13x32xf32>
    %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %148 = ttir.empty() : tensor<1x64x13x32xf32>
    %149 = "ttir.multiply"(%131, %147, %148) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %150 = ttir.empty() : tensor<1x64x13x32xbf16>
    %151 = "ttir.typecast"(%149, %150) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %152 = ttir.empty() : tensor<1x64x13x32xbf16>
    %153 = "ttir.subtract"(%127, %151, %152) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %154 = ttir.empty() : tensor<1x64x13x32xf32>
    %155 = "ttir.multiply"(%131, %123, %154) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %156 = ttir.empty() : tensor<1x64x13x32xbf16>
    %157 = "ttir.typecast"(%155, %156) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %158 = ttir.empty() : tensor<1x64x13x32xf32>
    %159 = "ttir.multiply"(%98, %147, %158) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %160 = ttir.empty() : tensor<1x64x13x32xbf16>
    %161 = "ttir.typecast"(%159, %160) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %162 = ttir.empty() : tensor<1x64x13x32xbf16>
    %163 = "ttir.add"(%157, %161, %162) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %164 = ttir.empty() : tensor<1x64x13x64xbf16>
    %165 = "ttir.concat"(%153, %163, %164) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %166 = ttir.empty() : tensor<64x13x64xbf16>
    %167 = "ttir.reshape"(%165, %166) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %168 = ttir.empty() : tensor<2880x512xbf16>
    %169 = "ttir.permute"(%arg9, %168) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %170 = "ttir.dot_general"(%79, %169) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %171 = ttir.empty() : tensor<1x13x512xbf16>
    %172 = "ttir.reshape"(%170, %171) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %173 = ttir.empty() : tensor<1x1x512xbf16>
    %174 = "ttir.reshape"(%arg8, %173) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %175 = ttir.empty() : tensor<1x13x512xbf16>
    %176 = "ttir.broadcast"(%174, %175) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %177 = ttir.empty() : tensor<1x13x512xbf16>
    %178 = "ttir.add"(%172, %176, %177) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %179 = ttir.empty() : tensor<1x13x8x64xbf16>
    %180 = "ttir.reshape"(%178, %179) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %181 = ttir.empty() : tensor<1x8x13x64xbf16>
    %182 = "ttir.permute"(%180, %181) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %183 = ttir.empty() : tensor<1x8x13x32xbf16>
    %184 = "ttir.slice_static"(%182, %183) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %185 = ttir.empty() : tensor<1x8x13x32xf32>
    %186 = "ttir.typecast"(%184, %185) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %187 = ttir.empty() : tensor<1x1x13x32xf32>
    %188 = "ttir.reshape"(%119, %187) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %189 = ttir.empty() : tensor<1x8x13x32xf32>
    %190 = "ttir.broadcast"(%188, %189) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %191 = ttir.empty() : tensor<1x8x13x32xf32>
    %192 = "ttir.multiply"(%186, %190, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %193 = ttir.empty() : tensor<1x8x13x32xbf16>
    %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %195 = ttir.empty() : tensor<1x8x13x32xbf16>
    %196 = "ttir.slice_static"(%182, %195) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %197 = ttir.empty() : tensor<1x8x13x32xf32>
    %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %199 = ttir.empty() : tensor<1x1x13x32xf32>
    %200 = "ttir.reshape"(%143, %199) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %201 = ttir.empty() : tensor<1x8x13x32xf32>
    %202 = "ttir.broadcast"(%200, %201) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %203 = ttir.empty() : tensor<1x8x13x32xf32>
    %204 = "ttir.multiply"(%198, %202, %203) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %205 = ttir.empty() : tensor<1x8x13x32xbf16>
    %206 = "ttir.typecast"(%204, %205) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %207 = ttir.empty() : tensor<1x8x13x32xbf16>
    %208 = "ttir.subtract"(%194, %206, %207) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %209 = ttir.empty() : tensor<1x8x13x32xf32>
    %210 = "ttir.multiply"(%198, %190, %209) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %211 = ttir.empty() : tensor<1x8x13x32xbf16>
    %212 = "ttir.typecast"(%210, %211) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %213 = ttir.empty() : tensor<1x8x13x32xf32>
    %214 = "ttir.multiply"(%186, %202, %213) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %215 = ttir.empty() : tensor<1x8x13x32xbf16>
    %216 = "ttir.typecast"(%214, %215) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %217 = ttir.empty() : tensor<1x8x13x32xbf16>
    %218 = "ttir.add"(%212, %216, %217) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %219 = ttir.empty() : tensor<1x8x13x64xbf16>
    %220 = "ttir.concat"(%208, %218, %219) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %221 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %222 = "ttir.reshape"(%220, %221) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %223 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %224 = "ttir.broadcast"(%222, %223) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %225 = ttir.empty() : tensor<1x64x13x64xbf16>
    %226 = "ttir.reshape"(%224, %225) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %227 = ttir.empty() : tensor<1x64x64x13xbf16>
    %228 = "ttir.permute"(%226, %227) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
    %229 = ttir.empty() : tensor<64x64x13xbf16>
    %230 = "ttir.reshape"(%228, %229) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %231 = "ttir.dot_general"(%167, %230) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %232 = ttir.empty() : tensor<1x64x13x13xbf16>
    %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %234 = ttir.empty() : tensor<1x64x13x13xf32>
    %235 = "ttir.typecast"(%233, %234) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %236 = ttir.empty() : tensor<1x1x1x1xf32>
    %237 = "ttir.reshape"(%arg18, %236) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %238 = ttir.empty() : tensor<1x64x13x13xf32>
    %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %240 = ttir.empty() : tensor<1x64x13x13xf32>
    %241 = "ttir.multiply"(%235, %239, %240) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %242 = ttir.empty() : tensor<1x64x13x13xbf16>
    %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %244 = ttir.empty() : tensor<1x13xi64>
    %245 = "ttir.reshape"(%1, %244) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xi64>, tensor<1x13xi64>) -> tensor<1x13xi64>
    %246 = ttir.empty() : tensor<13x13xi64>
    %247 = "ttir.broadcast"(%245, %246) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %248 = ttir.empty() : tensor<1xi64>
    %249 = "ttir.reshape"(%arg17, %248) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %250 = ttir.empty() : tensor<13xi64>
    %251 = "ttir.broadcast"(%249, %250) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xi64>, tensor<13xi64>) -> tensor<13xi64>
    %252 = ttir.empty() : tensor<13xi64>
    %253 = "ttir.subtract"(%1, %251, %252) : (tensor<13xi64>, tensor<13xi64>, tensor<13xi64>) -> tensor<13xi64>
    %254 = ttir.empty() : tensor<13x1xi64>
    %255 = "ttir.reshape"(%253, %254) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1xi64>) -> tensor<13x1xi64>
    %256 = ttir.empty() : tensor<13x13xi64>
    %257 = "ttir.broadcast"(%255, %256) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %258 = ttir.empty() : tensor<13x13xi1>
    %259 = "ttir.gt"(%247, %257, %258) : (tensor<13x13xi64>, tensor<13x13xi64>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %260 = ttir.empty() : tensor<13x13xui8>
    %261 = "ttir.typecast"(%259, %260) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %262 = ttir.empty() : tensor<13x13xui8>
    %263 = "ttir.bitwise_and"(%261, %25, %262) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %264 = ttir.empty() : tensor<13x13xi1>
    %265 = "ttir.ne"(%263, %33, %264) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %266 = ttir.empty() : tensor<13x13xui8>
    %267 = "ttir.typecast"(%265, %266) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %268 = ttir.empty() : tensor<13x1xi64>
    %269 = "ttir.reshape"(%1, %268) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1xi64>) -> tensor<13x1xi64>
    %270 = ttir.empty() : tensor<13x13xi64>
    %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %272 = ttir.empty() : tensor<13x13xi1>
    %273 = "ttir.le"(%247, %271, %272) : (tensor<13x13xi64>, tensor<13x13xi64>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %274 = ttir.empty() : tensor<13x13xui8>
    %275 = "ttir.typecast"(%273, %274) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %276 = ttir.empty() : tensor<13x13xui8>
    %277 = "ttir.bitwise_and"(%267, %275, %276) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %278 = ttir.empty() : tensor<13x13xi1>
    %279 = "ttir.ne"(%277, %33, %278) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %280 = ttir.empty() : tensor<1x1x13x13xi1>
    %281 = "ttir.reshape"(%279, %280) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xi1>, tensor<1x1x13x13xi1>) -> tensor<1x1x13x13xi1>
    %282 = ttir.empty() : tensor<1x1xbf16>
    %283 = "ttir.reshape"(%arg16, %282) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %284 = ttir.empty() : tensor<1x1x1x1xbf16>
    %285 = "ttir.reshape"(%283, %284) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xbf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %286 = ttir.empty() : tensor<1x1x13x13xbf16>
    %287 = "ttir.broadcast"(%285, %286) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %288 = ttir.empty() : tensor<1x1x13x13xbf16>
    %289 = "ttir.where"(%281, %21, %287, %288) : (tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %290 = ttir.empty() : tensor<1x13x13xbf16>
    %291 = "ttir.reshape"(%289, %290) <{shape = [1 : i32, 13 : i32, 13 : i32]}> : (tensor<1x1x13x13xbf16>, tensor<1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %292 = ttir.empty() : tensor<1x1x13x13xbf16>
    %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %294 = ttir.empty() : tensor<1x64x13x13xbf16>
    %295 = "ttir.broadcast"(%293, %294) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %296 = ttir.empty() : tensor<1x64x13x13xbf16>
    %297 = "ttir.add"(%243, %295, %296) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %298 = ttir.empty() : tensor<1x64x1xbf16>
    %299 = "ttir.reshape"(%arg15, %298) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1xbf16>) -> tensor<1x64x1xbf16>
    %300 = ttir.empty() : tensor<1x64x1x1xbf16>
    %301 = "ttir.reshape"(%299, %300) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x1xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %302 = ttir.empty() : tensor<1x64x13x1xbf16>
    %303 = "ttir.broadcast"(%301, %302) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %304 = ttir.empty() : tensor<1x64x13x14xbf16>
    %305 = "ttir.concat"(%297, %303, %304) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %306 = ttir.empty() : tensor<2880x512xbf16>
    %307 = "ttir.permute"(%arg1, %306) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %308 = "ttir.dot_general"(%79, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %309 = ttir.empty() : tensor<1x13x512xbf16>
    %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %311 = ttir.empty() : tensor<1x1x512xbf16>
    %312 = "ttir.reshape"(%arg0, %311) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %313 = ttir.empty() : tensor<1x13x512xbf16>
    %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %315 = ttir.empty() : tensor<1x13x512xbf16>
    %316 = "ttir.add"(%310, %314, %315) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %317 = ttir.empty() : tensor<1x13x8x64xbf16>
    %318 = "ttir.reshape"(%316, %317) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %319 = ttir.empty() : tensor<1x8x13x64xbf16>
    %320 = "ttir.permute"(%318, %319) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %321 = ttir.empty() : tensor<2880xf32>
    %322 = "ttir.typecast"(%arg30, %321) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %323 = ttir.empty() : tensor<1x1x2880xf32>
    %324 = "ttir.reshape"(%322, %323) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %325 = ttir.empty() : tensor<1x13x2880xf32>
    %326 = "ttir.broadcast"(%324, %325) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %327 = ttir.empty() : tensor<1x64x13xbf16>
    %328 = "ttir.max"(%305, %327) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %329 = ttir.empty() : tensor<1x64x13x1xbf16>
    %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %331 = ttir.empty() : tensor<1x64x13x14xbf16>
    %332 = "ttir.broadcast"(%330, %331) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %333 = ttir.empty() : tensor<1x64x13x14xbf16>
    %334 = "ttir.subtract"(%305, %332, %333) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %335 = ttir.empty() : tensor<1x64x13xbf16>
    %336 = "ttir.max"(%334, %335) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %337 = ttir.empty() : tensor<1x64x13x1xbf16>
    %338 = "ttir.reshape"(%336, %337) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %339 = ttir.empty() : tensor<1x64x13x14xbf16>
    %340 = "ttir.broadcast"(%338, %339) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %341 = ttir.empty() : tensor<1x64x13x14xbf16>
    %342 = "ttir.subtract"(%334, %340, %341) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %343 = ttir.empty() : tensor<1x64x13x14xbf16>
    %344 = "ttir.exp"(%342, %343) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %345 = ttir.empty() : tensor<1x64x13xbf16>
    %346 = "ttir.sum"(%344, %345) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %347 = ttir.empty() : tensor<1x64x13x1xbf16>
    %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %349 = ttir.empty() : tensor<1x64x13x14xbf16>
    %350 = "ttir.broadcast"(%348, %349) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %351 = ttir.empty() : tensor<1x64x13x14xbf16>
    %352 = "ttir.div"(%344, %350, %351) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %353 = ttir.empty() : tensor<1x64x13x13xbf16>
    %354 = "ttir.slice_static"(%352, %353) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %355 = ttir.empty() : tensor<64x13x13xbf16>
    %356 = "ttir.reshape"(%354, %355) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %357 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %358 = "ttir.reshape"(%320, %357) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %359 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %361 = ttir.empty() : tensor<64x13x64xbf16>
    %362 = "ttir.reshape"(%360, %361) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %363 = "ttir.dot_general"(%356, %362) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %364 = ttir.empty() : tensor<1x64x13x64xbf16>
    %365 = "ttir.reshape"(%363, %364) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %366 = ttir.empty() : tensor<1x13x64x64xbf16>
    %367 = "ttir.permute"(%365, %366) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %368 = ttir.empty() : tensor<13x4096xbf16>
    %369 = "ttir.reshape"(%367, %368) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
    %370 = ttir.empty() : tensor<4096x2880xbf16>
    %371 = "ttir.permute"(%arg14, %370) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
    %372 = "ttir.dot_general"(%369, %371) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %373 = ttir.empty() : tensor<1x13x2880xbf16>
    %374 = "ttir.reshape"(%372, %373) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %375 = ttir.empty() : tensor<1x1x2880xbf16>
    %376 = "ttir.reshape"(%arg13, %375) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
    %377 = ttir.empty() : tensor<1x13x2880xbf16>
    %378 = "ttir.broadcast"(%376, %377) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %379 = ttir.empty() : tensor<1x13x2880xbf16>
    %380 = "ttir.add"(%374, %378, %379) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %381 = ttir.empty() : tensor<1x13x2880xbf16>
    %382 = "ttir.add"(%47, %380, %381) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %383 = ttir.empty() : tensor<1x1x1xbf16>
    %384 = "ttir.reshape"(%arg29, %383) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %385 = ttir.empty() : tensor<32x13x2880xbf16>
    %386 = "ttir.broadcast"(%384, %385) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %387 = ttir.empty() : tensor<2880xf32>
    %388 = "ttir.typecast"(%arg21, %387) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %389 = ttir.empty() : tensor<1x1x2880xf32>
    %390 = "ttir.reshape"(%388, %389) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %391 = ttir.empty() : tensor<1x13x2880xf32>
    %392 = "ttir.broadcast"(%390, %391) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %393 = ttir.empty() : tensor<1x13x2880xf32>
    %394 = "ttir.typecast"(%382, %393) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %395 = ttir.empty() : tensor<1x13x2880xf32>
    %396 = "ttir.pow"(%394, %29, %395) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %397 = ttir.empty() : tensor<1x13xf32>
    %398 = "ttir.sum"(%396, %397) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %399 = ttir.empty() : tensor<1x13xf32>
    %400 = "ttir.multiply"(%398, %5, %399) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %401 = ttir.empty() : tensor<1x13x1xf32>
    %402 = "ttir.reshape"(%400, %401) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %403 = ttir.empty() : tensor<1x13x1xf32>
    %404 = "ttir.add"(%402, %61, %403) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %405 = ttir.empty() : tensor<1x13x1xf32>
    %406 = "ttir.rsqrt"(%404, %405) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %407 = ttir.empty() : tensor<1x13xf32>
    %408 = "ttir.reshape"(%406, %407) <{shape = [1 : i32, 13 : i32]}> : (tensor<1x13x1xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %409 = ttir.empty() : tensor<1x13x1xf32>
    %410 = "ttir.reshape"(%408, %409) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %411 = ttir.empty() : tensor<1x13x2880xf32>
    %412 = "ttir.broadcast"(%410, %411) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %413 = ttir.empty() : tensor<1x13x2880xf32>
    %414 = "ttir.multiply"(%394, %412, %413) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %415 = ttir.empty() : tensor<1x13x2880xf32>
    %416 = "ttir.multiply"(%392, %414, %415) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %417 = ttir.empty() : tensor<1x13x2880xbf16>
    %418 = "ttir.typecast"(%416, %417) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %419 = ttir.empty() : tensor<13x2880xbf16>
    %420 = "ttir.reshape"(%418, %419) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %421 = ttir.empty() : tensor<416x2880xbf16>
    %422 = "ttir.concat"(%420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %421) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
    %423 = ttir.empty() : tensor<32x13x2880xbf16>
    %424 = "ttir.reshape"(%422, %423) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %425 = "ttir.dot_general"(%424, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %426 = ttir.empty() : tensor<32x1x5760xbf16>
    %427 = "ttir.reshape"(%arg27, %426) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
    %428 = ttir.empty() : tensor<32x13x5760xbf16>
    %429 = "ttir.broadcast"(%427, %428) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %430 = ttir.empty() : tensor<32x13x5760xbf16>
    %431 = "ttir.add"(%425, %429, %430) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %432 = ttir.empty() : tensor<32x13x2880xbf16>
    %433 = "ttir.slice_static"(%431, %432) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %434 = ttir.empty() : tensor<1x1x1xbf16>
    %435 = "ttir.reshape"(%arg25, %434) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %436 = ttir.empty() : tensor<32x13x2880xbf16>
    %437 = "ttir.broadcast"(%435, %436) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %438 = ttir.empty() : tensor<32x13x2880xbf16>
    %439 = "ttir.clamp_tensor"(%433, %386, %437, %438) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %440 = ttir.empty() : tensor<32x13x2880xbf16>
    %441 = "ttir.add"(%439, %17, %440) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %442 = ttir.empty() : tensor<32x13x2880xf32>
    %443 = "ttir.typecast"(%441, %442) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %444 = ttir.empty() : tensor<1x1x1xbf16>
    %445 = "ttir.reshape"(%arg26, %444) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %446 = ttir.empty() : tensor<32x13x2880xbf16>
    %447 = "ttir.broadcast"(%445, %446) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %448 = ttir.empty() : tensor<32x13x2880xbf16>
    %449 = "ttir.slice_static"(%431, %448) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %450 = ttir.empty() : tensor<32x13x2880xbf16>
    %451 = "ttir.clamp_tensor"(%449, %447, %437, %450) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %452 = ttir.empty() : tensor<32x13x2880xf32>
    %453 = "ttir.typecast"(%451, %452) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %454 = ttir.empty() : tensor<1x1x1xf32>
    %455 = "ttir.reshape"(%arg24, %454) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %456 = ttir.empty() : tensor<32x13x2880xf32>
    %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %458 = ttir.empty() : tensor<32x13x2880xf32>
    %459 = "ttir.multiply"(%453, %457, %458) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %460 = ttir.empty() : tensor<32x13x2880xbf16>
    %461 = "ttir.typecast"(%459, %460) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %462 = ttir.empty() : tensor<32x13x2880xbf16>
    %463 = "ttir.sigmoid"(%461, %462) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %464 = ttir.empty() : tensor<32x13x2880xf32>
    %465 = "ttir.typecast"(%463, %464) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %466 = ttir.empty() : tensor<32x13x2880xf32>
    %467 = "ttir.multiply"(%453, %465, %466) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %468 = ttir.empty() : tensor<32x13x2880xbf16>
    %469 = "ttir.typecast"(%467, %468) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %470 = ttir.empty() : tensor<32x13x2880xf32>
    %471 = "ttir.typecast"(%469, %470) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %472 = ttir.empty() : tensor<32x13x2880xf32>
    %473 = "ttir.multiply"(%443, %471, %472) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %474 = ttir.empty() : tensor<32x13x2880xbf16>
    %475 = "ttir.typecast"(%473, %474) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %476 = "ttir.dot_general"(%475, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %477 = ttir.empty() : tensor<32x1x2880xbf16>
    %478 = "ttir.reshape"(%arg22, %477) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
    %479 = ttir.empty() : tensor<32x13x2880xbf16>
    %480 = "ttir.broadcast"(%478, %479) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %481 = ttir.empty() : tensor<32x13x2880xbf16>
    %482 = "ttir.add"(%476, %480, %481) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %483 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %484 = "ttir.reshape"(%482, %483) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %485 = ttir.empty() : tensor<32x1x13x2880xf32>
    %486 = "ttir.typecast"(%484, %485) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %487 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xi64>
    %488 = ttir.empty() : tensor<13x1x1xi64>
    %489 = "ttir.reshape"(%487, %488) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1x1xi64>) -> tensor<13x1x1xi64>
    %490 = ttir.empty() : tensor<13x4x1xi64>
    %491 = "ttir.broadcast"(%489, %490) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x1xi64>
    %492 = ttir.empty() : tensor<2880x32xbf16>
    %493 = "ttir.permute"(%arg12, %492) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
    %494 = "ttir.dot_general"(%420, %493) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %495 = ttir.empty() : tensor<1x32xbf16>
    %496 = "ttir.reshape"(%arg11, %495) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
    %497 = ttir.empty() : tensor<13x32xbf16>
    %498 = "ttir.broadcast"(%496, %497) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %499 = ttir.empty() : tensor<13x32xbf16>
    %500 = "ttir.add"(%494, %498, %499) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %501 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 32 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<32xi32>
    %502 = ttir.empty() : tensor<1x32xi32>
    %503 = "ttir.reshape"(%501, %502) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xi32>, tensor<1x32xi32>) -> tensor<1x32xi32>
    %504 = ttir.empty() : tensor<13x32xi32>
    %505 = "ttir.broadcast"(%503, %504) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xi32>, tensor<13x32xi32>) -> tensor<13x32xi32>
    %506 = ttir.empty() : tensor<13x32xbf16>
    %507 = ttir.empty() : tensor<13x32xi32>
    %values, %indices = "ttir.sort"(%500, %506, %507) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %508 = ttir.empty() : tensor<13x4xi32>
    %509 = "ttir.slice_static"(%indices, %508) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xi32>, tensor<13x4xi32>) -> tensor<13x4xi32>
    %510 = ttir.empty() : tensor<13x4xi64>
    %511 = "ttir.typecast"(%509, %510) <{conservative_folding = false}> : (tensor<13x4xi32>, tensor<13x4xi64>) -> tensor<13x4xi64>
    %512 = ttir.empty() : tensor<13x4x1xi64>
    %513 = "ttir.reshape"(%511, %512) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xi64>, tensor<13x4x1xi64>) -> tensor<13x4x1xi64>
    %514 = ttir.empty() : tensor<13x4x2xi64>
    %515 = "ttir.concat"(%491, %513, %514) <{dim = 2 : si32}> : (tensor<13x4x1xi64>, tensor<13x4x1xi64>, tensor<13x4x2xi64>) -> tensor<13x4x2xi64>
    %516 = ttir.empty() : tensor<13x4xbf16>
    %517 = "ttir.slice_static"(%values, %516) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %518 = ttir.empty() : tensor<13xbf16>
    %519 = "ttir.max"(%517, %518) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %520 = ttir.empty() : tensor<13x1xbf16>
    %521 = "ttir.reshape"(%519, %520) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %522 = ttir.empty() : tensor<13x4xbf16>
    %523 = "ttir.broadcast"(%521, %522) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %524 = ttir.empty() : tensor<13x4xbf16>
    %525 = "ttir.subtract"(%517, %523, %524) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %526 = ttir.empty() : tensor<13x4xbf16>
    %527 = "ttir.exp"(%525, %526) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %528 = ttir.empty() : tensor<13xbf16>
    %529 = "ttir.sum"(%527, %528) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %530 = ttir.empty() : tensor<13x1xbf16>
    %531 = "ttir.reshape"(%529, %530) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %532 = ttir.empty() : tensor<13x4xbf16>
    %533 = "ttir.broadcast"(%531, %532) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %534 = ttir.empty() : tensor<13x4xbf16>
    %535 = "ttir.div"(%527, %533, %534) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %536 = ttir.empty() : tensor<13x32xbf16>
    %537 = "ttir.scatter"(%13, %515, %535, %536) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %538 = ttir.empty() : tensor<32x13xbf16>
    %539 = "ttir.permute"(%537, %538) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
    %540 = ttir.empty() : tensor<32x1x13x1xbf16>
    %541 = "ttir.reshape"(%539, %540) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
    %542 = ttir.empty() : tensor<32x1x13x1xf32>
    %543 = "ttir.typecast"(%541, %542) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
    %544 = ttir.empty() : tensor<32x1x13xf32>
    %545 = "ttir.reshape"(%543, %544) <{shape = [32 : i32, 1 : i32, 13 : i32]}> : (tensor<32x1x13x1xf32>, tensor<32x1x13xf32>) -> tensor<32x1x13xf32>
    %546 = ttir.empty() : tensor<32x1x13x1xf32>
    %547 = "ttir.reshape"(%545, %546) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x1x13xf32>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
    %548 = ttir.empty() : tensor<32x1x13x2880xf32>
    %549 = "ttir.broadcast"(%547, %548) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %550 = ttir.empty() : tensor<32x1x13x2880xf32>
    %551 = "ttir.multiply"(%486, %549, %550) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %552 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %553 = "ttir.typecast"(%551, %552) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %554 = ttir.empty() : tensor<1x13x2880xbf16>
    %555 = "ttir.sum"(%553, %554) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %556 = ttir.empty() : tensor<1x13x2880xbf16>
    %557 = "ttir.add"(%382, %555, %556) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %558 = ttir.empty() : tensor<1x13x2880xf32>
    %559 = "ttir.typecast"(%557, %558) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %560 = ttir.empty() : tensor<1x13x2880xf32>
    %561 = "ttir.pow"(%559, %29, %560) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %562 = ttir.empty() : tensor<1x13xf32>
    %563 = "ttir.sum"(%561, %562) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %564 = ttir.empty() : tensor<1x13xf32>
    %565 = "ttir.multiply"(%563, %5, %564) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %566 = ttir.empty() : tensor<1x13x1xf32>
    %567 = "ttir.reshape"(%565, %566) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %568 = ttir.empty() : tensor<1x13x1xf32>
    %569 = "ttir.add"(%567, %61, %568) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %570 = ttir.empty() : tensor<1x13x1xf32>
    %571 = "ttir.rsqrt"(%569, %570) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %572 = ttir.empty() : tensor<1x13xf32>
    %573 = "ttir.reshape"(%571, %572) <{shape = [1 : i32, 13 : i32]}> : (tensor<1x13x1xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %574 = ttir.empty() : tensor<1x13x1xf32>
    %575 = "ttir.reshape"(%573, %574) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %576 = ttir.empty() : tensor<1x13x2880xf32>
    %577 = "ttir.broadcast"(%575, %576) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %578 = ttir.empty() : tensor<1x13x2880xf32>
    %579 = "ttir.multiply"(%559, %577, %578) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %580 = ttir.empty() : tensor<1x13x2880xf32>
    %581 = "ttir.multiply"(%326, %579, %580) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %582 = ttir.empty() : tensor<1x13x2880xbf16>
    %583 = "ttir.typecast"(%581, %582) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %584 = ttir.empty() : tensor<13x2880xbf16>
    %585 = "ttir.reshape"(%583, %584) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %586 = ttir.empty() : tensor<2880x201088xbf16>
    %587 = "ttir.permute"(%arg10, %586) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
    %588 = "ttir.dot_general"(%585, %587) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %589 = ttir.empty() : tensor<1x13x201088xbf16>
    %590 = "ttir.reshape"(%588, %589) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %316, %318, %320, %220, %588, %590 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}
// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>}> : () -> tensor<13xi64>
    %2 = "ttir.constant"() <{value = dense<0> : tensor<ui8>}> : () -> tensor<ui8>
    %3 = "ttir.constant"() <{value = dense<0xFF80> : tensor<bf16>}> : () -> tensor<bf16>
    %4 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %5 = "ttir.constant"() <{value = dense<3.47222231E-4> : tensor<1x13xf32>}> : () -> tensor<1x13xf32>
    %6 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
    %7 = "ttir.constant"() <{value = dense<1> : tensor<ui8>}> : () -> tensor<ui8>
    %8 = "ttir.constant"() <{value = dense<1.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %9 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %10 = ttir.empty() : tensor<1x1xbf16>
    %11 = "ttir.reshape"(%9, %10) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %12 = ttir.empty() : tensor<13x32xbf16>
    %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %14 = ttir.empty() : tensor<1x1x1xbf16>
    %15 = "ttir.reshape"(%8, %14) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %16 = ttir.empty() : tensor<32x13x2880xbf16>
    %17 = "ttir.broadcast"(%15, %16) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %18 = ttir.empty() : tensor<1x1x1x1xbf16>
    %19 = "ttir.reshape"(%9, %18) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %20 = ttir.empty() : tensor<1x1x13x13xbf16>
    %21 = "ttir.broadcast"(%19, %20) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %22 = ttir.empty() : tensor<1x1xui8>
    %23 = "ttir.reshape"(%7, %22) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %24 = ttir.empty() : tensor<13x13xui8>
    %25 = "ttir.broadcast"(%23, %24) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %26 = ttir.empty() : tensor<1x1x1xf32>
    %27 = "ttir.reshape"(%4, %26) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %28 = ttir.empty() : tensor<1x13x2880xf32>
    %29 = "ttir.broadcast"(%27, %28) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %30 = ttir.empty() : tensor<1x1xui8>
    %31 = "ttir.reshape"(%2, %30) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %32 = ttir.empty() : tensor<13x13xui8>
    %33 = "ttir.broadcast"(%31, %32) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %34 = ttir.empty() : tensor<2880xf32>
    %35 = "ttir.typecast"(%arg5, %34) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %36 = ttir.empty() : tensor<1x1x2880xf32>
    %37 = "ttir.reshape"(%35, %36) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %38 = ttir.empty() : tensor<1x13x2880xf32>
    %39 = "ttir.broadcast"(%37, %38) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %40 = ttir.empty() : tensor<13xi64>
    %41 = "ttir.reshape"(%arg3, %40) <{shape = [13 : i32]}> : (tensor<1x13xi64>, tensor<13xi64>) -> tensor<13xi64>
    %42 = ttir.empty() : tensor<13xui32>
    %43 = "ttir.typecast"(%41, %42) <{conservative_folding = false}> : (tensor<13xi64>, tensor<13xui32>) -> tensor<13xui32>
    %44 = ttir.empty() : tensor<13x2880xbf16>
    %45 = "ttir.gather"(%arg4, %43, %44) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %46 = ttir.empty() : tensor<1x13x2880xbf16>
    %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %48 = ttir.empty() : tensor<1x13x2880xf32>
    %49 = "ttir.typecast"(%47, %48) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %50 = ttir.empty() : tensor<1x13x2880xf32>
    %51 = "ttir.pow"(%49, %29, %50) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %52 = ttir.empty() : tensor<1x13xf32>
    %53 = "ttir.sum"(%51, %52) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %54 = ttir.empty() : tensor<1x13xf32>
    %55 = "ttir.multiply"(%53, %5, %54) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %56 = ttir.empty() : tensor<1x13x1xf32>
    %57 = "ttir.reshape"(%55, %56) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %58 = ttir.empty() : tensor<1x1x1xf32>
    %59 = "ttir.reshape"(%arg2, %58) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %60 = ttir.empty() : tensor<1x13x1xf32>
    %61 = "ttir.broadcast"(%59, %60) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %62 = ttir.empty() : tensor<1x13x1xf32>
    %63 = "ttir.add"(%57, %61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %64 = ttir.empty() : tensor<1x13x1xf32>
    %65 = "ttir.rsqrt"(%63, %64) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %66 = ttir.empty() : tensor<1x13xf32>
    %67 = "ttir.reshape"(%65, %66) <{shape = [1 : i32, 13 : i32]}> : (tensor<1x13x1xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %68 = ttir.empty() : tensor<1x13x1xf32>
    %69 = "ttir.reshape"(%67, %68) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %70 = ttir.empty() : tensor<1x13x2880xf32>
    %71 = "ttir.broadcast"(%69, %70) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %72 = ttir.empty() : tensor<1x13x2880xf32>
    %73 = "ttir.multiply"(%49, %71, %72) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %74 = ttir.empty() : tensor<1x13x2880xf32>
    %75 = "ttir.multiply"(%39, %73, %74) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %76 = ttir.empty() : tensor<1x13x2880xbf16>
    %77 = "ttir.typecast"(%75, %76) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %78 = ttir.empty() : tensor<13x2880xbf16>
    %79 = "ttir.reshape"(%77, %78) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %80 = ttir.empty() : tensor<2880x4096xbf16>
    %81 = "ttir.permute"(%arg20, %80) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
    %82 = "ttir.dot_general"(%79, %81) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %83 = ttir.empty() : tensor<1x13x4096xbf16>
    %84 = "ttir.reshape"(%82, %83) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %85 = ttir.empty() : tensor<1x1x4096xbf16>
    %86 = "ttir.reshape"(%arg19, %85) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %87 = ttir.empty() : tensor<1x13x4096xbf16>
    %88 = "ttir.broadcast"(%86, %87) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %89 = ttir.empty() : tensor<1x13x4096xbf16>
    %90 = "ttir.add"(%84, %88, %89) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %91 = ttir.empty() : tensor<1x13x64x64xbf16>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %93 = ttir.empty() : tensor<1x64x13x64xbf16>
    %94 = "ttir.permute"(%92, %93) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %95 = ttir.empty() : tensor<1x64x13x32xbf16>
    %96 = "ttir.slice_static"(%94, %95) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %97 = ttir.empty() : tensor<1x64x13x32xf32>
    %98 = "ttir.typecast"(%96, %97) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %99 = ttir.empty() : tensor<1x32x1xf32>
    %100 = "ttir.reshape"(%arg7, %99) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
    %101 = "ttir.dot_general"(%100, %6) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %102 = ttir.empty() : tensor<1x13x32xf32>
    %103 = "ttir.permute"(%101, %102) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %104 = ttir.empty() : tensor<1x13x32xf32>
    %105 = "ttir.cos"(%103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %106 = ttir.empty() : tensor<1x1x1xf32>
    %107 = "ttir.reshape"(%arg6, %106) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %108 = ttir.empty() : tensor<1x13x32xf32>
    %109 = "ttir.broadcast"(%107, %108) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %110 = ttir.empty() : tensor<1x13x32xf32>
    %111 = "ttir.multiply"(%105, %109, %110) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %112 = ttir.empty() : tensor<1x13x32xbf16>
    %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %114 = ttir.empty() : tensor<1x1x13x32xbf16>
    %115 = "ttir.reshape"(%113, %114) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %116 = ttir.empty() : tensor<1x1x13x32xf32>
    %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %118 = ttir.empty() : tensor<1x13x32xf32>
    %119 = "ttir.reshape"(%117, %118) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %120 = ttir.empty() : tensor<1x1x13x32xf32>
    %121 = "ttir.reshape"(%119, %120) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %122 = ttir.empty() : tensor<1x64x13x32xf32>
    %123 = "ttir.broadcast"(%121, %122) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %124 = ttir.empty() : tensor<1x64x13x32xf32>
    %125 = "ttir.multiply"(%98, %123, %124) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %126 = ttir.empty() : tensor<1x64x13x32xbf16>
    %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %128 = ttir.empty() : tensor<1x64x13x32xbf16>
    %129 = "ttir.slice_static"(%94, %128) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %130 = ttir.empty() : tensor<1x64x13x32xf32>
    %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %132 = ttir.empty() : tensor<1x13x32xf32>
    %133 = "ttir.sin"(%103, %132) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %134 = ttir.empty() : tensor<1x13x32xf32>
    %135 = "ttir.multiply"(%133, %109, %134) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %136 = ttir.empty() : tensor<1x13x32xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %138 = ttir.empty() : tensor<1x1x13x32xbf16>
    %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %140 = ttir.empty() : tensor<1x1x13x32xf32>
    %141 = "ttir.typecast"(%139, %140) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %142 = ttir.empty() : tensor<1x13x32xf32>
    %143 = "ttir.reshape"(%141, %142) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %144 = ttir.empty() : tensor<1x1x13x32xf32>
    %145 = "ttir.reshape"(%143, %144) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %146 = ttir.empty() : tensor<1x64x13x32xf32>
    %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %148 = ttir.empty() : tensor<1x64x13x32xf32>
    %149 = "ttir.multiply"(%131, %147, %148) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %150 = ttir.empty() : tensor<1x64x13x32xbf16>
    %151 = "ttir.typecast"(%149, %150) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %152 = ttir.empty() : tensor<1x64x13x32xbf16>
    %153 = "ttir.subtract"(%127, %151, %152) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %154 = ttir.empty() : tensor<1x64x13x32xf32>
    %155 = "ttir.multiply"(%131, %123, %154) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %156 = ttir.empty() : tensor<1x64x13x32xbf16>
    %157 = "ttir.typecast"(%155, %156) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %158 = ttir.empty() : tensor<1x64x13x32xf32>
    %159 = "ttir.multiply"(%98, %147, %158) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %160 = ttir.empty() : tensor<1x64x13x32xbf16>
    %161 = "ttir.typecast"(%159, %160) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %162 = ttir.empty() : tensor<1x64x13x32xbf16>
    %163 = "ttir.add"(%157, %161, %162) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %164 = ttir.empty() : tensor<1x64x13x64xbf16>
    %165 = "ttir.concat"(%153, %163, %164) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %166 = ttir.empty() : tensor<64x13x64xbf16>
    %167 = "ttir.reshape"(%165, %166) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %168 = ttir.empty() : tensor<2880x512xbf16>
    %169 = "ttir.permute"(%arg9, %168) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %170 = "ttir.dot_general"(%79, %169) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %171 = ttir.empty() : tensor<1x13x512xbf16>
    %172 = "ttir.reshape"(%170, %171) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %173 = ttir.empty() : tensor<1x1x512xbf16>
    %174 = "ttir.reshape"(%arg8, %173) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %175 = ttir.empty() : tensor<1x13x512xbf16>
    %176 = "ttir.broadcast"(%174, %175) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %177 = ttir.empty() : tensor<1x13x512xbf16>
    %178 = "ttir.add"(%172, %176, %177) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %179 = ttir.empty() : tensor<1x13x8x64xbf16>
    %180 = "ttir.reshape"(%178, %179) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %181 = ttir.empty() : tensor<1x8x13x64xbf16>
    %182 = "ttir.permute"(%180, %181) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %183 = ttir.empty() : tensor<1x8x13x32xbf16>
    %184 = "ttir.slice_static"(%182, %183) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %185 = ttir.empty() : tensor<1x8x13x32xf32>
    %186 = "ttir.typecast"(%184, %185) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %187 = ttir.empty() : tensor<1x1x13x32xf32>
    %188 = "ttir.reshape"(%119, %187) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %189 = ttir.empty() : tensor<1x8x13x32xf32>
    %190 = "ttir.broadcast"(%188, %189) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %191 = ttir.empty() : tensor<1x8x13x32xf32>
    %192 = "ttir.multiply"(%186, %190, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %193 = ttir.empty() : tensor<1x8x13x32xbf16>
    %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %195 = ttir.empty() : tensor<1x8x13x32xbf16>
    %196 = "ttir.slice_static"(%182, %195) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %197 = ttir.empty() : tensor<1x8x13x32xf32>
    %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %199 = ttir.empty() : tensor<1x1x13x32xf32>
    %200 = "ttir.reshape"(%143, %199) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %201 = ttir.empty() : tensor<1x8x13x32xf32>
    %202 = "ttir.broadcast"(%200, %201) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %203 = ttir.empty() : tensor<1x8x13x32xf32>
    %204 = "ttir.multiply"(%198, %202, %203) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %205 = ttir.empty() : tensor<1x8x13x32xbf16>
    %206 = "ttir.typecast"(%204, %205) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %207 = ttir.empty() : tensor<1x8x13x32xbf16>
    %208 = "ttir.subtract"(%194, %206, %207) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %209 = ttir.empty() : tensor<1x8x13x32xf32>
    %210 = "ttir.multiply"(%198, %190, %209) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %211 = ttir.empty() : tensor<1x8x13x32xbf16>
    %212 = "ttir.typecast"(%210, %211) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %213 = ttir.empty() : tensor<1x8x13x32xf32>
    %214 = "ttir.multiply"(%186, %202, %213) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %215 = ttir.empty() : tensor<1x8x13x32xbf16>
    %216 = "ttir.typecast"(%214, %215) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %217 = ttir.empty() : tensor<1x8x13x32xbf16>
    %218 = "ttir.add"(%212, %216, %217) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %219 = ttir.empty() : tensor<1x8x13x64xbf16>
    %220 = "ttir.concat"(%208, %218, %219) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %221 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %222 = "ttir.reshape"(%220, %221) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %223 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %224 = "ttir.broadcast"(%222, %223) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %225 = ttir.empty() : tensor<1x64x13x64xbf16>
    %226 = "ttir.reshape"(%224, %225) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %227 = ttir.empty() : tensor<1x64x64x13xbf16>
    %228 = "ttir.permute"(%226, %227) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
    %229 = ttir.empty() : tensor<64x64x13xbf16>
    %230 = "ttir.reshape"(%228, %229) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %231 = "ttir.dot_general"(%167, %230) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %232 = ttir.empty() : tensor<1x64x13x13xbf16>
    %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %234 = ttir.empty() : tensor<1x64x13x13xf32>
    %235 = "ttir.typecast"(%233, %234) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %236 = ttir.empty() : tensor<1x1x1x1xf32>
    %237 = "ttir.reshape"(%arg18, %236) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %238 = ttir.empty() : tensor<1x64x13x13xf32>
    %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %240 = ttir.empty() : tensor<1x64x13x13xf32>
    %241 = "ttir.multiply"(%235, %239, %240) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %242 = ttir.empty() : tensor<1x64x13x13xbf16>
    %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %244 = ttir.empty() : tensor<1x13xi64>
    %245 = "ttir.reshape"(%1, %244) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xi64>, tensor<1x13xi64>) -> tensor<1x13xi64>
    %246 = ttir.empty() : tensor<13x13xi64>
    %247 = "ttir.broadcast"(%245, %246) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %248 = ttir.empty() : tensor<1xi64>
    %249 = "ttir.reshape"(%arg17, %248) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %250 = ttir.empty() : tensor<13xi64>
    %251 = "ttir.broadcast"(%249, %250) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xi64>, tensor<13xi64>) -> tensor<13xi64>
    %252 = ttir.empty() : tensor<13xi64>
    %253 = "ttir.subtract"(%1, %251, %252) : (tensor<13xi64>, tensor<13xi64>, tensor<13xi64>) -> tensor<13xi64>
    %254 = ttir.empty() : tensor<13x1xi64>
    %255 = "ttir.reshape"(%253, %254) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1xi64>) -> tensor<13x1xi64>
    %256 = ttir.empty() : tensor<13x13xi64>
    %257 = "ttir.broadcast"(%255, %256) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %258 = ttir.empty() : tensor<13x13xi1>
    %259 = "ttir.gt"(%247, %257, %258) : (tensor<13x13xi64>, tensor<13x13xi64>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %260 = ttir.empty() : tensor<13x13xui8>
    %261 = "ttir.typecast"(%259, %260) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %262 = ttir.empty() : tensor<13x13xui8>
    %263 = "ttir.bitwise_and"(%261, %25, %262) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %264 = ttir.empty() : tensor<13x13xi1>
    %265 = "ttir.ne"(%263, %33, %264) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %266 = ttir.empty() : tensor<13x13xui8>
    %267 = "ttir.typecast"(%265, %266) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %268 = ttir.empty() : tensor<13x1xi64>
    %269 = "ttir.reshape"(%1, %268) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1xi64>) -> tensor<13x1xi64>
    %270 = ttir.empty() : tensor<13x13xi64>
    %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %272 = ttir.empty() : tensor<13x13xi1>
    %273 = "ttir.le"(%247, %271, %272) : (tensor<13x13xi64>, tensor<13x13xi64>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %274 = ttir.empty() : tensor<13x13xui8>
    %275 = "ttir.typecast"(%273, %274) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %276 = ttir.empty() : tensor<13x13xui8>
    %277 = "ttir.bitwise_and"(%267, %275, %276) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %278 = ttir.empty() : tensor<13x13xi1>
    %279 = "ttir.ne"(%277, %33, %278) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %280 = ttir.empty() : tensor<1x1x13x13xi1>
    %281 = "ttir.reshape"(%279, %280) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xi1>, tensor<1x1x13x13xi1>) -> tensor<1x1x13x13xi1>
    %282 = ttir.empty() : tensor<1x1xbf16>
    %283 = "ttir.reshape"(%arg16, %282) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %284 = ttir.empty() : tensor<1x1x1x1xbf16>
    %285 = "ttir.reshape"(%283, %284) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xbf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %286 = ttir.empty() : tensor<1x1x13x13xbf16>
    %287 = "ttir.broadcast"(%285, %286) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %288 = ttir.empty() : tensor<1x1x13x13xbf16>
    %289 = "ttir.where"(%281, %21, %287, %288) : (tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %290 = ttir.empty() : tensor<1x13x13xbf16>
    %291 = "ttir.reshape"(%289, %290) <{shape = [1 : i32, 13 : i32, 13 : i32]}> : (tensor<1x1x13x13xbf16>, tensor<1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %292 = ttir.empty() : tensor<1x1x13x13xbf16>
    %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %294 = ttir.empty() : tensor<1x64x13x13xbf16>
    %295 = "ttir.broadcast"(%293, %294) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %296 = ttir.empty() : tensor<1x64x13x13xbf16>
    %297 = "ttir.add"(%243, %295, %296) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %298 = ttir.empty() : tensor<1x64x1xbf16>
    %299 = "ttir.reshape"(%arg15, %298) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1xbf16>) -> tensor<1x64x1xbf16>
    %300 = ttir.empty() : tensor<1x64x1x1xbf16>
    %301 = "ttir.reshape"(%299, %300) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x1xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %302 = ttir.empty() : tensor<1x64x13x1xbf16>
    %303 = "ttir.broadcast"(%301, %302) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %304 = ttir.empty() : tensor<1x64x13x14xbf16>
    %305 = "ttir.concat"(%297, %303, %304) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %306 = ttir.empty() : tensor<2880x512xbf16>
    %307 = "ttir.permute"(%arg1, %306) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %308 = "ttir.dot_general"(%79, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %309 = ttir.empty() : tensor<1x13x512xbf16>
    %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %311 = ttir.empty() : tensor<1x1x512xbf16>
    %312 = "ttir.reshape"(%arg0, %311) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %313 = ttir.empty() : tensor<1x13x512xbf16>
    %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %315 = ttir.empty() : tensor<1x13x512xbf16>
    %316 = "ttir.add"(%310, %314, %315) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %317 = ttir.empty() : tensor<1x13x8x64xbf16>
    %318 = "ttir.reshape"(%316, %317) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %319 = ttir.empty() : tensor<1x8x13x64xbf16>
    %320 = "ttir.permute"(%318, %319) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %321 = ttir.empty() : tensor<2880xf32>
    %322 = "ttir.typecast"(%arg30, %321) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %323 = ttir.empty() : tensor<1x1x2880xf32>
    %324 = "ttir.reshape"(%322, %323) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %325 = ttir.empty() : tensor<1x13x2880xf32>
    %326 = "ttir.broadcast"(%324, %325) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %327 = ttir.empty() : tensor<1x64x13xbf16>
    %328 = "ttir.max"(%305, %327) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %329 = ttir.empty() : tensor<1x64x13x1xbf16>
    %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %331 = ttir.empty() : tensor<1x64x13x14xbf16>
    %332 = "ttir.broadcast"(%330, %331) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %333 = ttir.empty() : tensor<1x64x13x14xbf16>
    %334 = "ttir.subtract"(%305, %332, %333) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %335 = ttir.empty() : tensor<1x64x13xbf16>
    %336 = "ttir.max"(%334, %335) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %337 = ttir.empty() : tensor<1x64x13x1xbf16>
    %338 = "ttir.reshape"(%336, %337) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %339 = ttir.empty() : tensor<1x64x13x14xbf16>
    %340 = "ttir.broadcast"(%338, %339) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %341 = ttir.empty() : tensor<1x64x13x14xbf16>
    %342 = "ttir.subtract"(%334, %340, %341) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %343 = ttir.empty() : tensor<1x64x13x14xbf16>
    %344 = "ttir.exp"(%342, %343) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %345 = ttir.empty() : tensor<1x64x13xbf16>
    %346 = "ttir.sum"(%344, %345) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %347 = ttir.empty() : tensor<1x64x13x1xbf16>
    %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %349 = ttir.empty() : tensor<1x64x13x14xbf16>
    %350 = "ttir.broadcast"(%348, %349) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %351 = ttir.empty() : tensor<1x64x13x14xbf16>
    %352 = "ttir.div"(%344, %350, %351) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %353 = ttir.empty() : tensor<1x64x13x13xbf16>
    %354 = "ttir.slice_static"(%352, %353) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %355 = ttir.empty() : tensor<64x13x13xbf16>
    %356 = "ttir.reshape"(%354, %355) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %357 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %358 = "ttir.reshape"(%320, %357) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %359 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %361 = ttir.empty() : tensor<64x13x64xbf16>
    %362 = "ttir.reshape"(%360, %361) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %363 = "ttir.dot_general"(%356, %362) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %364 = ttir.empty() : tensor<1x64x13x64xbf16>
    %365 = "ttir.reshape"(%363, %364) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %366 = ttir.empty() : tensor<1x13x64x64xbf16>
    %367 = "ttir.permute"(%365, %366) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %368 = ttir.empty() : tensor<13x4096xbf16>
    %369 = "ttir.reshape"(%367, %368) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
    %370 = ttir.empty() : tensor<4096x2880xbf16>
    %371 = "ttir.permute"(%arg14, %370) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
    %372 = "ttir.dot_general"(%369, %371) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %373 = ttir.empty() : tensor<1x13x2880xbf16>
    %374 = "ttir.reshape"(%372, %373) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %375 = ttir.empty() : tensor<1x1x2880xbf16>
    %376 = "ttir.reshape"(%arg13, %375) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
    %377 = ttir.empty() : tensor<1x13x2880xbf16>
    %378 = "ttir.broadcast"(%376, %377) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %379 = ttir.empty() : tensor<1x13x2880xbf16>
    %380 = "ttir.add"(%374, %378, %379) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %381 = ttir.empty() : tensor<1x13x2880xbf16>
    %382 = "ttir.add"(%47, %380, %381) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %383 = ttir.empty() : tensor<1x1x1xbf16>
    %384 = "ttir.reshape"(%arg29, %383) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %385 = ttir.empty() : tensor<32x13x2880xbf16>
    %386 = "ttir.broadcast"(%384, %385) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %387 = ttir.empty() : tensor<2880xf32>
    %388 = "ttir.typecast"(%arg21, %387) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %389 = ttir.empty() : tensor<1x1x2880xf32>
    %390 = "ttir.reshape"(%388, %389) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %391 = ttir.empty() : tensor<1x13x2880xf32>
    %392 = "ttir.broadcast"(%390, %391) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %393 = ttir.empty() : tensor<1x13x2880xf32>
    %394 = "ttir.typecast"(%382, %393) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %395 = ttir.empty() : tensor<1x13x2880xf32>
    %396 = "ttir.pow"(%394, %29, %395) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %397 = ttir.empty() : tensor<1x13xf32>
    %398 = "ttir.sum"(%396, %397) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %399 = ttir.empty() : tensor<1x13xf32>
    %400 = "ttir.multiply"(%398, %5, %399) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %401 = ttir.empty() : tensor<1x13x1xf32>
    %402 = "ttir.reshape"(%400, %401) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %403 = ttir.empty() : tensor<1x13x1xf32>
    %404 = "ttir.add"(%402, %61, %403) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %405 = ttir.empty() : tensor<1x13x1xf32>
    %406 = "ttir.rsqrt"(%404, %405) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %407 = ttir.empty() : tensor<1x13xf32>
    %408 = "ttir.reshape"(%406, %407) <{shape = [1 : i32, 13 : i32]}> : (tensor<1x13x1xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %409 = ttir.empty() : tensor<1x13x1xf32>
    %410 = "ttir.reshape"(%408, %409) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %411 = ttir.empty() : tensor<1x13x2880xf32>
    %412 = "ttir.broadcast"(%410, %411) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %413 = ttir.empty() : tensor<1x13x2880xf32>
    %414 = "ttir.multiply"(%394, %412, %413) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %415 = ttir.empty() : tensor<1x13x2880xf32>
    %416 = "ttir.multiply"(%392, %414, %415) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %417 = ttir.empty() : tensor<1x13x2880xbf16>
    %418 = "ttir.typecast"(%416, %417) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %419 = ttir.empty() : tensor<13x2880xbf16>
    %420 = "ttir.reshape"(%418, %419) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %421 = ttir.empty() : tensor<416x2880xbf16>
    %422 = "ttir.concat"(%420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %420, %421) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
    %423 = ttir.empty() : tensor<32x13x2880xbf16>
    %424 = "ttir.reshape"(%422, %423) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %425 = "ttir.dot_general"(%424, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %426 = ttir.empty() : tensor<32x1x5760xbf16>
    %427 = "ttir.reshape"(%arg27, %426) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
    %428 = ttir.empty() : tensor<32x13x5760xbf16>
    %429 = "ttir.broadcast"(%427, %428) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %430 = ttir.empty() : tensor<32x13x5760xbf16>
    %431 = "ttir.add"(%425, %429, %430) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %432 = ttir.empty() : tensor<32x13x2880xbf16>
    %433 = "ttir.slice_static"(%431, %432) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %434 = ttir.empty() : tensor<1x1x1xbf16>
    %435 = "ttir.reshape"(%arg25, %434) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %436 = ttir.empty() : tensor<32x13x2880xbf16>
    %437 = "ttir.broadcast"(%435, %436) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %438 = ttir.empty() : tensor<32x13x2880xbf16>
    %439 = "ttir.clamp_tensor"(%433, %386, %437, %438) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %440 = ttir.empty() : tensor<32x13x2880xbf16>
    %441 = "ttir.add"(%439, %17, %440) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %442 = ttir.empty() : tensor<32x13x2880xf32>
    %443 = "ttir.typecast"(%441, %442) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %444 = ttir.empty() : tensor<1x1x1xbf16>
    %445 = "ttir.reshape"(%arg26, %444) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %446 = ttir.empty() : tensor<32x13x2880xbf16>
    %447 = "ttir.broadcast"(%445, %446) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %448 = ttir.empty() : tensor<32x13x2880xbf16>
    %449 = "ttir.slice_static"(%431, %448) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %450 = ttir.empty() : tensor<32x13x2880xbf16>
    %451 = "ttir.clamp_tensor"(%449, %447, %437, %450) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %452 = ttir.empty() : tensor<32x13x2880xf32>
    %453 = "ttir.typecast"(%451, %452) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %454 = ttir.empty() : tensor<1x1x1xf32>
    %455 = "ttir.reshape"(%arg24, %454) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %456 = ttir.empty() : tensor<32x13x2880xf32>
    %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %458 = ttir.empty() : tensor<32x13x2880xf32>
    %459 = "ttir.multiply"(%453, %457, %458) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %460 = ttir.empty() : tensor<32x13x2880xbf16>
    %461 = "ttir.typecast"(%459, %460) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %462 = ttir.empty() : tensor<32x13x2880xbf16>
    %463 = "ttir.sigmoid"(%461, %462) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %464 = ttir.empty() : tensor<32x13x2880xf32>
    %465 = "ttir.typecast"(%463, %464) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %466 = ttir.empty() : tensor<32x13x2880xf32>
    %467 = "ttir.multiply"(%453, %465, %466) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %468 = ttir.empty() : tensor<32x13x2880xbf16>
    %469 = "ttir.typecast"(%467, %468) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %470 = ttir.empty() : tensor<32x13x2880xf32>
    %471 = "ttir.typecast"(%469, %470) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %472 = ttir.empty() : tensor<32x13x2880xf32>
    %473 = "ttir.multiply"(%443, %471, %472) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %474 = ttir.empty() : tensor<32x13x2880xbf16>
    %475 = "ttir.typecast"(%473, %474) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %476 = "ttir.dot_general"(%475, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %477 = ttir.empty() : tensor<32x1x2880xbf16>
    %478 = "ttir.reshape"(%arg22, %477) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
    %479 = ttir.empty() : tensor<32x13x2880xbf16>
    %480 = "ttir.broadcast"(%478, %479) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %481 = ttir.empty() : tensor<32x13x2880xbf16>
    %482 = "ttir.add"(%476, %480, %481) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %483 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %484 = "ttir.reshape"(%482, %483) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %485 = ttir.empty() : tensor<32x1x13x2880xf32>
    %486 = "ttir.typecast"(%484, %485) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %487 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xi64>
    %488 = ttir.empty() : tensor<13x1x1xi64>
    %489 = "ttir.reshape"(%487, %488) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1x1xi64>) -> tensor<13x1x1xi64>
    %490 = ttir.empty() : tensor<13x4x1xi64>
    %491 = "ttir.broadcast"(%489, %490) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x1xi64>
    %492 = ttir.empty() : tensor<2880x32xbf16>
    %493 = "ttir.permute"(%arg12, %492) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
    %494 = "ttir.dot_general"(%420, %493) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %495 = ttir.empty() : tensor<1x32xbf16>
    %496 = "ttir.reshape"(%arg11, %495) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
    %497 = ttir.empty() : tensor<13x32xbf16>
    %498 = "ttir.broadcast"(%496, %497) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %499 = ttir.empty() : tensor<13x32xbf16>
    %500 = "ttir.add"(%494, %498, %499) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %501 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 32 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<32xi32>
    %502 = ttir.empty() : tensor<1x32xi32>
    %503 = "ttir.reshape"(%501, %502) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xi32>, tensor<1x32xi32>) -> tensor<1x32xi32>
    %504 = ttir.empty() : tensor<13x32xi32>
    %505 = "ttir.broadcast"(%503, %504) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xi32>, tensor<13x32xi32>) -> tensor<13x32xi32>
    %506 = ttir.empty() : tensor<13x32xbf16>
    %507 = ttir.empty() : tensor<13x32xi32>
    %values, %indices = "ttir.sort"(%500, %506, %507) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %508 = ttir.empty() : tensor<13x4xi32>
    %509 = "ttir.slice_static"(%indices, %508) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xi32>, tensor<13x4xi32>) -> tensor<13x4xi32>
    %510 = ttir.empty() : tensor<13x4xi64>
    %511 = "ttir.typecast"(%509, %510) <{conservative_folding = false}> : (tensor<13x4xi32>, tensor<13x4xi64>) -> tensor<13x4xi64>
    %512 = ttir.empty() : tensor<13x4x1xi64>
    %513 = "ttir.reshape"(%511, %512) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xi64>, tensor<13x4x1xi64>) -> tensor<13x4x1xi64>
    %514 = ttir.empty() : tensor<13x4x2xi64>
    %515 = "ttir.concat"(%491, %513, %514) <{dim = 2 : si32}> : (tensor<13x4x1xi64>, tensor<13x4x1xi64>, tensor<13x4x2xi64>) -> tensor<13x4x2xi64>
    %516 = ttir.empty() : tensor<13x4xbf16>
    %517 = "ttir.slice_static"(%values, %516) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %518 = ttir.empty() : tensor<13xbf16>
    %519 = "ttir.max"(%517, %518) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %520 = ttir.empty() : tensor<13x1xbf16>
    %521 = "ttir.reshape"(%519, %520) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %522 = ttir.empty() : tensor<13x4xbf16>
    %523 = "ttir.broadcast"(%521, %522) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %524 = ttir.empty() : tensor<13x4xbf16>
    %525 = "ttir.subtract"(%517, %523, %524) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %526 = ttir.empty() : tensor<13x4xbf16>
    %527 = "ttir.exp"(%525, %526) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %528 = ttir.empty() : tensor<13xbf16>
    %529 = "ttir.sum"(%527, %528) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %530 = ttir.empty() : tensor<13x1xbf16>
    %531 = "ttir.reshape"(%529, %530) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %532 = ttir.empty() : tensor<13x4xbf16>
    %533 = "ttir.broadcast"(%531, %532) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %534 = ttir.empty() : tensor<13x4xbf16>
    %535 = "ttir.div"(%527, %533, %534) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %536 = ttir.empty() : tensor<13x32xbf16>
    %537 = "ttir.scatter"(%13, %515, %535, %536) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %538 = ttir.empty() : tensor<32x13xbf16>
    %539 = "ttir.permute"(%537, %538) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
    %540 = ttir.empty() : tensor<32x1x13x1xbf16>
    %541 = "ttir.reshape"(%539, %540) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
    %542 = ttir.empty() : tensor<32x1x13x1xf32>
    %543 = "ttir.typecast"(%541, %542) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
    %544 = ttir.empty() : tensor<32x1x13xf32>
    %545 = "ttir.reshape"(%543, %544) <{shape = [32 : i32, 1 : i32, 13 : i32]}> : (tensor<32x1x13x1xf32>, tensor<32x1x13xf32>) -> tensor<32x1x13xf32>
    %546 = ttir.empty() : tensor<32x1x13x1xf32>
    %547 = "ttir.reshape"(%545, %546) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x1x13xf32>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
    %548 = ttir.empty() : tensor<32x1x13x2880xf32>
    %549 = "ttir.broadcast"(%547, %548) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %550 = ttir.empty() : tensor<32x1x13x2880xf32>
    %551 = "ttir.multiply"(%486, %549, %550) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %552 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %553 = "ttir.typecast"(%551, %552) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %554 = ttir.empty() : tensor<1x13x2880xbf16>
    %555 = "ttir.sum"(%553, %554) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %556 = ttir.empty() : tensor<1x13x2880xbf16>
    %557 = "ttir.add"(%382, %555, %556) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %558 = ttir.empty() : tensor<1x13x2880xf32>
    %559 = "ttir.typecast"(%557, %558) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %560 = ttir.empty() : tensor<1x13x2880xf32>
    %561 = "ttir.pow"(%559, %29, %560) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %562 = ttir.empty() : tensor<1x13xf32>
    %563 = "ttir.sum"(%561, %562) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %564 = ttir.empty() : tensor<1x13xf32>
    %565 = "ttir.multiply"(%563, %5, %564) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %566 = ttir.empty() : tensor<1x13x1xf32>
    %567 = "ttir.reshape"(%565, %566) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %568 = ttir.empty() : tensor<1x13x1xf32>
    %569 = "ttir.add"(%567, %61, %568) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %570 = ttir.empty() : tensor<1x13x1xf32>
    %571 = "ttir.rsqrt"(%569, %570) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %572 = ttir.empty() : tensor<1x13xf32>
    %573 = "ttir.reshape"(%571, %572) <{shape = [1 : i32, 13 : i32]}> : (tensor<1x13x1xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %574 = ttir.empty() : tensor<1x13x1xf32>
    %575 = "ttir.reshape"(%573, %574) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %576 = ttir.empty() : tensor<1x13x2880xf32>
    %577 = "ttir.broadcast"(%575, %576) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %578 = ttir.empty() : tensor<1x13x2880xf32>
    %579 = "ttir.multiply"(%559, %577, %578) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %580 = ttir.empty() : tensor<1x13x2880xf32>
    %581 = "ttir.multiply"(%326, %579, %580) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %582 = ttir.empty() : tensor<1x13x2880xbf16>
    %583 = "ttir.typecast"(%581, %582) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %584 = ttir.empty() : tensor<13x2880xbf16>
    %585 = "ttir.reshape"(%583, %584) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %586 = ttir.empty() : tensor<2880x201088xbf16>
    %587 = "ttir.permute"(%arg10, %586) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
    %588 = "ttir.dot_general"(%585, %587) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %589 = ttir.empty() : tensor<1x13x201088xbf16>
    %590 = "ttir.reshape"(%588, %589) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %316, %318, %320, %220, %588, %590 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
    %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>}> : () -> tensor<13xi64>
    %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
    %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
    %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %8 = ttir.empty() : tensor<1x1xbf16>
    %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %10 = ttir.empty() : tensor<13x32xbf16>
    %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %12 = ttir.empty() : tensor<1x1x1xbf16>
    %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %14 = ttir.empty() : tensor<32x13x2880xbf16>
    %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %16 = ttir.empty() : tensor<1x1x1x1xbf16>
    %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %18 = ttir.empty() : tensor<1x1x13x13xbf16>
    %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %20 = ttir.empty() : tensor<1x1xui8>
    %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %22 = ttir.empty() : tensor<13x13xui8>
    %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %24 = ttir.empty() : tensor<1x1x1xf32>
    %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %26 = ttir.empty() : tensor<1x13x2880xf32>
    %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %28 = ttir.empty() : tensor<1x1xui8>
    %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %30 = ttir.empty() : tensor<13x13xui8>
    %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %32 = ttir.empty() : tensor<2880xf32>
    %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %34 = ttir.empty() : tensor<1x1x2880xf32>
    %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %36 = ttir.empty() : tensor<1x13x2880xf32>
    %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %38 = ttir.empty() : tensor<13xi64>
    %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xi64>, tensor<13xi64>) -> tensor<13xi64>
    %40 = ttir.empty() : tensor<13xui32>
    %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xi64>, tensor<13xui32>) -> tensor<13xui32>
    %42 = ttir.empty() : tensor<13x2880xbf16>
    %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %44 = ttir.empty() : tensor<1x13x2880xbf16>
    %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %46 = ttir.empty() : tensor<1x13x2880xf32>
    %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %48 = ttir.empty() : tensor<1x13x2880xf32>
    %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %50 = ttir.empty() : tensor<1x13xf32>
    %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %52 = ttir.empty() : tensor<1x13xf32>
    %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %54 = ttir.empty() : tensor<1x13x1xf32>
    %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %56 = ttir.empty() : tensor<1x1x1xf32>
    %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %58 = ttir.empty() : tensor<1x13x1xf32>
    %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %60 = ttir.empty() : tensor<1x13x1xf32>
    %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %62 = ttir.empty() : tensor<1x13x1xf32>
    %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %64 = ttir.empty() : tensor<1x13x2880xf32>
    %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %66 = ttir.empty() : tensor<1x13x2880xf32>
    %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %68 = ttir.empty() : tensor<1x13x2880xf32>
    %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %70 = ttir.empty() : tensor<1x13x2880xbf16>
    %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %72 = ttir.empty() : tensor<13x2880xbf16>
    %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %74 = ttir.empty() : tensor<2880x4096xbf16>
    %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
    %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %77 = ttir.empty() : tensor<1x13x4096xbf16>
    %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %79 = ttir.empty() : tensor<1x1x4096xbf16>
    %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %81 = ttir.empty() : tensor<1x13x4096xbf16>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %83 = ttir.empty() : tensor<1x13x4096xbf16>
    %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %85 = ttir.empty() : tensor<1x13x64x64xbf16>
    %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %87 = ttir.empty() : tensor<1x64x13x64xbf16>
    %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = ttir.empty() : tensor<1x64x13x32xbf16>
    %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %91 = ttir.empty() : tensor<1x64x13x32xf32>
    %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %93 = ttir.empty() : tensor<1x32x1xf32>
    %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
    %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %96 = ttir.empty() : tensor<1x13x32xf32>
    %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %98 = ttir.empty() : tensor<1x13x32xf32>
    %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %100 = ttir.empty() : tensor<1x1x1xf32>
    %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %102 = ttir.empty() : tensor<1x13x32xf32>
    %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %104 = ttir.empty() : tensor<1x13x32xf32>
    %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %106 = ttir.empty() : tensor<1x13x32xbf16>
    %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %108 = ttir.empty() : tensor<1x1x13x32xbf16>
    %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %110 = ttir.empty() : tensor<1x1x13x32xf32>
    %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %112 = ttir.empty() : tensor<1x64x13x32xf32>
    %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %114 = ttir.empty() : tensor<1x64x13x32xf32>
    %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %116 = ttir.empty() : tensor<1x64x13x32xbf16>
    %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %118 = ttir.empty() : tensor<1x64x13x32xbf16>
    %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %120 = ttir.empty() : tensor<1x64x13x32xf32>
    %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %122 = ttir.empty() : tensor<1x13x32xf32>
    %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %124 = ttir.empty() : tensor<1x13x32xf32>
    %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %126 = ttir.empty() : tensor<1x13x32xbf16>
    %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %128 = ttir.empty() : tensor<1x1x13x32xbf16>
    %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %130 = ttir.empty() : tensor<1x1x13x32xf32>
    %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %132 = ttir.empty() : tensor<1x64x13x32xf32>
    %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %134 = ttir.empty() : tensor<1x64x13x32xf32>
    %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %136 = ttir.empty() : tensor<1x64x13x32xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %138 = ttir.empty() : tensor<1x64x13x32xbf16>
    %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %140 = ttir.empty() : tensor<1x64x13x32xf32>
    %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %142 = ttir.empty() : tensor<1x64x13x32xbf16>
    %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %144 = ttir.empty() : tensor<1x64x13x32xf32>
    %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %146 = ttir.empty() : tensor<1x64x13x32xbf16>
    %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %148 = ttir.empty() : tensor<1x64x13x32xbf16>
    %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %150 = ttir.empty() : tensor<1x64x13x64xbf16>
    %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %152 = ttir.empty() : tensor<64x13x64xbf16>
    %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %154 = ttir.empty() : tensor<2880x512xbf16>
    %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %157 = ttir.empty() : tensor<1x13x512xbf16>
    %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %159 = ttir.empty() : tensor<1x1x512xbf16>
    %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %161 = ttir.empty() : tensor<1x13x512xbf16>
    %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %163 = ttir.empty() : tensor<1x13x512xbf16>
    %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %165 = ttir.empty() : tensor<1x13x8x64xbf16>
    %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %167 = ttir.empty() : tensor<1x8x13x64xbf16>
    %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %169 = ttir.empty() : tensor<1x8x13x32xbf16>
    %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %171 = ttir.empty() : tensor<1x8x13x32xf32>
    %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %173 = ttir.empty() : tensor<1x8x13x32xf32>
    %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %175 = ttir.empty() : tensor<1x8x13x32xf32>
    %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %177 = ttir.empty() : tensor<1x8x13x32xbf16>
    %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %179 = ttir.empty() : tensor<1x8x13x32xbf16>
    %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %181 = ttir.empty() : tensor<1x8x13x32xf32>
    %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %183 = ttir.empty() : tensor<1x8x13x32xf32>
    %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %185 = ttir.empty() : tensor<1x8x13x32xf32>
    %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %187 = ttir.empty() : tensor<1x8x13x32xbf16>
    %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %189 = ttir.empty() : tensor<1x8x13x32xbf16>
    %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %191 = ttir.empty() : tensor<1x8x13x32xf32>
    %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %193 = ttir.empty() : tensor<1x8x13x32xbf16>
    %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %195 = ttir.empty() : tensor<1x8x13x32xf32>
    %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %197 = ttir.empty() : tensor<1x8x13x32xbf16>
    %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %199 = ttir.empty() : tensor<1x8x13x32xbf16>
    %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %201 = ttir.empty() : tensor<1x8x13x64xbf16>
    %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %207 = ttir.empty() : tensor<1x64x13x64xbf16>
    %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %209 = ttir.empty() : tensor<1x64x64x13xbf16>
    %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
    %211 = ttir.empty() : tensor<64x64x13xbf16>
    %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %214 = ttir.empty() : tensor<1x64x13x13xbf16>
    %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %216 = ttir.empty() : tensor<1x64x13x13xf32>
    %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %218 = ttir.empty() : tensor<1x1x1x1xf32>
    %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %220 = ttir.empty() : tensor<1x64x13x13xf32>
    %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %222 = ttir.empty() : tensor<1x64x13x13xf32>
    %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %224 = ttir.empty() : tensor<1x64x13x13xbf16>
    %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %226 = ttir.empty() : tensor<1x13xi64>
    %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xi64>, tensor<1x13xi64>) -> tensor<1x13xi64>
    %228 = ttir.empty() : tensor<13x13xi64>
    %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %230 = ttir.empty() : tensor<1xi64>
    %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %232 = ttir.empty() : tensor<13xi64>
    %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xi64>, tensor<13xi64>) -> tensor<13xi64>
    %234 = ttir.empty() : tensor<13xi64>
    %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xi64>, tensor<13xi64>, tensor<13xi64>) -> tensor<13xi64>
    %236 = ttir.empty() : tensor<13x1xi64>
    %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1xi64>) -> tensor<13x1xi64>
    %238 = ttir.empty() : tensor<13x13xi64>
    %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %240 = ttir.empty() : tensor<13x13xi1>
    %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xi64>, tensor<13x13xi64>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %242 = ttir.empty() : tensor<13x13xui8>
    %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %244 = ttir.empty() : tensor<13x13xui8>
    %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %246 = ttir.empty() : tensor<13x13xi1>
    %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %248 = ttir.empty() : tensor<13x13xui8>
    %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %250 = ttir.empty() : tensor<13x1xi64>
    %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1xi64>) -> tensor<13x1xi64>
    %252 = ttir.empty() : tensor<13x13xi64>
    %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %254 = ttir.empty() : tensor<13x13xi1>
    %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xi64>, tensor<13x13xi64>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %256 = ttir.empty() : tensor<13x13xui8>
    %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %258 = ttir.empty() : tensor<13x13xui8>
    %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %260 = ttir.empty() : tensor<13x13xi1>
    %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %262 = ttir.empty() : tensor<1x1x13x13xi1>
    %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xi1>, tensor<1x1x13x13xi1>) -> tensor<1x1x13x13xi1>
    %264 = ttir.empty() : tensor<1x1x1x1xbf16>
    %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %266 = ttir.empty() : tensor<1x1x13x13xbf16>
    %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %268 = ttir.empty() : tensor<1x1x13x13xbf16>
    %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %270 = ttir.empty() : tensor<1x64x13x13xbf16>
    %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %272 = ttir.empty() : tensor<1x64x13x13xbf16>
    %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %274 = ttir.empty() : tensor<1x64x1x1xbf16>
    %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %276 = ttir.empty() : tensor<1x64x13x1xbf16>
    %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %278 = ttir.empty() : tensor<1x64x13x14xbf16>
    %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %280 = ttir.empty() : tensor<2880x512xbf16>
    %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %283 = ttir.empty() : tensor<1x13x512xbf16>
    %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %285 = ttir.empty() : tensor<1x1x512xbf16>
    %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %287 = ttir.empty() : tensor<1x13x512xbf16>
    %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %289 = ttir.empty() : tensor<1x13x512xbf16>
    %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %291 = ttir.empty() : tensor<1x13x8x64xbf16>
    %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %293 = ttir.empty() : tensor<1x8x13x64xbf16>
    %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %295 = ttir.empty() : tensor<2880xf32>
    %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %297 = ttir.empty() : tensor<1x1x2880xf32>
    %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %299 = ttir.empty() : tensor<1x13x2880xf32>
    %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %301 = ttir.empty() : tensor<1x64x13xbf16>
    %302 = "ttir.max"(%279, %301) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %303 = ttir.empty() : tensor<1x64x13x1xbf16>
    %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %305 = ttir.empty() : tensor<1x64x13x14xbf16>
    %306 = "ttir.broadcast"(%304, %305) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %307 = ttir.empty() : tensor<1x64x13x14xbf16>
    %308 = "ttir.subtract"(%279, %306, %307) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %309 = ttir.empty() : tensor<1x64x13xbf16>
    %310 = "ttir.max"(%308, %309) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %311 = ttir.empty() : tensor<1x64x13x1xbf16>
    %312 = "ttir.reshape"(%310, %311) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %313 = ttir.empty() : tensor<1x64x13x14xbf16>
    %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %315 = ttir.empty() : tensor<1x64x13x14xbf16>
    %316 = "ttir.subtract"(%308, %314, %315) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %317 = ttir.empty() : tensor<1x64x13x14xbf16>
    %318 = "ttir.exp"(%316, %317) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %319 = ttir.empty() : tensor<1x64x13xbf16>
    %320 = "ttir.sum"(%318, %319) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %321 = ttir.empty() : tensor<1x64x13x1xbf16>
    %322 = "ttir.reshape"(%320, %321) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %323 = ttir.empty() : tensor<1x64x13x14xbf16>
    %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %325 = ttir.empty() : tensor<1x64x13x14xbf16>
    %326 = "ttir.div"(%318, %324, %325) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %327 = ttir.empty() : tensor<1x64x13x13xbf16>
    %328 = "ttir.slice_static"(%326, %327) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %329 = ttir.empty() : tensor<64x13x13xbf16>
    %330 = "ttir.reshape"(%328, %329) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %331 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %332 = "ttir.reshape"(%294, %331) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %333 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %334 = "ttir.broadcast"(%332, %333) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %335 = ttir.empty() : tensor<64x13x64xbf16>
    %336 = "ttir.reshape"(%334, %335) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %337 = "ttir.dot_general"(%330, %336) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %338 = ttir.empty() : tensor<1x64x13x64xbf16>
    %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %340 = ttir.empty() : tensor<1x13x64x64xbf16>
    %341 = "ttir.permute"(%339, %340) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %342 = ttir.empty() : tensor<13x4096xbf16>
    %343 = "ttir.reshape"(%341, %342) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
    %344 = ttir.empty() : tensor<4096x2880xbf16>
    %345 = "ttir.permute"(%arg14, %344) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
    %346 = "ttir.dot_general"(%343, %345) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %347 = ttir.empty() : tensor<1x13x2880xbf16>
    %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %349 = ttir.empty() : tensor<1x1x2880xbf16>
    %350 = "ttir.reshape"(%arg13, %349) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
    %351 = ttir.empty() : tensor<1x13x2880xbf16>
    %352 = "ttir.broadcast"(%350, %351) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %353 = ttir.empty() : tensor<1x13x2880xbf16>
    %354 = "ttir.add"(%348, %352, %353) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %355 = ttir.empty() : tensor<1x13x2880xbf16>
    %356 = "ttir.add"(%45, %354, %355) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %357 = ttir.empty() : tensor<1x1x1xbf16>
    %358 = "ttir.reshape"(%arg29, %357) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %359 = ttir.empty() : tensor<32x13x2880xbf16>
    %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %361 = ttir.empty() : tensor<2880xf32>
    %362 = "ttir.typecast"(%arg21, %361) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %363 = ttir.empty() : tensor<1x1x2880xf32>
    %364 = "ttir.reshape"(%362, %363) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %365 = ttir.empty() : tensor<1x13x2880xf32>
    %366 = "ttir.broadcast"(%364, %365) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %367 = ttir.empty() : tensor<1x13x2880xf32>
    %368 = "ttir.typecast"(%356, %367) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %369 = ttir.empty() : tensor<1x13x2880xf32>
    %370 = "ttir.pow"(%368, %27, %369) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %371 = ttir.empty() : tensor<1x13xf32>
    %372 = "ttir.sum"(%370, %371) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %373 = ttir.empty() : tensor<1x13xf32>
    %374 = "ttir.multiply"(%372, %4, %373) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %375 = ttir.empty() : tensor<1x13x1xf32>
    %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %377 = ttir.empty() : tensor<1x13x1xf32>
    %378 = "ttir.add"(%376, %59, %377) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %379 = ttir.empty() : tensor<1x13x1xf32>
    %380 = "ttir.rsqrt"(%378, %379) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %381 = ttir.empty() : tensor<1x13x2880xf32>
    %382 = "ttir.broadcast"(%380, %381) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %383 = ttir.empty() : tensor<1x13x2880xf32>
    %384 = "ttir.multiply"(%368, %382, %383) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %385 = ttir.empty() : tensor<1x13x2880xf32>
    %386 = "ttir.multiply"(%366, %384, %385) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %387 = ttir.empty() : tensor<1x13x2880xbf16>
    %388 = "ttir.typecast"(%386, %387) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %389 = ttir.empty() : tensor<13x2880xbf16>
    %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %391 = ttir.empty() : tensor<416x2880xbf16>
    %392 = "ttir.concat"(%390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %391) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
    %393 = ttir.empty() : tensor<32x13x2880xbf16>
    %394 = "ttir.reshape"(%392, %393) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %395 = "ttir.dot_general"(%394, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %396 = ttir.empty() : tensor<32x1x5760xbf16>
    %397 = "ttir.reshape"(%arg27, %396) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
    %398 = ttir.empty() : tensor<32x13x5760xbf16>
    %399 = "ttir.broadcast"(%397, %398) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %400 = ttir.empty() : tensor<32x13x5760xbf16>
    %401 = "ttir.add"(%395, %399, %400) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %402 = ttir.empty() : tensor<32x13x2880xbf16>
    %403 = "ttir.slice_static"(%401, %402) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %404 = ttir.empty() : tensor<1x1x1xbf16>
    %405 = "ttir.reshape"(%arg25, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %406 = ttir.empty() : tensor<32x13x2880xbf16>
    %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %408 = ttir.empty() : tensor<32x13x2880xbf16>
    %409 = "ttir.clamp_tensor"(%403, %360, %407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %410 = ttir.empty() : tensor<32x13x2880xbf16>
    %411 = "ttir.add"(%409, %15, %410) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %412 = ttir.empty() : tensor<32x13x2880xf32>
    %413 = "ttir.typecast"(%411, %412) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %414 = ttir.empty() : tensor<1x1x1xbf16>
    %415 = "ttir.reshape"(%arg26, %414) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %416 = ttir.empty() : tensor<32x13x2880xbf16>
    %417 = "ttir.broadcast"(%415, %416) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %418 = ttir.empty() : tensor<32x13x2880xbf16>
    %419 = "ttir.slice_static"(%401, %418) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %420 = ttir.empty() : tensor<32x13x2880xbf16>
    %421 = "ttir.clamp_tensor"(%419, %417, %407, %420) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %422 = ttir.empty() : tensor<32x13x2880xf32>
    %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %424 = ttir.empty() : tensor<1x1x1xf32>
    %425 = "ttir.reshape"(%arg24, %424) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %426 = ttir.empty() : tensor<32x13x2880xf32>
    %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %428 = ttir.empty() : tensor<32x13x2880xf32>
    %429 = "ttir.multiply"(%423, %427, %428) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %430 = ttir.empty() : tensor<32x13x2880xbf16>
    %431 = "ttir.typecast"(%429, %430) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %432 = ttir.empty() : tensor<32x13x2880xbf16>
    %433 = "ttir.sigmoid"(%431, %432) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %434 = ttir.empty() : tensor<32x13x2880xf32>
    %435 = "ttir.typecast"(%433, %434) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %436 = ttir.empty() : tensor<32x13x2880xf32>
    %437 = "ttir.multiply"(%423, %435, %436) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %438 = ttir.empty() : tensor<32x13x2880xf32>
    %439 = "ttir.multiply"(%413, %437, %438) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %440 = ttir.empty() : tensor<32x13x2880xbf16>
    %441 = "ttir.typecast"(%439, %440) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %442 = "ttir.dot_general"(%441, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %443 = ttir.empty() : tensor<32x1x2880xbf16>
    %444 = "ttir.reshape"(%arg22, %443) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
    %445 = ttir.empty() : tensor<32x13x2880xbf16>
    %446 = "ttir.broadcast"(%444, %445) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %447 = ttir.empty() : tensor<32x13x2880xbf16>
    %448 = "ttir.add"(%442, %446, %447) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %449 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %450 = "ttir.reshape"(%448, %449) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %451 = ttir.empty() : tensor<32x1x13x2880xf32>
    %452 = "ttir.typecast"(%450, %451) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %453 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xi64>
    %454 = ttir.empty() : tensor<13x1x1xi64>
    %455 = "ttir.reshape"(%453, %454) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1x1xi64>) -> tensor<13x1x1xi64>
    %456 = ttir.empty() : tensor<13x4x1xi64>
    %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x1xi64>
    %458 = ttir.empty() : tensor<2880x32xbf16>
    %459 = "ttir.permute"(%arg12, %458) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
    %460 = "ttir.dot_general"(%390, %459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %461 = ttir.empty() : tensor<1x32xbf16>
    %462 = "ttir.reshape"(%arg11, %461) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
    %463 = ttir.empty() : tensor<13x32xbf16>
    %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %465 = ttir.empty() : tensor<13x32xbf16>
    %466 = "ttir.add"(%460, %464, %465) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %467 = ttir.empty() : tensor<13x32xbf16>
    %468 = ttir.empty() : tensor<13x32xi32>
    %values, %indices = "ttir.sort"(%466, %467, %468) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %469 = ttir.empty() : tensor<13x4xi32>
    %470 = "ttir.slice_static"(%indices, %469) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xi32>, tensor<13x4xi32>) -> tensor<13x4xi32>
    %471 = ttir.empty() : tensor<13x4xi64>
    %472 = "ttir.typecast"(%470, %471) <{conservative_folding = false}> : (tensor<13x4xi32>, tensor<13x4xi64>) -> tensor<13x4xi64>
    %473 = ttir.empty() : tensor<13x4x1xi64>
    %474 = "ttir.reshape"(%472, %473) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xi64>, tensor<13x4x1xi64>) -> tensor<13x4x1xi64>
    %475 = ttir.empty() : tensor<13x4x2xi64>
    %476 = "ttir.concat"(%457, %474, %475) <{dim = 2 : si32}> : (tensor<13x4x1xi64>, tensor<13x4x1xi64>, tensor<13x4x2xi64>) -> tensor<13x4x2xi64>
    %477 = ttir.empty() : tensor<13x4xbf16>
    %478 = "ttir.slice_static"(%values, %477) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %479 = ttir.empty() : tensor<13xbf16>
    %480 = "ttir.max"(%478, %479) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %481 = ttir.empty() : tensor<13x1xbf16>
    %482 = "ttir.reshape"(%480, %481) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %483 = ttir.empty() : tensor<13x4xbf16>
    %484 = "ttir.broadcast"(%482, %483) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %485 = ttir.empty() : tensor<13x4xbf16>
    %486 = "ttir.subtract"(%478, %484, %485) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %487 = ttir.empty() : tensor<13x4xbf16>
    %488 = "ttir.exp"(%486, %487) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %489 = ttir.empty() : tensor<13xbf16>
    %490 = "ttir.sum"(%488, %489) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %491 = ttir.empty() : tensor<13x1xbf16>
    %492 = "ttir.reshape"(%490, %491) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %493 = ttir.empty() : tensor<13x4xbf16>
    %494 = "ttir.broadcast"(%492, %493) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %495 = ttir.empty() : tensor<13x4xbf16>
    %496 = "ttir.div"(%488, %494, %495) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %497 = ttir.empty() : tensor<13x32xbf16>
    %498 = "ttir.scatter"(%11, %476, %496, %497) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %499 = ttir.empty() : tensor<32x13xbf16>
    %500 = "ttir.permute"(%498, %499) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
    %501 = ttir.empty() : tensor<32x1x13x1xbf16>
    %502 = "ttir.reshape"(%500, %501) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
    %503 = ttir.empty() : tensor<32x1x13x1xf32>
    %504 = "ttir.typecast"(%502, %503) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
    %505 = ttir.empty() : tensor<32x1x13x2880xf32>
    %506 = "ttir.broadcast"(%504, %505) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %507 = ttir.empty() : tensor<32x1x13x2880xf32>
    %508 = "ttir.multiply"(%452, %506, %507) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %509 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %510 = "ttir.typecast"(%508, %509) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %511 = ttir.empty() : tensor<1x13x2880xbf16>
    %512 = "ttir.sum"(%510, %511) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %513 = ttir.empty() : tensor<1x13x2880xbf16>
    %514 = "ttir.add"(%356, %512, %513) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %515 = ttir.empty() : tensor<1x13x2880xf32>
    %516 = "ttir.typecast"(%514, %515) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %517 = ttir.empty() : tensor<1x13x2880xf32>
    %518 = "ttir.pow"(%516, %27, %517) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %519 = ttir.empty() : tensor<1x13xf32>
    %520 = "ttir.sum"(%518, %519) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %521 = ttir.empty() : tensor<1x13xf32>
    %522 = "ttir.multiply"(%520, %4, %521) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %523 = ttir.empty() : tensor<1x13x1xf32>
    %524 = "ttir.reshape"(%522, %523) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %525 = ttir.empty() : tensor<1x13x1xf32>
    %526 = "ttir.add"(%524, %59, %525) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %527 = ttir.empty() : tensor<1x13x1xf32>
    %528 = "ttir.rsqrt"(%526, %527) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %529 = ttir.empty() : tensor<1x13x2880xf32>
    %530 = "ttir.broadcast"(%528, %529) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %531 = ttir.empty() : tensor<1x13x2880xf32>
    %532 = "ttir.multiply"(%516, %530, %531) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %533 = ttir.empty() : tensor<1x13x2880xf32>
    %534 = "ttir.multiply"(%300, %532, %533) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %535 = ttir.empty() : tensor<1x13x2880xbf16>
    %536 = "ttir.typecast"(%534, %535) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %537 = ttir.empty() : tensor<13x2880xbf16>
    %538 = "ttir.reshape"(%536, %537) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %539 = ttir.empty() : tensor<2880x201088xbf16>
    %540 = "ttir.permute"(%arg10, %539) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
    %541 = "ttir.dot_general"(%538, %540) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %542 = ttir.empty() : tensor<1x13x201088xbf16>
    %543 = "ttir.reshape"(%541, %542) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %290, %292, %294, %202, %541, %543 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before ElementTypeNormalization (ttir-element-type-normalization) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
    %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>}> : () -> tensor<13xi64>
    %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
    %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
    %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %8 = ttir.empty() : tensor<1x1xbf16>
    %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %10 = ttir.empty() : tensor<13x32xbf16>
    %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %12 = ttir.empty() : tensor<1x1x1xbf16>
    %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %14 = ttir.empty() : tensor<32x13x2880xbf16>
    %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %16 = ttir.empty() : tensor<1x1x1x1xbf16>
    %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %18 = ttir.empty() : tensor<1x1x13x13xbf16>
    %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %20 = ttir.empty() : tensor<1x1xui8>
    %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %22 = ttir.empty() : tensor<13x13xui8>
    %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %24 = ttir.empty() : tensor<1x1x1xf32>
    %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %26 = ttir.empty() : tensor<1x13x2880xf32>
    %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %28 = ttir.empty() : tensor<1x1xui8>
    %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %30 = ttir.empty() : tensor<13x13xui8>
    %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %32 = ttir.empty() : tensor<2880xf32>
    %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %34 = ttir.empty() : tensor<1x1x2880xf32>
    %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %36 = ttir.empty() : tensor<1x13x2880xf32>
    %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %38 = ttir.empty() : tensor<13xi64>
    %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xi64>, tensor<13xi64>) -> tensor<13xi64>
    %40 = ttir.empty() : tensor<13xui32>
    %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xi64>, tensor<13xui32>) -> tensor<13xui32>
    %42 = ttir.empty() : tensor<13x2880xbf16>
    %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %44 = ttir.empty() : tensor<1x13x2880xbf16>
    %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %46 = ttir.empty() : tensor<1x13x2880xf32>
    %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %48 = ttir.empty() : tensor<1x13x2880xf32>
    %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %50 = ttir.empty() : tensor<1x13xf32>
    %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %52 = ttir.empty() : tensor<1x13xf32>
    %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %54 = ttir.empty() : tensor<1x13x1xf32>
    %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %56 = ttir.empty() : tensor<1x1x1xf32>
    %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %58 = ttir.empty() : tensor<1x13x1xf32>
    %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %60 = ttir.empty() : tensor<1x13x1xf32>
    %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %62 = ttir.empty() : tensor<1x13x1xf32>
    %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %64 = ttir.empty() : tensor<1x13x2880xf32>
    %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %66 = ttir.empty() : tensor<1x13x2880xf32>
    %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %68 = ttir.empty() : tensor<1x13x2880xf32>
    %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %70 = ttir.empty() : tensor<1x13x2880xbf16>
    %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %72 = ttir.empty() : tensor<13x2880xbf16>
    %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %74 = ttir.empty() : tensor<2880x4096xbf16>
    %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
    %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %77 = ttir.empty() : tensor<1x13x4096xbf16>
    %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %79 = ttir.empty() : tensor<1x1x4096xbf16>
    %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %81 = ttir.empty() : tensor<1x13x4096xbf16>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %83 = ttir.empty() : tensor<1x13x4096xbf16>
    %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %85 = ttir.empty() : tensor<1x13x64x64xbf16>
    %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %87 = ttir.empty() : tensor<1x64x13x64xbf16>
    %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = ttir.empty() : tensor<1x64x13x32xbf16>
    %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %91 = ttir.empty() : tensor<1x64x13x32xf32>
    %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %93 = ttir.empty() : tensor<1x32x1xf32>
    %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
    %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %96 = ttir.empty() : tensor<1x13x32xf32>
    %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %98 = ttir.empty() : tensor<1x13x32xf32>
    %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %100 = ttir.empty() : tensor<1x1x1xf32>
    %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %102 = ttir.empty() : tensor<1x13x32xf32>
    %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %104 = ttir.empty() : tensor<1x13x32xf32>
    %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %106 = ttir.empty() : tensor<1x13x32xbf16>
    %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %108 = ttir.empty() : tensor<1x1x13x32xbf16>
    %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %110 = ttir.empty() : tensor<1x1x13x32xf32>
    %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %112 = ttir.empty() : tensor<1x64x13x32xf32>
    %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %114 = ttir.empty() : tensor<1x64x13x32xf32>
    %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %116 = ttir.empty() : tensor<1x64x13x32xbf16>
    %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %118 = ttir.empty() : tensor<1x64x13x32xbf16>
    %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %120 = ttir.empty() : tensor<1x64x13x32xf32>
    %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %122 = ttir.empty() : tensor<1x13x32xf32>
    %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %124 = ttir.empty() : tensor<1x13x32xf32>
    %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %126 = ttir.empty() : tensor<1x13x32xbf16>
    %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %128 = ttir.empty() : tensor<1x1x13x32xbf16>
    %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %130 = ttir.empty() : tensor<1x1x13x32xf32>
    %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %132 = ttir.empty() : tensor<1x64x13x32xf32>
    %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %134 = ttir.empty() : tensor<1x64x13x32xf32>
    %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %136 = ttir.empty() : tensor<1x64x13x32xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %138 = ttir.empty() : tensor<1x64x13x32xbf16>
    %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %140 = ttir.empty() : tensor<1x64x13x32xf32>
    %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %142 = ttir.empty() : tensor<1x64x13x32xbf16>
    %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %144 = ttir.empty() : tensor<1x64x13x32xf32>
    %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %146 = ttir.empty() : tensor<1x64x13x32xbf16>
    %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %148 = ttir.empty() : tensor<1x64x13x32xbf16>
    %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %150 = ttir.empty() : tensor<1x64x13x64xbf16>
    %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %152 = ttir.empty() : tensor<64x13x64xbf16>
    %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %154 = ttir.empty() : tensor<2880x512xbf16>
    %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %157 = ttir.empty() : tensor<1x13x512xbf16>
    %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %159 = ttir.empty() : tensor<1x1x512xbf16>
    %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %161 = ttir.empty() : tensor<1x13x512xbf16>
    %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %163 = ttir.empty() : tensor<1x13x512xbf16>
    %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %165 = ttir.empty() : tensor<1x13x8x64xbf16>
    %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %167 = ttir.empty() : tensor<1x8x13x64xbf16>
    %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %169 = ttir.empty() : tensor<1x8x13x32xbf16>
    %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %171 = ttir.empty() : tensor<1x8x13x32xf32>
    %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %173 = ttir.empty() : tensor<1x8x13x32xf32>
    %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %175 = ttir.empty() : tensor<1x8x13x32xf32>
    %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %177 = ttir.empty() : tensor<1x8x13x32xbf16>
    %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %179 = ttir.empty() : tensor<1x8x13x32xbf16>
    %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %181 = ttir.empty() : tensor<1x8x13x32xf32>
    %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %183 = ttir.empty() : tensor<1x8x13x32xf32>
    %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %185 = ttir.empty() : tensor<1x8x13x32xf32>
    %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %187 = ttir.empty() : tensor<1x8x13x32xbf16>
    %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %189 = ttir.empty() : tensor<1x8x13x32xbf16>
    %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %191 = ttir.empty() : tensor<1x8x13x32xf32>
    %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %193 = ttir.empty() : tensor<1x8x13x32xbf16>
    %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %195 = ttir.empty() : tensor<1x8x13x32xf32>
    %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %197 = ttir.empty() : tensor<1x8x13x32xbf16>
    %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %199 = ttir.empty() : tensor<1x8x13x32xbf16>
    %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %201 = ttir.empty() : tensor<1x8x13x64xbf16>
    %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %207 = ttir.empty() : tensor<1x64x13x64xbf16>
    %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %209 = ttir.empty() : tensor<1x64x64x13xbf16>
    %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
    %211 = ttir.empty() : tensor<64x64x13xbf16>
    %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %214 = ttir.empty() : tensor<1x64x13x13xbf16>
    %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %216 = ttir.empty() : tensor<1x64x13x13xf32>
    %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %218 = ttir.empty() : tensor<1x1x1x1xf32>
    %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %220 = ttir.empty() : tensor<1x64x13x13xf32>
    %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %222 = ttir.empty() : tensor<1x64x13x13xf32>
    %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %224 = ttir.empty() : tensor<1x64x13x13xbf16>
    %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %226 = ttir.empty() : tensor<1x13xi64>
    %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xi64>, tensor<1x13xi64>) -> tensor<1x13xi64>
    %228 = ttir.empty() : tensor<13x13xi64>
    %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %230 = ttir.empty() : tensor<1xi64>
    %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %232 = ttir.empty() : tensor<13xi64>
    %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xi64>, tensor<13xi64>) -> tensor<13xi64>
    %234 = ttir.empty() : tensor<13xi64>
    %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xi64>, tensor<13xi64>, tensor<13xi64>) -> tensor<13xi64>
    %236 = ttir.empty() : tensor<13x1xi64>
    %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1xi64>) -> tensor<13x1xi64>
    %238 = ttir.empty() : tensor<13x13xi64>
    %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %240 = ttir.empty() : tensor<13x13xi1>
    %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xi64>, tensor<13x13xi64>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %242 = ttir.empty() : tensor<13x13xui8>
    %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %244 = ttir.empty() : tensor<13x13xui8>
    %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %246 = ttir.empty() : tensor<13x13xi1>
    %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %248 = ttir.empty() : tensor<13x13xui8>
    %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %250 = ttir.empty() : tensor<13x1xi64>
    %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1xi64>) -> tensor<13x1xi64>
    %252 = ttir.empty() : tensor<13x13xi64>
    %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xi64>, tensor<13x13xi64>) -> tensor<13x13xi64>
    %254 = ttir.empty() : tensor<13x13xi1>
    %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xi64>, tensor<13x13xi64>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %256 = ttir.empty() : tensor<13x13xui8>
    %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xi1>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %258 = ttir.empty() : tensor<13x13xui8>
    %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %260 = ttir.empty() : tensor<13x13xi1>
    %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xi1>) -> tensor<13x13xi1>
    %262 = ttir.empty() : tensor<1x1x13x13xi1>
    %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xi1>, tensor<1x1x13x13xi1>) -> tensor<1x1x13x13xi1>
    %264 = ttir.empty() : tensor<1x1x1x1xbf16>
    %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %266 = ttir.empty() : tensor<1x1x13x13xbf16>
    %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %268 = ttir.empty() : tensor<1x1x13x13xbf16>
    %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %270 = ttir.empty() : tensor<1x64x13x13xbf16>
    %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %272 = ttir.empty() : tensor<1x64x13x13xbf16>
    %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %274 = ttir.empty() : tensor<1x64x1x1xbf16>
    %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %276 = ttir.empty() : tensor<1x64x13x1xbf16>
    %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %278 = ttir.empty() : tensor<1x64x13x14xbf16>
    %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %280 = ttir.empty() : tensor<2880x512xbf16>
    %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %283 = ttir.empty() : tensor<1x13x512xbf16>
    %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %285 = ttir.empty() : tensor<1x1x512xbf16>
    %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %287 = ttir.empty() : tensor<1x13x512xbf16>
    %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %289 = ttir.empty() : tensor<1x13x512xbf16>
    %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %291 = ttir.empty() : tensor<1x13x8x64xbf16>
    %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %293 = ttir.empty() : tensor<1x8x13x64xbf16>
    %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %295 = ttir.empty() : tensor<2880xf32>
    %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %297 = ttir.empty() : tensor<1x1x2880xf32>
    %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %299 = ttir.empty() : tensor<1x13x2880xf32>
    %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %301 = ttir.empty() : tensor<1x64x13xbf16>
    %302 = "ttir.max"(%279, %301) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %303 = ttir.empty() : tensor<1x64x13x1xbf16>
    %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %305 = ttir.empty() : tensor<1x64x13x14xbf16>
    %306 = "ttir.broadcast"(%304, %305) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %307 = ttir.empty() : tensor<1x64x13x14xbf16>
    %308 = "ttir.subtract"(%279, %306, %307) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %309 = ttir.empty() : tensor<1x64x13xbf16>
    %310 = "ttir.max"(%308, %309) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %311 = ttir.empty() : tensor<1x64x13x1xbf16>
    %312 = "ttir.reshape"(%310, %311) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %313 = ttir.empty() : tensor<1x64x13x14xbf16>
    %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %315 = ttir.empty() : tensor<1x64x13x14xbf16>
    %316 = "ttir.subtract"(%308, %314, %315) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %317 = ttir.empty() : tensor<1x64x13x14xbf16>
    %318 = "ttir.exp"(%316, %317) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %319 = ttir.empty() : tensor<1x64x13xbf16>
    %320 = "ttir.sum"(%318, %319) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %321 = ttir.empty() : tensor<1x64x13x1xbf16>
    %322 = "ttir.reshape"(%320, %321) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %323 = ttir.empty() : tensor<1x64x13x14xbf16>
    %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %325 = ttir.empty() : tensor<1x64x13x14xbf16>
    %326 = "ttir.div"(%318, %324, %325) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %327 = ttir.empty() : tensor<1x64x13x13xbf16>
    %328 = "ttir.slice_static"(%326, %327) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %329 = ttir.empty() : tensor<64x13x13xbf16>
    %330 = "ttir.reshape"(%328, %329) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %331 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %332 = "ttir.reshape"(%294, %331) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %333 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %334 = "ttir.broadcast"(%332, %333) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %335 = ttir.empty() : tensor<64x13x64xbf16>
    %336 = "ttir.reshape"(%334, %335) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %337 = "ttir.dot_general"(%330, %336) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %338 = ttir.empty() : tensor<1x64x13x64xbf16>
    %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %340 = ttir.empty() : tensor<1x13x64x64xbf16>
    %341 = "ttir.permute"(%339, %340) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %342 = ttir.empty() : tensor<13x4096xbf16>
    %343 = "ttir.reshape"(%341, %342) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
    %344 = ttir.empty() : tensor<4096x2880xbf16>
    %345 = "ttir.permute"(%arg14, %344) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
    %346 = "ttir.dot_general"(%343, %345) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %347 = ttir.empty() : tensor<1x13x2880xbf16>
    %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %349 = ttir.empty() : tensor<1x1x2880xbf16>
    %350 = "ttir.reshape"(%arg13, %349) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
    %351 = ttir.empty() : tensor<1x13x2880xbf16>
    %352 = "ttir.broadcast"(%350, %351) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %353 = ttir.empty() : tensor<1x13x2880xbf16>
    %354 = "ttir.add"(%348, %352, %353) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %355 = ttir.empty() : tensor<1x13x2880xbf16>
    %356 = "ttir.add"(%45, %354, %355) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %357 = ttir.empty() : tensor<1x1x1xbf16>
    %358 = "ttir.reshape"(%arg29, %357) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %359 = ttir.empty() : tensor<32x13x2880xbf16>
    %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %361 = ttir.empty() : tensor<2880xf32>
    %362 = "ttir.typecast"(%arg21, %361) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %363 = ttir.empty() : tensor<1x1x2880xf32>
    %364 = "ttir.reshape"(%362, %363) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %365 = ttir.empty() : tensor<1x13x2880xf32>
    %366 = "ttir.broadcast"(%364, %365) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %367 = ttir.empty() : tensor<1x13x2880xf32>
    %368 = "ttir.typecast"(%356, %367) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %369 = ttir.empty() : tensor<1x13x2880xf32>
    %370 = "ttir.pow"(%368, %27, %369) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %371 = ttir.empty() : tensor<1x13xf32>
    %372 = "ttir.sum"(%370, %371) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %373 = ttir.empty() : tensor<1x13xf32>
    %374 = "ttir.multiply"(%372, %4, %373) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %375 = ttir.empty() : tensor<1x13x1xf32>
    %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %377 = ttir.empty() : tensor<1x13x1xf32>
    %378 = "ttir.add"(%376, %59, %377) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %379 = ttir.empty() : tensor<1x13x1xf32>
    %380 = "ttir.rsqrt"(%378, %379) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %381 = ttir.empty() : tensor<1x13x2880xf32>
    %382 = "ttir.broadcast"(%380, %381) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %383 = ttir.empty() : tensor<1x13x2880xf32>
    %384 = "ttir.multiply"(%368, %382, %383) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %385 = ttir.empty() : tensor<1x13x2880xf32>
    %386 = "ttir.multiply"(%366, %384, %385) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %387 = ttir.empty() : tensor<1x13x2880xbf16>
    %388 = "ttir.typecast"(%386, %387) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %389 = ttir.empty() : tensor<13x2880xbf16>
    %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %391 = ttir.empty() : tensor<416x2880xbf16>
    %392 = "ttir.concat"(%390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %391) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
    %393 = ttir.empty() : tensor<32x13x2880xbf16>
    %394 = "ttir.reshape"(%392, %393) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %395 = "ttir.dot_general"(%394, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %396 = ttir.empty() : tensor<32x1x5760xbf16>
    %397 = "ttir.reshape"(%arg27, %396) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
    %398 = ttir.empty() : tensor<32x13x5760xbf16>
    %399 = "ttir.broadcast"(%397, %398) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %400 = ttir.empty() : tensor<32x13x5760xbf16>
    %401 = "ttir.add"(%395, %399, %400) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %402 = ttir.empty() : tensor<32x13x2880xbf16>
    %403 = "ttir.slice_static"(%401, %402) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %404 = ttir.empty() : tensor<1x1x1xbf16>
    %405 = "ttir.reshape"(%arg25, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %406 = ttir.empty() : tensor<32x13x2880xbf16>
    %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %408 = ttir.empty() : tensor<32x13x2880xbf16>
    %409 = "ttir.clamp_tensor"(%403, %360, %407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %410 = ttir.empty() : tensor<32x13x2880xbf16>
    %411 = "ttir.add"(%409, %15, %410) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %412 = ttir.empty() : tensor<32x13x2880xf32>
    %413 = "ttir.typecast"(%411, %412) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %414 = ttir.empty() : tensor<1x1x1xbf16>
    %415 = "ttir.reshape"(%arg26, %414) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %416 = ttir.empty() : tensor<32x13x2880xbf16>
    %417 = "ttir.broadcast"(%415, %416) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %418 = ttir.empty() : tensor<32x13x2880xbf16>
    %419 = "ttir.slice_static"(%401, %418) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %420 = ttir.empty() : tensor<32x13x2880xbf16>
    %421 = "ttir.clamp_tensor"(%419, %417, %407, %420) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %422 = ttir.empty() : tensor<32x13x2880xf32>
    %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %424 = ttir.empty() : tensor<1x1x1xf32>
    %425 = "ttir.reshape"(%arg24, %424) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %426 = ttir.empty() : tensor<32x13x2880xf32>
    %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %428 = ttir.empty() : tensor<32x13x2880xf32>
    %429 = "ttir.multiply"(%423, %427, %428) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %430 = ttir.empty() : tensor<32x13x2880xbf16>
    %431 = "ttir.typecast"(%429, %430) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %432 = ttir.empty() : tensor<32x13x2880xbf16>
    %433 = "ttir.sigmoid"(%431, %432) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %434 = ttir.empty() : tensor<32x13x2880xf32>
    %435 = "ttir.typecast"(%433, %434) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %436 = ttir.empty() : tensor<32x13x2880xf32>
    %437 = "ttir.multiply"(%423, %435, %436) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %438 = ttir.empty() : tensor<32x13x2880xf32>
    %439 = "ttir.multiply"(%413, %437, %438) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %440 = ttir.empty() : tensor<32x13x2880xbf16>
    %441 = "ttir.typecast"(%439, %440) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %442 = "ttir.dot_general"(%441, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %443 = ttir.empty() : tensor<32x1x2880xbf16>
    %444 = "ttir.reshape"(%arg22, %443) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
    %445 = ttir.empty() : tensor<32x13x2880xbf16>
    %446 = "ttir.broadcast"(%444, %445) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %447 = ttir.empty() : tensor<32x13x2880xbf16>
    %448 = "ttir.add"(%442, %446, %447) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %449 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %450 = "ttir.reshape"(%448, %449) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %451 = ttir.empty() : tensor<32x1x13x2880xf32>
    %452 = "ttir.typecast"(%450, %451) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %453 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xi64>
    %454 = ttir.empty() : tensor<13x1x1xi64>
    %455 = "ttir.reshape"(%453, %454) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xi64>, tensor<13x1x1xi64>) -> tensor<13x1x1xi64>
    %456 = ttir.empty() : tensor<13x4x1xi64>
    %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x1xi64>
    %458 = ttir.empty() : tensor<2880x32xbf16>
    %459 = "ttir.permute"(%arg12, %458) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
    %460 = "ttir.dot_general"(%390, %459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %461 = ttir.empty() : tensor<1x32xbf16>
    %462 = "ttir.reshape"(%arg11, %461) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
    %463 = ttir.empty() : tensor<13x32xbf16>
    %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %465 = ttir.empty() : tensor<13x32xbf16>
    %466 = "ttir.add"(%460, %464, %465) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %467 = ttir.empty() : tensor<13x32xbf16>
    %468 = ttir.empty() : tensor<13x32xi32>
    %values, %indices = "ttir.sort"(%466, %467, %468) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %469 = ttir.empty() : tensor<13x4xi32>
    %470 = "ttir.slice_static"(%indices, %469) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xi32>, tensor<13x4xi32>) -> tensor<13x4xi32>
    %471 = ttir.empty() : tensor<13x4xi64>
    %472 = "ttir.typecast"(%470, %471) <{conservative_folding = false}> : (tensor<13x4xi32>, tensor<13x4xi64>) -> tensor<13x4xi64>
    %473 = ttir.empty() : tensor<13x4x1xi64>
    %474 = "ttir.reshape"(%472, %473) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xi64>, tensor<13x4x1xi64>) -> tensor<13x4x1xi64>
    %475 = ttir.empty() : tensor<13x4x2xi64>
    %476 = "ttir.concat"(%457, %474, %475) <{dim = 2 : si32}> : (tensor<13x4x1xi64>, tensor<13x4x1xi64>, tensor<13x4x2xi64>) -> tensor<13x4x2xi64>
    %477 = ttir.empty() : tensor<13x4xbf16>
    %478 = "ttir.slice_static"(%values, %477) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %479 = ttir.empty() : tensor<13xbf16>
    %480 = "ttir.max"(%478, %479) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %481 = ttir.empty() : tensor<13x1xbf16>
    %482 = "ttir.reshape"(%480, %481) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %483 = ttir.empty() : tensor<13x4xbf16>
    %484 = "ttir.broadcast"(%482, %483) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %485 = ttir.empty() : tensor<13x4xbf16>
    %486 = "ttir.subtract"(%478, %484, %485) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %487 = ttir.empty() : tensor<13x4xbf16>
    %488 = "ttir.exp"(%486, %487) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %489 = ttir.empty() : tensor<13xbf16>
    %490 = "ttir.sum"(%488, %489) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %491 = ttir.empty() : tensor<13x1xbf16>
    %492 = "ttir.reshape"(%490, %491) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %493 = ttir.empty() : tensor<13x4xbf16>
    %494 = "ttir.broadcast"(%492, %493) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %495 = ttir.empty() : tensor<13x4xbf16>
    %496 = "ttir.div"(%488, %494, %495) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %497 = ttir.empty() : tensor<13x32xbf16>
    %498 = "ttir.scatter"(%11, %476, %496, %497) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %499 = ttir.empty() : tensor<32x13xbf16>
    %500 = "ttir.permute"(%498, %499) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
    %501 = ttir.empty() : tensor<32x1x13x1xbf16>
    %502 = "ttir.reshape"(%500, %501) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
    %503 = ttir.empty() : tensor<32x1x13x1xf32>
    %504 = "ttir.typecast"(%502, %503) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
    %505 = ttir.empty() : tensor<32x1x13x2880xf32>
    %506 = "ttir.broadcast"(%504, %505) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %507 = ttir.empty() : tensor<32x1x13x2880xf32>
    %508 = "ttir.multiply"(%452, %506, %507) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %509 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %510 = "ttir.typecast"(%508, %509) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %511 = ttir.empty() : tensor<1x13x2880xbf16>
    %512 = "ttir.sum"(%510, %511) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %513 = ttir.empty() : tensor<1x13x2880xbf16>
    %514 = "ttir.add"(%356, %512, %513) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %515 = ttir.empty() : tensor<1x13x2880xf32>
    %516 = "ttir.typecast"(%514, %515) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %517 = ttir.empty() : tensor<1x13x2880xf32>
    %518 = "ttir.pow"(%516, %27, %517) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %519 = ttir.empty() : tensor<1x13xf32>
    %520 = "ttir.sum"(%518, %519) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %521 = ttir.empty() : tensor<1x13xf32>
    %522 = "ttir.multiply"(%520, %4, %521) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %523 = ttir.empty() : tensor<1x13x1xf32>
    %524 = "ttir.reshape"(%522, %523) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %525 = ttir.empty() : tensor<1x13x1xf32>
    %526 = "ttir.add"(%524, %59, %525) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %527 = ttir.empty() : tensor<1x13x1xf32>
    %528 = "ttir.rsqrt"(%526, %527) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %529 = ttir.empty() : tensor<1x13x2880xf32>
    %530 = "ttir.broadcast"(%528, %529) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %531 = ttir.empty() : tensor<1x13x2880xf32>
    %532 = "ttir.multiply"(%516, %530, %531) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %533 = ttir.empty() : tensor<1x13x2880xf32>
    %534 = "ttir.multiply"(%300, %532, %533) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %535 = ttir.empty() : tensor<1x13x2880xbf16>
    %536 = "ttir.typecast"(%534, %535) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %537 = ttir.empty() : tensor<13x2880xbf16>
    %538 = "ttir.reshape"(%536, %537) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %539 = ttir.empty() : tensor<2880x201088xbf16>
    %540 = "ttir.permute"(%arg10, %539) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
    %541 = "ttir.dot_general"(%538, %540) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %542 = ttir.empty() : tensor<1x13x201088xbf16>
    %543 = "ttir.reshape"(%541, %542) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %290, %292, %294, %202, %541, %543 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After ElementTypeNormalization (ttir-element-type-normalization) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
    %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
    %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
    %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
    %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %8 = ttir.empty() : tensor<1x1xbf16>
    %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %10 = ttir.empty() : tensor<13x32xbf16>
    %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %12 = ttir.empty() : tensor<1x1x1xbf16>
    %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %14 = ttir.empty() : tensor<32x13x2880xbf16>
    %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %16 = ttir.empty() : tensor<1x1x1x1xbf16>
    %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %18 = ttir.empty() : tensor<1x1x13x13xbf16>
    %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %20 = ttir.empty() : tensor<1x1xui8>
    %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %22 = ttir.empty() : tensor<13x13xui8>
    %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %24 = ttir.empty() : tensor<1x1x1xf32>
    %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %26 = ttir.empty() : tensor<1x13x2880xf32>
    %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %28 = ttir.empty() : tensor<1x1xui8>
    %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %30 = ttir.empty() : tensor<13x13xui8>
    %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %32 = ttir.empty() : tensor<2880xf32>
    %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %34 = ttir.empty() : tensor<1x1x2880xf32>
    %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %36 = ttir.empty() : tensor<1x13x2880xf32>
    %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %38 = ttir.empty() : tensor<13xsi32>
    %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
    %40 = ttir.empty() : tensor<13xui32>
    %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
    %42 = ttir.empty() : tensor<13x2880xbf16>
    %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %44 = ttir.empty() : tensor<1x13x2880xbf16>
    %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %46 = ttir.empty() : tensor<1x13x2880xf32>
    %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %48 = ttir.empty() : tensor<1x13x2880xf32>
    %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %50 = ttir.empty() : tensor<1x13xf32>
    %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %52 = ttir.empty() : tensor<1x13xf32>
    %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %54 = ttir.empty() : tensor<1x13x1xf32>
    %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %56 = ttir.empty() : tensor<1x1x1xf32>
    %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %58 = ttir.empty() : tensor<1x13x1xf32>
    %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %60 = ttir.empty() : tensor<1x13x1xf32>
    %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %62 = ttir.empty() : tensor<1x13x1xf32>
    %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %64 = ttir.empty() : tensor<1x13x2880xf32>
    %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %66 = ttir.empty() : tensor<1x13x2880xf32>
    %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %68 = ttir.empty() : tensor<1x13x2880xf32>
    %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %70 = ttir.empty() : tensor<1x13x2880xbf16>
    %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %72 = ttir.empty() : tensor<13x2880xbf16>
    %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %74 = ttir.empty() : tensor<2880x4096xbf16>
    %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
    %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %77 = ttir.empty() : tensor<1x13x4096xbf16>
    %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %79 = ttir.empty() : tensor<1x1x4096xbf16>
    %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %81 = ttir.empty() : tensor<1x13x4096xbf16>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %83 = ttir.empty() : tensor<1x13x4096xbf16>
    %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %85 = ttir.empty() : tensor<1x13x64x64xbf16>
    %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %87 = ttir.empty() : tensor<1x64x13x64xbf16>
    %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = ttir.empty() : tensor<1x64x13x32xbf16>
    %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %91 = ttir.empty() : tensor<1x64x13x32xf32>
    %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %93 = ttir.empty() : tensor<1x32x1xf32>
    %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
    %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %96 = ttir.empty() : tensor<1x13x32xf32>
    %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %98 = ttir.empty() : tensor<1x13x32xf32>
    %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %100 = ttir.empty() : tensor<1x1x1xf32>
    %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %102 = ttir.empty() : tensor<1x13x32xf32>
    %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %104 = ttir.empty() : tensor<1x13x32xf32>
    %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %106 = ttir.empty() : tensor<1x13x32xbf16>
    %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %108 = ttir.empty() : tensor<1x1x13x32xbf16>
    %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %110 = ttir.empty() : tensor<1x1x13x32xf32>
    %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %112 = ttir.empty() : tensor<1x64x13x32xf32>
    %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %114 = ttir.empty() : tensor<1x64x13x32xf32>
    %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %116 = ttir.empty() : tensor<1x64x13x32xbf16>
    %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %118 = ttir.empty() : tensor<1x64x13x32xbf16>
    %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %120 = ttir.empty() : tensor<1x64x13x32xf32>
    %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %122 = ttir.empty() : tensor<1x13x32xf32>
    %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %124 = ttir.empty() : tensor<1x13x32xf32>
    %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %126 = ttir.empty() : tensor<1x13x32xbf16>
    %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %128 = ttir.empty() : tensor<1x1x13x32xbf16>
    %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %130 = ttir.empty() : tensor<1x1x13x32xf32>
    %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %132 = ttir.empty() : tensor<1x64x13x32xf32>
    %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %134 = ttir.empty() : tensor<1x64x13x32xf32>
    %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %136 = ttir.empty() : tensor<1x64x13x32xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %138 = ttir.empty() : tensor<1x64x13x32xbf16>
    %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %140 = ttir.empty() : tensor<1x64x13x32xf32>
    %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %142 = ttir.empty() : tensor<1x64x13x32xbf16>
    %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %144 = ttir.empty() : tensor<1x64x13x32xf32>
    %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %146 = ttir.empty() : tensor<1x64x13x32xbf16>
    %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %148 = ttir.empty() : tensor<1x64x13x32xbf16>
    %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %150 = ttir.empty() : tensor<1x64x13x64xbf16>
    %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %152 = ttir.empty() : tensor<64x13x64xbf16>
    %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %154 = ttir.empty() : tensor<2880x512xbf16>
    %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %157 = ttir.empty() : tensor<1x13x512xbf16>
    %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %159 = ttir.empty() : tensor<1x1x512xbf16>
    %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %161 = ttir.empty() : tensor<1x13x512xbf16>
    %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %163 = ttir.empty() : tensor<1x13x512xbf16>
    %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %165 = ttir.empty() : tensor<1x13x8x64xbf16>
    %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %167 = ttir.empty() : tensor<1x8x13x64xbf16>
    %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %169 = ttir.empty() : tensor<1x8x13x32xbf16>
    %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %171 = ttir.empty() : tensor<1x8x13x32xf32>
    %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %173 = ttir.empty() : tensor<1x8x13x32xf32>
    %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %175 = ttir.empty() : tensor<1x8x13x32xf32>
    %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %177 = ttir.empty() : tensor<1x8x13x32xbf16>
    %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %179 = ttir.empty() : tensor<1x8x13x32xbf16>
    %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %181 = ttir.empty() : tensor<1x8x13x32xf32>
    %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %183 = ttir.empty() : tensor<1x8x13x32xf32>
    %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %185 = ttir.empty() : tensor<1x8x13x32xf32>
    %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %187 = ttir.empty() : tensor<1x8x13x32xbf16>
    %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %189 = ttir.empty() : tensor<1x8x13x32xbf16>
    %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %191 = ttir.empty() : tensor<1x8x13x32xf32>
    %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %193 = ttir.empty() : tensor<1x8x13x32xbf16>
    %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %195 = ttir.empty() : tensor<1x8x13x32xf32>
    %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %197 = ttir.empty() : tensor<1x8x13x32xbf16>
    %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %199 = ttir.empty() : tensor<1x8x13x32xbf16>
    %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %201 = ttir.empty() : tensor<1x8x13x64xbf16>
    %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %207 = ttir.empty() : tensor<1x64x13x64xbf16>
    %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %209 = ttir.empty() : tensor<1x64x64x13xbf16>
    %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
    %211 = ttir.empty() : tensor<64x64x13xbf16>
    %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %214 = ttir.empty() : tensor<1x64x13x13xbf16>
    %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %216 = ttir.empty() : tensor<1x64x13x13xf32>
    %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %218 = ttir.empty() : tensor<1x1x1x1xf32>
    %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %220 = ttir.empty() : tensor<1x64x13x13xf32>
    %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %222 = ttir.empty() : tensor<1x64x13x13xf32>
    %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %224 = ttir.empty() : tensor<1x64x13x13xbf16>
    %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %226 = ttir.empty() : tensor<1x13xsi32>
    %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
    %228 = ttir.empty() : tensor<13x13xsi32>
    %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
    %230 = ttir.empty() : tensor<1xsi32>
    %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
    %232 = ttir.empty() : tensor<13xsi32>
    %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
    %234 = ttir.empty() : tensor<13xsi32>
    %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
    %236 = ttir.empty() : tensor<13x1xsi32>
    %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
    %238 = ttir.empty() : tensor<13x13xsi32>
    %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
    %240 = ttir.empty() : tensor<13x13xbf16>
    %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
    %242 = ttir.empty() : tensor<13x13xui8>
    %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %244 = ttir.empty() : tensor<13x13xui8>
    %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %246 = ttir.empty() : tensor<13x13xbf16>
    %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
    %248 = ttir.empty() : tensor<13x13xui8>
    %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %250 = ttir.empty() : tensor<13x1xsi32>
    %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
    %252 = ttir.empty() : tensor<13x13xsi32>
    %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
    %254 = ttir.empty() : tensor<13x13xbf16>
    %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
    %256 = ttir.empty() : tensor<13x13xui8>
    %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %258 = ttir.empty() : tensor<13x13xui8>
    %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %260 = ttir.empty() : tensor<13x13xbf16>
    %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
    %262 = ttir.empty() : tensor<1x1x13x13xbf16>
    %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %264 = ttir.empty() : tensor<1x1x1x1xbf16>
    %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %266 = ttir.empty() : tensor<1x1x13x13xbf16>
    %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %268 = ttir.empty() : tensor<1x1x13x13xbf16>
    %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %270 = ttir.empty() : tensor<1x64x13x13xbf16>
    %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %272 = ttir.empty() : tensor<1x64x13x13xbf16>
    %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %274 = ttir.empty() : tensor<1x64x1x1xbf16>
    %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %276 = ttir.empty() : tensor<1x64x13x1xbf16>
    %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %278 = ttir.empty() : tensor<1x64x13x14xbf16>
    %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %280 = ttir.empty() : tensor<2880x512xbf16>
    %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %283 = ttir.empty() : tensor<1x13x512xbf16>
    %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %285 = ttir.empty() : tensor<1x1x512xbf16>
    %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %287 = ttir.empty() : tensor<1x13x512xbf16>
    %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %289 = ttir.empty() : tensor<1x13x512xbf16>
    %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %291 = ttir.empty() : tensor<1x13x8x64xbf16>
    %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %293 = ttir.empty() : tensor<1x8x13x64xbf16>
    %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %295 = ttir.empty() : tensor<2880xf32>
    %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %297 = ttir.empty() : tensor<1x1x2880xf32>
    %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %299 = ttir.empty() : tensor<1x13x2880xf32>
    %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %301 = ttir.empty() : tensor<1x64x13xbf16>
    %302 = "ttir.max"(%279, %301) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %303 = ttir.empty() : tensor<1x64x13x1xbf16>
    %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %305 = ttir.empty() : tensor<1x64x13x14xbf16>
    %306 = "ttir.broadcast"(%304, %305) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %307 = ttir.empty() : tensor<1x64x13x14xbf16>
    %308 = "ttir.subtract"(%279, %306, %307) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %309 = ttir.empty() : tensor<1x64x13xbf16>
    %310 = "ttir.max"(%308, %309) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %311 = ttir.empty() : tensor<1x64x13x1xbf16>
    %312 = "ttir.reshape"(%310, %311) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %313 = ttir.empty() : tensor<1x64x13x14xbf16>
    %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %315 = ttir.empty() : tensor<1x64x13x14xbf16>
    %316 = "ttir.subtract"(%308, %314, %315) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %317 = ttir.empty() : tensor<1x64x13x14xbf16>
    %318 = "ttir.exp"(%316, %317) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %319 = ttir.empty() : tensor<1x64x13xbf16>
    %320 = "ttir.sum"(%318, %319) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %321 = ttir.empty() : tensor<1x64x13x1xbf16>
    %322 = "ttir.reshape"(%320, %321) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %323 = ttir.empty() : tensor<1x64x13x14xbf16>
    %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %325 = ttir.empty() : tensor<1x64x13x14xbf16>
    %326 = "ttir.div"(%318, %324, %325) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %327 = ttir.empty() : tensor<1x64x13x13xbf16>
    %328 = "ttir.slice_static"(%326, %327) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %329 = ttir.empty() : tensor<64x13x13xbf16>
    %330 = "ttir.reshape"(%328, %329) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %331 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %332 = "ttir.reshape"(%294, %331) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %333 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %334 = "ttir.broadcast"(%332, %333) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %335 = ttir.empty() : tensor<64x13x64xbf16>
    %336 = "ttir.reshape"(%334, %335) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %337 = "ttir.dot_general"(%330, %336) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %338 = ttir.empty() : tensor<1x64x13x64xbf16>
    %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %340 = ttir.empty() : tensor<1x13x64x64xbf16>
    %341 = "ttir.permute"(%339, %340) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %342 = ttir.empty() : tensor<13x4096xbf16>
    %343 = "ttir.reshape"(%341, %342) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
    %344 = ttir.empty() : tensor<4096x2880xbf16>
    %345 = "ttir.permute"(%arg14, %344) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
    %346 = "ttir.dot_general"(%343, %345) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %347 = ttir.empty() : tensor<1x13x2880xbf16>
    %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %349 = ttir.empty() : tensor<1x1x2880xbf16>
    %350 = "ttir.reshape"(%arg13, %349) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
    %351 = ttir.empty() : tensor<1x13x2880xbf16>
    %352 = "ttir.broadcast"(%350, %351) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %353 = ttir.empty() : tensor<1x13x2880xbf16>
    %354 = "ttir.add"(%348, %352, %353) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %355 = ttir.empty() : tensor<1x13x2880xbf16>
    %356 = "ttir.add"(%45, %354, %355) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %357 = ttir.empty() : tensor<1x1x1xbf16>
    %358 = "ttir.reshape"(%arg29, %357) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %359 = ttir.empty() : tensor<32x13x2880xbf16>
    %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %361 = ttir.empty() : tensor<2880xf32>
    %362 = "ttir.typecast"(%arg21, %361) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %363 = ttir.empty() : tensor<1x1x2880xf32>
    %364 = "ttir.reshape"(%362, %363) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %365 = ttir.empty() : tensor<1x13x2880xf32>
    %366 = "ttir.broadcast"(%364, %365) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %367 = ttir.empty() : tensor<1x13x2880xf32>
    %368 = "ttir.typecast"(%356, %367) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %369 = ttir.empty() : tensor<1x13x2880xf32>
    %370 = "ttir.pow"(%368, %27, %369) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %371 = ttir.empty() : tensor<1x13xf32>
    %372 = "ttir.sum"(%370, %371) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %373 = ttir.empty() : tensor<1x13xf32>
    %374 = "ttir.multiply"(%372, %4, %373) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %375 = ttir.empty() : tensor<1x13x1xf32>
    %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %377 = ttir.empty() : tensor<1x13x1xf32>
    %378 = "ttir.add"(%376, %59, %377) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %379 = ttir.empty() : tensor<1x13x1xf32>
    %380 = "ttir.rsqrt"(%378, %379) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %381 = ttir.empty() : tensor<1x13x2880xf32>
    %382 = "ttir.broadcast"(%380, %381) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %383 = ttir.empty() : tensor<1x13x2880xf32>
    %384 = "ttir.multiply"(%368, %382, %383) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %385 = ttir.empty() : tensor<1x13x2880xf32>
    %386 = "ttir.multiply"(%366, %384, %385) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %387 = ttir.empty() : tensor<1x13x2880xbf16>
    %388 = "ttir.typecast"(%386, %387) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %389 = ttir.empty() : tensor<13x2880xbf16>
    %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %391 = ttir.empty() : tensor<416x2880xbf16>
    %392 = "ttir.concat"(%390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %391) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
    %393 = ttir.empty() : tensor<32x13x2880xbf16>
    %394 = "ttir.reshape"(%392, %393) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %395 = "ttir.dot_general"(%394, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %396 = ttir.empty() : tensor<32x1x5760xbf16>
    %397 = "ttir.reshape"(%arg27, %396) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
    %398 = ttir.empty() : tensor<32x13x5760xbf16>
    %399 = "ttir.broadcast"(%397, %398) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %400 = ttir.empty() : tensor<32x13x5760xbf16>
    %401 = "ttir.add"(%395, %399, %400) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %402 = ttir.empty() : tensor<32x13x2880xbf16>
    %403 = "ttir.slice_static"(%401, %402) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %404 = ttir.empty() : tensor<1x1x1xbf16>
    %405 = "ttir.reshape"(%arg25, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %406 = ttir.empty() : tensor<32x13x2880xbf16>
    %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %408 = ttir.empty() : tensor<32x13x2880xbf16>
    %409 = "ttir.clamp_tensor"(%403, %360, %407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %410 = ttir.empty() : tensor<32x13x2880xbf16>
    %411 = "ttir.add"(%409, %15, %410) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %412 = ttir.empty() : tensor<32x13x2880xf32>
    %413 = "ttir.typecast"(%411, %412) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %414 = ttir.empty() : tensor<1x1x1xbf16>
    %415 = "ttir.reshape"(%arg26, %414) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %416 = ttir.empty() : tensor<32x13x2880xbf16>
    %417 = "ttir.broadcast"(%415, %416) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %418 = ttir.empty() : tensor<32x13x2880xbf16>
    %419 = "ttir.slice_static"(%401, %418) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %420 = ttir.empty() : tensor<32x13x2880xbf16>
    %421 = "ttir.clamp_tensor"(%419, %417, %407, %420) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %422 = ttir.empty() : tensor<32x13x2880xf32>
    %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %424 = ttir.empty() : tensor<1x1x1xf32>
    %425 = "ttir.reshape"(%arg24, %424) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %426 = ttir.empty() : tensor<32x13x2880xf32>
    %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %428 = ttir.empty() : tensor<32x13x2880xf32>
    %429 = "ttir.multiply"(%423, %427, %428) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %430 = ttir.empty() : tensor<32x13x2880xbf16>
    %431 = "ttir.typecast"(%429, %430) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %432 = ttir.empty() : tensor<32x13x2880xbf16>
    %433 = "ttir.sigmoid"(%431, %432) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %434 = ttir.empty() : tensor<32x13x2880xf32>
    %435 = "ttir.typecast"(%433, %434) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %436 = ttir.empty() : tensor<32x13x2880xf32>
    %437 = "ttir.multiply"(%423, %435, %436) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %438 = ttir.empty() : tensor<32x13x2880xf32>
    %439 = "ttir.multiply"(%413, %437, %438) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %440 = ttir.empty() : tensor<32x13x2880xbf16>
    %441 = "ttir.typecast"(%439, %440) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %442 = "ttir.dot_general"(%441, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %443 = ttir.empty() : tensor<32x1x2880xbf16>
    %444 = "ttir.reshape"(%arg22, %443) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
    %445 = ttir.empty() : tensor<32x13x2880xbf16>
    %446 = "ttir.broadcast"(%444, %445) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %447 = ttir.empty() : tensor<32x13x2880xbf16>
    %448 = "ttir.add"(%442, %446, %447) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %449 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %450 = "ttir.reshape"(%448, %449) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %451 = ttir.empty() : tensor<32x1x13x2880xf32>
    %452 = "ttir.typecast"(%450, %451) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %453 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
    %454 = ttir.empty() : tensor<13x1x1xsi32>
    %455 = "ttir.reshape"(%453, %454) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
    %456 = ttir.empty() : tensor<13x4x1xsi32>
    %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
    %458 = ttir.empty() : tensor<2880x32xbf16>
    %459 = "ttir.permute"(%arg12, %458) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
    %460 = "ttir.dot_general"(%390, %459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %461 = ttir.empty() : tensor<1x32xbf16>
    %462 = "ttir.reshape"(%arg11, %461) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
    %463 = ttir.empty() : tensor<13x32xbf16>
    %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %465 = ttir.empty() : tensor<13x32xbf16>
    %466 = "ttir.add"(%460, %464, %465) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %467 = ttir.empty() : tensor<13x32xbf16>
    %468 = ttir.empty() : tensor<13x32xsi32>
    %values, %indices = "ttir.sort"(%466, %467, %468) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
    %469 = ttir.empty() : tensor<13x4xsi32>
    %470 = "ttir.slice_static"(%indices, %469) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
    %471 = ttir.empty() : tensor<13x4x1xsi32>
    %472 = "ttir.reshape"(%470, %471) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
    %473 = ttir.empty() : tensor<13x4x2xsi32>
    %474 = "ttir.concat"(%457, %472, %473) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
    %475 = ttir.empty() : tensor<13x4xbf16>
    %476 = "ttir.slice_static"(%values, %475) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %477 = ttir.empty() : tensor<13xbf16>
    %478 = "ttir.max"(%476, %477) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %479 = ttir.empty() : tensor<13x1xbf16>
    %480 = "ttir.reshape"(%478, %479) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %481 = ttir.empty() : tensor<13x4xbf16>
    %482 = "ttir.broadcast"(%480, %481) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %483 = ttir.empty() : tensor<13x4xbf16>
    %484 = "ttir.subtract"(%476, %482, %483) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %485 = ttir.empty() : tensor<13x4xbf16>
    %486 = "ttir.exp"(%484, %485) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %487 = ttir.empty() : tensor<13xbf16>
    %488 = "ttir.sum"(%486, %487) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %489 = ttir.empty() : tensor<13x1xbf16>
    %490 = "ttir.reshape"(%488, %489) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %491 = ttir.empty() : tensor<13x4xbf16>
    %492 = "ttir.broadcast"(%490, %491) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %493 = ttir.empty() : tensor<13x4xbf16>
    %494 = "ttir.div"(%486, %492, %493) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %495 = ttir.empty() : tensor<13x32xbf16>
    %496 = "ttir.scatter"(%11, %474, %494, %495) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %497 = ttir.empty() : tensor<32x13xbf16>
    %498 = "ttir.permute"(%496, %497) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
    %499 = ttir.empty() : tensor<32x1x13x1xbf16>
    %500 = "ttir.reshape"(%498, %499) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
    %501 = ttir.empty() : tensor<32x1x13x1xf32>
    %502 = "ttir.typecast"(%500, %501) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
    %503 = ttir.empty() : tensor<32x1x13x2880xf32>
    %504 = "ttir.broadcast"(%502, %503) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %505 = ttir.empty() : tensor<32x1x13x2880xf32>
    %506 = "ttir.multiply"(%452, %504, %505) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %507 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %508 = "ttir.typecast"(%506, %507) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %509 = ttir.empty() : tensor<1x13x2880xbf16>
    %510 = "ttir.sum"(%508, %509) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %511 = ttir.empty() : tensor<1x13x2880xbf16>
    %512 = "ttir.add"(%356, %510, %511) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %513 = ttir.empty() : tensor<1x13x2880xf32>
    %514 = "ttir.typecast"(%512, %513) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %515 = ttir.empty() : tensor<1x13x2880xf32>
    %516 = "ttir.pow"(%514, %27, %515) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %517 = ttir.empty() : tensor<1x13xf32>
    %518 = "ttir.sum"(%516, %517) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %519 = ttir.empty() : tensor<1x13xf32>
    %520 = "ttir.multiply"(%518, %4, %519) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %521 = ttir.empty() : tensor<1x13x1xf32>
    %522 = "ttir.reshape"(%520, %521) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %523 = ttir.empty() : tensor<1x13x1xf32>
    %524 = "ttir.add"(%522, %59, %523) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %525 = ttir.empty() : tensor<1x13x1xf32>
    %526 = "ttir.rsqrt"(%524, %525) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %527 = ttir.empty() : tensor<1x13x2880xf32>
    %528 = "ttir.broadcast"(%526, %527) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %529 = ttir.empty() : tensor<1x13x2880xf32>
    %530 = "ttir.multiply"(%514, %528, %529) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %531 = ttir.empty() : tensor<1x13x2880xf32>
    %532 = "ttir.multiply"(%300, %530, %531) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %533 = ttir.empty() : tensor<1x13x2880xbf16>
    %534 = "ttir.typecast"(%532, %533) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %535 = ttir.empty() : tensor<13x2880xbf16>
    %536 = "ttir.reshape"(%534, %535) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %537 = ttir.empty() : tensor<2880x201088xbf16>
    %538 = "ttir.permute"(%arg10, %537) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
    %539 = "ttir.dot_general"(%536, %538) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %540 = ttir.empty() : tensor<1x13x201088xbf16>
    %541 = "ttir.reshape"(%539, %540) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %290, %292, %294, %202, %539, %541 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before TTCoreWrapDeviceModulePass (ttcore-wrap-device-module) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
    %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
    %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
    %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
    %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %8 = ttir.empty() : tensor<1x1xbf16>
    %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
    %10 = ttir.empty() : tensor<13x32xbf16>
    %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %12 = ttir.empty() : tensor<1x1x1xbf16>
    %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %14 = ttir.empty() : tensor<32x13x2880xbf16>
    %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %16 = ttir.empty() : tensor<1x1x1x1xbf16>
    %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %18 = ttir.empty() : tensor<1x1x13x13xbf16>
    %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %20 = ttir.empty() : tensor<1x1xui8>
    %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %22 = ttir.empty() : tensor<13x13xui8>
    %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %24 = ttir.empty() : tensor<1x1x1xf32>
    %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %26 = ttir.empty() : tensor<1x13x2880xf32>
    %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %28 = ttir.empty() : tensor<1x1xui8>
    %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
    %30 = ttir.empty() : tensor<13x13xui8>
    %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %32 = ttir.empty() : tensor<2880xf32>
    %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %34 = ttir.empty() : tensor<1x1x2880xf32>
    %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %36 = ttir.empty() : tensor<1x13x2880xf32>
    %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %38 = ttir.empty() : tensor<13xsi32>
    %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
    %40 = ttir.empty() : tensor<13xui32>
    %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
    %42 = ttir.empty() : tensor<13x2880xbf16>
    %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %44 = ttir.empty() : tensor<1x13x2880xbf16>
    %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %46 = ttir.empty() : tensor<1x13x2880xf32>
    %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %48 = ttir.empty() : tensor<1x13x2880xf32>
    %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %50 = ttir.empty() : tensor<1x13xf32>
    %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %52 = ttir.empty() : tensor<1x13xf32>
    %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %54 = ttir.empty() : tensor<1x13x1xf32>
    %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %56 = ttir.empty() : tensor<1x1x1xf32>
    %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %58 = ttir.empty() : tensor<1x13x1xf32>
    %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %60 = ttir.empty() : tensor<1x13x1xf32>
    %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %62 = ttir.empty() : tensor<1x13x1xf32>
    %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %64 = ttir.empty() : tensor<1x13x2880xf32>
    %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %66 = ttir.empty() : tensor<1x13x2880xf32>
    %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %68 = ttir.empty() : tensor<1x13x2880xf32>
    %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %70 = ttir.empty() : tensor<1x13x2880xbf16>
    %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %72 = ttir.empty() : tensor<13x2880xbf16>
    %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %74 = ttir.empty() : tensor<2880x4096xbf16>
    %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
    %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %77 = ttir.empty() : tensor<1x13x4096xbf16>
    %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %79 = ttir.empty() : tensor<1x1x4096xbf16>
    %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
    %81 = ttir.empty() : tensor<1x13x4096xbf16>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %83 = ttir.empty() : tensor<1x13x4096xbf16>
    %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %85 = ttir.empty() : tensor<1x13x64x64xbf16>
    %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %87 = ttir.empty() : tensor<1x64x13x64xbf16>
    %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = ttir.empty() : tensor<1x64x13x32xbf16>
    %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %91 = ttir.empty() : tensor<1x64x13x32xf32>
    %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %93 = ttir.empty() : tensor<1x32x1xf32>
    %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
    %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %96 = ttir.empty() : tensor<1x13x32xf32>
    %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %98 = ttir.empty() : tensor<1x13x32xf32>
    %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %100 = ttir.empty() : tensor<1x1x1xf32>
    %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %102 = ttir.empty() : tensor<1x13x32xf32>
    %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %104 = ttir.empty() : tensor<1x13x32xf32>
    %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %106 = ttir.empty() : tensor<1x13x32xbf16>
    %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %108 = ttir.empty() : tensor<1x1x13x32xbf16>
    %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %110 = ttir.empty() : tensor<1x1x13x32xf32>
    %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %112 = ttir.empty() : tensor<1x64x13x32xf32>
    %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %114 = ttir.empty() : tensor<1x64x13x32xf32>
    %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %116 = ttir.empty() : tensor<1x64x13x32xbf16>
    %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %118 = ttir.empty() : tensor<1x64x13x32xbf16>
    %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %120 = ttir.empty() : tensor<1x64x13x32xf32>
    %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %122 = ttir.empty() : tensor<1x13x32xf32>
    %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %124 = ttir.empty() : tensor<1x13x32xf32>
    %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
    %126 = ttir.empty() : tensor<1x13x32xbf16>
    %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
    %128 = ttir.empty() : tensor<1x1x13x32xbf16>
    %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %130 = ttir.empty() : tensor<1x1x13x32xf32>
    %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
    %132 = ttir.empty() : tensor<1x64x13x32xf32>
    %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %134 = ttir.empty() : tensor<1x64x13x32xf32>
    %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %136 = ttir.empty() : tensor<1x64x13x32xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %138 = ttir.empty() : tensor<1x64x13x32xbf16>
    %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %140 = ttir.empty() : tensor<1x64x13x32xf32>
    %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %142 = ttir.empty() : tensor<1x64x13x32xbf16>
    %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %144 = ttir.empty() : tensor<1x64x13x32xf32>
    %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
    %146 = ttir.empty() : tensor<1x64x13x32xbf16>
    %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %148 = ttir.empty() : tensor<1x64x13x32xbf16>
    %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
    %150 = ttir.empty() : tensor<1x64x13x64xbf16>
    %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %152 = ttir.empty() : tensor<64x13x64xbf16>
    %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %154 = ttir.empty() : tensor<2880x512xbf16>
    %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %157 = ttir.empty() : tensor<1x13x512xbf16>
    %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %159 = ttir.empty() : tensor<1x1x512xbf16>
    %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %161 = ttir.empty() : tensor<1x13x512xbf16>
    %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %163 = ttir.empty() : tensor<1x13x512xbf16>
    %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %165 = ttir.empty() : tensor<1x13x8x64xbf16>
    %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %167 = ttir.empty() : tensor<1x8x13x64xbf16>
    %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %169 = ttir.empty() : tensor<1x8x13x32xbf16>
    %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %171 = ttir.empty() : tensor<1x8x13x32xf32>
    %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %173 = ttir.empty() : tensor<1x8x13x32xf32>
    %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %175 = ttir.empty() : tensor<1x8x13x32xf32>
    %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %177 = ttir.empty() : tensor<1x8x13x32xbf16>
    %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %179 = ttir.empty() : tensor<1x8x13x32xbf16>
    %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %181 = ttir.empty() : tensor<1x8x13x32xf32>
    %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %183 = ttir.empty() : tensor<1x8x13x32xf32>
    %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %185 = ttir.empty() : tensor<1x8x13x32xf32>
    %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %187 = ttir.empty() : tensor<1x8x13x32xbf16>
    %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %189 = ttir.empty() : tensor<1x8x13x32xbf16>
    %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %191 = ttir.empty() : tensor<1x8x13x32xf32>
    %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %193 = ttir.empty() : tensor<1x8x13x32xbf16>
    %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %195 = ttir.empty() : tensor<1x8x13x32xf32>
    %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
    %197 = ttir.empty() : tensor<1x8x13x32xbf16>
    %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %199 = ttir.empty() : tensor<1x8x13x32xbf16>
    %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
    %201 = ttir.empty() : tensor<1x8x13x64xbf16>
    %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %207 = ttir.empty() : tensor<1x64x13x64xbf16>
    %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %209 = ttir.empty() : tensor<1x64x64x13xbf16>
    %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
    %211 = ttir.empty() : tensor<64x64x13xbf16>
    %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %214 = ttir.empty() : tensor<1x64x13x13xbf16>
    %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %216 = ttir.empty() : tensor<1x64x13x13xf32>
    %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %218 = ttir.empty() : tensor<1x1x1x1xf32>
    %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %220 = ttir.empty() : tensor<1x64x13x13xf32>
    %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %222 = ttir.empty() : tensor<1x64x13x13xf32>
    %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %224 = ttir.empty() : tensor<1x64x13x13xbf16>
    %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %226 = ttir.empty() : tensor<1x13xsi32>
    %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
    %228 = ttir.empty() : tensor<13x13xsi32>
    %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
    %230 = ttir.empty() : tensor<1xsi32>
    %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
    %232 = ttir.empty() : tensor<13xsi32>
    %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
    %234 = ttir.empty() : tensor<13xsi32>
    %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
    %236 = ttir.empty() : tensor<13x1xsi32>
    %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
    %238 = ttir.empty() : tensor<13x13xsi32>
    %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
    %240 = ttir.empty() : tensor<13x13xbf16>
    %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
    %242 = ttir.empty() : tensor<13x13xui8>
    %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %244 = ttir.empty() : tensor<13x13xui8>
    %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %246 = ttir.empty() : tensor<13x13xbf16>
    %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
    %248 = ttir.empty() : tensor<13x13xui8>
    %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %250 = ttir.empty() : tensor<13x1xsi32>
    %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
    %252 = ttir.empty() : tensor<13x13xsi32>
    %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
    %254 = ttir.empty() : tensor<13x13xbf16>
    %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
    %256 = ttir.empty() : tensor<13x13xui8>
    %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %258 = ttir.empty() : tensor<13x13xui8>
    %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
    %260 = ttir.empty() : tensor<13x13xbf16>
    %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
    %262 = ttir.empty() : tensor<1x1x13x13xbf16>
    %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %264 = ttir.empty() : tensor<1x1x1x1xbf16>
    %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %266 = ttir.empty() : tensor<1x1x13x13xbf16>
    %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %268 = ttir.empty() : tensor<1x1x13x13xbf16>
    %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
    %270 = ttir.empty() : tensor<1x64x13x13xbf16>
    %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %272 = ttir.empty() : tensor<1x64x13x13xbf16>
    %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %274 = ttir.empty() : tensor<1x64x1x1xbf16>
    %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %276 = ttir.empty() : tensor<1x64x13x1xbf16>
    %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %278 = ttir.empty() : tensor<1x64x13x14xbf16>
    %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %280 = ttir.empty() : tensor<2880x512xbf16>
    %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
    %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %283 = ttir.empty() : tensor<1x13x512xbf16>
    %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %285 = ttir.empty() : tensor<1x1x512xbf16>
    %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
    %287 = ttir.empty() : tensor<1x13x512xbf16>
    %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %289 = ttir.empty() : tensor<1x13x512xbf16>
    %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
    %291 = ttir.empty() : tensor<1x13x8x64xbf16>
    %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
    %293 = ttir.empty() : tensor<1x8x13x64xbf16>
    %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
    %295 = ttir.empty() : tensor<2880xf32>
    %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %297 = ttir.empty() : tensor<1x1x2880xf32>
    %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %299 = ttir.empty() : tensor<1x13x2880xf32>
    %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %301 = ttir.empty() : tensor<1x64x13xbf16>
    %302 = "ttir.max"(%279, %301) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %303 = ttir.empty() : tensor<1x64x13x1xbf16>
    %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %305 = ttir.empty() : tensor<1x64x13x14xbf16>
    %306 = "ttir.broadcast"(%304, %305) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %307 = ttir.empty() : tensor<1x64x13x14xbf16>
    %308 = "ttir.subtract"(%279, %306, %307) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %309 = ttir.empty() : tensor<1x64x13xbf16>
    %310 = "ttir.max"(%308, %309) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %311 = ttir.empty() : tensor<1x64x13x1xbf16>
    %312 = "ttir.reshape"(%310, %311) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %313 = ttir.empty() : tensor<1x64x13x14xbf16>
    %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %315 = ttir.empty() : tensor<1x64x13x14xbf16>
    %316 = "ttir.subtract"(%308, %314, %315) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %317 = ttir.empty() : tensor<1x64x13x14xbf16>
    %318 = "ttir.exp"(%316, %317) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %319 = ttir.empty() : tensor<1x64x13xbf16>
    %320 = "ttir.sum"(%318, %319) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
    %321 = ttir.empty() : tensor<1x64x13x1xbf16>
    %322 = "ttir.reshape"(%320, %321) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
    %323 = ttir.empty() : tensor<1x64x13x14xbf16>
    %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %325 = ttir.empty() : tensor<1x64x13x14xbf16>
    %326 = "ttir.div"(%318, %324, %325) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
    %327 = ttir.empty() : tensor<1x64x13x13xbf16>
    %328 = "ttir.slice_static"(%326, %327) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %329 = ttir.empty() : tensor<64x13x13xbf16>
    %330 = "ttir.reshape"(%328, %329) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %331 = ttir.empty() : tensor<1x8x1x13x64xbf16>
    %332 = "ttir.reshape"(%294, %331) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
    %333 = ttir.empty() : tensor<1x8x8x13x64xbf16>
    %334 = "ttir.broadcast"(%332, %333) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %335 = ttir.empty() : tensor<64x13x64xbf16>
    %336 = "ttir.reshape"(%334, %335) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %337 = "ttir.dot_general"(%330, %336) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %338 = ttir.empty() : tensor<1x64x13x64xbf16>
    %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %340 = ttir.empty() : tensor<1x13x64x64xbf16>
    %341 = "ttir.permute"(%339, %340) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
    %342 = ttir.empty() : tensor<13x4096xbf16>
    %343 = "ttir.reshape"(%341, %342) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
    %344 = ttir.empty() : tensor<4096x2880xbf16>
    %345 = "ttir.permute"(%arg14, %344) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
    %346 = "ttir.dot_general"(%343, %345) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %347 = ttir.empty() : tensor<1x13x2880xbf16>
    %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %349 = ttir.empty() : tensor<1x1x2880xbf16>
    %350 = "ttir.reshape"(%arg13, %349) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
    %351 = ttir.empty() : tensor<1x13x2880xbf16>
    %352 = "ttir.broadcast"(%350, %351) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %353 = ttir.empty() : tensor<1x13x2880xbf16>
    %354 = "ttir.add"(%348, %352, %353) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %355 = ttir.empty() : tensor<1x13x2880xbf16>
    %356 = "ttir.add"(%45, %354, %355) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %357 = ttir.empty() : tensor<1x1x1xbf16>
    %358 = "ttir.reshape"(%arg29, %357) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %359 = ttir.empty() : tensor<32x13x2880xbf16>
    %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %361 = ttir.empty() : tensor<2880xf32>
    %362 = "ttir.typecast"(%arg21, %361) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
    %363 = ttir.empty() : tensor<1x1x2880xf32>
    %364 = "ttir.reshape"(%362, %363) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
    %365 = ttir.empty() : tensor<1x13x2880xf32>
    %366 = "ttir.broadcast"(%364, %365) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %367 = ttir.empty() : tensor<1x13x2880xf32>
    %368 = "ttir.typecast"(%356, %367) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %369 = ttir.empty() : tensor<1x13x2880xf32>
    %370 = "ttir.pow"(%368, %27, %369) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %371 = ttir.empty() : tensor<1x13xf32>
    %372 = "ttir.sum"(%370, %371) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %373 = ttir.empty() : tensor<1x13xf32>
    %374 = "ttir.multiply"(%372, %4, %373) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %375 = ttir.empty() : tensor<1x13x1xf32>
    %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %377 = ttir.empty() : tensor<1x13x1xf32>
    %378 = "ttir.add"(%376, %59, %377) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %379 = ttir.empty() : tensor<1x13x1xf32>
    %380 = "ttir.rsqrt"(%378, %379) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %381 = ttir.empty() : tensor<1x13x2880xf32>
    %382 = "ttir.broadcast"(%380, %381) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %383 = ttir.empty() : tensor<1x13x2880xf32>
    %384 = "ttir.multiply"(%368, %382, %383) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %385 = ttir.empty() : tensor<1x13x2880xf32>
    %386 = "ttir.multiply"(%366, %384, %385) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %387 = ttir.empty() : tensor<1x13x2880xbf16>
    %388 = "ttir.typecast"(%386, %387) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %389 = ttir.empty() : tensor<13x2880xbf16>
    %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %391 = ttir.empty() : tensor<416x2880xbf16>
    %392 = "ttir.concat"(%390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %391) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
    %393 = ttir.empty() : tensor<32x13x2880xbf16>
    %394 = "ttir.reshape"(%392, %393) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %395 = "ttir.dot_general"(%394, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %396 = ttir.empty() : tensor<32x1x5760xbf16>
    %397 = "ttir.reshape"(%arg27, %396) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
    %398 = ttir.empty() : tensor<32x13x5760xbf16>
    %399 = "ttir.broadcast"(%397, %398) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %400 = ttir.empty() : tensor<32x13x5760xbf16>
    %401 = "ttir.add"(%395, %399, %400) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
    %402 = ttir.empty() : tensor<32x13x2880xbf16>
    %403 = "ttir.slice_static"(%401, %402) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %404 = ttir.empty() : tensor<1x1x1xbf16>
    %405 = "ttir.reshape"(%arg25, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %406 = ttir.empty() : tensor<32x13x2880xbf16>
    %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %408 = ttir.empty() : tensor<32x13x2880xbf16>
    %409 = "ttir.clamp_tensor"(%403, %360, %407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %410 = ttir.empty() : tensor<32x13x2880xbf16>
    %411 = "ttir.add"(%409, %15, %410) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %412 = ttir.empty() : tensor<32x13x2880xf32>
    %413 = "ttir.typecast"(%411, %412) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %414 = ttir.empty() : tensor<1x1x1xbf16>
    %415 = "ttir.reshape"(%arg26, %414) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
    %416 = ttir.empty() : tensor<32x13x2880xbf16>
    %417 = "ttir.broadcast"(%415, %416) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %418 = ttir.empty() : tensor<32x13x2880xbf16>
    %419 = "ttir.slice_static"(%401, %418) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %420 = ttir.empty() : tensor<32x13x2880xbf16>
    %421 = "ttir.clamp_tensor"(%419, %417, %407, %420) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %422 = ttir.empty() : tensor<32x13x2880xf32>
    %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %424 = ttir.empty() : tensor<1x1x1xf32>
    %425 = "ttir.reshape"(%arg24, %424) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %426 = ttir.empty() : tensor<32x13x2880xf32>
    %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %428 = ttir.empty() : tensor<32x13x2880xf32>
    %429 = "ttir.multiply"(%423, %427, %428) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %430 = ttir.empty() : tensor<32x13x2880xbf16>
    %431 = "ttir.typecast"(%429, %430) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %432 = ttir.empty() : tensor<32x13x2880xbf16>
    %433 = "ttir.sigmoid"(%431, %432) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %434 = ttir.empty() : tensor<32x13x2880xf32>
    %435 = "ttir.typecast"(%433, %434) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %436 = ttir.empty() : tensor<32x13x2880xf32>
    %437 = "ttir.multiply"(%423, %435, %436) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %438 = ttir.empty() : tensor<32x13x2880xf32>
    %439 = "ttir.multiply"(%413, %437, %438) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
    %440 = ttir.empty() : tensor<32x13x2880xbf16>
    %441 = "ttir.typecast"(%439, %440) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %442 = "ttir.dot_general"(%441, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %443 = ttir.empty() : tensor<32x1x2880xbf16>
    %444 = "ttir.reshape"(%arg22, %443) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
    %445 = ttir.empty() : tensor<32x13x2880xbf16>
    %446 = "ttir.broadcast"(%444, %445) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %447 = ttir.empty() : tensor<32x13x2880xbf16>
    %448 = "ttir.add"(%442, %446, %447) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
    %449 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %450 = "ttir.reshape"(%448, %449) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %451 = ttir.empty() : tensor<32x1x13x2880xf32>
    %452 = "ttir.typecast"(%450, %451) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %453 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
    %454 = ttir.empty() : tensor<13x1x1xsi32>
    %455 = "ttir.reshape"(%453, %454) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
    %456 = ttir.empty() : tensor<13x4x1xsi32>
    %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
    %458 = ttir.empty() : tensor<2880x32xbf16>
    %459 = "ttir.permute"(%arg12, %458) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
    %460 = "ttir.dot_general"(%390, %459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %461 = ttir.empty() : tensor<1x32xbf16>
    %462 = "ttir.reshape"(%arg11, %461) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
    %463 = ttir.empty() : tensor<13x32xbf16>
    %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %465 = ttir.empty() : tensor<13x32xbf16>
    %466 = "ttir.add"(%460, %464, %465) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %467 = ttir.empty() : tensor<13x32xbf16>
    %468 = ttir.empty() : tensor<13x32xsi32>
    %values, %indices = "ttir.sort"(%466, %467, %468) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
    %469 = ttir.empty() : tensor<13x4xsi32>
    %470 = "ttir.slice_static"(%indices, %469) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
    %471 = ttir.empty() : tensor<13x4x1xsi32>
    %472 = "ttir.reshape"(%470, %471) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
    %473 = ttir.empty() : tensor<13x4x2xsi32>
    %474 = "ttir.concat"(%457, %472, %473) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
    %475 = ttir.empty() : tensor<13x4xbf16>
    %476 = "ttir.slice_static"(%values, %475) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %477 = ttir.empty() : tensor<13xbf16>
    %478 = "ttir.max"(%476, %477) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %479 = ttir.empty() : tensor<13x1xbf16>
    %480 = "ttir.reshape"(%478, %479) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %481 = ttir.empty() : tensor<13x4xbf16>
    %482 = "ttir.broadcast"(%480, %481) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %483 = ttir.empty() : tensor<13x4xbf16>
    %484 = "ttir.subtract"(%476, %482, %483) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %485 = ttir.empty() : tensor<13x4xbf16>
    %486 = "ttir.exp"(%484, %485) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %487 = ttir.empty() : tensor<13xbf16>
    %488 = "ttir.sum"(%486, %487) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
    %489 = ttir.empty() : tensor<13x1xbf16>
    %490 = "ttir.reshape"(%488, %489) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
    %491 = ttir.empty() : tensor<13x4xbf16>
    %492 = "ttir.broadcast"(%490, %491) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %493 = ttir.empty() : tensor<13x4xbf16>
    %494 = "ttir.div"(%486, %492, %493) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
    %495 = ttir.empty() : tensor<13x32xbf16>
    %496 = "ttir.scatter"(%11, %474, %494, %495) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
    %497 = ttir.empty() : tensor<32x13xbf16>
    %498 = "ttir.permute"(%496, %497) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
    %499 = ttir.empty() : tensor<32x1x13x1xbf16>
    %500 = "ttir.reshape"(%498, %499) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
    %501 = ttir.empty() : tensor<32x1x13x1xf32>
    %502 = "ttir.typecast"(%500, %501) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
    %503 = ttir.empty() : tensor<32x1x13x2880xf32>
    %504 = "ttir.broadcast"(%502, %503) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %505 = ttir.empty() : tensor<32x1x13x2880xf32>
    %506 = "ttir.multiply"(%452, %504, %505) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %507 = ttir.empty() : tensor<32x1x13x2880xbf16>
    %508 = "ttir.typecast"(%506, %507) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %509 = ttir.empty() : tensor<1x13x2880xbf16>
    %510 = "ttir.sum"(%508, %509) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %511 = ttir.empty() : tensor<1x13x2880xbf16>
    %512 = "ttir.add"(%356, %510, %511) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %513 = ttir.empty() : tensor<1x13x2880xf32>
    %514 = "ttir.typecast"(%512, %513) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %515 = ttir.empty() : tensor<1x13x2880xf32>
    %516 = "ttir.pow"(%514, %27, %515) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %517 = ttir.empty() : tensor<1x13xf32>
    %518 = "ttir.sum"(%516, %517) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %519 = ttir.empty() : tensor<1x13xf32>
    %520 = "ttir.multiply"(%518, %4, %519) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
    %521 = ttir.empty() : tensor<1x13x1xf32>
    %522 = "ttir.reshape"(%520, %521) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %523 = ttir.empty() : tensor<1x13x1xf32>
    %524 = "ttir.add"(%522, %59, %523) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %525 = ttir.empty() : tensor<1x13x1xf32>
    %526 = "ttir.rsqrt"(%524, %525) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
    %527 = ttir.empty() : tensor<1x13x2880xf32>
    %528 = "ttir.broadcast"(%526, %527) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %529 = ttir.empty() : tensor<1x13x2880xf32>
    %530 = "ttir.multiply"(%514, %528, %529) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %531 = ttir.empty() : tensor<1x13x2880xf32>
    %532 = "ttir.multiply"(%300, %530, %531) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
    %533 = ttir.empty() : tensor<1x13x2880xbf16>
    %534 = "ttir.typecast"(%532, %533) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %535 = ttir.empty() : tensor<13x2880xbf16>
    %536 = "ttir.reshape"(%534, %535) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
    %537 = ttir.empty() : tensor<2880x201088xbf16>
    %538 = "ttir.permute"(%arg10, %537) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
    %539 = "ttir.dot_general"(%536, %538) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %540 = ttir.empty() : tensor<1x13x201088xbf16>
    %541 = "ttir.reshape"(%539, %540) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %290, %292, %294, %202, %539, %541 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After TTCoreWrapDeviceModulePass (ttcore-wrap-device-module) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<2880x4096xbf16>
        %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
        %77 = ttir.empty() : tensor<1x13x4096xbf16>
        %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %79 = ttir.empty() : tensor<1x1x4096xbf16>
        %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %81 = ttir.empty() : tensor<1x13x4096xbf16>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %83 = ttir.empty() : tensor<1x13x4096xbf16>
        %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %85 = ttir.empty() : tensor<1x13x64x64xbf16>
        %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %87 = ttir.empty() : tensor<1x64x13x64xbf16>
        %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %89 = ttir.empty() : tensor<1x64x13x32xbf16>
        %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %91 = ttir.empty() : tensor<1x64x13x32xf32>
        %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %93 = ttir.empty() : tensor<1x32x1xf32>
        %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<2880x512xbf16>
        %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %157 = ttir.empty() : tensor<1x13x512xbf16>
        %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %159 = ttir.empty() : tensor<1x1x512xbf16>
        %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %161 = ttir.empty() : tensor<1x13x512xbf16>
        %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %163 = ttir.empty() : tensor<1x13x512xbf16>
        %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %165 = ttir.empty() : tensor<1x13x8x64xbf16>
        %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %167 = ttir.empty() : tensor<1x8x13x64xbf16>
        %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %169 = ttir.empty() : tensor<1x8x13x32xbf16>
        %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %171 = ttir.empty() : tensor<1x8x13x32xf32>
        %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %173 = ttir.empty() : tensor<1x8x13x32xf32>
        %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %175 = ttir.empty() : tensor<1x8x13x32xf32>
        %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %177 = ttir.empty() : tensor<1x8x13x32xbf16>
        %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %179 = ttir.empty() : tensor<1x8x13x32xbf16>
        %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %181 = ttir.empty() : tensor<1x8x13x32xf32>
        %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %183 = ttir.empty() : tensor<1x8x13x32xf32>
        %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %185 = ttir.empty() : tensor<1x8x13x32xf32>
        %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %187 = ttir.empty() : tensor<1x8x13x32xbf16>
        %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %189 = ttir.empty() : tensor<1x8x13x32xbf16>
        %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %191 = ttir.empty() : tensor<1x8x13x32xf32>
        %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %193 = ttir.empty() : tensor<1x8x13x32xbf16>
        %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %195 = ttir.empty() : tensor<1x8x13x32xf32>
        %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %197 = ttir.empty() : tensor<1x8x13x32xbf16>
        %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %199 = ttir.empty() : tensor<1x8x13x32xbf16>
        %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %201 = ttir.empty() : tensor<1x8x13x64xbf16>
        %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %207 = ttir.empty() : tensor<1x64x13x64xbf16>
        %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %209 = ttir.empty() : tensor<1x64x64x13xbf16>
        %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %211 = ttir.empty() : tensor<64x64x13xbf16>
        %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<2880x512xbf16>
        %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %283 = ttir.empty() : tensor<1x13x512xbf16>
        %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %285 = ttir.empty() : tensor<1x1x512xbf16>
        %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %287 = ttir.empty() : tensor<1x13x512xbf16>
        %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %289 = ttir.empty() : tensor<1x13x512xbf16>
        %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %291 = ttir.empty() : tensor<1x13x8x64xbf16>
        %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %293 = ttir.empty() : tensor<1x8x13x64xbf16>
        %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %295 = ttir.empty() : tensor<2880xf32>
        %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %297 = ttir.empty() : tensor<1x1x2880xf32>
        %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %299 = ttir.empty() : tensor<1x13x2880xf32>
        %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %301 = ttir.empty() : tensor<1x64x13xbf16>
        %302 = "ttir.max"(%279, %301) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %303 = ttir.empty() : tensor<1x64x13x1xbf16>
        %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %305 = ttir.empty() : tensor<1x64x13x14xbf16>
        %306 = "ttir.broadcast"(%304, %305) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %307 = ttir.empty() : tensor<1x64x13x14xbf16>
        %308 = "ttir.subtract"(%279, %306, %307) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %309 = ttir.empty() : tensor<1x64x13xbf16>
        %310 = "ttir.max"(%308, %309) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %311 = ttir.empty() : tensor<1x64x13x1xbf16>
        %312 = "ttir.reshape"(%310, %311) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %313 = ttir.empty() : tensor<1x64x13x14xbf16>
        %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %315 = ttir.empty() : tensor<1x64x13x14xbf16>
        %316 = "ttir.subtract"(%308, %314, %315) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %317 = ttir.empty() : tensor<1x64x13x14xbf16>
        %318 = "ttir.exp"(%316, %317) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %319 = ttir.empty() : tensor<1x64x13xbf16>
        %320 = "ttir.sum"(%318, %319) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %321 = ttir.empty() : tensor<1x64x13x1xbf16>
        %322 = "ttir.reshape"(%320, %321) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %323 = ttir.empty() : tensor<1x64x13x14xbf16>
        %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %325 = ttir.empty() : tensor<1x64x13x14xbf16>
        %326 = "ttir.div"(%318, %324, %325) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %327 = ttir.empty() : tensor<1x64x13x13xbf16>
        %328 = "ttir.slice_static"(%326, %327) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %329 = ttir.empty() : tensor<64x13x13xbf16>
        %330 = "ttir.reshape"(%328, %329) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %331 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %332 = "ttir.reshape"(%294, %331) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %333 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %334 = "ttir.broadcast"(%332, %333) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %335 = ttir.empty() : tensor<64x13x64xbf16>
        %336 = "ttir.reshape"(%334, %335) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %337 = "ttir.dot_general"(%330, %336) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %338 = ttir.empty() : tensor<1x64x13x64xbf16>
        %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %340 = ttir.empty() : tensor<1x13x64x64xbf16>
        %341 = "ttir.permute"(%339, %340) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %342 = ttir.empty() : tensor<13x4096xbf16>
        %343 = "ttir.reshape"(%341, %342) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %344 = ttir.empty() : tensor<4096x2880xbf16>
        %345 = "ttir.permute"(%arg14, %344) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %346 = "ttir.dot_general"(%343, %345) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
        %347 = ttir.empty() : tensor<1x13x2880xbf16>
        %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %349 = ttir.empty() : tensor<1x1x2880xbf16>
        %350 = "ttir.reshape"(%arg13, %349) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %351 = ttir.empty() : tensor<1x13x2880xbf16>
        %352 = "ttir.broadcast"(%350, %351) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %353 = ttir.empty() : tensor<1x13x2880xbf16>
        %354 = "ttir.add"(%348, %352, %353) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %355 = ttir.empty() : tensor<1x13x2880xbf16>
        %356 = "ttir.add"(%45, %354, %355) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %357 = ttir.empty() : tensor<1x1x1xbf16>
        %358 = "ttir.reshape"(%arg29, %357) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %359 = ttir.empty() : tensor<32x13x2880xbf16>
        %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %361 = ttir.empty() : tensor<2880xf32>
        %362 = "ttir.typecast"(%arg21, %361) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %363 = ttir.empty() : tensor<1x1x2880xf32>
        %364 = "ttir.reshape"(%362, %363) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %365 = ttir.empty() : tensor<1x13x2880xf32>
        %366 = "ttir.broadcast"(%364, %365) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %367 = ttir.empty() : tensor<1x13x2880xf32>
        %368 = "ttir.typecast"(%356, %367) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %369 = ttir.empty() : tensor<1x13x2880xf32>
        %370 = "ttir.pow"(%368, %27, %369) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %371 = ttir.empty() : tensor<1x13xf32>
        %372 = "ttir.sum"(%370, %371) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %373 = ttir.empty() : tensor<1x13xf32>
        %374 = "ttir.multiply"(%372, %4, %373) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %375 = ttir.empty() : tensor<1x13x1xf32>
        %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %377 = ttir.empty() : tensor<1x13x1xf32>
        %378 = "ttir.add"(%376, %59, %377) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %379 = ttir.empty() : tensor<1x13x1xf32>
        %380 = "ttir.rsqrt"(%378, %379) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %381 = ttir.empty() : tensor<1x13x2880xf32>
        %382 = "ttir.broadcast"(%380, %381) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %383 = ttir.empty() : tensor<1x13x2880xf32>
        %384 = "ttir.multiply"(%368, %382, %383) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %385 = ttir.empty() : tensor<1x13x2880xf32>
        %386 = "ttir.multiply"(%366, %384, %385) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %387 = ttir.empty() : tensor<1x13x2880xbf16>
        %388 = "ttir.typecast"(%386, %387) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %389 = ttir.empty() : tensor<13x2880xbf16>
        %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %391 = ttir.empty() : tensor<416x2880xbf16>
        %392 = "ttir.concat"(%390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %391) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %393 = ttir.empty() : tensor<32x13x2880xbf16>
        %394 = "ttir.reshape"(%392, %393) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %395 = "ttir.dot_general"(%394, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
        %396 = ttir.empty() : tensor<32x1x5760xbf16>
        %397 = "ttir.reshape"(%arg27, %396) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %398 = ttir.empty() : tensor<32x13x5760xbf16>
        %399 = "ttir.broadcast"(%397, %398) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %400 = ttir.empty() : tensor<32x13x5760xbf16>
        %401 = "ttir.add"(%395, %399, %400) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %402 = ttir.empty() : tensor<32x13x2880xbf16>
        %403 = "ttir.slice_static"(%401, %402) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %404 = ttir.empty() : tensor<1x1x1xbf16>
        %405 = "ttir.reshape"(%arg25, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.clamp_tensor"(%403, %360, %407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xbf16>
        %411 = "ttir.add"(%409, %15, %410) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.typecast"(%411, %412) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<1x1x1xbf16>
        %415 = "ttir.reshape"(%arg26, %414) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.broadcast"(%415, %416) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xbf16>
        %419 = "ttir.slice_static"(%401, %418) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %420 = ttir.empty() : tensor<32x13x2880xbf16>
        %421 = "ttir.clamp_tensor"(%419, %417, %407, %420) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xf32>
        %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %424 = ttir.empty() : tensor<1x1x1xf32>
        %425 = "ttir.reshape"(%arg24, %424) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %426 = ttir.empty() : tensor<32x13x2880xf32>
        %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %428 = ttir.empty() : tensor<32x13x2880xf32>
        %429 = "ttir.multiply"(%423, %427, %428) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %430 = ttir.empty() : tensor<32x13x2880xbf16>
        %431 = "ttir.typecast"(%429, %430) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %432 = ttir.empty() : tensor<32x13x2880xbf16>
        %433 = "ttir.sigmoid"(%431, %432) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %434 = ttir.empty() : tensor<32x13x2880xf32>
        %435 = "ttir.typecast"(%433, %434) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %436 = ttir.empty() : tensor<32x13x2880xf32>
        %437 = "ttir.multiply"(%423, %435, %436) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %438 = ttir.empty() : tensor<32x13x2880xf32>
        %439 = "ttir.multiply"(%413, %437, %438) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %440 = ttir.empty() : tensor<32x13x2880xbf16>
        %441 = "ttir.typecast"(%439, %440) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %442 = "ttir.dot_general"(%441, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
        %443 = ttir.empty() : tensor<32x1x2880xbf16>
        %444 = "ttir.reshape"(%arg22, %443) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %445 = ttir.empty() : tensor<32x13x2880xbf16>
        %446 = "ttir.broadcast"(%444, %445) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %447 = ttir.empty() : tensor<32x13x2880xbf16>
        %448 = "ttir.add"(%442, %446, %447) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %449 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %450 = "ttir.reshape"(%448, %449) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %451 = ttir.empty() : tensor<32x1x13x2880xf32>
        %452 = "ttir.typecast"(%450, %451) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %453 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %454 = ttir.empty() : tensor<13x1x1xsi32>
        %455 = "ttir.reshape"(%453, %454) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %456 = ttir.empty() : tensor<13x4x1xsi32>
        %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %458 = ttir.empty() : tensor<2880x32xbf16>
        %459 = "ttir.permute"(%arg12, %458) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %460 = "ttir.dot_general"(%390, %459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
        %461 = ttir.empty() : tensor<1x32xbf16>
        %462 = "ttir.reshape"(%arg11, %461) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %463 = ttir.empty() : tensor<13x32xbf16>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %465 = ttir.empty() : tensor<13x32xbf16>
        %466 = "ttir.add"(%460, %464, %465) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %467 = ttir.empty() : tensor<13x32xbf16>
        %468 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%466, %467, %468) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %469 = ttir.empty() : tensor<13x4xsi32>
        %470 = "ttir.slice_static"(%indices, %469) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %471 = ttir.empty() : tensor<13x4x1xsi32>
        %472 = "ttir.reshape"(%470, %471) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %473 = ttir.empty() : tensor<13x4x2xsi32>
        %474 = "ttir.concat"(%457, %472, %473) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %475 = ttir.empty() : tensor<13x4xbf16>
        %476 = "ttir.slice_static"(%values, %475) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %477 = ttir.empty() : tensor<13xbf16>
        %478 = "ttir.max"(%476, %477) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %479 = ttir.empty() : tensor<13x1xbf16>
        %480 = "ttir.reshape"(%478, %479) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %481 = ttir.empty() : tensor<13x4xbf16>
        %482 = "ttir.broadcast"(%480, %481) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %483 = ttir.empty() : tensor<13x4xbf16>
        %484 = "ttir.subtract"(%476, %482, %483) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %485 = ttir.empty() : tensor<13x4xbf16>
        %486 = "ttir.exp"(%484, %485) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %487 = ttir.empty() : tensor<13xbf16>
        %488 = "ttir.sum"(%486, %487) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %489 = ttir.empty() : tensor<13x1xbf16>
        %490 = "ttir.reshape"(%488, %489) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %491 = ttir.empty() : tensor<13x4xbf16>
        %492 = "ttir.broadcast"(%490, %491) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %493 = ttir.empty() : tensor<13x4xbf16>
        %494 = "ttir.div"(%486, %492, %493) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %495 = ttir.empty() : tensor<13x32xbf16>
        %496 = "ttir.scatter"(%11, %474, %494, %495) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %497 = ttir.empty() : tensor<32x13xbf16>
        %498 = "ttir.permute"(%496, %497) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %499 = ttir.empty() : tensor<32x1x13x1xbf16>
        %500 = "ttir.reshape"(%498, %499) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %501 = ttir.empty() : tensor<32x1x13x1xf32>
        %502 = "ttir.typecast"(%500, %501) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %503 = ttir.empty() : tensor<32x1x13x2880xf32>
        %504 = "ttir.broadcast"(%502, %503) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %505 = ttir.empty() : tensor<32x1x13x2880xf32>
        %506 = "ttir.multiply"(%452, %504, %505) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %507 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %508 = "ttir.typecast"(%506, %507) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %509 = ttir.empty() : tensor<1x13x2880xbf16>
        %510 = "ttir.sum"(%508, %509) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %511 = ttir.empty() : tensor<1x13x2880xbf16>
        %512 = "ttir.add"(%356, %510, %511) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %513 = ttir.empty() : tensor<1x13x2880xf32>
        %514 = "ttir.typecast"(%512, %513) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %515 = ttir.empty() : tensor<1x13x2880xf32>
        %516 = "ttir.pow"(%514, %27, %515) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %517 = ttir.empty() : tensor<1x13xf32>
        %518 = "ttir.sum"(%516, %517) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %519 = ttir.empty() : tensor<1x13xf32>
        %520 = "ttir.multiply"(%518, %4, %519) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %521 = ttir.empty() : tensor<1x13x1xf32>
        %522 = "ttir.reshape"(%520, %521) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %523 = ttir.empty() : tensor<1x13x1xf32>
        %524 = "ttir.add"(%522, %59, %523) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %525 = ttir.empty() : tensor<1x13x1xf32>
        %526 = "ttir.rsqrt"(%524, %525) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %527 = ttir.empty() : tensor<1x13x2880xf32>
        %528 = "ttir.broadcast"(%526, %527) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %529 = ttir.empty() : tensor<1x13x2880xf32>
        %530 = "ttir.multiply"(%514, %528, %529) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %531 = ttir.empty() : tensor<1x13x2880xf32>
        %532 = "ttir.multiply"(%300, %530, %531) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %533 = ttir.empty() : tensor<1x13x2880xbf16>
        %534 = "ttir.typecast"(%532, %533) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %535 = ttir.empty() : tensor<13x2880xbf16>
        %536 = "ttir.reshape"(%534, %535) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %537 = ttir.empty() : tensor<2880x201088xbf16>
        %538 = "ttir.permute"(%arg10, %537) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %539 = "ttir.dot_general"(%536, %538) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
        %540 = ttir.empty() : tensor<1x13x201088xbf16>
        %541 = "ttir.reshape"(%539, %540) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %290, %292, %294, %202, %539, %541 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRHoistTransform (ttir-cpu-hoist-transform) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<2880x4096xbf16>
        %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
        %77 = ttir.empty() : tensor<1x13x4096xbf16>
        %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %79 = ttir.empty() : tensor<1x1x4096xbf16>
        %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %81 = ttir.empty() : tensor<1x13x4096xbf16>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %83 = ttir.empty() : tensor<1x13x4096xbf16>
        %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %85 = ttir.empty() : tensor<1x13x64x64xbf16>
        %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %87 = ttir.empty() : tensor<1x64x13x64xbf16>
        %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %89 = ttir.empty() : tensor<1x64x13x32xbf16>
        %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %91 = ttir.empty() : tensor<1x64x13x32xf32>
        %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %93 = ttir.empty() : tensor<1x32x1xf32>
        %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<2880x512xbf16>
        %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %157 = ttir.empty() : tensor<1x13x512xbf16>
        %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %159 = ttir.empty() : tensor<1x1x512xbf16>
        %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %161 = ttir.empty() : tensor<1x13x512xbf16>
        %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %163 = ttir.empty() : tensor<1x13x512xbf16>
        %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %165 = ttir.empty() : tensor<1x13x8x64xbf16>
        %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %167 = ttir.empty() : tensor<1x8x13x64xbf16>
        %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %169 = ttir.empty() : tensor<1x8x13x32xbf16>
        %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %171 = ttir.empty() : tensor<1x8x13x32xf32>
        %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %173 = ttir.empty() : tensor<1x8x13x32xf32>
        %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %175 = ttir.empty() : tensor<1x8x13x32xf32>
        %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %177 = ttir.empty() : tensor<1x8x13x32xbf16>
        %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %179 = ttir.empty() : tensor<1x8x13x32xbf16>
        %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %181 = ttir.empty() : tensor<1x8x13x32xf32>
        %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %183 = ttir.empty() : tensor<1x8x13x32xf32>
        %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %185 = ttir.empty() : tensor<1x8x13x32xf32>
        %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %187 = ttir.empty() : tensor<1x8x13x32xbf16>
        %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %189 = ttir.empty() : tensor<1x8x13x32xbf16>
        %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %191 = ttir.empty() : tensor<1x8x13x32xf32>
        %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %193 = ttir.empty() : tensor<1x8x13x32xbf16>
        %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %195 = ttir.empty() : tensor<1x8x13x32xf32>
        %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %197 = ttir.empty() : tensor<1x8x13x32xbf16>
        %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %199 = ttir.empty() : tensor<1x8x13x32xbf16>
        %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %201 = ttir.empty() : tensor<1x8x13x64xbf16>
        %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %207 = ttir.empty() : tensor<1x64x13x64xbf16>
        %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %209 = ttir.empty() : tensor<1x64x64x13xbf16>
        %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %211 = ttir.empty() : tensor<64x64x13xbf16>
        %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<2880x512xbf16>
        %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %283 = ttir.empty() : tensor<1x13x512xbf16>
        %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %285 = ttir.empty() : tensor<1x1x512xbf16>
        %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %287 = ttir.empty() : tensor<1x13x512xbf16>
        %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %289 = ttir.empty() : tensor<1x13x512xbf16>
        %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %291 = ttir.empty() : tensor<1x13x8x64xbf16>
        %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %293 = ttir.empty() : tensor<1x8x13x64xbf16>
        %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %295 = ttir.empty() : tensor<2880xf32>
        %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %297 = ttir.empty() : tensor<1x1x2880xf32>
        %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %299 = ttir.empty() : tensor<1x13x2880xf32>
        %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %301 = ttir.empty() : tensor<1x64x13xbf16>
        %302 = "ttir.max"(%279, %301) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %303 = ttir.empty() : tensor<1x64x13x1xbf16>
        %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %305 = ttir.empty() : tensor<1x64x13x14xbf16>
        %306 = "ttir.broadcast"(%304, %305) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %307 = ttir.empty() : tensor<1x64x13x14xbf16>
        %308 = "ttir.subtract"(%279, %306, %307) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %309 = ttir.empty() : tensor<1x64x13xbf16>
        %310 = "ttir.max"(%308, %309) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %311 = ttir.empty() : tensor<1x64x13x1xbf16>
        %312 = "ttir.reshape"(%310, %311) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %313 = ttir.empty() : tensor<1x64x13x14xbf16>
        %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %315 = ttir.empty() : tensor<1x64x13x14xbf16>
        %316 = "ttir.subtract"(%308, %314, %315) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %317 = ttir.empty() : tensor<1x64x13x14xbf16>
        %318 = "ttir.exp"(%316, %317) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %319 = ttir.empty() : tensor<1x64x13xbf16>
        %320 = "ttir.sum"(%318, %319) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %321 = ttir.empty() : tensor<1x64x13x1xbf16>
        %322 = "ttir.reshape"(%320, %321) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %323 = ttir.empty() : tensor<1x64x13x14xbf16>
        %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %325 = ttir.empty() : tensor<1x64x13x14xbf16>
        %326 = "ttir.div"(%318, %324, %325) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %327 = ttir.empty() : tensor<1x64x13x13xbf16>
        %328 = "ttir.slice_static"(%326, %327) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %329 = ttir.empty() : tensor<64x13x13xbf16>
        %330 = "ttir.reshape"(%328, %329) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %331 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %332 = "ttir.reshape"(%294, %331) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %333 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %334 = "ttir.broadcast"(%332, %333) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %335 = ttir.empty() : tensor<64x13x64xbf16>
        %336 = "ttir.reshape"(%334, %335) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %337 = "ttir.dot_general"(%330, %336) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %338 = ttir.empty() : tensor<1x64x13x64xbf16>
        %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %340 = ttir.empty() : tensor<1x13x64x64xbf16>
        %341 = "ttir.permute"(%339, %340) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %342 = ttir.empty() : tensor<13x4096xbf16>
        %343 = "ttir.reshape"(%341, %342) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %344 = ttir.empty() : tensor<4096x2880xbf16>
        %345 = "ttir.permute"(%arg14, %344) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %346 = "ttir.dot_general"(%343, %345) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
        %347 = ttir.empty() : tensor<1x13x2880xbf16>
        %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %349 = ttir.empty() : tensor<1x1x2880xbf16>
        %350 = "ttir.reshape"(%arg13, %349) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %351 = ttir.empty() : tensor<1x13x2880xbf16>
        %352 = "ttir.broadcast"(%350, %351) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %353 = ttir.empty() : tensor<1x13x2880xbf16>
        %354 = "ttir.add"(%348, %352, %353) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %355 = ttir.empty() : tensor<1x13x2880xbf16>
        %356 = "ttir.add"(%45, %354, %355) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %357 = ttir.empty() : tensor<1x1x1xbf16>
        %358 = "ttir.reshape"(%arg29, %357) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %359 = ttir.empty() : tensor<32x13x2880xbf16>
        %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %361 = ttir.empty() : tensor<2880xf32>
        %362 = "ttir.typecast"(%arg21, %361) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %363 = ttir.empty() : tensor<1x1x2880xf32>
        %364 = "ttir.reshape"(%362, %363) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %365 = ttir.empty() : tensor<1x13x2880xf32>
        %366 = "ttir.broadcast"(%364, %365) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %367 = ttir.empty() : tensor<1x13x2880xf32>
        %368 = "ttir.typecast"(%356, %367) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %369 = ttir.empty() : tensor<1x13x2880xf32>
        %370 = "ttir.pow"(%368, %27, %369) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %371 = ttir.empty() : tensor<1x13xf32>
        %372 = "ttir.sum"(%370, %371) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %373 = ttir.empty() : tensor<1x13xf32>
        %374 = "ttir.multiply"(%372, %4, %373) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %375 = ttir.empty() : tensor<1x13x1xf32>
        %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %377 = ttir.empty() : tensor<1x13x1xf32>
        %378 = "ttir.add"(%376, %59, %377) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %379 = ttir.empty() : tensor<1x13x1xf32>
        %380 = "ttir.rsqrt"(%378, %379) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %381 = ttir.empty() : tensor<1x13x2880xf32>
        %382 = "ttir.broadcast"(%380, %381) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %383 = ttir.empty() : tensor<1x13x2880xf32>
        %384 = "ttir.multiply"(%368, %382, %383) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %385 = ttir.empty() : tensor<1x13x2880xf32>
        %386 = "ttir.multiply"(%366, %384, %385) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %387 = ttir.empty() : tensor<1x13x2880xbf16>
        %388 = "ttir.typecast"(%386, %387) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %389 = ttir.empty() : tensor<13x2880xbf16>
        %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %391 = ttir.empty() : tensor<416x2880xbf16>
        %392 = "ttir.concat"(%390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %391) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %393 = ttir.empty() : tensor<32x13x2880xbf16>
        %394 = "ttir.reshape"(%392, %393) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %395 = "ttir.dot_general"(%394, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
        %396 = ttir.empty() : tensor<32x1x5760xbf16>
        %397 = "ttir.reshape"(%arg27, %396) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %398 = ttir.empty() : tensor<32x13x5760xbf16>
        %399 = "ttir.broadcast"(%397, %398) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %400 = ttir.empty() : tensor<32x13x5760xbf16>
        %401 = "ttir.add"(%395, %399, %400) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %402 = ttir.empty() : tensor<32x13x2880xbf16>
        %403 = "ttir.slice_static"(%401, %402) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %404 = ttir.empty() : tensor<1x1x1xbf16>
        %405 = "ttir.reshape"(%arg25, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.clamp_tensor"(%403, %360, %407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xbf16>
        %411 = "ttir.add"(%409, %15, %410) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.typecast"(%411, %412) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<1x1x1xbf16>
        %415 = "ttir.reshape"(%arg26, %414) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.broadcast"(%415, %416) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xbf16>
        %419 = "ttir.slice_static"(%401, %418) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %420 = ttir.empty() : tensor<32x13x2880xbf16>
        %421 = "ttir.clamp_tensor"(%419, %417, %407, %420) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xf32>
        %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %424 = ttir.empty() : tensor<1x1x1xf32>
        %425 = "ttir.reshape"(%arg24, %424) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %426 = ttir.empty() : tensor<32x13x2880xf32>
        %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %428 = ttir.empty() : tensor<32x13x2880xf32>
        %429 = "ttir.multiply"(%423, %427, %428) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %430 = ttir.empty() : tensor<32x13x2880xbf16>
        %431 = "ttir.typecast"(%429, %430) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %432 = ttir.empty() : tensor<32x13x2880xbf16>
        %433 = "ttir.sigmoid"(%431, %432) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %434 = ttir.empty() : tensor<32x13x2880xf32>
        %435 = "ttir.typecast"(%433, %434) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %436 = ttir.empty() : tensor<32x13x2880xf32>
        %437 = "ttir.multiply"(%423, %435, %436) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %438 = ttir.empty() : tensor<32x13x2880xf32>
        %439 = "ttir.multiply"(%413, %437, %438) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %440 = ttir.empty() : tensor<32x13x2880xbf16>
        %441 = "ttir.typecast"(%439, %440) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %442 = "ttir.dot_general"(%441, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
        %443 = ttir.empty() : tensor<32x1x2880xbf16>
        %444 = "ttir.reshape"(%arg22, %443) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %445 = ttir.empty() : tensor<32x13x2880xbf16>
        %446 = "ttir.broadcast"(%444, %445) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %447 = ttir.empty() : tensor<32x13x2880xbf16>
        %448 = "ttir.add"(%442, %446, %447) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %449 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %450 = "ttir.reshape"(%448, %449) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %451 = ttir.empty() : tensor<32x1x13x2880xf32>
        %452 = "ttir.typecast"(%450, %451) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %453 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %454 = ttir.empty() : tensor<13x1x1xsi32>
        %455 = "ttir.reshape"(%453, %454) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %456 = ttir.empty() : tensor<13x4x1xsi32>
        %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %458 = ttir.empty() : tensor<2880x32xbf16>
        %459 = "ttir.permute"(%arg12, %458) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %460 = "ttir.dot_general"(%390, %459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
        %461 = ttir.empty() : tensor<1x32xbf16>
        %462 = "ttir.reshape"(%arg11, %461) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %463 = ttir.empty() : tensor<13x32xbf16>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %465 = ttir.empty() : tensor<13x32xbf16>
        %466 = "ttir.add"(%460, %464, %465) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %467 = ttir.empty() : tensor<13x32xbf16>
        %468 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%466, %467, %468) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %469 = ttir.empty() : tensor<13x4xsi32>
        %470 = "ttir.slice_static"(%indices, %469) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %471 = ttir.empty() : tensor<13x4x1xsi32>
        %472 = "ttir.reshape"(%470, %471) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %473 = ttir.empty() : tensor<13x4x2xsi32>
        %474 = "ttir.concat"(%457, %472, %473) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %475 = ttir.empty() : tensor<13x4xbf16>
        %476 = "ttir.slice_static"(%values, %475) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %477 = ttir.empty() : tensor<13xbf16>
        %478 = "ttir.max"(%476, %477) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %479 = ttir.empty() : tensor<13x1xbf16>
        %480 = "ttir.reshape"(%478, %479) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %481 = ttir.empty() : tensor<13x4xbf16>
        %482 = "ttir.broadcast"(%480, %481) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %483 = ttir.empty() : tensor<13x4xbf16>
        %484 = "ttir.subtract"(%476, %482, %483) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %485 = ttir.empty() : tensor<13x4xbf16>
        %486 = "ttir.exp"(%484, %485) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %487 = ttir.empty() : tensor<13xbf16>
        %488 = "ttir.sum"(%486, %487) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %489 = ttir.empty() : tensor<13x1xbf16>
        %490 = "ttir.reshape"(%488, %489) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %491 = ttir.empty() : tensor<13x4xbf16>
        %492 = "ttir.broadcast"(%490, %491) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %493 = ttir.empty() : tensor<13x4xbf16>
        %494 = "ttir.div"(%486, %492, %493) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %495 = ttir.empty() : tensor<13x32xbf16>
        %496 = "ttir.scatter"(%11, %474, %494, %495) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %497 = ttir.empty() : tensor<32x13xbf16>
        %498 = "ttir.permute"(%496, %497) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %499 = ttir.empty() : tensor<32x1x13x1xbf16>
        %500 = "ttir.reshape"(%498, %499) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %501 = ttir.empty() : tensor<32x1x13x1xf32>
        %502 = "ttir.typecast"(%500, %501) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %503 = ttir.empty() : tensor<32x1x13x2880xf32>
        %504 = "ttir.broadcast"(%502, %503) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %505 = ttir.empty() : tensor<32x1x13x2880xf32>
        %506 = "ttir.multiply"(%452, %504, %505) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %507 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %508 = "ttir.typecast"(%506, %507) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %509 = ttir.empty() : tensor<1x13x2880xbf16>
        %510 = "ttir.sum"(%508, %509) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %511 = ttir.empty() : tensor<1x13x2880xbf16>
        %512 = "ttir.add"(%356, %510, %511) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %513 = ttir.empty() : tensor<1x13x2880xf32>
        %514 = "ttir.typecast"(%512, %513) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %515 = ttir.empty() : tensor<1x13x2880xf32>
        %516 = "ttir.pow"(%514, %27, %515) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %517 = ttir.empty() : tensor<1x13xf32>
        %518 = "ttir.sum"(%516, %517) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %519 = ttir.empty() : tensor<1x13xf32>
        %520 = "ttir.multiply"(%518, %4, %519) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %521 = ttir.empty() : tensor<1x13x1xf32>
        %522 = "ttir.reshape"(%520, %521) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %523 = ttir.empty() : tensor<1x13x1xf32>
        %524 = "ttir.add"(%522, %59, %523) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %525 = ttir.empty() : tensor<1x13x1xf32>
        %526 = "ttir.rsqrt"(%524, %525) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %527 = ttir.empty() : tensor<1x13x2880xf32>
        %528 = "ttir.broadcast"(%526, %527) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %529 = ttir.empty() : tensor<1x13x2880xf32>
        %530 = "ttir.multiply"(%514, %528, %529) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %531 = ttir.empty() : tensor<1x13x2880xf32>
        %532 = "ttir.multiply"(%300, %530, %531) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %533 = ttir.empty() : tensor<1x13x2880xbf16>
        %534 = "ttir.typecast"(%532, %533) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %535 = ttir.empty() : tensor<13x2880xbf16>
        %536 = "ttir.reshape"(%534, %535) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %537 = ttir.empty() : tensor<2880x201088xbf16>
        %538 = "ttir.permute"(%arg10, %537) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %539 = "ttir.dot_general"(%536, %538) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
        %540 = ttir.empty() : tensor<1x13x201088xbf16>
        %541 = "ttir.reshape"(%539, %540) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %290, %292, %294, %202, %539, %541 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTCoreRegisterDevicePass (ttcore-register-device) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<2880x4096xbf16>
        %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
        %77 = ttir.empty() : tensor<1x13x4096xbf16>
        %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %79 = ttir.empty() : tensor<1x1x4096xbf16>
        %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %81 = ttir.empty() : tensor<1x13x4096xbf16>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %83 = ttir.empty() : tensor<1x13x4096xbf16>
        %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %85 = ttir.empty() : tensor<1x13x64x64xbf16>
        %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %87 = ttir.empty() : tensor<1x64x13x64xbf16>
        %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %89 = ttir.empty() : tensor<1x64x13x32xbf16>
        %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %91 = ttir.empty() : tensor<1x64x13x32xf32>
        %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %93 = ttir.empty() : tensor<1x32x1xf32>
        %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<2880x512xbf16>
        %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %157 = ttir.empty() : tensor<1x13x512xbf16>
        %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %159 = ttir.empty() : tensor<1x1x512xbf16>
        %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %161 = ttir.empty() : tensor<1x13x512xbf16>
        %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %163 = ttir.empty() : tensor<1x13x512xbf16>
        %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %165 = ttir.empty() : tensor<1x13x8x64xbf16>
        %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %167 = ttir.empty() : tensor<1x8x13x64xbf16>
        %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %169 = ttir.empty() : tensor<1x8x13x32xbf16>
        %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %171 = ttir.empty() : tensor<1x8x13x32xf32>
        %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %173 = ttir.empty() : tensor<1x8x13x32xf32>
        %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %175 = ttir.empty() : tensor<1x8x13x32xf32>
        %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %177 = ttir.empty() : tensor<1x8x13x32xbf16>
        %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %179 = ttir.empty() : tensor<1x8x13x32xbf16>
        %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %181 = ttir.empty() : tensor<1x8x13x32xf32>
        %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %183 = ttir.empty() : tensor<1x8x13x32xf32>
        %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %185 = ttir.empty() : tensor<1x8x13x32xf32>
        %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %187 = ttir.empty() : tensor<1x8x13x32xbf16>
        %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %189 = ttir.empty() : tensor<1x8x13x32xbf16>
        %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %191 = ttir.empty() : tensor<1x8x13x32xf32>
        %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %193 = ttir.empty() : tensor<1x8x13x32xbf16>
        %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %195 = ttir.empty() : tensor<1x8x13x32xf32>
        %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %197 = ttir.empty() : tensor<1x8x13x32xbf16>
        %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %199 = ttir.empty() : tensor<1x8x13x32xbf16>
        %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %201 = ttir.empty() : tensor<1x8x13x64xbf16>
        %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %207 = ttir.empty() : tensor<1x64x13x64xbf16>
        %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %209 = ttir.empty() : tensor<1x64x64x13xbf16>
        %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %211 = ttir.empty() : tensor<64x64x13xbf16>
        %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<2880x512xbf16>
        %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %283 = ttir.empty() : tensor<1x13x512xbf16>
        %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %285 = ttir.empty() : tensor<1x1x512xbf16>
        %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %287 = ttir.empty() : tensor<1x13x512xbf16>
        %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %289 = ttir.empty() : tensor<1x13x512xbf16>
        %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %291 = ttir.empty() : tensor<1x13x8x64xbf16>
        %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %293 = ttir.empty() : tensor<1x8x13x64xbf16>
        %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %295 = ttir.empty() : tensor<2880xf32>
        %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %297 = ttir.empty() : tensor<1x1x2880xf32>
        %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %299 = ttir.empty() : tensor<1x13x2880xf32>
        %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %301 = ttir.empty() : tensor<1x64x13xbf16>
        %302 = "ttir.max"(%279, %301) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %303 = ttir.empty() : tensor<1x64x13x1xbf16>
        %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %305 = ttir.empty() : tensor<1x64x13x14xbf16>
        %306 = "ttir.broadcast"(%304, %305) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %307 = ttir.empty() : tensor<1x64x13x14xbf16>
        %308 = "ttir.subtract"(%279, %306, %307) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %309 = ttir.empty() : tensor<1x64x13xbf16>
        %310 = "ttir.max"(%308, %309) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %311 = ttir.empty() : tensor<1x64x13x1xbf16>
        %312 = "ttir.reshape"(%310, %311) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %313 = ttir.empty() : tensor<1x64x13x14xbf16>
        %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %315 = ttir.empty() : tensor<1x64x13x14xbf16>
        %316 = "ttir.subtract"(%308, %314, %315) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %317 = ttir.empty() : tensor<1x64x13x14xbf16>
        %318 = "ttir.exp"(%316, %317) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %319 = ttir.empty() : tensor<1x64x13xbf16>
        %320 = "ttir.sum"(%318, %319) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %321 = ttir.empty() : tensor<1x64x13x1xbf16>
        %322 = "ttir.reshape"(%320, %321) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %323 = ttir.empty() : tensor<1x64x13x14xbf16>
        %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %325 = ttir.empty() : tensor<1x64x13x14xbf16>
        %326 = "ttir.div"(%318, %324, %325) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %327 = ttir.empty() : tensor<1x64x13x13xbf16>
        %328 = "ttir.slice_static"(%326, %327) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %329 = ttir.empty() : tensor<64x13x13xbf16>
        %330 = "ttir.reshape"(%328, %329) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %331 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %332 = "ttir.reshape"(%294, %331) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %333 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %334 = "ttir.broadcast"(%332, %333) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %335 = ttir.empty() : tensor<64x13x64xbf16>
        %336 = "ttir.reshape"(%334, %335) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %337 = "ttir.dot_general"(%330, %336) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %338 = ttir.empty() : tensor<1x64x13x64xbf16>
        %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %340 = ttir.empty() : tensor<1x13x64x64xbf16>
        %341 = "ttir.permute"(%339, %340) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %342 = ttir.empty() : tensor<13x4096xbf16>
        %343 = "ttir.reshape"(%341, %342) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %344 = ttir.empty() : tensor<4096x2880xbf16>
        %345 = "ttir.permute"(%arg14, %344) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %346 = "ttir.dot_general"(%343, %345) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
        %347 = ttir.empty() : tensor<1x13x2880xbf16>
        %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %349 = ttir.empty() : tensor<1x1x2880xbf16>
        %350 = "ttir.reshape"(%arg13, %349) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %351 = ttir.empty() : tensor<1x13x2880xbf16>
        %352 = "ttir.broadcast"(%350, %351) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %353 = ttir.empty() : tensor<1x13x2880xbf16>
        %354 = "ttir.add"(%348, %352, %353) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %355 = ttir.empty() : tensor<1x13x2880xbf16>
        %356 = "ttir.add"(%45, %354, %355) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %357 = ttir.empty() : tensor<1x1x1xbf16>
        %358 = "ttir.reshape"(%arg29, %357) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %359 = ttir.empty() : tensor<32x13x2880xbf16>
        %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %361 = ttir.empty() : tensor<2880xf32>
        %362 = "ttir.typecast"(%arg21, %361) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %363 = ttir.empty() : tensor<1x1x2880xf32>
        %364 = "ttir.reshape"(%362, %363) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %365 = ttir.empty() : tensor<1x13x2880xf32>
        %366 = "ttir.broadcast"(%364, %365) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %367 = ttir.empty() : tensor<1x13x2880xf32>
        %368 = "ttir.typecast"(%356, %367) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %369 = ttir.empty() : tensor<1x13x2880xf32>
        %370 = "ttir.pow"(%368, %27, %369) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %371 = ttir.empty() : tensor<1x13xf32>
        %372 = "ttir.sum"(%370, %371) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %373 = ttir.empty() : tensor<1x13xf32>
        %374 = "ttir.multiply"(%372, %4, %373) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %375 = ttir.empty() : tensor<1x13x1xf32>
        %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %377 = ttir.empty() : tensor<1x13x1xf32>
        %378 = "ttir.add"(%376, %59, %377) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %379 = ttir.empty() : tensor<1x13x1xf32>
        %380 = "ttir.rsqrt"(%378, %379) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %381 = ttir.empty() : tensor<1x13x2880xf32>
        %382 = "ttir.broadcast"(%380, %381) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %383 = ttir.empty() : tensor<1x13x2880xf32>
        %384 = "ttir.multiply"(%368, %382, %383) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %385 = ttir.empty() : tensor<1x13x2880xf32>
        %386 = "ttir.multiply"(%366, %384, %385) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %387 = ttir.empty() : tensor<1x13x2880xbf16>
        %388 = "ttir.typecast"(%386, %387) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %389 = ttir.empty() : tensor<13x2880xbf16>
        %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %391 = ttir.empty() : tensor<416x2880xbf16>
        %392 = "ttir.concat"(%390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %391) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %393 = ttir.empty() : tensor<32x13x2880xbf16>
        %394 = "ttir.reshape"(%392, %393) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %395 = "ttir.dot_general"(%394, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
        %396 = ttir.empty() : tensor<32x1x5760xbf16>
        %397 = "ttir.reshape"(%arg27, %396) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %398 = ttir.empty() : tensor<32x13x5760xbf16>
        %399 = "ttir.broadcast"(%397, %398) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %400 = ttir.empty() : tensor<32x13x5760xbf16>
        %401 = "ttir.add"(%395, %399, %400) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %402 = ttir.empty() : tensor<32x13x2880xbf16>
        %403 = "ttir.slice_static"(%401, %402) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %404 = ttir.empty() : tensor<1x1x1xbf16>
        %405 = "ttir.reshape"(%arg25, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.clamp_tensor"(%403, %360, %407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xbf16>
        %411 = "ttir.add"(%409, %15, %410) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.typecast"(%411, %412) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<1x1x1xbf16>
        %415 = "ttir.reshape"(%arg26, %414) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.broadcast"(%415, %416) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xbf16>
        %419 = "ttir.slice_static"(%401, %418) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %420 = ttir.empty() : tensor<32x13x2880xbf16>
        %421 = "ttir.clamp_tensor"(%419, %417, %407, %420) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xf32>
        %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %424 = ttir.empty() : tensor<1x1x1xf32>
        %425 = "ttir.reshape"(%arg24, %424) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %426 = ttir.empty() : tensor<32x13x2880xf32>
        %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %428 = ttir.empty() : tensor<32x13x2880xf32>
        %429 = "ttir.multiply"(%423, %427, %428) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %430 = ttir.empty() : tensor<32x13x2880xbf16>
        %431 = "ttir.typecast"(%429, %430) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %432 = ttir.empty() : tensor<32x13x2880xbf16>
        %433 = "ttir.sigmoid"(%431, %432) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %434 = ttir.empty() : tensor<32x13x2880xf32>
        %435 = "ttir.typecast"(%433, %434) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %436 = ttir.empty() : tensor<32x13x2880xf32>
        %437 = "ttir.multiply"(%423, %435, %436) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %438 = ttir.empty() : tensor<32x13x2880xf32>
        %439 = "ttir.multiply"(%413, %437, %438) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %440 = ttir.empty() : tensor<32x13x2880xbf16>
        %441 = "ttir.typecast"(%439, %440) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %442 = "ttir.dot_general"(%441, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
        %443 = ttir.empty() : tensor<32x1x2880xbf16>
        %444 = "ttir.reshape"(%arg22, %443) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %445 = ttir.empty() : tensor<32x13x2880xbf16>
        %446 = "ttir.broadcast"(%444, %445) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %447 = ttir.empty() : tensor<32x13x2880xbf16>
        %448 = "ttir.add"(%442, %446, %447) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %449 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %450 = "ttir.reshape"(%448, %449) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %451 = ttir.empty() : tensor<32x1x13x2880xf32>
        %452 = "ttir.typecast"(%450, %451) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %453 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %454 = ttir.empty() : tensor<13x1x1xsi32>
        %455 = "ttir.reshape"(%453, %454) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %456 = ttir.empty() : tensor<13x4x1xsi32>
        %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %458 = ttir.empty() : tensor<2880x32xbf16>
        %459 = "ttir.permute"(%arg12, %458) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %460 = "ttir.dot_general"(%390, %459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
        %461 = ttir.empty() : tensor<1x32xbf16>
        %462 = "ttir.reshape"(%arg11, %461) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %463 = ttir.empty() : tensor<13x32xbf16>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %465 = ttir.empty() : tensor<13x32xbf16>
        %466 = "ttir.add"(%460, %464, %465) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %467 = ttir.empty() : tensor<13x32xbf16>
        %468 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%466, %467, %468) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %469 = ttir.empty() : tensor<13x4xsi32>
        %470 = "ttir.slice_static"(%indices, %469) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %471 = ttir.empty() : tensor<13x4x1xsi32>
        %472 = "ttir.reshape"(%470, %471) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %473 = ttir.empty() : tensor<13x4x2xsi32>
        %474 = "ttir.concat"(%457, %472, %473) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %475 = ttir.empty() : tensor<13x4xbf16>
        %476 = "ttir.slice_static"(%values, %475) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %477 = ttir.empty() : tensor<13xbf16>
        %478 = "ttir.max"(%476, %477) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %479 = ttir.empty() : tensor<13x1xbf16>
        %480 = "ttir.reshape"(%478, %479) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %481 = ttir.empty() : tensor<13x4xbf16>
        %482 = "ttir.broadcast"(%480, %481) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %483 = ttir.empty() : tensor<13x4xbf16>
        %484 = "ttir.subtract"(%476, %482, %483) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %485 = ttir.empty() : tensor<13x4xbf16>
        %486 = "ttir.exp"(%484, %485) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %487 = ttir.empty() : tensor<13xbf16>
        %488 = "ttir.sum"(%486, %487) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %489 = ttir.empty() : tensor<13x1xbf16>
        %490 = "ttir.reshape"(%488, %489) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %491 = ttir.empty() : tensor<13x4xbf16>
        %492 = "ttir.broadcast"(%490, %491) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %493 = ttir.empty() : tensor<13x4xbf16>
        %494 = "ttir.div"(%486, %492, %493) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %495 = ttir.empty() : tensor<13x32xbf16>
        %496 = "ttir.scatter"(%11, %474, %494, %495) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %497 = ttir.empty() : tensor<32x13xbf16>
        %498 = "ttir.permute"(%496, %497) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %499 = ttir.empty() : tensor<32x1x13x1xbf16>
        %500 = "ttir.reshape"(%498, %499) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %501 = ttir.empty() : tensor<32x1x13x1xf32>
        %502 = "ttir.typecast"(%500, %501) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %503 = ttir.empty() : tensor<32x1x13x2880xf32>
        %504 = "ttir.broadcast"(%502, %503) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %505 = ttir.empty() : tensor<32x1x13x2880xf32>
        %506 = "ttir.multiply"(%452, %504, %505) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %507 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %508 = "ttir.typecast"(%506, %507) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %509 = ttir.empty() : tensor<1x13x2880xbf16>
        %510 = "ttir.sum"(%508, %509) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %511 = ttir.empty() : tensor<1x13x2880xbf16>
        %512 = "ttir.add"(%356, %510, %511) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %513 = ttir.empty() : tensor<1x13x2880xf32>
        %514 = "ttir.typecast"(%512, %513) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %515 = ttir.empty() : tensor<1x13x2880xf32>
        %516 = "ttir.pow"(%514, %27, %515) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %517 = ttir.empty() : tensor<1x13xf32>
        %518 = "ttir.sum"(%516, %517) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %519 = ttir.empty() : tensor<1x13xf32>
        %520 = "ttir.multiply"(%518, %4, %519) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %521 = ttir.empty() : tensor<1x13x1xf32>
        %522 = "ttir.reshape"(%520, %521) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %523 = ttir.empty() : tensor<1x13x1xf32>
        %524 = "ttir.add"(%522, %59, %523) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %525 = ttir.empty() : tensor<1x13x1xf32>
        %526 = "ttir.rsqrt"(%524, %525) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %527 = ttir.empty() : tensor<1x13x2880xf32>
        %528 = "ttir.broadcast"(%526, %527) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %529 = ttir.empty() : tensor<1x13x2880xf32>
        %530 = "ttir.multiply"(%514, %528, %529) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %531 = ttir.empty() : tensor<1x13x2880xf32>
        %532 = "ttir.multiply"(%300, %530, %531) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %533 = ttir.empty() : tensor<1x13x2880xbf16>
        %534 = "ttir.typecast"(%532, %533) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %535 = ttir.empty() : tensor<13x2880xbf16>
        %536 = "ttir.reshape"(%534, %535) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %537 = ttir.empty() : tensor<2880x201088xbf16>
        %538 = "ttir.permute"(%arg10, %537) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %539 = "ttir.dot_general"(%536, %538) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
        %540 = ttir.empty() : tensor<1x13x201088xbf16>
        %541 = "ttir.reshape"(%539, %540) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %290, %292, %294, %202, %539, %541 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump After TTCoreRegisterDevicePass (ttcore-register-device) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<2880x4096xbf16>
        %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
        %77 = ttir.empty() : tensor<1x13x4096xbf16>
        %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %79 = ttir.empty() : tensor<1x1x4096xbf16>
        %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %81 = ttir.empty() : tensor<1x13x4096xbf16>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %83 = ttir.empty() : tensor<1x13x4096xbf16>
        %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %85 = ttir.empty() : tensor<1x13x64x64xbf16>
        %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %87 = ttir.empty() : tensor<1x64x13x64xbf16>
        %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %89 = ttir.empty() : tensor<1x64x13x32xbf16>
        %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %91 = ttir.empty() : tensor<1x64x13x32xf32>
        %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %93 = ttir.empty() : tensor<1x32x1xf32>
        %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<2880x512xbf16>
        %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %157 = ttir.empty() : tensor<1x13x512xbf16>
        %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %159 = ttir.empty() : tensor<1x1x512xbf16>
        %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %161 = ttir.empty() : tensor<1x13x512xbf16>
        %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %163 = ttir.empty() : tensor<1x13x512xbf16>
        %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %165 = ttir.empty() : tensor<1x13x8x64xbf16>
        %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %167 = ttir.empty() : tensor<1x8x13x64xbf16>
        %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %169 = ttir.empty() : tensor<1x8x13x32xbf16>
        %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %171 = ttir.empty() : tensor<1x8x13x32xf32>
        %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %173 = ttir.empty() : tensor<1x8x13x32xf32>
        %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %175 = ttir.empty() : tensor<1x8x13x32xf32>
        %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %177 = ttir.empty() : tensor<1x8x13x32xbf16>
        %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %179 = ttir.empty() : tensor<1x8x13x32xbf16>
        %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %181 = ttir.empty() : tensor<1x8x13x32xf32>
        %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %183 = ttir.empty() : tensor<1x8x13x32xf32>
        %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %185 = ttir.empty() : tensor<1x8x13x32xf32>
        %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %187 = ttir.empty() : tensor<1x8x13x32xbf16>
        %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %189 = ttir.empty() : tensor<1x8x13x32xbf16>
        %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %191 = ttir.empty() : tensor<1x8x13x32xf32>
        %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %193 = ttir.empty() : tensor<1x8x13x32xbf16>
        %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %195 = ttir.empty() : tensor<1x8x13x32xf32>
        %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %197 = ttir.empty() : tensor<1x8x13x32xbf16>
        %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %199 = ttir.empty() : tensor<1x8x13x32xbf16>
        %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %201 = ttir.empty() : tensor<1x8x13x64xbf16>
        %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %207 = ttir.empty() : tensor<1x64x13x64xbf16>
        %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %209 = ttir.empty() : tensor<1x64x64x13xbf16>
        %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %211 = ttir.empty() : tensor<64x64x13xbf16>
        %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<2880x512xbf16>
        %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %283 = ttir.empty() : tensor<1x13x512xbf16>
        %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %285 = ttir.empty() : tensor<1x1x512xbf16>
        %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %287 = ttir.empty() : tensor<1x13x512xbf16>
        %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %289 = ttir.empty() : tensor<1x13x512xbf16>
        %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %291 = ttir.empty() : tensor<1x13x8x64xbf16>
        %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %293 = ttir.empty() : tensor<1x8x13x64xbf16>
        %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %295 = ttir.empty() : tensor<2880xf32>
        %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %297 = ttir.empty() : tensor<1x1x2880xf32>
        %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %299 = ttir.empty() : tensor<1x13x2880xf32>
        %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %301 = ttir.empty() : tensor<1x64x13xbf16>
        %302 = "ttir.max"(%279, %301) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %303 = ttir.empty() : tensor<1x64x13x1xbf16>
        %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %305 = ttir.empty() : tensor<1x64x13x14xbf16>
        %306 = "ttir.broadcast"(%304, %305) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %307 = ttir.empty() : tensor<1x64x13x14xbf16>
        %308 = "ttir.subtract"(%279, %306, %307) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %309 = ttir.empty() : tensor<1x64x13xbf16>
        %310 = "ttir.max"(%308, %309) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %311 = ttir.empty() : tensor<1x64x13x1xbf16>
        %312 = "ttir.reshape"(%310, %311) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %313 = ttir.empty() : tensor<1x64x13x14xbf16>
        %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %315 = ttir.empty() : tensor<1x64x13x14xbf16>
        %316 = "ttir.subtract"(%308, %314, %315) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %317 = ttir.empty() : tensor<1x64x13x14xbf16>
        %318 = "ttir.exp"(%316, %317) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %319 = ttir.empty() : tensor<1x64x13xbf16>
        %320 = "ttir.sum"(%318, %319) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %321 = ttir.empty() : tensor<1x64x13x1xbf16>
        %322 = "ttir.reshape"(%320, %321) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %323 = ttir.empty() : tensor<1x64x13x14xbf16>
        %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %325 = ttir.empty() : tensor<1x64x13x14xbf16>
        %326 = "ttir.div"(%318, %324, %325) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %327 = ttir.empty() : tensor<1x64x13x13xbf16>
        %328 = "ttir.slice_static"(%326, %327) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %329 = ttir.empty() : tensor<64x13x13xbf16>
        %330 = "ttir.reshape"(%328, %329) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %331 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %332 = "ttir.reshape"(%294, %331) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %333 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %334 = "ttir.broadcast"(%332, %333) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %335 = ttir.empty() : tensor<64x13x64xbf16>
        %336 = "ttir.reshape"(%334, %335) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %337 = "ttir.dot_general"(%330, %336) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %338 = ttir.empty() : tensor<1x64x13x64xbf16>
        %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %340 = ttir.empty() : tensor<1x13x64x64xbf16>
        %341 = "ttir.permute"(%339, %340) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %342 = ttir.empty() : tensor<13x4096xbf16>
        %343 = "ttir.reshape"(%341, %342) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %344 = ttir.empty() : tensor<4096x2880xbf16>
        %345 = "ttir.permute"(%arg14, %344) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %346 = "ttir.dot_general"(%343, %345) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
        %347 = ttir.empty() : tensor<1x13x2880xbf16>
        %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %349 = ttir.empty() : tensor<1x1x2880xbf16>
        %350 = "ttir.reshape"(%arg13, %349) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %351 = ttir.empty() : tensor<1x13x2880xbf16>
        %352 = "ttir.broadcast"(%350, %351) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %353 = ttir.empty() : tensor<1x13x2880xbf16>
        %354 = "ttir.add"(%348, %352, %353) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %355 = ttir.empty() : tensor<1x13x2880xbf16>
        %356 = "ttir.add"(%45, %354, %355) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %357 = ttir.empty() : tensor<1x1x1xbf16>
        %358 = "ttir.reshape"(%arg29, %357) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %359 = ttir.empty() : tensor<32x13x2880xbf16>
        %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %361 = ttir.empty() : tensor<2880xf32>
        %362 = "ttir.typecast"(%arg21, %361) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %363 = ttir.empty() : tensor<1x1x2880xf32>
        %364 = "ttir.reshape"(%362, %363) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %365 = ttir.empty() : tensor<1x13x2880xf32>
        %366 = "ttir.broadcast"(%364, %365) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %367 = ttir.empty() : tensor<1x13x2880xf32>
        %368 = "ttir.typecast"(%356, %367) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %369 = ttir.empty() : tensor<1x13x2880xf32>
        %370 = "ttir.pow"(%368, %27, %369) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %371 = ttir.empty() : tensor<1x13xf32>
        %372 = "ttir.sum"(%370, %371) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %373 = ttir.empty() : tensor<1x13xf32>
        %374 = "ttir.multiply"(%372, %4, %373) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %375 = ttir.empty() : tensor<1x13x1xf32>
        %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %377 = ttir.empty() : tensor<1x13x1xf32>
        %378 = "ttir.add"(%376, %59, %377) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %379 = ttir.empty() : tensor<1x13x1xf32>
        %380 = "ttir.rsqrt"(%378, %379) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %381 = ttir.empty() : tensor<1x13x2880xf32>
        %382 = "ttir.broadcast"(%380, %381) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %383 = ttir.empty() : tensor<1x13x2880xf32>
        %384 = "ttir.multiply"(%368, %382, %383) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %385 = ttir.empty() : tensor<1x13x2880xf32>
        %386 = "ttir.multiply"(%366, %384, %385) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %387 = ttir.empty() : tensor<1x13x2880xbf16>
        %388 = "ttir.typecast"(%386, %387) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %389 = ttir.empty() : tensor<13x2880xbf16>
        %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %391 = ttir.empty() : tensor<416x2880xbf16>
        %392 = "ttir.concat"(%390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %391) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %393 = ttir.empty() : tensor<32x13x2880xbf16>
        %394 = "ttir.reshape"(%392, %393) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %395 = "ttir.dot_general"(%394, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
        %396 = ttir.empty() : tensor<32x1x5760xbf16>
        %397 = "ttir.reshape"(%arg27, %396) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %398 = ttir.empty() : tensor<32x13x5760xbf16>
        %399 = "ttir.broadcast"(%397, %398) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %400 = ttir.empty() : tensor<32x13x5760xbf16>
        %401 = "ttir.add"(%395, %399, %400) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %402 = ttir.empty() : tensor<32x13x2880xbf16>
        %403 = "ttir.slice_static"(%401, %402) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %404 = ttir.empty() : tensor<1x1x1xbf16>
        %405 = "ttir.reshape"(%arg25, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.clamp_tensor"(%403, %360, %407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xbf16>
        %411 = "ttir.add"(%409, %15, %410) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.typecast"(%411, %412) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<1x1x1xbf16>
        %415 = "ttir.reshape"(%arg26, %414) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.broadcast"(%415, %416) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xbf16>
        %419 = "ttir.slice_static"(%401, %418) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %420 = ttir.empty() : tensor<32x13x2880xbf16>
        %421 = "ttir.clamp_tensor"(%419, %417, %407, %420) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xf32>
        %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %424 = ttir.empty() : tensor<1x1x1xf32>
        %425 = "ttir.reshape"(%arg24, %424) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %426 = ttir.empty() : tensor<32x13x2880xf32>
        %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %428 = ttir.empty() : tensor<32x13x2880xf32>
        %429 = "ttir.multiply"(%423, %427, %428) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %430 = ttir.empty() : tensor<32x13x2880xbf16>
        %431 = "ttir.typecast"(%429, %430) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %432 = ttir.empty() : tensor<32x13x2880xbf16>
        %433 = "ttir.sigmoid"(%431, %432) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %434 = ttir.empty() : tensor<32x13x2880xf32>
        %435 = "ttir.typecast"(%433, %434) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %436 = ttir.empty() : tensor<32x13x2880xf32>
        %437 = "ttir.multiply"(%423, %435, %436) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %438 = ttir.empty() : tensor<32x13x2880xf32>
        %439 = "ttir.multiply"(%413, %437, %438) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %440 = ttir.empty() : tensor<32x13x2880xbf16>
        %441 = "ttir.typecast"(%439, %440) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %442 = "ttir.dot_general"(%441, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
        %443 = ttir.empty() : tensor<32x1x2880xbf16>
        %444 = "ttir.reshape"(%arg22, %443) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %445 = ttir.empty() : tensor<32x13x2880xbf16>
        %446 = "ttir.broadcast"(%444, %445) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %447 = ttir.empty() : tensor<32x13x2880xbf16>
        %448 = "ttir.add"(%442, %446, %447) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %449 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %450 = "ttir.reshape"(%448, %449) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %451 = ttir.empty() : tensor<32x1x13x2880xf32>
        %452 = "ttir.typecast"(%450, %451) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %453 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %454 = ttir.empty() : tensor<13x1x1xsi32>
        %455 = "ttir.reshape"(%453, %454) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %456 = ttir.empty() : tensor<13x4x1xsi32>
        %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %458 = ttir.empty() : tensor<2880x32xbf16>
        %459 = "ttir.permute"(%arg12, %458) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %460 = "ttir.dot_general"(%390, %459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
        %461 = ttir.empty() : tensor<1x32xbf16>
        %462 = "ttir.reshape"(%arg11, %461) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %463 = ttir.empty() : tensor<13x32xbf16>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %465 = ttir.empty() : tensor<13x32xbf16>
        %466 = "ttir.add"(%460, %464, %465) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %467 = ttir.empty() : tensor<13x32xbf16>
        %468 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%466, %467, %468) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %469 = ttir.empty() : tensor<13x4xsi32>
        %470 = "ttir.slice_static"(%indices, %469) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %471 = ttir.empty() : tensor<13x4x1xsi32>
        %472 = "ttir.reshape"(%470, %471) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %473 = ttir.empty() : tensor<13x4x2xsi32>
        %474 = "ttir.concat"(%457, %472, %473) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %475 = ttir.empty() : tensor<13x4xbf16>
        %476 = "ttir.slice_static"(%values, %475) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %477 = ttir.empty() : tensor<13xbf16>
        %478 = "ttir.max"(%476, %477) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %479 = ttir.empty() : tensor<13x1xbf16>
        %480 = "ttir.reshape"(%478, %479) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %481 = ttir.empty() : tensor<13x4xbf16>
        %482 = "ttir.broadcast"(%480, %481) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %483 = ttir.empty() : tensor<13x4xbf16>
        %484 = "ttir.subtract"(%476, %482, %483) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %485 = ttir.empty() : tensor<13x4xbf16>
        %486 = "ttir.exp"(%484, %485) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %487 = ttir.empty() : tensor<13xbf16>
        %488 = "ttir.sum"(%486, %487) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %489 = ttir.empty() : tensor<13x1xbf16>
        %490 = "ttir.reshape"(%488, %489) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %491 = ttir.empty() : tensor<13x4xbf16>
        %492 = "ttir.broadcast"(%490, %491) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %493 = ttir.empty() : tensor<13x4xbf16>
        %494 = "ttir.div"(%486, %492, %493) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %495 = ttir.empty() : tensor<13x32xbf16>
        %496 = "ttir.scatter"(%11, %474, %494, %495) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %497 = ttir.empty() : tensor<32x13xbf16>
        %498 = "ttir.permute"(%496, %497) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %499 = ttir.empty() : tensor<32x1x13x1xbf16>
        %500 = "ttir.reshape"(%498, %499) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %501 = ttir.empty() : tensor<32x1x13x1xf32>
        %502 = "ttir.typecast"(%500, %501) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %503 = ttir.empty() : tensor<32x1x13x2880xf32>
        %504 = "ttir.broadcast"(%502, %503) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %505 = ttir.empty() : tensor<32x1x13x2880xf32>
        %506 = "ttir.multiply"(%452, %504, %505) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %507 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %508 = "ttir.typecast"(%506, %507) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %509 = ttir.empty() : tensor<1x13x2880xbf16>
        %510 = "ttir.sum"(%508, %509) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %511 = ttir.empty() : tensor<1x13x2880xbf16>
        %512 = "ttir.add"(%356, %510, %511) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %513 = ttir.empty() : tensor<1x13x2880xf32>
        %514 = "ttir.typecast"(%512, %513) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %515 = ttir.empty() : tensor<1x13x2880xf32>
        %516 = "ttir.pow"(%514, %27, %515) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %517 = ttir.empty() : tensor<1x13xf32>
        %518 = "ttir.sum"(%516, %517) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %519 = ttir.empty() : tensor<1x13xf32>
        %520 = "ttir.multiply"(%518, %4, %519) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %521 = ttir.empty() : tensor<1x13x1xf32>
        %522 = "ttir.reshape"(%520, %521) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %523 = ttir.empty() : tensor<1x13x1xf32>
        %524 = "ttir.add"(%522, %59, %523) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %525 = ttir.empty() : tensor<1x13x1xf32>
        %526 = "ttir.rsqrt"(%524, %525) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %527 = ttir.empty() : tensor<1x13x2880xf32>
        %528 = "ttir.broadcast"(%526, %527) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %529 = ttir.empty() : tensor<1x13x2880xf32>
        %530 = "ttir.multiply"(%514, %528, %529) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %531 = ttir.empty() : tensor<1x13x2880xf32>
        %532 = "ttir.multiply"(%300, %530, %531) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %533 = ttir.empty() : tensor<1x13x2880xbf16>
        %534 = "ttir.typecast"(%532, %533) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %535 = ttir.empty() : tensor<13x2880xbf16>
        %536 = "ttir.reshape"(%534, %535) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %537 = ttir.empty() : tensor<2880x201088xbf16>
        %538 = "ttir.permute"(%arg10, %537) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %539 = "ttir.dot_general"(%536, %538) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
        %540 = ttir.empty() : tensor<1x13x201088xbf16>
        %541 = "ttir.reshape"(%539, %540) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %290, %292, %294, %202, %539, %541 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<2880x4096xbf16>
        %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
        %77 = ttir.empty() : tensor<1x13x4096xbf16>
        %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %79 = ttir.empty() : tensor<1x1x4096xbf16>
        %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %81 = ttir.empty() : tensor<1x13x4096xbf16>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %83 = ttir.empty() : tensor<1x13x4096xbf16>
        %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %85 = ttir.empty() : tensor<1x13x64x64xbf16>
        %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %87 = ttir.empty() : tensor<1x64x13x64xbf16>
        %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %89 = ttir.empty() : tensor<1x64x13x32xbf16>
        %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %91 = ttir.empty() : tensor<1x64x13x32xf32>
        %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %93 = ttir.empty() : tensor<1x32x1xf32>
        %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<2880x512xbf16>
        %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %157 = ttir.empty() : tensor<1x13x512xbf16>
        %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %159 = ttir.empty() : tensor<1x1x512xbf16>
        %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %161 = ttir.empty() : tensor<1x13x512xbf16>
        %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %163 = ttir.empty() : tensor<1x13x512xbf16>
        %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %165 = ttir.empty() : tensor<1x13x8x64xbf16>
        %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %167 = ttir.empty() : tensor<1x8x13x64xbf16>
        %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %169 = ttir.empty() : tensor<1x8x13x32xbf16>
        %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %171 = ttir.empty() : tensor<1x8x13x32xf32>
        %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %173 = ttir.empty() : tensor<1x8x13x32xf32>
        %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %175 = ttir.empty() : tensor<1x8x13x32xf32>
        %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %177 = ttir.empty() : tensor<1x8x13x32xbf16>
        %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %179 = ttir.empty() : tensor<1x8x13x32xbf16>
        %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %181 = ttir.empty() : tensor<1x8x13x32xf32>
        %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %183 = ttir.empty() : tensor<1x8x13x32xf32>
        %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %185 = ttir.empty() : tensor<1x8x13x32xf32>
        %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %187 = ttir.empty() : tensor<1x8x13x32xbf16>
        %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %189 = ttir.empty() : tensor<1x8x13x32xbf16>
        %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %191 = ttir.empty() : tensor<1x8x13x32xf32>
        %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %193 = ttir.empty() : tensor<1x8x13x32xbf16>
        %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %195 = ttir.empty() : tensor<1x8x13x32xf32>
        %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %197 = ttir.empty() : tensor<1x8x13x32xbf16>
        %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %199 = ttir.empty() : tensor<1x8x13x32xbf16>
        %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %201 = ttir.empty() : tensor<1x8x13x64xbf16>
        %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %207 = ttir.empty() : tensor<1x64x13x64xbf16>
        %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %209 = ttir.empty() : tensor<1x64x64x13xbf16>
        %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %211 = ttir.empty() : tensor<64x64x13xbf16>
        %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<2880x512xbf16>
        %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %283 = ttir.empty() : tensor<1x13x512xbf16>
        %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %285 = ttir.empty() : tensor<1x1x512xbf16>
        %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %287 = ttir.empty() : tensor<1x13x512xbf16>
        %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %289 = ttir.empty() : tensor<1x13x512xbf16>
        %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %291 = ttir.empty() : tensor<1x13x8x64xbf16>
        %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %293 = ttir.empty() : tensor<1x8x13x64xbf16>
        %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %295 = ttir.empty() : tensor<2880xf32>
        %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %297 = ttir.empty() : tensor<1x1x2880xf32>
        %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %299 = ttir.empty() : tensor<1x13x2880xf32>
        %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %301 = ttir.empty() : tensor<1x64x13xbf16>
        %302 = "ttir.max"(%279, %301) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %303 = ttir.empty() : tensor<1x64x13x1xbf16>
        %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %305 = ttir.empty() : tensor<1x64x13x14xbf16>
        %306 = "ttir.broadcast"(%304, %305) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %307 = ttir.empty() : tensor<1x64x13x14xbf16>
        %308 = "ttir.subtract"(%279, %306, %307) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %309 = ttir.empty() : tensor<1x64x13xbf16>
        %310 = "ttir.max"(%308, %309) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %311 = ttir.empty() : tensor<1x64x13x1xbf16>
        %312 = "ttir.reshape"(%310, %311) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %313 = ttir.empty() : tensor<1x64x13x14xbf16>
        %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %315 = ttir.empty() : tensor<1x64x13x14xbf16>
        %316 = "ttir.subtract"(%308, %314, %315) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %317 = ttir.empty() : tensor<1x64x13x14xbf16>
        %318 = "ttir.exp"(%316, %317) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %319 = ttir.empty() : tensor<1x64x13xbf16>
        %320 = "ttir.sum"(%318, %319) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %321 = ttir.empty() : tensor<1x64x13x1xbf16>
        %322 = "ttir.reshape"(%320, %321) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %323 = ttir.empty() : tensor<1x64x13x14xbf16>
        %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %325 = ttir.empty() : tensor<1x64x13x14xbf16>
        %326 = "ttir.div"(%318, %324, %325) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %327 = ttir.empty() : tensor<1x64x13x13xbf16>
        %328 = "ttir.slice_static"(%326, %327) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %329 = ttir.empty() : tensor<64x13x13xbf16>
        %330 = "ttir.reshape"(%328, %329) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %331 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %332 = "ttir.reshape"(%294, %331) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %333 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %334 = "ttir.broadcast"(%332, %333) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %335 = ttir.empty() : tensor<64x13x64xbf16>
        %336 = "ttir.reshape"(%334, %335) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %337 = "ttir.dot_general"(%330, %336) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %338 = ttir.empty() : tensor<1x64x13x64xbf16>
        %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %340 = ttir.empty() : tensor<1x13x64x64xbf16>
        %341 = "ttir.permute"(%339, %340) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %342 = ttir.empty() : tensor<13x4096xbf16>
        %343 = "ttir.reshape"(%341, %342) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %344 = ttir.empty() : tensor<4096x2880xbf16>
        %345 = "ttir.permute"(%arg14, %344) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %346 = "ttir.dot_general"(%343, %345) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
        %347 = ttir.empty() : tensor<1x13x2880xbf16>
        %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %349 = ttir.empty() : tensor<1x1x2880xbf16>
        %350 = "ttir.reshape"(%arg13, %349) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %351 = ttir.empty() : tensor<1x13x2880xbf16>
        %352 = "ttir.broadcast"(%350, %351) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %353 = ttir.empty() : tensor<1x13x2880xbf16>
        %354 = "ttir.add"(%348, %352, %353) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %355 = ttir.empty() : tensor<1x13x2880xbf16>
        %356 = "ttir.add"(%45, %354, %355) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %357 = ttir.empty() : tensor<1x1x1xbf16>
        %358 = "ttir.reshape"(%arg29, %357) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %359 = ttir.empty() : tensor<32x13x2880xbf16>
        %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %361 = ttir.empty() : tensor<2880xf32>
        %362 = "ttir.typecast"(%arg21, %361) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %363 = ttir.empty() : tensor<1x1x2880xf32>
        %364 = "ttir.reshape"(%362, %363) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %365 = ttir.empty() : tensor<1x13x2880xf32>
        %366 = "ttir.broadcast"(%364, %365) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %367 = ttir.empty() : tensor<1x13x2880xf32>
        %368 = "ttir.typecast"(%356, %367) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %369 = ttir.empty() : tensor<1x13x2880xf32>
        %370 = "ttir.pow"(%368, %27, %369) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %371 = ttir.empty() : tensor<1x13xf32>
        %372 = "ttir.sum"(%370, %371) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %373 = ttir.empty() : tensor<1x13xf32>
        %374 = "ttir.multiply"(%372, %4, %373) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %375 = ttir.empty() : tensor<1x13x1xf32>
        %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %377 = ttir.empty() : tensor<1x13x1xf32>
        %378 = "ttir.add"(%376, %59, %377) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %379 = ttir.empty() : tensor<1x13x1xf32>
        %380 = "ttir.rsqrt"(%378, %379) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %381 = ttir.empty() : tensor<1x13x2880xf32>
        %382 = "ttir.broadcast"(%380, %381) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %383 = ttir.empty() : tensor<1x13x2880xf32>
        %384 = "ttir.multiply"(%368, %382, %383) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %385 = ttir.empty() : tensor<1x13x2880xf32>
        %386 = "ttir.multiply"(%366, %384, %385) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %387 = ttir.empty() : tensor<1x13x2880xbf16>
        %388 = "ttir.typecast"(%386, %387) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %389 = ttir.empty() : tensor<13x2880xbf16>
        %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %391 = ttir.empty() : tensor<416x2880xbf16>
        %392 = "ttir.concat"(%390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %391) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %393 = ttir.empty() : tensor<32x13x2880xbf16>
        %394 = "ttir.reshape"(%392, %393) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %395 = "ttir.dot_general"(%394, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
        %396 = ttir.empty() : tensor<32x1x5760xbf16>
        %397 = "ttir.reshape"(%arg27, %396) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %398 = ttir.empty() : tensor<32x13x5760xbf16>
        %399 = "ttir.broadcast"(%397, %398) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %400 = ttir.empty() : tensor<32x13x5760xbf16>
        %401 = "ttir.add"(%395, %399, %400) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %402 = ttir.empty() : tensor<32x13x2880xbf16>
        %403 = "ttir.slice_static"(%401, %402) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %404 = ttir.empty() : tensor<1x1x1xbf16>
        %405 = "ttir.reshape"(%arg25, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.clamp_tensor"(%403, %360, %407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xbf16>
        %411 = "ttir.add"(%409, %15, %410) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.typecast"(%411, %412) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<1x1x1xbf16>
        %415 = "ttir.reshape"(%arg26, %414) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.broadcast"(%415, %416) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xbf16>
        %419 = "ttir.slice_static"(%401, %418) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %420 = ttir.empty() : tensor<32x13x2880xbf16>
        %421 = "ttir.clamp_tensor"(%419, %417, %407, %420) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xf32>
        %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %424 = ttir.empty() : tensor<1x1x1xf32>
        %425 = "ttir.reshape"(%arg24, %424) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %426 = ttir.empty() : tensor<32x13x2880xf32>
        %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %428 = ttir.empty() : tensor<32x13x2880xf32>
        %429 = "ttir.multiply"(%423, %427, %428) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %430 = ttir.empty() : tensor<32x13x2880xbf16>
        %431 = "ttir.typecast"(%429, %430) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %432 = ttir.empty() : tensor<32x13x2880xbf16>
        %433 = "ttir.sigmoid"(%431, %432) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %434 = ttir.empty() : tensor<32x13x2880xf32>
        %435 = "ttir.typecast"(%433, %434) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %436 = ttir.empty() : tensor<32x13x2880xf32>
        %437 = "ttir.multiply"(%423, %435, %436) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %438 = ttir.empty() : tensor<32x13x2880xf32>
        %439 = "ttir.multiply"(%413, %437, %438) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %440 = ttir.empty() : tensor<32x13x2880xbf16>
        %441 = "ttir.typecast"(%439, %440) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %442 = "ttir.dot_general"(%441, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
        %443 = ttir.empty() : tensor<32x1x2880xbf16>
        %444 = "ttir.reshape"(%arg22, %443) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %445 = ttir.empty() : tensor<32x13x2880xbf16>
        %446 = "ttir.broadcast"(%444, %445) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %447 = ttir.empty() : tensor<32x13x2880xbf16>
        %448 = "ttir.add"(%442, %446, %447) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %449 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %450 = "ttir.reshape"(%448, %449) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %451 = ttir.empty() : tensor<32x1x13x2880xf32>
        %452 = "ttir.typecast"(%450, %451) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %453 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %454 = ttir.empty() : tensor<13x1x1xsi32>
        %455 = "ttir.reshape"(%453, %454) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %456 = ttir.empty() : tensor<13x4x1xsi32>
        %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %458 = ttir.empty() : tensor<2880x32xbf16>
        %459 = "ttir.permute"(%arg12, %458) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %460 = "ttir.dot_general"(%390, %459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
        %461 = ttir.empty() : tensor<1x32xbf16>
        %462 = "ttir.reshape"(%arg11, %461) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %463 = ttir.empty() : tensor<13x32xbf16>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %465 = ttir.empty() : tensor<13x32xbf16>
        %466 = "ttir.add"(%460, %464, %465) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %467 = ttir.empty() : tensor<13x32xbf16>
        %468 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%466, %467, %468) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %469 = ttir.empty() : tensor<13x4xsi32>
        %470 = "ttir.slice_static"(%indices, %469) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %471 = ttir.empty() : tensor<13x4x1xsi32>
        %472 = "ttir.reshape"(%470, %471) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %473 = ttir.empty() : tensor<13x4x2xsi32>
        %474 = "ttir.concat"(%457, %472, %473) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %475 = ttir.empty() : tensor<13x4xbf16>
        %476 = "ttir.slice_static"(%values, %475) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %477 = ttir.empty() : tensor<13xbf16>
        %478 = "ttir.max"(%476, %477) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %479 = ttir.empty() : tensor<13x1xbf16>
        %480 = "ttir.reshape"(%478, %479) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %481 = ttir.empty() : tensor<13x4xbf16>
        %482 = "ttir.broadcast"(%480, %481) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %483 = ttir.empty() : tensor<13x4xbf16>
        %484 = "ttir.subtract"(%476, %482, %483) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %485 = ttir.empty() : tensor<13x4xbf16>
        %486 = "ttir.exp"(%484, %485) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %487 = ttir.empty() : tensor<13xbf16>
        %488 = "ttir.sum"(%486, %487) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %489 = ttir.empty() : tensor<13x1xbf16>
        %490 = "ttir.reshape"(%488, %489) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %491 = ttir.empty() : tensor<13x4xbf16>
        %492 = "ttir.broadcast"(%490, %491) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %493 = ttir.empty() : tensor<13x4xbf16>
        %494 = "ttir.div"(%486, %492, %493) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %495 = ttir.empty() : tensor<13x32xbf16>
        %496 = "ttir.scatter"(%11, %474, %494, %495) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %497 = ttir.empty() : tensor<32x13xbf16>
        %498 = "ttir.permute"(%496, %497) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %499 = ttir.empty() : tensor<32x1x13x1xbf16>
        %500 = "ttir.reshape"(%498, %499) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %501 = ttir.empty() : tensor<32x1x13x1xf32>
        %502 = "ttir.typecast"(%500, %501) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %503 = ttir.empty() : tensor<32x1x13x2880xf32>
        %504 = "ttir.broadcast"(%502, %503) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %505 = ttir.empty() : tensor<32x1x13x2880xf32>
        %506 = "ttir.multiply"(%452, %504, %505) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %507 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %508 = "ttir.typecast"(%506, %507) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %509 = ttir.empty() : tensor<1x13x2880xbf16>
        %510 = "ttir.sum"(%508, %509) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %511 = ttir.empty() : tensor<1x13x2880xbf16>
        %512 = "ttir.add"(%356, %510, %511) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %513 = ttir.empty() : tensor<1x13x2880xf32>
        %514 = "ttir.typecast"(%512, %513) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %515 = ttir.empty() : tensor<1x13x2880xf32>
        %516 = "ttir.pow"(%514, %27, %515) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %517 = ttir.empty() : tensor<1x13xf32>
        %518 = "ttir.sum"(%516, %517) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %519 = ttir.empty() : tensor<1x13xf32>
        %520 = "ttir.multiply"(%518, %4, %519) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %521 = ttir.empty() : tensor<1x13x1xf32>
        %522 = "ttir.reshape"(%520, %521) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %523 = ttir.empty() : tensor<1x13x1xf32>
        %524 = "ttir.add"(%522, %59, %523) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %525 = ttir.empty() : tensor<1x13x1xf32>
        %526 = "ttir.rsqrt"(%524, %525) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %527 = ttir.empty() : tensor<1x13x2880xf32>
        %528 = "ttir.broadcast"(%526, %527) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %529 = ttir.empty() : tensor<1x13x2880xf32>
        %530 = "ttir.multiply"(%514, %528, %529) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %531 = ttir.empty() : tensor<1x13x2880xf32>
        %532 = "ttir.multiply"(%300, %530, %531) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %533 = ttir.empty() : tensor<1x13x2880xbf16>
        %534 = "ttir.typecast"(%532, %533) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %535 = ttir.empty() : tensor<13x2880xbf16>
        %536 = "ttir.reshape"(%534, %535) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %537 = ttir.empty() : tensor<2880x201088xbf16>
        %538 = "ttir.permute"(%arg10, %537) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %539 = "ttir.dot_general"(%536, %538) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
        %540 = ttir.empty() : tensor<1x13x201088xbf16>
        %541 = "ttir.reshape"(%539, %540) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %290, %292, %294, %202, %539, %541 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<2880x4096xbf16>
        %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
        %77 = ttir.empty() : tensor<1x13x4096xbf16>
        %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %79 = ttir.empty() : tensor<1x1x4096xbf16>
        %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %81 = ttir.empty() : tensor<1x13x4096xbf16>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %83 = ttir.empty() : tensor<1x13x4096xbf16>
        %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %85 = ttir.empty() : tensor<1x13x64x64xbf16>
        %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %87 = ttir.empty() : tensor<1x64x13x64xbf16>
        %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %89 = ttir.empty() : tensor<1x64x13x32xbf16>
        %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %91 = ttir.empty() : tensor<1x64x13x32xf32>
        %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %93 = ttir.empty() : tensor<1x32x1xf32>
        %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<2880x512xbf16>
        %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %157 = ttir.empty() : tensor<1x13x512xbf16>
        %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %159 = ttir.empty() : tensor<1x1x512xbf16>
        %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %161 = ttir.empty() : tensor<1x13x512xbf16>
        %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %163 = ttir.empty() : tensor<1x13x512xbf16>
        %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %165 = ttir.empty() : tensor<1x13x8x64xbf16>
        %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %167 = ttir.empty() : tensor<1x8x13x64xbf16>
        %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %169 = ttir.empty() : tensor<1x8x13x32xbf16>
        %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %171 = ttir.empty() : tensor<1x8x13x32xf32>
        %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %173 = ttir.empty() : tensor<1x8x13x32xf32>
        %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %175 = ttir.empty() : tensor<1x8x13x32xf32>
        %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %177 = ttir.empty() : tensor<1x8x13x32xbf16>
        %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %179 = ttir.empty() : tensor<1x8x13x32xbf16>
        %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %181 = ttir.empty() : tensor<1x8x13x32xf32>
        %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %183 = ttir.empty() : tensor<1x8x13x32xf32>
        %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %185 = ttir.empty() : tensor<1x8x13x32xf32>
        %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %187 = ttir.empty() : tensor<1x8x13x32xbf16>
        %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %189 = ttir.empty() : tensor<1x8x13x32xbf16>
        %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %191 = ttir.empty() : tensor<1x8x13x32xf32>
        %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %193 = ttir.empty() : tensor<1x8x13x32xbf16>
        %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %195 = ttir.empty() : tensor<1x8x13x32xf32>
        %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %197 = ttir.empty() : tensor<1x8x13x32xbf16>
        %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %199 = ttir.empty() : tensor<1x8x13x32xbf16>
        %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %201 = ttir.empty() : tensor<1x8x13x64xbf16>
        %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %207 = ttir.empty() : tensor<1x64x13x64xbf16>
        %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %209 = ttir.empty() : tensor<1x64x64x13xbf16>
        %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %211 = ttir.empty() : tensor<64x64x13xbf16>
        %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<2880x512xbf16>
        %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %283 = ttir.empty() : tensor<1x13x512xbf16>
        %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %285 = ttir.empty() : tensor<1x1x512xbf16>
        %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %287 = ttir.empty() : tensor<1x13x512xbf16>
        %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %289 = ttir.empty() : tensor<1x13x512xbf16>
        %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %291 = ttir.empty() : tensor<1x13x8x64xbf16>
        %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %293 = ttir.empty() : tensor<1x8x13x64xbf16>
        %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %295 = ttir.empty() : tensor<2880xf32>
        %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %297 = ttir.empty() : tensor<1x1x2880xf32>
        %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %299 = ttir.empty() : tensor<1x13x2880xf32>
        %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %301 = ttir.empty() : tensor<1x64x13xbf16>
        %302 = "ttir.max"(%279, %301) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %303 = ttir.empty() : tensor<1x64x13x1xbf16>
        %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %305 = ttir.empty() : tensor<1x64x13x14xbf16>
        %306 = "ttir.broadcast"(%304, %305) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %307 = ttir.empty() : tensor<1x64x13x14xbf16>
        %308 = "ttir.subtract"(%279, %306, %307) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %309 = ttir.empty() : tensor<1x64x13xbf16>
        %310 = "ttir.max"(%308, %309) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %311 = ttir.empty() : tensor<1x64x13x1xbf16>
        %312 = "ttir.reshape"(%310, %311) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %313 = ttir.empty() : tensor<1x64x13x14xbf16>
        %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %315 = ttir.empty() : tensor<1x64x13x14xbf16>
        %316 = "ttir.subtract"(%308, %314, %315) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %317 = ttir.empty() : tensor<1x64x13x14xbf16>
        %318 = "ttir.exp"(%316, %317) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %319 = ttir.empty() : tensor<1x64x13xbf16>
        %320 = "ttir.sum"(%318, %319) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %321 = ttir.empty() : tensor<1x64x13x1xbf16>
        %322 = "ttir.reshape"(%320, %321) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %323 = ttir.empty() : tensor<1x64x13x14xbf16>
        %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %325 = ttir.empty() : tensor<1x64x13x14xbf16>
        %326 = "ttir.div"(%318, %324, %325) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %327 = ttir.empty() : tensor<1x64x13x13xbf16>
        %328 = "ttir.slice_static"(%326, %327) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %329 = ttir.empty() : tensor<64x13x13xbf16>
        %330 = "ttir.reshape"(%328, %329) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %331 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %332 = "ttir.reshape"(%294, %331) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %333 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %334 = "ttir.broadcast"(%332, %333) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %335 = ttir.empty() : tensor<64x13x64xbf16>
        %336 = "ttir.reshape"(%334, %335) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %337 = "ttir.dot_general"(%330, %336) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %338 = ttir.empty() : tensor<1x64x13x64xbf16>
        %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %340 = ttir.empty() : tensor<1x13x64x64xbf16>
        %341 = "ttir.permute"(%339, %340) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %342 = ttir.empty() : tensor<13x4096xbf16>
        %343 = "ttir.reshape"(%341, %342) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %344 = ttir.empty() : tensor<4096x2880xbf16>
        %345 = "ttir.permute"(%arg14, %344) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %346 = "ttir.dot_general"(%343, %345) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
        %347 = ttir.empty() : tensor<1x13x2880xbf16>
        %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %349 = ttir.empty() : tensor<1x1x2880xbf16>
        %350 = "ttir.reshape"(%arg13, %349) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %351 = ttir.empty() : tensor<1x13x2880xbf16>
        %352 = "ttir.broadcast"(%350, %351) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %353 = ttir.empty() : tensor<1x13x2880xbf16>
        %354 = "ttir.add"(%348, %352, %353) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %355 = ttir.empty() : tensor<1x13x2880xbf16>
        %356 = "ttir.add"(%45, %354, %355) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %357 = ttir.empty() : tensor<1x1x1xbf16>
        %358 = "ttir.reshape"(%arg29, %357) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %359 = ttir.empty() : tensor<32x13x2880xbf16>
        %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %361 = ttir.empty() : tensor<2880xf32>
        %362 = "ttir.typecast"(%arg21, %361) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %363 = ttir.empty() : tensor<1x1x2880xf32>
        %364 = "ttir.reshape"(%362, %363) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %365 = ttir.empty() : tensor<1x13x2880xf32>
        %366 = "ttir.broadcast"(%364, %365) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %367 = ttir.empty() : tensor<1x13x2880xf32>
        %368 = "ttir.typecast"(%356, %367) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %369 = ttir.empty() : tensor<1x13x2880xf32>
        %370 = "ttir.pow"(%368, %27, %369) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %371 = ttir.empty() : tensor<1x13xf32>
        %372 = "ttir.sum"(%370, %371) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %373 = ttir.empty() : tensor<1x13xf32>
        %374 = "ttir.multiply"(%372, %4, %373) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %375 = ttir.empty() : tensor<1x13x1xf32>
        %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %377 = ttir.empty() : tensor<1x13x1xf32>
        %378 = "ttir.add"(%376, %59, %377) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %379 = ttir.empty() : tensor<1x13x1xf32>
        %380 = "ttir.rsqrt"(%378, %379) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %381 = ttir.empty() : tensor<1x13x2880xf32>
        %382 = "ttir.broadcast"(%380, %381) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %383 = ttir.empty() : tensor<1x13x2880xf32>
        %384 = "ttir.multiply"(%368, %382, %383) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %385 = ttir.empty() : tensor<1x13x2880xf32>
        %386 = "ttir.multiply"(%366, %384, %385) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %387 = ttir.empty() : tensor<1x13x2880xbf16>
        %388 = "ttir.typecast"(%386, %387) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %389 = ttir.empty() : tensor<13x2880xbf16>
        %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %391 = ttir.empty() : tensor<416x2880xbf16>
        %392 = "ttir.concat"(%390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %391) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %393 = ttir.empty() : tensor<32x13x2880xbf16>
        %394 = "ttir.reshape"(%392, %393) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %395 = "ttir.dot_general"(%394, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
        %396 = ttir.empty() : tensor<32x1x5760xbf16>
        %397 = "ttir.reshape"(%arg27, %396) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %398 = ttir.empty() : tensor<32x13x5760xbf16>
        %399 = "ttir.broadcast"(%397, %398) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %400 = ttir.empty() : tensor<32x13x5760xbf16>
        %401 = "ttir.add"(%395, %399, %400) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %402 = ttir.empty() : tensor<32x13x2880xbf16>
        %403 = "ttir.slice_static"(%401, %402) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %404 = ttir.empty() : tensor<1x1x1xbf16>
        %405 = "ttir.reshape"(%arg25, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.clamp_tensor"(%403, %360, %407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xbf16>
        %411 = "ttir.add"(%409, %15, %410) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.typecast"(%411, %412) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<1x1x1xbf16>
        %415 = "ttir.reshape"(%arg26, %414) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.broadcast"(%415, %416) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xbf16>
        %419 = "ttir.slice_static"(%401, %418) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %420 = ttir.empty() : tensor<32x13x2880xbf16>
        %421 = "ttir.clamp_tensor"(%419, %417, %407, %420) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xf32>
        %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %424 = ttir.empty() : tensor<1x1x1xf32>
        %425 = "ttir.reshape"(%arg24, %424) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %426 = ttir.empty() : tensor<32x13x2880xf32>
        %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %428 = ttir.empty() : tensor<32x13x2880xf32>
        %429 = "ttir.multiply"(%423, %427, %428) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %430 = ttir.empty() : tensor<32x13x2880xbf16>
        %431 = "ttir.typecast"(%429, %430) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %432 = ttir.empty() : tensor<32x13x2880xbf16>
        %433 = "ttir.sigmoid"(%431, %432) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %434 = ttir.empty() : tensor<32x13x2880xf32>
        %435 = "ttir.typecast"(%433, %434) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %436 = ttir.empty() : tensor<32x13x2880xf32>
        %437 = "ttir.multiply"(%423, %435, %436) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %438 = ttir.empty() : tensor<32x13x2880xf32>
        %439 = "ttir.multiply"(%413, %437, %438) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %440 = ttir.empty() : tensor<32x13x2880xbf16>
        %441 = "ttir.typecast"(%439, %440) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %442 = "ttir.dot_general"(%441, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
        %443 = ttir.empty() : tensor<32x1x2880xbf16>
        %444 = "ttir.reshape"(%arg22, %443) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %445 = ttir.empty() : tensor<32x13x2880xbf16>
        %446 = "ttir.broadcast"(%444, %445) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %447 = ttir.empty() : tensor<32x13x2880xbf16>
        %448 = "ttir.add"(%442, %446, %447) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %449 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %450 = "ttir.reshape"(%448, %449) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %451 = ttir.empty() : tensor<32x1x13x2880xf32>
        %452 = "ttir.typecast"(%450, %451) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %453 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %454 = ttir.empty() : tensor<13x1x1xsi32>
        %455 = "ttir.reshape"(%453, %454) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %456 = ttir.empty() : tensor<13x4x1xsi32>
        %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %458 = ttir.empty() : tensor<2880x32xbf16>
        %459 = "ttir.permute"(%arg12, %458) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %460 = "ttir.dot_general"(%390, %459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
        %461 = ttir.empty() : tensor<1x32xbf16>
        %462 = "ttir.reshape"(%arg11, %461) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %463 = ttir.empty() : tensor<13x32xbf16>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %465 = ttir.empty() : tensor<13x32xbf16>
        %466 = "ttir.add"(%460, %464, %465) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %467 = ttir.empty() : tensor<13x32xbf16>
        %468 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%466, %467, %468) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %469 = ttir.empty() : tensor<13x4xsi32>
        %470 = "ttir.slice_static"(%indices, %469) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %471 = ttir.empty() : tensor<13x4x1xsi32>
        %472 = "ttir.reshape"(%470, %471) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %473 = ttir.empty() : tensor<13x4x2xsi32>
        %474 = "ttir.concat"(%457, %472, %473) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %475 = ttir.empty() : tensor<13x4xbf16>
        %476 = "ttir.slice_static"(%values, %475) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %477 = ttir.empty() : tensor<13xbf16>
        %478 = "ttir.max"(%476, %477) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %479 = ttir.empty() : tensor<13x1xbf16>
        %480 = "ttir.reshape"(%478, %479) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %481 = ttir.empty() : tensor<13x4xbf16>
        %482 = "ttir.broadcast"(%480, %481) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %483 = ttir.empty() : tensor<13x4xbf16>
        %484 = "ttir.subtract"(%476, %482, %483) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %485 = ttir.empty() : tensor<13x4xbf16>
        %486 = "ttir.exp"(%484, %485) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %487 = ttir.empty() : tensor<13xbf16>
        %488 = "ttir.sum"(%486, %487) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %489 = ttir.empty() : tensor<13x1xbf16>
        %490 = "ttir.reshape"(%488, %489) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %491 = ttir.empty() : tensor<13x4xbf16>
        %492 = "ttir.broadcast"(%490, %491) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %493 = ttir.empty() : tensor<13x4xbf16>
        %494 = "ttir.div"(%486, %492, %493) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %495 = ttir.empty() : tensor<13x32xbf16>
        %496 = "ttir.scatter"(%11, %474, %494, %495) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %497 = ttir.empty() : tensor<32x13xbf16>
        %498 = "ttir.permute"(%496, %497) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %499 = ttir.empty() : tensor<32x1x13x1xbf16>
        %500 = "ttir.reshape"(%498, %499) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %501 = ttir.empty() : tensor<32x1x13x1xf32>
        %502 = "ttir.typecast"(%500, %501) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %503 = ttir.empty() : tensor<32x1x13x2880xf32>
        %504 = "ttir.broadcast"(%502, %503) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %505 = ttir.empty() : tensor<32x1x13x2880xf32>
        %506 = "ttir.multiply"(%452, %504, %505) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %507 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %508 = "ttir.typecast"(%506, %507) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %509 = ttir.empty() : tensor<1x13x2880xbf16>
        %510 = "ttir.sum"(%508, %509) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %511 = ttir.empty() : tensor<1x13x2880xbf16>
        %512 = "ttir.add"(%356, %510, %511) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %513 = ttir.empty() : tensor<1x13x2880xf32>
        %514 = "ttir.typecast"(%512, %513) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %515 = ttir.empty() : tensor<1x13x2880xf32>
        %516 = "ttir.pow"(%514, %27, %515) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %517 = ttir.empty() : tensor<1x13xf32>
        %518 = "ttir.sum"(%516, %517) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %519 = ttir.empty() : tensor<1x13xf32>
        %520 = "ttir.multiply"(%518, %4, %519) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %521 = ttir.empty() : tensor<1x13x1xf32>
        %522 = "ttir.reshape"(%520, %521) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %523 = ttir.empty() : tensor<1x13x1xf32>
        %524 = "ttir.add"(%522, %59, %523) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %525 = ttir.empty() : tensor<1x13x1xf32>
        %526 = "ttir.rsqrt"(%524, %525) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %527 = ttir.empty() : tensor<1x13x2880xf32>
        %528 = "ttir.broadcast"(%526, %527) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %529 = ttir.empty() : tensor<1x13x2880xf32>
        %530 = "ttir.multiply"(%514, %528, %529) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %531 = ttir.empty() : tensor<1x13x2880xf32>
        %532 = "ttir.multiply"(%300, %530, %531) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %533 = ttir.empty() : tensor<1x13x2880xbf16>
        %534 = "ttir.typecast"(%532, %533) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %535 = ttir.empty() : tensor<13x2880xbf16>
        %536 = "ttir.reshape"(%534, %535) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %537 = ttir.empty() : tensor<2880x201088xbf16>
        %538 = "ttir.permute"(%arg10, %537) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %539 = "ttir.dot_general"(%536, %538) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
        %540 = ttir.empty() : tensor<1x13x201088xbf16>
        %541 = "ttir.reshape"(%539, %540) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %290, %292, %294, %202, %539, %541 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<2880x4096xbf16>
        %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
        %77 = ttir.empty() : tensor<1x13x4096xbf16>
        %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %79 = ttir.empty() : tensor<1x1x4096xbf16>
        %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %81 = ttir.empty() : tensor<1x13x4096xbf16>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %83 = ttir.empty() : tensor<1x13x4096xbf16>
        %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %85 = ttir.empty() : tensor<1x13x64x64xbf16>
        %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %87 = ttir.empty() : tensor<1x64x13x64xbf16>
        %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %89 = ttir.empty() : tensor<1x64x13x32xbf16>
        %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %91 = ttir.empty() : tensor<1x64x13x32xf32>
        %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %93 = ttir.empty() : tensor<1x32x1xf32>
        %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<2880x512xbf16>
        %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %157 = ttir.empty() : tensor<1x13x512xbf16>
        %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %159 = ttir.empty() : tensor<1x1x512xbf16>
        %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %161 = ttir.empty() : tensor<1x13x512xbf16>
        %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %163 = ttir.empty() : tensor<1x13x512xbf16>
        %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %165 = ttir.empty() : tensor<1x13x8x64xbf16>
        %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %167 = ttir.empty() : tensor<1x8x13x64xbf16>
        %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %169 = ttir.empty() : tensor<1x8x13x32xbf16>
        %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %171 = ttir.empty() : tensor<1x8x13x32xf32>
        %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %173 = ttir.empty() : tensor<1x8x13x32xf32>
        %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %175 = ttir.empty() : tensor<1x8x13x32xf32>
        %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %177 = ttir.empty() : tensor<1x8x13x32xbf16>
        %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %179 = ttir.empty() : tensor<1x8x13x32xbf16>
        %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %181 = ttir.empty() : tensor<1x8x13x32xf32>
        %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %183 = ttir.empty() : tensor<1x8x13x32xf32>
        %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %185 = ttir.empty() : tensor<1x8x13x32xf32>
        %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %187 = ttir.empty() : tensor<1x8x13x32xbf16>
        %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %189 = ttir.empty() : tensor<1x8x13x32xbf16>
        %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %191 = ttir.empty() : tensor<1x8x13x32xf32>
        %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %193 = ttir.empty() : tensor<1x8x13x32xbf16>
        %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %195 = ttir.empty() : tensor<1x8x13x32xf32>
        %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %197 = ttir.empty() : tensor<1x8x13x32xbf16>
        %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %199 = ttir.empty() : tensor<1x8x13x32xbf16>
        %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %201 = ttir.empty() : tensor<1x8x13x64xbf16>
        %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %207 = ttir.empty() : tensor<1x64x13x64xbf16>
        %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %209 = ttir.empty() : tensor<1x64x64x13xbf16>
        %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %211 = ttir.empty() : tensor<64x64x13xbf16>
        %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<2880x512xbf16>
        %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %283 = ttir.empty() : tensor<1x13x512xbf16>
        %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %285 = ttir.empty() : tensor<1x1x512xbf16>
        %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %287 = ttir.empty() : tensor<1x13x512xbf16>
        %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %289 = ttir.empty() : tensor<1x13x512xbf16>
        %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %291 = ttir.empty() : tensor<1x13x8x64xbf16>
        %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %293 = ttir.empty() : tensor<1x8x13x64xbf16>
        %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %295 = ttir.empty() : tensor<2880xf32>
        %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %297 = ttir.empty() : tensor<1x1x2880xf32>
        %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %299 = ttir.empty() : tensor<1x13x2880xf32>
        %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %301 = ttir.empty() : tensor<1x64x13xbf16>
        %302 = "ttir.max"(%279, %301) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %303 = ttir.empty() : tensor<1x64x13x1xbf16>
        %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %305 = ttir.empty() : tensor<1x64x13x14xbf16>
        %306 = "ttir.broadcast"(%304, %305) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %307 = ttir.empty() : tensor<1x64x13x14xbf16>
        %308 = "ttir.subtract"(%279, %306, %307) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %309 = ttir.empty() : tensor<1x64x13xbf16>
        %310 = "ttir.max"(%308, %309) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %311 = ttir.empty() : tensor<1x64x13x1xbf16>
        %312 = "ttir.reshape"(%310, %311) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %313 = ttir.empty() : tensor<1x64x13x14xbf16>
        %314 = "ttir.broadcast"(%312, %313) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %315 = ttir.empty() : tensor<1x64x13x14xbf16>
        %316 = "ttir.subtract"(%308, %314, %315) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %317 = ttir.empty() : tensor<1x64x13x14xbf16>
        %318 = "ttir.exp"(%316, %317) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %319 = ttir.empty() : tensor<1x64x13xbf16>
        %320 = "ttir.sum"(%318, %319) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13xbf16>) -> tensor<1x64x13xbf16>
        %321 = ttir.empty() : tensor<1x64x13x1xbf16>
        %322 = "ttir.reshape"(%320, %321) <{shape = [1 : i32, 64 : i32, 13 : i32, 1 : i32]}> : (tensor<1x64x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %323 = ttir.empty() : tensor<1x64x13x14xbf16>
        %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 1, 1, 14>}> : (tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %325 = ttir.empty() : tensor<1x64x13x14xbf16>
        %326 = "ttir.div"(%318, %324, %325) : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %327 = ttir.empty() : tensor<1x64x13x13xbf16>
        %328 = "ttir.slice_static"(%326, %327) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %329 = ttir.empty() : tensor<64x13x13xbf16>
        %330 = "ttir.reshape"(%328, %329) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %331 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %332 = "ttir.reshape"(%294, %331) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %333 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %334 = "ttir.broadcast"(%332, %333) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %335 = ttir.empty() : tensor<64x13x64xbf16>
        %336 = "ttir.reshape"(%334, %335) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %337 = "ttir.dot_general"(%330, %336) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %338 = ttir.empty() : tensor<1x64x13x64xbf16>
        %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %340 = ttir.empty() : tensor<1x13x64x64xbf16>
        %341 = "ttir.permute"(%339, %340) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x64x13x64xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %342 = ttir.empty() : tensor<13x4096xbf16>
        %343 = "ttir.reshape"(%341, %342) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x64x64xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %344 = ttir.empty() : tensor<4096x2880xbf16>
        %345 = "ttir.permute"(%arg14, %344) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %346 = "ttir.dot_general"(%343, %345) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
        %347 = ttir.empty() : tensor<1x13x2880xbf16>
        %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %349 = ttir.empty() : tensor<1x1x2880xbf16>
        %350 = "ttir.reshape"(%arg13, %349) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %351 = ttir.empty() : tensor<1x13x2880xbf16>
        %352 = "ttir.broadcast"(%350, %351) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %353 = ttir.empty() : tensor<1x13x2880xbf16>
        %354 = "ttir.add"(%348, %352, %353) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %355 = ttir.empty() : tensor<1x13x2880xbf16>
        %356 = "ttir.add"(%45, %354, %355) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %357 = ttir.empty() : tensor<1x1x1xbf16>
        %358 = "ttir.reshape"(%arg29, %357) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %359 = ttir.empty() : tensor<32x13x2880xbf16>
        %360 = "ttir.broadcast"(%358, %359) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %361 = ttir.empty() : tensor<2880xf32>
        %362 = "ttir.typecast"(%arg21, %361) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %363 = ttir.empty() : tensor<1x1x2880xf32>
        %364 = "ttir.reshape"(%362, %363) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %365 = ttir.empty() : tensor<1x13x2880xf32>
        %366 = "ttir.broadcast"(%364, %365) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %367 = ttir.empty() : tensor<1x13x2880xf32>
        %368 = "ttir.typecast"(%356, %367) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %369 = ttir.empty() : tensor<1x13x2880xf32>
        %370 = "ttir.pow"(%368, %27, %369) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %371 = ttir.empty() : tensor<1x13xf32>
        %372 = "ttir.sum"(%370, %371) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %373 = ttir.empty() : tensor<1x13xf32>
        %374 = "ttir.multiply"(%372, %4, %373) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %375 = ttir.empty() : tensor<1x13x1xf32>
        %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %377 = ttir.empty() : tensor<1x13x1xf32>
        %378 = "ttir.add"(%376, %59, %377) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %379 = ttir.empty() : tensor<1x13x1xf32>
        %380 = "ttir.rsqrt"(%378, %379) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %381 = ttir.empty() : tensor<1x13x2880xf32>
        %382 = "ttir.broadcast"(%380, %381) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %383 = ttir.empty() : tensor<1x13x2880xf32>
        %384 = "ttir.multiply"(%368, %382, %383) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %385 = ttir.empty() : tensor<1x13x2880xf32>
        %386 = "ttir.multiply"(%366, %384, %385) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %387 = ttir.empty() : tensor<1x13x2880xbf16>
        %388 = "ttir.typecast"(%386, %387) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %389 = ttir.empty() : tensor<13x2880xbf16>
        %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %391 = ttir.empty() : tensor<416x2880xbf16>
        %392 = "ttir.concat"(%390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %390, %391) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %393 = ttir.empty() : tensor<32x13x2880xbf16>
        %394 = "ttir.reshape"(%392, %393) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %395 = "ttir.dot_general"(%394, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
        %396 = ttir.empty() : tensor<32x1x5760xbf16>
        %397 = "ttir.reshape"(%arg27, %396) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %398 = ttir.empty() : tensor<32x13x5760xbf16>
        %399 = "ttir.broadcast"(%397, %398) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %400 = ttir.empty() : tensor<32x13x5760xbf16>
        %401 = "ttir.add"(%395, %399, %400) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %402 = ttir.empty() : tensor<32x13x2880xbf16>
        %403 = "ttir.slice_static"(%401, %402) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %404 = ttir.empty() : tensor<1x1x1xbf16>
        %405 = "ttir.reshape"(%arg25, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.clamp_tensor"(%403, %360, %407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xbf16>
        %411 = "ttir.add"(%409, %15, %410) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.typecast"(%411, %412) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<1x1x1xbf16>
        %415 = "ttir.reshape"(%arg26, %414) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.broadcast"(%415, %416) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xbf16>
        %419 = "ttir.slice_static"(%401, %418) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %420 = ttir.empty() : tensor<32x13x2880xbf16>
        %421 = "ttir.clamp_tensor"(%419, %417, %407, %420) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xf32>
        %423 = "ttir.typecast"(%421, %422) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %424 = ttir.empty() : tensor<1x1x1xf32>
        %425 = "ttir.reshape"(%arg24, %424) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %426 = ttir.empty() : tensor<32x13x2880xf32>
        %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %428 = ttir.empty() : tensor<32x13x2880xf32>
        %429 = "ttir.multiply"(%423, %427, %428) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %430 = ttir.empty() : tensor<32x13x2880xbf16>
        %431 = "ttir.typecast"(%429, %430) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %432 = ttir.empty() : tensor<32x13x2880xbf16>
        %433 = "ttir.sigmoid"(%431, %432) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %434 = ttir.empty() : tensor<32x13x2880xf32>
        %435 = "ttir.typecast"(%433, %434) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %436 = ttir.empty() : tensor<32x13x2880xf32>
        %437 = "ttir.multiply"(%423, %435, %436) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %438 = ttir.empty() : tensor<32x13x2880xf32>
        %439 = "ttir.multiply"(%413, %437, %438) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %440 = ttir.empty() : tensor<32x13x2880xbf16>
        %441 = "ttir.typecast"(%439, %440) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %442 = "ttir.dot_general"(%441, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
        %443 = ttir.empty() : tensor<32x1x2880xbf16>
        %444 = "ttir.reshape"(%arg22, %443) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %445 = ttir.empty() : tensor<32x13x2880xbf16>
        %446 = "ttir.broadcast"(%444, %445) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %447 = ttir.empty() : tensor<32x13x2880xbf16>
        %448 = "ttir.add"(%442, %446, %447) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %449 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %450 = "ttir.reshape"(%448, %449) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %451 = ttir.empty() : tensor<32x1x13x2880xf32>
        %452 = "ttir.typecast"(%450, %451) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %453 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %454 = ttir.empty() : tensor<13x1x1xsi32>
        %455 = "ttir.reshape"(%453, %454) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %456 = ttir.empty() : tensor<13x4x1xsi32>
        %457 = "ttir.broadcast"(%455, %456) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %458 = ttir.empty() : tensor<2880x32xbf16>
        %459 = "ttir.permute"(%arg12, %458) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %460 = "ttir.dot_general"(%390, %459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
        %461 = ttir.empty() : tensor<1x32xbf16>
        %462 = "ttir.reshape"(%arg11, %461) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %463 = ttir.empty() : tensor<13x32xbf16>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %465 = ttir.empty() : tensor<13x32xbf16>
        %466 = "ttir.add"(%460, %464, %465) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %467 = ttir.empty() : tensor<13x32xbf16>
        %468 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%466, %467, %468) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %469 = ttir.empty() : tensor<13x4xsi32>
        %470 = "ttir.slice_static"(%indices, %469) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %471 = ttir.empty() : tensor<13x4x1xsi32>
        %472 = "ttir.reshape"(%470, %471) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %473 = ttir.empty() : tensor<13x4x2xsi32>
        %474 = "ttir.concat"(%457, %472, %473) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %475 = ttir.empty() : tensor<13x4xbf16>
        %476 = "ttir.slice_static"(%values, %475) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %477 = ttir.empty() : tensor<13xbf16>
        %478 = "ttir.max"(%476, %477) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %479 = ttir.empty() : tensor<13x1xbf16>
        %480 = "ttir.reshape"(%478, %479) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %481 = ttir.empty() : tensor<13x4xbf16>
        %482 = "ttir.broadcast"(%480, %481) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %483 = ttir.empty() : tensor<13x4xbf16>
        %484 = "ttir.subtract"(%476, %482, %483) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %485 = ttir.empty() : tensor<13x4xbf16>
        %486 = "ttir.exp"(%484, %485) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %487 = ttir.empty() : tensor<13xbf16>
        %488 = "ttir.sum"(%486, %487) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<13x4xbf16>, tensor<13xbf16>) -> tensor<13xbf16>
        %489 = ttir.empty() : tensor<13x1xbf16>
        %490 = "ttir.reshape"(%488, %489) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xbf16>, tensor<13x1xbf16>) -> tensor<13x1xbf16>
        %491 = ttir.empty() : tensor<13x4xbf16>
        %492 = "ttir.broadcast"(%490, %491) <{broadcast_dimensions = array<i64: 1, 4>}> : (tensor<13x1xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %493 = ttir.empty() : tensor<13x4xbf16>
        %494 = "ttir.div"(%486, %492, %493) : (tensor<13x4xbf16>, tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %495 = ttir.empty() : tensor<13x32xbf16>
        %496 = "ttir.scatter"(%11, %474, %494, %495) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %497 = ttir.empty() : tensor<32x13xbf16>
        %498 = "ttir.permute"(%496, %497) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %499 = ttir.empty() : tensor<32x1x13x1xbf16>
        %500 = "ttir.reshape"(%498, %499) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %501 = ttir.empty() : tensor<32x1x13x1xf32>
        %502 = "ttir.typecast"(%500, %501) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %503 = ttir.empty() : tensor<32x1x13x2880xf32>
        %504 = "ttir.broadcast"(%502, %503) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %505 = ttir.empty() : tensor<32x1x13x2880xf32>
        %506 = "ttir.multiply"(%452, %504, %505) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %507 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %508 = "ttir.typecast"(%506, %507) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %509 = ttir.empty() : tensor<1x13x2880xbf16>
        %510 = "ttir.sum"(%508, %509) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %511 = ttir.empty() : tensor<1x13x2880xbf16>
        %512 = "ttir.add"(%356, %510, %511) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %513 = ttir.empty() : tensor<1x13x2880xf32>
        %514 = "ttir.typecast"(%512, %513) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %515 = ttir.empty() : tensor<1x13x2880xf32>
        %516 = "ttir.pow"(%514, %27, %515) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %517 = ttir.empty() : tensor<1x13xf32>
        %518 = "ttir.sum"(%516, %517) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %519 = ttir.empty() : tensor<1x13xf32>
        %520 = "ttir.multiply"(%518, %4, %519) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %521 = ttir.empty() : tensor<1x13x1xf32>
        %522 = "ttir.reshape"(%520, %521) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %523 = ttir.empty() : tensor<1x13x1xf32>
        %524 = "ttir.add"(%522, %59, %523) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %525 = ttir.empty() : tensor<1x13x1xf32>
        %526 = "ttir.rsqrt"(%524, %525) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %527 = ttir.empty() : tensor<1x13x2880xf32>
        %528 = "ttir.broadcast"(%526, %527) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %529 = ttir.empty() : tensor<1x13x2880xf32>
        %530 = "ttir.multiply"(%514, %528, %529) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %531 = ttir.empty() : tensor<1x13x2880xf32>
        %532 = "ttir.multiply"(%300, %530, %531) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %533 = ttir.empty() : tensor<1x13x2880xbf16>
        %534 = "ttir.typecast"(%532, %533) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %535 = ttir.empty() : tensor<13x2880xbf16>
        %536 = "ttir.reshape"(%534, %535) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %537 = ttir.empty() : tensor<2880x201088xbf16>
        %538 = "ttir.permute"(%arg10, %537) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %539 = "ttir.dot_general"(%536, %538) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
        %540 = ttir.empty() : tensor<1x13x201088xbf16>
        %541 = "ttir.reshape"(%539, %540) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %290, %292, %294, %202, %539, %541 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<2880x4096xbf16>
        %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
        %77 = ttir.empty() : tensor<1x13x4096xbf16>
        %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %79 = ttir.empty() : tensor<1x1x4096xbf16>
        %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %81 = ttir.empty() : tensor<1x13x4096xbf16>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %83 = ttir.empty() : tensor<1x13x4096xbf16>
        %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %85 = ttir.empty() : tensor<1x13x64x64xbf16>
        %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %87 = ttir.empty() : tensor<1x64x13x64xbf16>
        %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %89 = ttir.empty() : tensor<1x64x13x32xbf16>
        %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %91 = ttir.empty() : tensor<1x64x13x32xf32>
        %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %93 = ttir.empty() : tensor<1x32x1xf32>
        %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<2880x512xbf16>
        %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %157 = ttir.empty() : tensor<1x13x512xbf16>
        %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %159 = ttir.empty() : tensor<1x1x512xbf16>
        %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %161 = ttir.empty() : tensor<1x13x512xbf16>
        %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %163 = ttir.empty() : tensor<1x13x512xbf16>
        %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %165 = ttir.empty() : tensor<1x13x8x64xbf16>
        %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %167 = ttir.empty() : tensor<1x8x13x64xbf16>
        %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %169 = ttir.empty() : tensor<1x8x13x32xbf16>
        %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %171 = ttir.empty() : tensor<1x8x13x32xf32>
        %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %173 = ttir.empty() : tensor<1x8x13x32xf32>
        %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %175 = ttir.empty() : tensor<1x8x13x32xf32>
        %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %177 = ttir.empty() : tensor<1x8x13x32xbf16>
        %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %179 = ttir.empty() : tensor<1x8x13x32xbf16>
        %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %181 = ttir.empty() : tensor<1x8x13x32xf32>
        %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %183 = ttir.empty() : tensor<1x8x13x32xf32>
        %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %185 = ttir.empty() : tensor<1x8x13x32xf32>
        %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %187 = ttir.empty() : tensor<1x8x13x32xbf16>
        %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %189 = ttir.empty() : tensor<1x8x13x32xbf16>
        %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %191 = ttir.empty() : tensor<1x8x13x32xf32>
        %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %193 = ttir.empty() : tensor<1x8x13x32xbf16>
        %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %195 = ttir.empty() : tensor<1x8x13x32xf32>
        %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %197 = ttir.empty() : tensor<1x8x13x32xbf16>
        %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %199 = ttir.empty() : tensor<1x8x13x32xbf16>
        %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %201 = ttir.empty() : tensor<1x8x13x64xbf16>
        %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %207 = ttir.empty() : tensor<1x64x13x64xbf16>
        %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %209 = ttir.empty() : tensor<1x64x64x13xbf16>
        %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %211 = ttir.empty() : tensor<64x64x13xbf16>
        %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<2880x512xbf16>
        %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %283 = ttir.empty() : tensor<1x13x512xbf16>
        %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %285 = ttir.empty() : tensor<1x1x512xbf16>
        %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %287 = ttir.empty() : tensor<1x13x512xbf16>
        %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %289 = ttir.empty() : tensor<1x13x512xbf16>
        %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %291 = ttir.empty() : tensor<1x13x8x64xbf16>
        %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %293 = ttir.empty() : tensor<1x8x13x64xbf16>
        %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %295 = ttir.empty() : tensor<2880xf32>
        %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %297 = ttir.empty() : tensor<1x1x2880xf32>
        %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %299 = ttir.empty() : tensor<1x13x2880xf32>
        %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %301 = ttir.empty() : tensor<1x64x13x14xbf16>
        %302 = "ttir.softmax"(%279, %301) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %303 = ttir.empty() : tensor<1x64x13x13xbf16>
        %304 = "ttir.slice_static"(%302, %303) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %305 = ttir.empty() : tensor<64x13x13xbf16>
        %306 = "ttir.reshape"(%304, %305) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %307 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %308 = "ttir.reshape"(%294, %307) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %309 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %310 = "ttir.broadcast"(%308, %309) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %311 = ttir.empty() : tensor<64x13x64xbf16>
        %312 = "ttir.reshape"(%310, %311) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %313 = "ttir.dot_general"(%306, %312) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %314 = ttir.empty() : tensor<1x64x13x64xbf16>
        %315 = "ttir.reshape"(%313, %314) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %316 = ttir.empty() : tensor<13x4096xbf16>
        %317 = ttir.empty() : tensor<1x13x4096xbf16>
        %318 = "ttir.concatenate_heads"(%315, %317) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %319 = "ttir.reshape"(%318, %316) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %320 = ttir.empty() : tensor<4096x2880xbf16>
        %321 = "ttir.permute"(%arg14, %320) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %322 = "ttir.dot_general"(%319, %321) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
        %323 = ttir.empty() : tensor<1x13x2880xbf16>
        %324 = "ttir.reshape"(%322, %323) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %325 = ttir.empty() : tensor<1x1x2880xbf16>
        %326 = "ttir.reshape"(%arg13, %325) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %327 = ttir.empty() : tensor<1x13x2880xbf16>
        %328 = "ttir.broadcast"(%326, %327) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %329 = ttir.empty() : tensor<1x13x2880xbf16>
        %330 = "ttir.add"(%324, %328, %329) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %331 = ttir.empty() : tensor<1x13x2880xbf16>
        %332 = "ttir.add"(%45, %330, %331) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %333 = ttir.empty() : tensor<1x1x1xbf16>
        %334 = "ttir.reshape"(%arg29, %333) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %335 = ttir.empty() : tensor<32x13x2880xbf16>
        %336 = "ttir.broadcast"(%334, %335) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %337 = ttir.empty() : tensor<2880xf32>
        %338 = "ttir.typecast"(%arg21, %337) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %339 = ttir.empty() : tensor<1x1x2880xf32>
        %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %341 = ttir.empty() : tensor<1x13x2880xf32>
        %342 = "ttir.broadcast"(%340, %341) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %343 = ttir.empty() : tensor<1x13x2880xf32>
        %344 = "ttir.typecast"(%332, %343) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %345 = ttir.empty() : tensor<1x13x2880xf32>
        %346 = "ttir.pow"(%344, %27, %345) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %347 = ttir.empty() : tensor<1x13xf32>
        %348 = "ttir.sum"(%346, %347) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %349 = ttir.empty() : tensor<1x13xf32>
        %350 = "ttir.multiply"(%348, %4, %349) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %351 = ttir.empty() : tensor<1x13x1xf32>
        %352 = "ttir.reshape"(%350, %351) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %353 = ttir.empty() : tensor<1x13x1xf32>
        %354 = "ttir.add"(%352, %59, %353) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %355 = ttir.empty() : tensor<1x13x1xf32>
        %356 = "ttir.rsqrt"(%354, %355) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %357 = ttir.empty() : tensor<1x13x2880xf32>
        %358 = "ttir.broadcast"(%356, %357) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %359 = ttir.empty() : tensor<1x13x2880xf32>
        %360 = "ttir.multiply"(%344, %358, %359) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %361 = ttir.empty() : tensor<1x13x2880xf32>
        %362 = "ttir.multiply"(%342, %360, %361) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %363 = ttir.empty() : tensor<1x13x2880xbf16>
        %364 = "ttir.typecast"(%362, %363) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %365 = ttir.empty() : tensor<13x2880xbf16>
        %366 = "ttir.reshape"(%364, %365) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %367 = ttir.empty() : tensor<416x2880xbf16>
        %368 = "ttir.concat"(%366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %367) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %369 = ttir.empty() : tensor<32x13x2880xbf16>
        %370 = "ttir.reshape"(%368, %369) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %371 = "ttir.dot_general"(%370, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
        %372 = ttir.empty() : tensor<32x1x5760xbf16>
        %373 = "ttir.reshape"(%arg27, %372) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %374 = ttir.empty() : tensor<32x13x5760xbf16>
        %375 = "ttir.broadcast"(%373, %374) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %376 = ttir.empty() : tensor<32x13x5760xbf16>
        %377 = "ttir.add"(%371, %375, %376) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %378 = ttir.empty() : tensor<32x13x2880xbf16>
        %379 = "ttir.slice_static"(%377, %378) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %380 = ttir.empty() : tensor<1x1x1xbf16>
        %381 = "ttir.reshape"(%arg25, %380) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %382 = ttir.empty() : tensor<32x13x2880xbf16>
        %383 = "ttir.broadcast"(%381, %382) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %384 = ttir.empty() : tensor<32x13x2880xbf16>
        %385 = "ttir.clamp_tensor"(%379, %336, %383, %384) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.add"(%385, %15, %386) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<32x13x2880xf32>
        %389 = "ttir.typecast"(%387, %388) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %390 = ttir.empty() : tensor<1x1x1xbf16>
        %391 = "ttir.reshape"(%arg26, %390) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xbf16>
        %393 = "ttir.broadcast"(%391, %392) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %394 = ttir.empty() : tensor<32x13x2880xbf16>
        %395 = "ttir.slice_static"(%377, %394) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xbf16>
        %397 = "ttir.clamp_tensor"(%395, %393, %383, %396) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %398 = ttir.empty() : tensor<32x13x2880xf32>
        %399 = "ttir.typecast"(%397, %398) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %400 = ttir.empty() : tensor<1x1x1xf32>
        %401 = "ttir.reshape"(%arg24, %400) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %402 = ttir.empty() : tensor<32x13x2880xf32>
        %403 = "ttir.broadcast"(%401, %402) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %404 = ttir.empty() : tensor<32x13x2880xf32>
        %405 = "ttir.multiply"(%399, %403, %404) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.typecast"(%405, %406) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.sigmoid"(%407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xf32>
        %411 = "ttir.typecast"(%409, %410) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.multiply"(%399, %411, %412) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<32x13x2880xf32>
        %415 = "ttir.multiply"(%389, %413, %414) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.typecast"(%415, %416) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = "ttir.dot_general"(%417, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
        %419 = ttir.empty() : tensor<32x1x2880xbf16>
        %420 = "ttir.reshape"(%arg22, %419) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %421 = ttir.empty() : tensor<32x13x2880xbf16>
        %422 = "ttir.broadcast"(%420, %421) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %423 = ttir.empty() : tensor<32x13x2880xbf16>
        %424 = "ttir.add"(%418, %422, %423) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %425 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %426 = "ttir.reshape"(%424, %425) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %427 = ttir.empty() : tensor<32x1x13x2880xf32>
        %428 = "ttir.typecast"(%426, %427) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %429 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %430 = ttir.empty() : tensor<13x1x1xsi32>
        %431 = "ttir.reshape"(%429, %430) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %432 = ttir.empty() : tensor<13x4x1xsi32>
        %433 = "ttir.broadcast"(%431, %432) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %434 = ttir.empty() : tensor<2880x32xbf16>
        %435 = "ttir.permute"(%arg12, %434) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %436 = "ttir.dot_general"(%366, %435) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
        %437 = ttir.empty() : tensor<1x32xbf16>
        %438 = "ttir.reshape"(%arg11, %437) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %439 = ttir.empty() : tensor<13x32xbf16>
        %440 = "ttir.broadcast"(%438, %439) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %441 = ttir.empty() : tensor<13x32xbf16>
        %442 = "ttir.add"(%436, %440, %441) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %443 = ttir.empty() : tensor<13x32xbf16>
        %444 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%442, %443, %444) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %445 = ttir.empty() : tensor<13x4xsi32>
        %446 = "ttir.slice_static"(%indices, %445) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %447 = ttir.empty() : tensor<13x4x1xsi32>
        %448 = "ttir.reshape"(%446, %447) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %449 = ttir.empty() : tensor<13x4x2xsi32>
        %450 = "ttir.concat"(%433, %448, %449) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %451 = ttir.empty() : tensor<13x4xbf16>
        %452 = "ttir.slice_static"(%values, %451) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %453 = ttir.empty() : tensor<13x4xbf16>
        %454 = "ttir.softmax"(%452, %453) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %455 = ttir.empty() : tensor<13x32xbf16>
        %456 = "ttir.scatter"(%11, %450, %454, %455) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %457 = ttir.empty() : tensor<32x13xbf16>
        %458 = "ttir.permute"(%456, %457) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %459 = ttir.empty() : tensor<32x1x13x1xbf16>
        %460 = "ttir.reshape"(%458, %459) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %461 = ttir.empty() : tensor<32x1x13x1xf32>
        %462 = "ttir.typecast"(%460, %461) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %463 = ttir.empty() : tensor<32x1x13x2880xf32>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %465 = ttir.empty() : tensor<32x1x13x2880xf32>
        %466 = "ttir.multiply"(%428, %464, %465) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %467 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %468 = "ttir.typecast"(%466, %467) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %469 = ttir.empty() : tensor<1x13x2880xbf16>
        %470 = "ttir.sum"(%468, %469) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %471 = ttir.empty() : tensor<1x13x2880xbf16>
        %472 = "ttir.add"(%332, %470, %471) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %473 = ttir.empty() : tensor<1x13x2880xf32>
        %474 = "ttir.typecast"(%472, %473) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %475 = ttir.empty() : tensor<1x13x2880xf32>
        %476 = "ttir.pow"(%474, %27, %475) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %477 = ttir.empty() : tensor<1x13xf32>
        %478 = "ttir.sum"(%476, %477) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %479 = ttir.empty() : tensor<1x13xf32>
        %480 = "ttir.multiply"(%478, %4, %479) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %481 = ttir.empty() : tensor<1x13x1xf32>
        %482 = "ttir.reshape"(%480, %481) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %483 = ttir.empty() : tensor<1x13x1xf32>
        %484 = "ttir.add"(%482, %59, %483) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %485 = ttir.empty() : tensor<1x13x1xf32>
        %486 = "ttir.rsqrt"(%484, %485) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %487 = ttir.empty() : tensor<1x13x2880xf32>
        %488 = "ttir.broadcast"(%486, %487) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %489 = ttir.empty() : tensor<1x13x2880xf32>
        %490 = "ttir.multiply"(%474, %488, %489) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %491 = ttir.empty() : tensor<1x13x2880xf32>
        %492 = "ttir.multiply"(%300, %490, %491) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %493 = ttir.empty() : tensor<1x13x2880xbf16>
        %494 = "ttir.typecast"(%492, %493) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %495 = ttir.empty() : tensor<13x2880xbf16>
        %496 = "ttir.reshape"(%494, %495) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %497 = ttir.empty() : tensor<2880x201088xbf16>
        %498 = "ttir.permute"(%arg10, %497) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %499 = "ttir.dot_general"(%496, %498) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
        %500 = ttir.empty() : tensor<1x13x201088xbf16>
        %501 = "ttir.reshape"(%499, %500) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %290, %292, %294, %202, %499, %501 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRQuantDequantConversion (ttir-quant-dequant-conversion) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<2880x4096xbf16>
        %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
        %77 = ttir.empty() : tensor<1x13x4096xbf16>
        %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %79 = ttir.empty() : tensor<1x1x4096xbf16>
        %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %81 = ttir.empty() : tensor<1x13x4096xbf16>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %83 = ttir.empty() : tensor<1x13x4096xbf16>
        %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %85 = ttir.empty() : tensor<1x13x64x64xbf16>
        %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %87 = ttir.empty() : tensor<1x64x13x64xbf16>
        %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %89 = ttir.empty() : tensor<1x64x13x32xbf16>
        %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %91 = ttir.empty() : tensor<1x64x13x32xf32>
        %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %93 = ttir.empty() : tensor<1x32x1xf32>
        %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<2880x512xbf16>
        %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %157 = ttir.empty() : tensor<1x13x512xbf16>
        %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %159 = ttir.empty() : tensor<1x1x512xbf16>
        %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %161 = ttir.empty() : tensor<1x13x512xbf16>
        %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %163 = ttir.empty() : tensor<1x13x512xbf16>
        %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %165 = ttir.empty() : tensor<1x13x8x64xbf16>
        %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %167 = ttir.empty() : tensor<1x8x13x64xbf16>
        %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %169 = ttir.empty() : tensor<1x8x13x32xbf16>
        %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %171 = ttir.empty() : tensor<1x8x13x32xf32>
        %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %173 = ttir.empty() : tensor<1x8x13x32xf32>
        %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %175 = ttir.empty() : tensor<1x8x13x32xf32>
        %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %177 = ttir.empty() : tensor<1x8x13x32xbf16>
        %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %179 = ttir.empty() : tensor<1x8x13x32xbf16>
        %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %181 = ttir.empty() : tensor<1x8x13x32xf32>
        %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %183 = ttir.empty() : tensor<1x8x13x32xf32>
        %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %185 = ttir.empty() : tensor<1x8x13x32xf32>
        %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %187 = ttir.empty() : tensor<1x8x13x32xbf16>
        %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %189 = ttir.empty() : tensor<1x8x13x32xbf16>
        %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %191 = ttir.empty() : tensor<1x8x13x32xf32>
        %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %193 = ttir.empty() : tensor<1x8x13x32xbf16>
        %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %195 = ttir.empty() : tensor<1x8x13x32xf32>
        %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %197 = ttir.empty() : tensor<1x8x13x32xbf16>
        %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %199 = ttir.empty() : tensor<1x8x13x32xbf16>
        %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %201 = ttir.empty() : tensor<1x8x13x64xbf16>
        %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %207 = ttir.empty() : tensor<1x64x13x64xbf16>
        %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %209 = ttir.empty() : tensor<1x64x64x13xbf16>
        %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %211 = ttir.empty() : tensor<64x64x13xbf16>
        %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<2880x512xbf16>
        %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %283 = ttir.empty() : tensor<1x13x512xbf16>
        %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %285 = ttir.empty() : tensor<1x1x512xbf16>
        %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %287 = ttir.empty() : tensor<1x13x512xbf16>
        %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %289 = ttir.empty() : tensor<1x13x512xbf16>
        %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %291 = ttir.empty() : tensor<1x13x8x64xbf16>
        %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %293 = ttir.empty() : tensor<1x8x13x64xbf16>
        %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %295 = ttir.empty() : tensor<2880xf32>
        %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %297 = ttir.empty() : tensor<1x1x2880xf32>
        %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %299 = ttir.empty() : tensor<1x13x2880xf32>
        %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %301 = ttir.empty() : tensor<1x64x13x14xbf16>
        %302 = "ttir.softmax"(%279, %301) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %303 = ttir.empty() : tensor<1x64x13x13xbf16>
        %304 = "ttir.slice_static"(%302, %303) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %305 = ttir.empty() : tensor<64x13x13xbf16>
        %306 = "ttir.reshape"(%304, %305) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %307 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %308 = "ttir.reshape"(%294, %307) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %309 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %310 = "ttir.broadcast"(%308, %309) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %311 = ttir.empty() : tensor<64x13x64xbf16>
        %312 = "ttir.reshape"(%310, %311) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %313 = "ttir.dot_general"(%306, %312) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %314 = ttir.empty() : tensor<1x64x13x64xbf16>
        %315 = "ttir.reshape"(%313, %314) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %316 = ttir.empty() : tensor<13x4096xbf16>
        %317 = ttir.empty() : tensor<1x13x4096xbf16>
        %318 = "ttir.concatenate_heads"(%315, %317) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %319 = "ttir.reshape"(%318, %316) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %320 = ttir.empty() : tensor<4096x2880xbf16>
        %321 = "ttir.permute"(%arg14, %320) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %322 = "ttir.dot_general"(%319, %321) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
        %323 = ttir.empty() : tensor<1x13x2880xbf16>
        %324 = "ttir.reshape"(%322, %323) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %325 = ttir.empty() : tensor<1x1x2880xbf16>
        %326 = "ttir.reshape"(%arg13, %325) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %327 = ttir.empty() : tensor<1x13x2880xbf16>
        %328 = "ttir.broadcast"(%326, %327) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %329 = ttir.empty() : tensor<1x13x2880xbf16>
        %330 = "ttir.add"(%324, %328, %329) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %331 = ttir.empty() : tensor<1x13x2880xbf16>
        %332 = "ttir.add"(%45, %330, %331) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %333 = ttir.empty() : tensor<1x1x1xbf16>
        %334 = "ttir.reshape"(%arg29, %333) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %335 = ttir.empty() : tensor<32x13x2880xbf16>
        %336 = "ttir.broadcast"(%334, %335) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %337 = ttir.empty() : tensor<2880xf32>
        %338 = "ttir.typecast"(%arg21, %337) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %339 = ttir.empty() : tensor<1x1x2880xf32>
        %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %341 = ttir.empty() : tensor<1x13x2880xf32>
        %342 = "ttir.broadcast"(%340, %341) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %343 = ttir.empty() : tensor<1x13x2880xf32>
        %344 = "ttir.typecast"(%332, %343) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %345 = ttir.empty() : tensor<1x13x2880xf32>
        %346 = "ttir.pow"(%344, %27, %345) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %347 = ttir.empty() : tensor<1x13xf32>
        %348 = "ttir.sum"(%346, %347) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %349 = ttir.empty() : tensor<1x13xf32>
        %350 = "ttir.multiply"(%348, %4, %349) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %351 = ttir.empty() : tensor<1x13x1xf32>
        %352 = "ttir.reshape"(%350, %351) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %353 = ttir.empty() : tensor<1x13x1xf32>
        %354 = "ttir.add"(%352, %59, %353) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %355 = ttir.empty() : tensor<1x13x1xf32>
        %356 = "ttir.rsqrt"(%354, %355) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %357 = ttir.empty() : tensor<1x13x2880xf32>
        %358 = "ttir.broadcast"(%356, %357) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %359 = ttir.empty() : tensor<1x13x2880xf32>
        %360 = "ttir.multiply"(%344, %358, %359) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %361 = ttir.empty() : tensor<1x13x2880xf32>
        %362 = "ttir.multiply"(%342, %360, %361) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %363 = ttir.empty() : tensor<1x13x2880xbf16>
        %364 = "ttir.typecast"(%362, %363) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %365 = ttir.empty() : tensor<13x2880xbf16>
        %366 = "ttir.reshape"(%364, %365) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %367 = ttir.empty() : tensor<416x2880xbf16>
        %368 = "ttir.concat"(%366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %367) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %369 = ttir.empty() : tensor<32x13x2880xbf16>
        %370 = "ttir.reshape"(%368, %369) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %371 = "ttir.dot_general"(%370, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
        %372 = ttir.empty() : tensor<32x1x5760xbf16>
        %373 = "ttir.reshape"(%arg27, %372) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %374 = ttir.empty() : tensor<32x13x5760xbf16>
        %375 = "ttir.broadcast"(%373, %374) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %376 = ttir.empty() : tensor<32x13x5760xbf16>
        %377 = "ttir.add"(%371, %375, %376) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %378 = ttir.empty() : tensor<32x13x2880xbf16>
        %379 = "ttir.slice_static"(%377, %378) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %380 = ttir.empty() : tensor<1x1x1xbf16>
        %381 = "ttir.reshape"(%arg25, %380) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %382 = ttir.empty() : tensor<32x13x2880xbf16>
        %383 = "ttir.broadcast"(%381, %382) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %384 = ttir.empty() : tensor<32x13x2880xbf16>
        %385 = "ttir.clamp_tensor"(%379, %336, %383, %384) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.add"(%385, %15, %386) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<32x13x2880xf32>
        %389 = "ttir.typecast"(%387, %388) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %390 = ttir.empty() : tensor<1x1x1xbf16>
        %391 = "ttir.reshape"(%arg26, %390) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xbf16>
        %393 = "ttir.broadcast"(%391, %392) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %394 = ttir.empty() : tensor<32x13x2880xbf16>
        %395 = "ttir.slice_static"(%377, %394) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xbf16>
        %397 = "ttir.clamp_tensor"(%395, %393, %383, %396) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %398 = ttir.empty() : tensor<32x13x2880xf32>
        %399 = "ttir.typecast"(%397, %398) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %400 = ttir.empty() : tensor<1x1x1xf32>
        %401 = "ttir.reshape"(%arg24, %400) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %402 = ttir.empty() : tensor<32x13x2880xf32>
        %403 = "ttir.broadcast"(%401, %402) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %404 = ttir.empty() : tensor<32x13x2880xf32>
        %405 = "ttir.multiply"(%399, %403, %404) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.typecast"(%405, %406) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.sigmoid"(%407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xf32>
        %411 = "ttir.typecast"(%409, %410) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.multiply"(%399, %411, %412) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<32x13x2880xf32>
        %415 = "ttir.multiply"(%389, %413, %414) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.typecast"(%415, %416) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = "ttir.dot_general"(%417, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
        %419 = ttir.empty() : tensor<32x1x2880xbf16>
        %420 = "ttir.reshape"(%arg22, %419) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %421 = ttir.empty() : tensor<32x13x2880xbf16>
        %422 = "ttir.broadcast"(%420, %421) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %423 = ttir.empty() : tensor<32x13x2880xbf16>
        %424 = "ttir.add"(%418, %422, %423) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %425 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %426 = "ttir.reshape"(%424, %425) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %427 = ttir.empty() : tensor<32x1x13x2880xf32>
        %428 = "ttir.typecast"(%426, %427) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %429 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %430 = ttir.empty() : tensor<13x1x1xsi32>
        %431 = "ttir.reshape"(%429, %430) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %432 = ttir.empty() : tensor<13x4x1xsi32>
        %433 = "ttir.broadcast"(%431, %432) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %434 = ttir.empty() : tensor<2880x32xbf16>
        %435 = "ttir.permute"(%arg12, %434) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %436 = "ttir.dot_general"(%366, %435) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
        %437 = ttir.empty() : tensor<1x32xbf16>
        %438 = "ttir.reshape"(%arg11, %437) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %439 = ttir.empty() : tensor<13x32xbf16>
        %440 = "ttir.broadcast"(%438, %439) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %441 = ttir.empty() : tensor<13x32xbf16>
        %442 = "ttir.add"(%436, %440, %441) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %443 = ttir.empty() : tensor<13x32xbf16>
        %444 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%442, %443, %444) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %445 = ttir.empty() : tensor<13x4xsi32>
        %446 = "ttir.slice_static"(%indices, %445) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %447 = ttir.empty() : tensor<13x4x1xsi32>
        %448 = "ttir.reshape"(%446, %447) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %449 = ttir.empty() : tensor<13x4x2xsi32>
        %450 = "ttir.concat"(%433, %448, %449) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %451 = ttir.empty() : tensor<13x4xbf16>
        %452 = "ttir.slice_static"(%values, %451) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %453 = ttir.empty() : tensor<13x4xbf16>
        %454 = "ttir.softmax"(%452, %453) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %455 = ttir.empty() : tensor<13x32xbf16>
        %456 = "ttir.scatter"(%11, %450, %454, %455) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %457 = ttir.empty() : tensor<32x13xbf16>
        %458 = "ttir.permute"(%456, %457) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %459 = ttir.empty() : tensor<32x1x13x1xbf16>
        %460 = "ttir.reshape"(%458, %459) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %461 = ttir.empty() : tensor<32x1x13x1xf32>
        %462 = "ttir.typecast"(%460, %461) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %463 = ttir.empty() : tensor<32x1x13x2880xf32>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %465 = ttir.empty() : tensor<32x1x13x2880xf32>
        %466 = "ttir.multiply"(%428, %464, %465) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %467 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %468 = "ttir.typecast"(%466, %467) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %469 = ttir.empty() : tensor<1x13x2880xbf16>
        %470 = "ttir.sum"(%468, %469) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %471 = ttir.empty() : tensor<1x13x2880xbf16>
        %472 = "ttir.add"(%332, %470, %471) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %473 = ttir.empty() : tensor<1x13x2880xf32>
        %474 = "ttir.typecast"(%472, %473) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %475 = ttir.empty() : tensor<1x13x2880xf32>
        %476 = "ttir.pow"(%474, %27, %475) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %477 = ttir.empty() : tensor<1x13xf32>
        %478 = "ttir.sum"(%476, %477) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %479 = ttir.empty() : tensor<1x13xf32>
        %480 = "ttir.multiply"(%478, %4, %479) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %481 = ttir.empty() : tensor<1x13x1xf32>
        %482 = "ttir.reshape"(%480, %481) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %483 = ttir.empty() : tensor<1x13x1xf32>
        %484 = "ttir.add"(%482, %59, %483) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %485 = ttir.empty() : tensor<1x13x1xf32>
        %486 = "ttir.rsqrt"(%484, %485) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %487 = ttir.empty() : tensor<1x13x2880xf32>
        %488 = "ttir.broadcast"(%486, %487) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %489 = ttir.empty() : tensor<1x13x2880xf32>
        %490 = "ttir.multiply"(%474, %488, %489) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %491 = ttir.empty() : tensor<1x13x2880xf32>
        %492 = "ttir.multiply"(%300, %490, %491) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %493 = ttir.empty() : tensor<1x13x2880xbf16>
        %494 = "ttir.typecast"(%492, %493) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %495 = ttir.empty() : tensor<13x2880xbf16>
        %496 = "ttir.reshape"(%494, %495) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %497 = ttir.empty() : tensor<2880x201088xbf16>
        %498 = "ttir.permute"(%arg10, %497) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %499 = "ttir.dot_general"(%496, %498) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
        %500 = ttir.empty() : tensor<1x13x201088xbf16>
        %501 = "ttir.reshape"(%499, %500) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %290, %292, %294, %202, %499, %501 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.gather"(%arg4, %41, %42) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2880>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<201088x2880xbf16>, tensor<13xui32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<2880x4096xbf16>
        %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %76 = "ttir.dot_general"(%73, %75) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
        %77 = ttir.empty() : tensor<1x13x4096xbf16>
        %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %79 = ttir.empty() : tensor<1x1x4096xbf16>
        %80 = "ttir.reshape"(%arg19, %79) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %81 = ttir.empty() : tensor<1x13x4096xbf16>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %83 = ttir.empty() : tensor<1x13x4096xbf16>
        %84 = "ttir.add"(%78, %82, %83) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %85 = ttir.empty() : tensor<1x13x64x64xbf16>
        %86 = "ttir.reshape"(%84, %85) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %87 = ttir.empty() : tensor<1x64x13x64xbf16>
        %88 = "ttir.permute"(%86, %87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %89 = ttir.empty() : tensor<1x64x13x32xbf16>
        %90 = "ttir.slice_static"(%88, %89) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %91 = ttir.empty() : tensor<1x64x13x32xf32>
        %92 = "ttir.typecast"(%90, %91) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %93 = ttir.empty() : tensor<1x32x1xf32>
        %94 = "ttir.reshape"(%arg7, %93) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %95 = "ttir.dot_general"(%94, %0) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%92, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%88, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%92, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<2880x512xbf16>
        %155 = "ttir.permute"(%arg9, %154) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %156 = "ttir.dot_general"(%73, %155) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %157 = ttir.empty() : tensor<1x13x512xbf16>
        %158 = "ttir.reshape"(%156, %157) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %159 = ttir.empty() : tensor<1x1x512xbf16>
        %160 = "ttir.reshape"(%arg8, %159) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %161 = ttir.empty() : tensor<1x13x512xbf16>
        %162 = "ttir.broadcast"(%160, %161) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %163 = ttir.empty() : tensor<1x13x512xbf16>
        %164 = "ttir.add"(%158, %162, %163) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %165 = ttir.empty() : tensor<1x13x8x64xbf16>
        %166 = "ttir.reshape"(%164, %165) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %167 = ttir.empty() : tensor<1x8x13x64xbf16>
        %168 = "ttir.permute"(%166, %167) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %169 = ttir.empty() : tensor<1x8x13x32xbf16>
        %170 = "ttir.slice_static"(%168, %169) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %171 = ttir.empty() : tensor<1x8x13x32xf32>
        %172 = "ttir.typecast"(%170, %171) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %173 = ttir.empty() : tensor<1x8x13x32xf32>
        %174 = "ttir.broadcast"(%111, %173) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %175 = ttir.empty() : tensor<1x8x13x32xf32>
        %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %177 = ttir.empty() : tensor<1x8x13x32xbf16>
        %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %179 = ttir.empty() : tensor<1x8x13x32xbf16>
        %180 = "ttir.slice_static"(%168, %179) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %181 = ttir.empty() : tensor<1x8x13x32xf32>
        %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %183 = ttir.empty() : tensor<1x8x13x32xf32>
        %184 = "ttir.broadcast"(%131, %183) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %185 = ttir.empty() : tensor<1x8x13x32xf32>
        %186 = "ttir.multiply"(%182, %184, %185) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %187 = ttir.empty() : tensor<1x8x13x32xbf16>
        %188 = "ttir.typecast"(%186, %187) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %189 = ttir.empty() : tensor<1x8x13x32xbf16>
        %190 = "ttir.subtract"(%178, %188, %189) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %191 = ttir.empty() : tensor<1x8x13x32xf32>
        %192 = "ttir.multiply"(%182, %174, %191) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %193 = ttir.empty() : tensor<1x8x13x32xbf16>
        %194 = "ttir.typecast"(%192, %193) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %195 = ttir.empty() : tensor<1x8x13x32xf32>
        %196 = "ttir.multiply"(%172, %184, %195) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %197 = ttir.empty() : tensor<1x8x13x32xbf16>
        %198 = "ttir.typecast"(%196, %197) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %199 = ttir.empty() : tensor<1x8x13x32xbf16>
        %200 = "ttir.add"(%194, %198, %199) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %201 = ttir.empty() : tensor<1x8x13x64xbf16>
        %202 = "ttir.concat"(%190, %200, %201) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %203 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %205 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %207 = ttir.empty() : tensor<1x64x13x64xbf16>
        %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %209 = ttir.empty() : tensor<1x64x64x13xbf16>
        %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %211 = ttir.empty() : tensor<64x64x13xbf16>
        %212 = "ttir.reshape"(%210, %211) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %213 = "ttir.dot_general"(%153, %212) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<2880x512xbf16>
        %281 = "ttir.permute"(%arg1, %280) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %282 = "ttir.dot_general"(%73, %281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
        %283 = ttir.empty() : tensor<1x13x512xbf16>
        %284 = "ttir.reshape"(%282, %283) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %285 = ttir.empty() : tensor<1x1x512xbf16>
        %286 = "ttir.reshape"(%arg0, %285) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %287 = ttir.empty() : tensor<1x13x512xbf16>
        %288 = "ttir.broadcast"(%286, %287) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %289 = ttir.empty() : tensor<1x13x512xbf16>
        %290 = "ttir.add"(%284, %288, %289) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %291 = ttir.empty() : tensor<1x13x8x64xbf16>
        %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %293 = ttir.empty() : tensor<1x8x13x64xbf16>
        %294 = "ttir.permute"(%292, %293) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %295 = ttir.empty() : tensor<2880xf32>
        %296 = "ttir.typecast"(%arg30, %295) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %297 = ttir.empty() : tensor<1x1x2880xf32>
        %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %299 = ttir.empty() : tensor<1x13x2880xf32>
        %300 = "ttir.broadcast"(%298, %299) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %301 = ttir.empty() : tensor<1x64x13x14xbf16>
        %302 = "ttir.softmax"(%279, %301) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %303 = ttir.empty() : tensor<1x64x13x13xbf16>
        %304 = "ttir.slice_static"(%302, %303) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %305 = ttir.empty() : tensor<64x13x13xbf16>
        %306 = "ttir.reshape"(%304, %305) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %307 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %308 = "ttir.reshape"(%294, %307) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %309 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %310 = "ttir.broadcast"(%308, %309) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %311 = ttir.empty() : tensor<64x13x64xbf16>
        %312 = "ttir.reshape"(%310, %311) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %313 = "ttir.dot_general"(%306, %312) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %314 = ttir.empty() : tensor<1x64x13x64xbf16>
        %315 = "ttir.reshape"(%313, %314) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %316 = ttir.empty() : tensor<13x4096xbf16>
        %317 = ttir.empty() : tensor<1x13x4096xbf16>
        %318 = "ttir.concatenate_heads"(%315, %317) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %319 = "ttir.reshape"(%318, %316) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %320 = ttir.empty() : tensor<4096x2880xbf16>
        %321 = "ttir.permute"(%arg14, %320) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %322 = "ttir.dot_general"(%319, %321) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
        %323 = ttir.empty() : tensor<1x13x2880xbf16>
        %324 = "ttir.reshape"(%322, %323) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %325 = ttir.empty() : tensor<1x1x2880xbf16>
        %326 = "ttir.reshape"(%arg13, %325) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %327 = ttir.empty() : tensor<1x13x2880xbf16>
        %328 = "ttir.broadcast"(%326, %327) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %329 = ttir.empty() : tensor<1x13x2880xbf16>
        %330 = "ttir.add"(%324, %328, %329) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %331 = ttir.empty() : tensor<1x13x2880xbf16>
        %332 = "ttir.add"(%45, %330, %331) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %333 = ttir.empty() : tensor<1x1x1xbf16>
        %334 = "ttir.reshape"(%arg29, %333) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %335 = ttir.empty() : tensor<32x13x2880xbf16>
        %336 = "ttir.broadcast"(%334, %335) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %337 = ttir.empty() : tensor<2880xf32>
        %338 = "ttir.typecast"(%arg21, %337) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %339 = ttir.empty() : tensor<1x1x2880xf32>
        %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %341 = ttir.empty() : tensor<1x13x2880xf32>
        %342 = "ttir.broadcast"(%340, %341) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %343 = ttir.empty() : tensor<1x13x2880xf32>
        %344 = "ttir.typecast"(%332, %343) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %345 = ttir.empty() : tensor<1x13x2880xf32>
        %346 = "ttir.pow"(%344, %27, %345) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %347 = ttir.empty() : tensor<1x13xf32>
        %348 = "ttir.sum"(%346, %347) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %349 = ttir.empty() : tensor<1x13xf32>
        %350 = "ttir.multiply"(%348, %4, %349) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %351 = ttir.empty() : tensor<1x13x1xf32>
        %352 = "ttir.reshape"(%350, %351) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %353 = ttir.empty() : tensor<1x13x1xf32>
        %354 = "ttir.add"(%352, %59, %353) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %355 = ttir.empty() : tensor<1x13x1xf32>
        %356 = "ttir.rsqrt"(%354, %355) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %357 = ttir.empty() : tensor<1x13x2880xf32>
        %358 = "ttir.broadcast"(%356, %357) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %359 = ttir.empty() : tensor<1x13x2880xf32>
        %360 = "ttir.multiply"(%344, %358, %359) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %361 = ttir.empty() : tensor<1x13x2880xf32>
        %362 = "ttir.multiply"(%342, %360, %361) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %363 = ttir.empty() : tensor<1x13x2880xbf16>
        %364 = "ttir.typecast"(%362, %363) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %365 = ttir.empty() : tensor<13x2880xbf16>
        %366 = "ttir.reshape"(%364, %365) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %367 = ttir.empty() : tensor<416x2880xbf16>
        %368 = "ttir.concat"(%366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %366, %367) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %369 = ttir.empty() : tensor<32x13x2880xbf16>
        %370 = "ttir.reshape"(%368, %369) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %371 = "ttir.dot_general"(%370, %arg28) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
        %372 = ttir.empty() : tensor<32x1x5760xbf16>
        %373 = "ttir.reshape"(%arg27, %372) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %374 = ttir.empty() : tensor<32x13x5760xbf16>
        %375 = "ttir.broadcast"(%373, %374) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %376 = ttir.empty() : tensor<32x13x5760xbf16>
        %377 = "ttir.add"(%371, %375, %376) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %378 = ttir.empty() : tensor<32x13x2880xbf16>
        %379 = "ttir.slice_static"(%377, %378) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %380 = ttir.empty() : tensor<1x1x1xbf16>
        %381 = "ttir.reshape"(%arg25, %380) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %382 = ttir.empty() : tensor<32x13x2880xbf16>
        %383 = "ttir.broadcast"(%381, %382) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %384 = ttir.empty() : tensor<32x13x2880xbf16>
        %385 = "ttir.clamp_tensor"(%379, %336, %383, %384) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.add"(%385, %15, %386) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<32x13x2880xf32>
        %389 = "ttir.typecast"(%387, %388) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %390 = ttir.empty() : tensor<1x1x1xbf16>
        %391 = "ttir.reshape"(%arg26, %390) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xbf16>
        %393 = "ttir.broadcast"(%391, %392) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %394 = ttir.empty() : tensor<32x13x2880xbf16>
        %395 = "ttir.slice_static"(%377, %394) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xbf16>
        %397 = "ttir.clamp_tensor"(%395, %393, %383, %396) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %398 = ttir.empty() : tensor<32x13x2880xf32>
        %399 = "ttir.typecast"(%397, %398) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %400 = ttir.empty() : tensor<1x1x1xf32>
        %401 = "ttir.reshape"(%arg24, %400) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %402 = ttir.empty() : tensor<32x13x2880xf32>
        %403 = "ttir.broadcast"(%401, %402) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %404 = ttir.empty() : tensor<32x13x2880xf32>
        %405 = "ttir.multiply"(%399, %403, %404) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.typecast"(%405, %406) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.sigmoid"(%407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xf32>
        %411 = "ttir.typecast"(%409, %410) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.multiply"(%399, %411, %412) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<32x13x2880xf32>
        %415 = "ttir.multiply"(%389, %413, %414) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.typecast"(%415, %416) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = "ttir.dot_general"(%417, %arg23) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
        %419 = ttir.empty() : tensor<32x1x2880xbf16>
        %420 = "ttir.reshape"(%arg22, %419) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %421 = ttir.empty() : tensor<32x13x2880xbf16>
        %422 = "ttir.broadcast"(%420, %421) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %423 = ttir.empty() : tensor<32x13x2880xbf16>
        %424 = "ttir.add"(%418, %422, %423) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %425 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %426 = "ttir.reshape"(%424, %425) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %427 = ttir.empty() : tensor<32x1x13x2880xf32>
        %428 = "ttir.typecast"(%426, %427) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %429 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %430 = ttir.empty() : tensor<13x1x1xsi32>
        %431 = "ttir.reshape"(%429, %430) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %432 = ttir.empty() : tensor<13x4x1xsi32>
        %433 = "ttir.broadcast"(%431, %432) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %434 = ttir.empty() : tensor<2880x32xbf16>
        %435 = "ttir.permute"(%arg12, %434) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %436 = "ttir.dot_general"(%366, %435) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
        %437 = ttir.empty() : tensor<1x32xbf16>
        %438 = "ttir.reshape"(%arg11, %437) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %439 = ttir.empty() : tensor<13x32xbf16>
        %440 = "ttir.broadcast"(%438, %439) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %441 = ttir.empty() : tensor<13x32xbf16>
        %442 = "ttir.add"(%436, %440, %441) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %443 = ttir.empty() : tensor<13x32xbf16>
        %444 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%442, %443, %444) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %445 = ttir.empty() : tensor<13x4xsi32>
        %446 = "ttir.slice_static"(%indices, %445) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %447 = ttir.empty() : tensor<13x4x1xsi32>
        %448 = "ttir.reshape"(%446, %447) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %449 = ttir.empty() : tensor<13x4x2xsi32>
        %450 = "ttir.concat"(%433, %448, %449) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %451 = ttir.empty() : tensor<13x4xbf16>
        %452 = "ttir.slice_static"(%values, %451) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %453 = ttir.empty() : tensor<13x4xbf16>
        %454 = "ttir.softmax"(%452, %453) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %455 = ttir.empty() : tensor<13x32xbf16>
        %456 = "ttir.scatter"(%11, %450, %454, %455) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %457 = ttir.empty() : tensor<32x13xbf16>
        %458 = "ttir.permute"(%456, %457) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %459 = ttir.empty() : tensor<32x1x13x1xbf16>
        %460 = "ttir.reshape"(%458, %459) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %461 = ttir.empty() : tensor<32x1x13x1xf32>
        %462 = "ttir.typecast"(%460, %461) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %463 = ttir.empty() : tensor<32x1x13x2880xf32>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %465 = ttir.empty() : tensor<32x1x13x2880xf32>
        %466 = "ttir.multiply"(%428, %464, %465) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %467 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %468 = "ttir.typecast"(%466, %467) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %469 = ttir.empty() : tensor<1x13x2880xbf16>
        %470 = "ttir.sum"(%468, %469) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %471 = ttir.empty() : tensor<1x13x2880xbf16>
        %472 = "ttir.add"(%332, %470, %471) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %473 = ttir.empty() : tensor<1x13x2880xf32>
        %474 = "ttir.typecast"(%472, %473) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %475 = ttir.empty() : tensor<1x13x2880xf32>
        %476 = "ttir.pow"(%474, %27, %475) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %477 = ttir.empty() : tensor<1x13xf32>
        %478 = "ttir.sum"(%476, %477) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %479 = ttir.empty() : tensor<1x13xf32>
        %480 = "ttir.multiply"(%478, %4, %479) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %481 = ttir.empty() : tensor<1x13x1xf32>
        %482 = "ttir.reshape"(%480, %481) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %483 = ttir.empty() : tensor<1x13x1xf32>
        %484 = "ttir.add"(%482, %59, %483) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %485 = ttir.empty() : tensor<1x13x1xf32>
        %486 = "ttir.rsqrt"(%484, %485) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %487 = ttir.empty() : tensor<1x13x2880xf32>
        %488 = "ttir.broadcast"(%486, %487) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %489 = ttir.empty() : tensor<1x13x2880xf32>
        %490 = "ttir.multiply"(%474, %488, %489) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %491 = ttir.empty() : tensor<1x13x2880xf32>
        %492 = "ttir.multiply"(%300, %490, %491) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %493 = ttir.empty() : tensor<1x13x2880xbf16>
        %494 = "ttir.typecast"(%492, %493) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %495 = ttir.empty() : tensor<13x2880xbf16>
        %496 = "ttir.reshape"(%494, %495) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %497 = ttir.empty() : tensor<2880x201088xbf16>
        %498 = "ttir.permute"(%arg10, %497) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %499 = "ttir.dot_general"(%496, %498) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
        %500 = ttir.empty() : tensor<1x13x201088xbf16>
        %501 = "ttir.reshape"(%499, %500) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %290, %292, %294, %202, %499, %501 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = ttir.empty() : tensor<201088x2880xbf16>
        %44 = "ttir.permute"(%arg4, %43) <{permutation = array<i64: 0, 1>}> : (tensor<201088x2880xbf16>, tensor<201088x2880xbf16>) -> tensor<201088x2880xbf16>
        %45 = ttir.empty() : tensor<201088x2880xbf16>
        %46 = "ttir.reshape"(%44, %45) <{shape = [201088 : i32, 2880 : i32]}> : (tensor<201088x2880xbf16>, tensor<201088x2880xbf16>) -> tensor<201088x2880xbf16>
        %47 = ttir.empty() : tensor<13x2880xbf16>
        %48 = "ttir.embedding"(%41, %46, %47) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %49 = ttir.empty() : tensor<13x2880xbf16>
        %50 = "ttir.reshape"(%48, %49) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %51 = ttir.empty() : tensor<13x2880xbf16>
        %52 = "ttir.permute"(%50, %51) <{permutation = array<i64: 0, 1>}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %53 = ttir.empty() : tensor<1x13x2880xbf16>
        %54 = "ttir.reshape"(%52, %53) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %55 = ttir.empty() : tensor<1x13x2880xf32>
        %56 = "ttir.typecast"(%54, %55) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %57 = ttir.empty() : tensor<1x13x2880xf32>
        %58 = "ttir.pow"(%56, %27, %57) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %59 = ttir.empty() : tensor<1x13xf32>
        %60 = "ttir.sum"(%58, %59) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %61 = ttir.empty() : tensor<1x13xf32>
        %62 = "ttir.multiply"(%60, %4, %61) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %63 = ttir.empty() : tensor<1x13x1xf32>
        %64 = "ttir.reshape"(%62, %63) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %65 = ttir.empty() : tensor<1x1x1xf32>
        %66 = "ttir.reshape"(%arg2, %65) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %67 = ttir.empty() : tensor<1x13x1xf32>
        %68 = "ttir.broadcast"(%66, %67) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %69 = ttir.empty() : tensor<1x13x1xf32>
        %70 = "ttir.add"(%64, %68, %69) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %71 = ttir.empty() : tensor<1x13x1xf32>
        %72 = "ttir.rsqrt"(%70, %71) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %73 = ttir.empty() : tensor<1x13x2880xf32>
        %74 = "ttir.broadcast"(%72, %73) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %75 = ttir.empty() : tensor<1x13x2880xf32>
        %76 = "ttir.multiply"(%56, %74, %75) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %77 = ttir.empty() : tensor<1x13x2880xf32>
        %78 = "ttir.multiply"(%37, %76, %77) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %79 = ttir.empty() : tensor<1x13x2880xbf16>
        %80 = "ttir.typecast"(%78, %79) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %81 = ttir.empty() : tensor<13x2880xbf16>
        %82 = "ttir.reshape"(%80, %81) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %83 = ttir.empty() : tensor<2880x4096xbf16>
        %84 = "ttir.permute"(%arg20, %83) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %85 = ttir.empty() : tensor<13x2880xbf16>
        %86 = "ttir.permute"(%82, %85) <{permutation = array<i64: 0, 1>}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %87 = ttir.empty() : tensor<2880x4096xbf16>
        %88 = "ttir.permute"(%84, %87) <{permutation = array<i64: 0, 1>}> : (tensor<2880x4096xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %89 = ttir.empty() : tensor<13x2880xbf16>
        %90 = "ttir.reshape"(%86, %89) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %91 = ttir.empty() : tensor<2880x4096xbf16>
        %92 = "ttir.reshape"(%88, %91) <{shape = [2880 : i32, 4096 : i32]}> : (tensor<2880x4096xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %93 = ttir.empty() : tensor<13x4096xbf16>
        %94 = "ttir.matmul"(%90, %92, %93) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %95 = ttir.empty() : tensor<13x4096xbf16>
        %96 = "ttir.reshape"(%94, %95) <{shape = [13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %97 = ttir.empty() : tensor<1x13x4096xbf16>
        %98 = "ttir.reshape"(%96, %97) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %99 = ttir.empty() : tensor<1x1x4096xbf16>
        %100 = "ttir.reshape"(%arg19, %99) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %101 = ttir.empty() : tensor<1x13x4096xbf16>
        %102 = "ttir.broadcast"(%100, %101) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %103 = ttir.empty() : tensor<1x13x4096xbf16>
        %104 = "ttir.add"(%98, %102, %103) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %105 = ttir.empty() : tensor<1x13x64x64xbf16>
        %106 = "ttir.reshape"(%104, %105) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %107 = ttir.empty() : tensor<1x64x13x64xbf16>
        %108 = "ttir.permute"(%106, %107) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %109 = ttir.empty() : tensor<1x64x13x32xbf16>
        %110 = "ttir.slice_static"(%108, %109) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %111 = ttir.empty() : tensor<1x64x13x32xf32>
        %112 = "ttir.typecast"(%110, %111) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %113 = ttir.empty() : tensor<1x32x1xf32>
        %114 = "ttir.reshape"(%arg7, %113) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %115 = ttir.empty() : tensor<1x32x1xf32>
        %116 = "ttir.permute"(%114, %115) <{permutation = array<i64: 0, 1, 2>}> : (tensor<1x32x1xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %117 = ttir.empty() : tensor<1x1x13xf32>
        %118 = "ttir.permute"(%0, %117) <{permutation = array<i64: 0, 1, 2>}> : (tensor<1x1x13xf32>, tensor<1x1x13xf32>) -> tensor<1x1x13xf32>
        %119 = ttir.empty() : tensor<1x32x1xf32>
        %120 = "ttir.reshape"(%116, %119) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<1x32x1xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %121 = ttir.empty() : tensor<1x1x13xf32>
        %122 = "ttir.reshape"(%118, %121) <{shape = [1 : i32, 1 : i32, 13 : i32]}> : (tensor<1x1x13xf32>, tensor<1x1x13xf32>) -> tensor<1x1x13xf32>
        %123 = ttir.empty() : tensor<1x32x13xf32>
        %124 = "ttir.matmul"(%120, %122, %123) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %125 = ttir.empty() : tensor<1x32x13xf32>
        %126 = "ttir.reshape"(%124, %125) <{shape = [1 : i32, 32 : i32, 13 : i32]}> : (tensor<1x32x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %127 = ttir.empty() : tensor<1x13x32xf32>
        %128 = "ttir.permute"(%126, %127) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %129 = ttir.empty() : tensor<1x13x32xf32>
        %130 = "ttir.cos"(%128, %129) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %131 = ttir.empty() : tensor<1x1x1xf32>
        %132 = "ttir.reshape"(%arg6, %131) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %133 = ttir.empty() : tensor<1x13x32xf32>
        %134 = "ttir.broadcast"(%132, %133) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %135 = ttir.empty() : tensor<1x13x32xf32>
        %136 = "ttir.multiply"(%130, %134, %135) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %137 = ttir.empty() : tensor<1x13x32xbf16>
        %138 = "ttir.typecast"(%136, %137) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %139 = ttir.empty() : tensor<1x1x13x32xbf16>
        %140 = "ttir.reshape"(%138, %139) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %141 = ttir.empty() : tensor<1x1x13x32xf32>
        %142 = "ttir.typecast"(%140, %141) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %143 = ttir.empty() : tensor<1x64x13x32xf32>
        %144 = "ttir.broadcast"(%142, %143) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %145 = ttir.empty() : tensor<1x64x13x32xf32>
        %146 = "ttir.multiply"(%112, %144, %145) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %147 = ttir.empty() : tensor<1x64x13x32xbf16>
        %148 = "ttir.typecast"(%146, %147) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %149 = ttir.empty() : tensor<1x64x13x32xbf16>
        %150 = "ttir.slice_static"(%108, %149) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %151 = ttir.empty() : tensor<1x64x13x32xf32>
        %152 = "ttir.typecast"(%150, %151) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %153 = ttir.empty() : tensor<1x13x32xf32>
        %154 = "ttir.sin"(%128, %153) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %155 = ttir.empty() : tensor<1x13x32xf32>
        %156 = "ttir.multiply"(%154, %134, %155) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %157 = ttir.empty() : tensor<1x13x32xbf16>
        %158 = "ttir.typecast"(%156, %157) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %159 = ttir.empty() : tensor<1x1x13x32xbf16>
        %160 = "ttir.reshape"(%158, %159) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %161 = ttir.empty() : tensor<1x1x13x32xf32>
        %162 = "ttir.typecast"(%160, %161) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %163 = ttir.empty() : tensor<1x64x13x32xf32>
        %164 = "ttir.broadcast"(%162, %163) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %165 = ttir.empty() : tensor<1x64x13x32xf32>
        %166 = "ttir.multiply"(%152, %164, %165) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %167 = ttir.empty() : tensor<1x64x13x32xbf16>
        %168 = "ttir.typecast"(%166, %167) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %169 = ttir.empty() : tensor<1x64x13x32xbf16>
        %170 = "ttir.subtract"(%148, %168, %169) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %171 = ttir.empty() : tensor<1x64x13x32xf32>
        %172 = "ttir.multiply"(%152, %144, %171) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %173 = ttir.empty() : tensor<1x64x13x32xbf16>
        %174 = "ttir.typecast"(%172, %173) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %175 = ttir.empty() : tensor<1x64x13x32xf32>
        %176 = "ttir.multiply"(%112, %164, %175) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %177 = ttir.empty() : tensor<1x64x13x32xbf16>
        %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %179 = ttir.empty() : tensor<1x64x13x32xbf16>
        %180 = "ttir.add"(%174, %178, %179) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %181 = ttir.empty() : tensor<1x64x13x64xbf16>
        %182 = "ttir.concat"(%170, %180, %181) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %183 = ttir.empty() : tensor<64x13x64xbf16>
        %184 = "ttir.reshape"(%182, %183) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %185 = ttir.empty() : tensor<2880x512xbf16>
        %186 = "ttir.permute"(%arg9, %185) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %187 = ttir.empty() : tensor<13x2880xbf16>
        %188 = "ttir.permute"(%82, %187) <{permutation = array<i64: 0, 1>}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %189 = ttir.empty() : tensor<2880x512xbf16>
        %190 = "ttir.permute"(%186, %189) <{permutation = array<i64: 0, 1>}> : (tensor<2880x512xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %191 = ttir.empty() : tensor<13x2880xbf16>
        %192 = "ttir.reshape"(%188, %191) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %193 = ttir.empty() : tensor<2880x512xbf16>
        %194 = "ttir.reshape"(%190, %193) <{shape = [2880 : i32, 512 : i32]}> : (tensor<2880x512xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %195 = ttir.empty() : tensor<13x512xbf16>
        %196 = "ttir.matmul"(%192, %194, %195) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %197 = ttir.empty() : tensor<13x512xbf16>
        %198 = "ttir.reshape"(%196, %197) <{shape = [13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %199 = ttir.empty() : tensor<1x13x512xbf16>
        %200 = "ttir.reshape"(%198, %199) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %201 = ttir.empty() : tensor<1x1x512xbf16>
        %202 = "ttir.reshape"(%arg8, %201) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %203 = ttir.empty() : tensor<1x13x512xbf16>
        %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %205 = ttir.empty() : tensor<1x13x512xbf16>
        %206 = "ttir.add"(%200, %204, %205) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %207 = ttir.empty() : tensor<1x13x8x64xbf16>
        %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %209 = ttir.empty() : tensor<1x8x13x64xbf16>
        %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %211 = ttir.empty() : tensor<1x8x13x32xbf16>
        %212 = "ttir.slice_static"(%210, %211) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %213 = ttir.empty() : tensor<1x8x13x32xf32>
        %214 = "ttir.typecast"(%212, %213) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %215 = ttir.empty() : tensor<1x8x13x32xf32>
        %216 = "ttir.broadcast"(%142, %215) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %217 = ttir.empty() : tensor<1x8x13x32xf32>
        %218 = "ttir.multiply"(%214, %216, %217) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %219 = ttir.empty() : tensor<1x8x13x32xbf16>
        %220 = "ttir.typecast"(%218, %219) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %221 = ttir.empty() : tensor<1x8x13x32xbf16>
        %222 = "ttir.slice_static"(%210, %221) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %223 = ttir.empty() : tensor<1x8x13x32xf32>
        %224 = "ttir.typecast"(%222, %223) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %225 = ttir.empty() : tensor<1x8x13x32xf32>
        %226 = "ttir.broadcast"(%162, %225) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %227 = ttir.empty() : tensor<1x8x13x32xf32>
        %228 = "ttir.multiply"(%224, %226, %227) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %229 = ttir.empty() : tensor<1x8x13x32xbf16>
        %230 = "ttir.typecast"(%228, %229) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %231 = ttir.empty() : tensor<1x8x13x32xbf16>
        %232 = "ttir.subtract"(%220, %230, %231) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %233 = ttir.empty() : tensor<1x8x13x32xf32>
        %234 = "ttir.multiply"(%224, %216, %233) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %235 = ttir.empty() : tensor<1x8x13x32xbf16>
        %236 = "ttir.typecast"(%234, %235) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %237 = ttir.empty() : tensor<1x8x13x32xf32>
        %238 = "ttir.multiply"(%214, %226, %237) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %239 = ttir.empty() : tensor<1x8x13x32xbf16>
        %240 = "ttir.typecast"(%238, %239) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %241 = ttir.empty() : tensor<1x8x13x32xbf16>
        %242 = "ttir.add"(%236, %240, %241) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %243 = ttir.empty() : tensor<1x8x13x64xbf16>
        %244 = "ttir.concat"(%232, %242, %243) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %245 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %246 = "ttir.reshape"(%244, %245) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %247 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %248 = "ttir.broadcast"(%246, %247) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %249 = ttir.empty() : tensor<1x64x13x64xbf16>
        %250 = "ttir.reshape"(%248, %249) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %251 = ttir.empty() : tensor<1x64x64x13xbf16>
        %252 = "ttir.permute"(%250, %251) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %253 = ttir.empty() : tensor<64x64x13xbf16>
        %254 = "ttir.reshape"(%252, %253) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %255 = ttir.empty() : tensor<64x13x64xbf16>
        %256 = "ttir.permute"(%184, %255) <{permutation = array<i64: 0, 1, 2>}> : (tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %257 = ttir.empty() : tensor<64x64x13xbf16>
        %258 = "ttir.permute"(%254, %257) <{permutation = array<i64: 0, 1, 2>}> : (tensor<64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %259 = ttir.empty() : tensor<64x13x64xbf16>
        %260 = "ttir.reshape"(%256, %259) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %261 = ttir.empty() : tensor<64x64x13xbf16>
        %262 = "ttir.reshape"(%258, %261) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %263 = ttir.empty() : tensor<64x13x13xbf16>
        %264 = "ttir.matmul"(%260, %262, %263) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %265 = ttir.empty() : tensor<64x13x13xbf16>
        %266 = "ttir.reshape"(%264, %265) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %267 = ttir.empty() : tensor<1x64x13x13xbf16>
        %268 = "ttir.reshape"(%266, %267) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %269 = ttir.empty() : tensor<1x64x13x13xf32>
        %270 = "ttir.typecast"(%268, %269) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %271 = ttir.empty() : tensor<1x1x1x1xf32>
        %272 = "ttir.reshape"(%arg18, %271) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %273 = ttir.empty() : tensor<1x64x13x13xf32>
        %274 = "ttir.broadcast"(%272, %273) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %275 = ttir.empty() : tensor<1x64x13x13xf32>
        %276 = "ttir.multiply"(%270, %274, %275) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %277 = ttir.empty() : tensor<1x64x13x13xbf16>
        %278 = "ttir.typecast"(%276, %277) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %279 = ttir.empty() : tensor<1x13xsi32>
        %280 = "ttir.reshape"(%1, %279) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %281 = ttir.empty() : tensor<13x13xsi32>
        %282 = "ttir.broadcast"(%280, %281) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %283 = ttir.empty() : tensor<1xsi32>
        %284 = "ttir.reshape"(%arg17, %283) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %285 = ttir.empty() : tensor<13xsi32>
        %286 = "ttir.broadcast"(%284, %285) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %287 = ttir.empty() : tensor<13xsi32>
        %288 = "ttir.subtract"(%1, %286, %287) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %289 = ttir.empty() : tensor<13x1xsi32>
        %290 = "ttir.reshape"(%288, %289) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %291 = ttir.empty() : tensor<13x13xsi32>
        %292 = "ttir.broadcast"(%290, %291) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %293 = ttir.empty() : tensor<13x13xbf16>
        %294 = "ttir.gt"(%282, %292, %293) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %295 = ttir.empty() : tensor<13x13xui8>
        %296 = "ttir.typecast"(%294, %295) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %297 = ttir.empty() : tensor<13x13xui8>
        %298 = "ttir.bitwise_and"(%296, %23, %297) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %299 = ttir.empty() : tensor<13x13xbf16>
        %300 = "ttir.ne"(%298, %31, %299) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %301 = ttir.empty() : tensor<13x13xui8>
        %302 = "ttir.typecast"(%300, %301) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %303 = ttir.empty() : tensor<13x1xsi32>
        %304 = "ttir.reshape"(%1, %303) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %305 = ttir.empty() : tensor<13x13xsi32>
        %306 = "ttir.broadcast"(%304, %305) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %307 = ttir.empty() : tensor<13x13xbf16>
        %308 = "ttir.le"(%282, %306, %307) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %309 = ttir.empty() : tensor<13x13xui8>
        %310 = "ttir.typecast"(%308, %309) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %311 = ttir.empty() : tensor<13x13xui8>
        %312 = "ttir.bitwise_and"(%302, %310, %311) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %313 = ttir.empty() : tensor<13x13xbf16>
        %314 = "ttir.ne"(%312, %31, %313) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %315 = ttir.empty() : tensor<1x1x13x13xbf16>
        %316 = "ttir.reshape"(%314, %315) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %317 = ttir.empty() : tensor<1x1x1x1xbf16>
        %318 = "ttir.reshape"(%arg16, %317) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %319 = ttir.empty() : tensor<1x1x13x13xbf16>
        %320 = "ttir.broadcast"(%318, %319) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %321 = ttir.empty() : tensor<1x1x13x13xbf16>
        %322 = "ttir.where"(%316, %19, %320, %321) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %323 = ttir.empty() : tensor<1x64x13x13xbf16>
        %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %325 = ttir.empty() : tensor<1x64x13x13xbf16>
        %326 = "ttir.add"(%278, %324, %325) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %327 = ttir.empty() : tensor<1x64x1x1xbf16>
        %328 = "ttir.reshape"(%arg15, %327) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %329 = ttir.empty() : tensor<1x64x13x1xbf16>
        %330 = "ttir.broadcast"(%328, %329) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %331 = ttir.empty() : tensor<1x64x13x14xbf16>
        %332 = "ttir.concat"(%326, %330, %331) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %333 = ttir.empty() : tensor<2880x512xbf16>
        %334 = "ttir.permute"(%arg1, %333) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %335 = ttir.empty() : tensor<13x2880xbf16>
        %336 = "ttir.permute"(%82, %335) <{permutation = array<i64: 0, 1>}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %337 = ttir.empty() : tensor<2880x512xbf16>
        %338 = "ttir.permute"(%334, %337) <{permutation = array<i64: 0, 1>}> : (tensor<2880x512xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %339 = ttir.empty() : tensor<13x2880xbf16>
        %340 = "ttir.reshape"(%336, %339) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %341 = ttir.empty() : tensor<2880x512xbf16>
        %342 = "ttir.reshape"(%338, %341) <{shape = [2880 : i32, 512 : i32]}> : (tensor<2880x512xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %343 = ttir.empty() : tensor<13x512xbf16>
        %344 = "ttir.matmul"(%340, %342, %343) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %345 = ttir.empty() : tensor<13x512xbf16>
        %346 = "ttir.reshape"(%344, %345) <{shape = [13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %347 = ttir.empty() : tensor<1x13x512xbf16>
        %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %349 = ttir.empty() : tensor<1x1x512xbf16>
        %350 = "ttir.reshape"(%arg0, %349) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %351 = ttir.empty() : tensor<1x13x512xbf16>
        %352 = "ttir.broadcast"(%350, %351) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %353 = ttir.empty() : tensor<1x13x512xbf16>
        %354 = "ttir.add"(%348, %352, %353) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %355 = ttir.empty() : tensor<1x13x8x64xbf16>
        %356 = "ttir.reshape"(%354, %355) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %357 = ttir.empty() : tensor<1x8x13x64xbf16>
        %358 = "ttir.permute"(%356, %357) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %359 = ttir.empty() : tensor<2880xf32>
        %360 = "ttir.typecast"(%arg30, %359) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %361 = ttir.empty() : tensor<1x1x2880xf32>
        %362 = "ttir.reshape"(%360, %361) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %363 = ttir.empty() : tensor<1x13x2880xf32>
        %364 = "ttir.broadcast"(%362, %363) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %365 = ttir.empty() : tensor<1x64x13x14xbf16>
        %366 = "ttir.softmax"(%332, %365) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %367 = ttir.empty() : tensor<1x64x13x13xbf16>
        %368 = "ttir.slice_static"(%366, %367) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %369 = ttir.empty() : tensor<64x13x13xbf16>
        %370 = "ttir.reshape"(%368, %369) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %371 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %372 = "ttir.reshape"(%358, %371) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %373 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %374 = "ttir.broadcast"(%372, %373) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %375 = ttir.empty() : tensor<64x13x64xbf16>
        %376 = "ttir.reshape"(%374, %375) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %377 = ttir.empty() : tensor<64x13x13xbf16>
        %378 = "ttir.permute"(%370, %377) <{permutation = array<i64: 0, 1, 2>}> : (tensor<64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %379 = ttir.empty() : tensor<64x13x64xbf16>
        %380 = "ttir.permute"(%376, %379) <{permutation = array<i64: 0, 1, 2>}> : (tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %381 = ttir.empty() : tensor<64x13x13xbf16>
        %382 = "ttir.reshape"(%378, %381) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %383 = ttir.empty() : tensor<64x13x64xbf16>
        %384 = "ttir.reshape"(%380, %383) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %385 = ttir.empty() : tensor<64x13x64xbf16>
        %386 = "ttir.matmul"(%382, %384, %385) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %387 = ttir.empty() : tensor<64x13x64xbf16>
        %388 = "ttir.reshape"(%386, %387) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %389 = ttir.empty() : tensor<1x64x13x64xbf16>
        %390 = "ttir.reshape"(%388, %389) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %391 = ttir.empty() : tensor<13x4096xbf16>
        %392 = ttir.empty() : tensor<1x13x4096xbf16>
        %393 = "ttir.concatenate_heads"(%390, %392) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %394 = "ttir.reshape"(%393, %391) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %395 = ttir.empty() : tensor<4096x2880xbf16>
        %396 = "ttir.permute"(%arg14, %395) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %397 = ttir.empty() : tensor<13x4096xbf16>
        %398 = "ttir.permute"(%394, %397) <{permutation = array<i64: 0, 1>}> : (tensor<13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %399 = ttir.empty() : tensor<4096x2880xbf16>
        %400 = "ttir.permute"(%396, %399) <{permutation = array<i64: 0, 1>}> : (tensor<4096x2880xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %401 = ttir.empty() : tensor<13x4096xbf16>
        %402 = "ttir.reshape"(%398, %401) <{shape = [13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %403 = ttir.empty() : tensor<4096x2880xbf16>
        %404 = "ttir.reshape"(%400, %403) <{shape = [4096 : i32, 2880 : i32]}> : (tensor<4096x2880xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %405 = ttir.empty() : tensor<13x2880xbf16>
        %406 = "ttir.matmul"(%402, %404, %405) <{transpose_a = false, transpose_b = false}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %407 = ttir.empty() : tensor<13x2880xbf16>
        %408 = "ttir.reshape"(%406, %407) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %409 = ttir.empty() : tensor<1x13x2880xbf16>
        %410 = "ttir.reshape"(%408, %409) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %411 = ttir.empty() : tensor<1x1x2880xbf16>
        %412 = "ttir.reshape"(%arg13, %411) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %413 = ttir.empty() : tensor<1x13x2880xbf16>
        %414 = "ttir.broadcast"(%412, %413) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %415 = ttir.empty() : tensor<1x13x2880xbf16>
        %416 = "ttir.add"(%410, %414, %415) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %417 = ttir.empty() : tensor<1x13x2880xbf16>
        %418 = "ttir.add"(%54, %416, %417) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %419 = ttir.empty() : tensor<1x1x1xbf16>
        %420 = "ttir.reshape"(%arg29, %419) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %421 = ttir.empty() : tensor<32x13x2880xbf16>
        %422 = "ttir.broadcast"(%420, %421) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %423 = ttir.empty() : tensor<2880xf32>
        %424 = "ttir.typecast"(%arg21, %423) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %425 = ttir.empty() : tensor<1x1x2880xf32>
        %426 = "ttir.reshape"(%424, %425) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %427 = ttir.empty() : tensor<1x13x2880xf32>
        %428 = "ttir.broadcast"(%426, %427) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %429 = ttir.empty() : tensor<1x13x2880xf32>
        %430 = "ttir.typecast"(%418, %429) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %431 = ttir.empty() : tensor<1x13x2880xf32>
        %432 = "ttir.pow"(%430, %27, %431) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %433 = ttir.empty() : tensor<1x13xf32>
        %434 = "ttir.sum"(%432, %433) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %435 = ttir.empty() : tensor<1x13xf32>
        %436 = "ttir.multiply"(%434, %4, %435) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %437 = ttir.empty() : tensor<1x13x1xf32>
        %438 = "ttir.reshape"(%436, %437) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %439 = ttir.empty() : tensor<1x13x1xf32>
        %440 = "ttir.add"(%438, %68, %439) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %441 = ttir.empty() : tensor<1x13x1xf32>
        %442 = "ttir.rsqrt"(%440, %441) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %443 = ttir.empty() : tensor<1x13x2880xf32>
        %444 = "ttir.broadcast"(%442, %443) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %445 = ttir.empty() : tensor<1x13x2880xf32>
        %446 = "ttir.multiply"(%430, %444, %445) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %447 = ttir.empty() : tensor<1x13x2880xf32>
        %448 = "ttir.multiply"(%428, %446, %447) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %449 = ttir.empty() : tensor<1x13x2880xbf16>
        %450 = "ttir.typecast"(%448, %449) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %451 = ttir.empty() : tensor<13x2880xbf16>
        %452 = "ttir.reshape"(%450, %451) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %453 = ttir.empty() : tensor<416x2880xbf16>
        %454 = "ttir.concat"(%452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %453) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %455 = ttir.empty() : tensor<32x13x2880xbf16>
        %456 = "ttir.reshape"(%454, %455) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %457 = ttir.empty() : tensor<32x13x2880xbf16>
        %458 = "ttir.permute"(%456, %457) <{permutation = array<i64: 0, 1, 2>}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %459 = ttir.empty() : tensor<32x2880x5760xbf16>
        %460 = "ttir.permute"(%arg28, %459) <{permutation = array<i64: 0, 1, 2>}> : (tensor<32x2880x5760xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x2880x5760xbf16>
        %461 = ttir.empty() : tensor<32x13x2880xbf16>
        %462 = "ttir.reshape"(%458, %461) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %463 = ttir.empty() : tensor<32x2880x5760xbf16>
        %464 = "ttir.reshape"(%460, %463) <{shape = [32 : i32, 2880 : i32, 5760 : i32]}> : (tensor<32x2880x5760xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x2880x5760xbf16>
        %465 = ttir.empty() : tensor<32x13x5760xbf16>
        %466 = "ttir.matmul"(%462, %464, %465) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %467 = ttir.empty() : tensor<32x13x5760xbf16>
        %468 = "ttir.reshape"(%466, %467) <{shape = [32 : i32, 13 : i32, 5760 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %469 = ttir.empty() : tensor<32x1x5760xbf16>
        %470 = "ttir.reshape"(%arg27, %469) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %471 = ttir.empty() : tensor<32x13x5760xbf16>
        %472 = "ttir.broadcast"(%470, %471) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %473 = ttir.empty() : tensor<32x13x5760xbf16>
        %474 = "ttir.add"(%468, %472, %473) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %475 = ttir.empty() : tensor<32x13x2880xbf16>
        %476 = "ttir.slice_static"(%474, %475) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %477 = ttir.empty() : tensor<1x1x1xbf16>
        %478 = "ttir.reshape"(%arg25, %477) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %479 = ttir.empty() : tensor<32x13x2880xbf16>
        %480 = "ttir.broadcast"(%478, %479) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %481 = ttir.empty() : tensor<32x13x2880xbf16>
        %482 = "ttir.clamp_tensor"(%476, %422, %480, %481) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %483 = ttir.empty() : tensor<32x13x2880xbf16>
        %484 = "ttir.add"(%482, %15, %483) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %485 = ttir.empty() : tensor<32x13x2880xf32>
        %486 = "ttir.typecast"(%484, %485) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %487 = ttir.empty() : tensor<1x1x1xbf16>
        %488 = "ttir.reshape"(%arg26, %487) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %489 = ttir.empty() : tensor<32x13x2880xbf16>
        %490 = "ttir.broadcast"(%488, %489) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %491 = ttir.empty() : tensor<32x13x2880xbf16>
        %492 = "ttir.slice_static"(%474, %491) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %493 = ttir.empty() : tensor<32x13x2880xbf16>
        %494 = "ttir.clamp_tensor"(%492, %490, %480, %493) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %495 = ttir.empty() : tensor<32x13x2880xf32>
        %496 = "ttir.typecast"(%494, %495) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %497 = ttir.empty() : tensor<1x1x1xf32>
        %498 = "ttir.reshape"(%arg24, %497) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %499 = ttir.empty() : tensor<32x13x2880xf32>
        %500 = "ttir.broadcast"(%498, %499) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %501 = ttir.empty() : tensor<32x13x2880xf32>
        %502 = "ttir.multiply"(%496, %500, %501) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %503 = ttir.empty() : tensor<32x13x2880xbf16>
        %504 = "ttir.typecast"(%502, %503) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %505 = ttir.empty() : tensor<32x13x2880xbf16>
        %506 = "ttir.sigmoid"(%504, %505) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %507 = ttir.empty() : tensor<32x13x2880xf32>
        %508 = "ttir.typecast"(%506, %507) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %509 = ttir.empty() : tensor<32x13x2880xf32>
        %510 = "ttir.multiply"(%496, %508, %509) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %511 = ttir.empty() : tensor<32x13x2880xf32>
        %512 = "ttir.multiply"(%486, %510, %511) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %513 = ttir.empty() : tensor<32x13x2880xbf16>
        %514 = "ttir.typecast"(%512, %513) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %515 = ttir.empty() : tensor<32x13x2880xbf16>
        %516 = "ttir.permute"(%514, %515) <{permutation = array<i64: 0, 1, 2>}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %517 = ttir.empty() : tensor<32x2880x2880xbf16>
        %518 = "ttir.permute"(%arg23, %517) <{permutation = array<i64: 0, 1, 2>}> : (tensor<32x2880x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x2880x2880xbf16>
        %519 = ttir.empty() : tensor<32x13x2880xbf16>
        %520 = "ttir.reshape"(%516, %519) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %521 = ttir.empty() : tensor<32x2880x2880xbf16>
        %522 = "ttir.reshape"(%518, %521) <{shape = [32 : i32, 2880 : i32, 2880 : i32]}> : (tensor<32x2880x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x2880x2880xbf16>
        %523 = ttir.empty() : tensor<32x13x2880xbf16>
        %524 = "ttir.matmul"(%520, %522, %523) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %525 = ttir.empty() : tensor<32x13x2880xbf16>
        %526 = "ttir.reshape"(%524, %525) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %527 = ttir.empty() : tensor<32x1x2880xbf16>
        %528 = "ttir.reshape"(%arg22, %527) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %529 = ttir.empty() : tensor<32x13x2880xbf16>
        %530 = "ttir.broadcast"(%528, %529) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %531 = ttir.empty() : tensor<32x13x2880xbf16>
        %532 = "ttir.add"(%526, %530, %531) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %533 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %534 = "ttir.reshape"(%532, %533) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %535 = ttir.empty() : tensor<32x1x13x2880xf32>
        %536 = "ttir.typecast"(%534, %535) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %537 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %538 = ttir.empty() : tensor<13x1x1xsi32>
        %539 = "ttir.reshape"(%537, %538) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %540 = ttir.empty() : tensor<13x4x1xsi32>
        %541 = "ttir.broadcast"(%539, %540) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %542 = ttir.empty() : tensor<2880x32xbf16>
        %543 = "ttir.permute"(%arg12, %542) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %544 = ttir.empty() : tensor<13x2880xbf16>
        %545 = "ttir.permute"(%452, %544) <{permutation = array<i64: 0, 1>}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %546 = ttir.empty() : tensor<2880x32xbf16>
        %547 = "ttir.permute"(%543, %546) <{permutation = array<i64: 0, 1>}> : (tensor<2880x32xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %548 = ttir.empty() : tensor<13x2880xbf16>
        %549 = "ttir.reshape"(%545, %548) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %550 = ttir.empty() : tensor<2880x32xbf16>
        %551 = "ttir.reshape"(%547, %550) <{shape = [2880 : i32, 32 : i32]}> : (tensor<2880x32xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %552 = ttir.empty() : tensor<13x32xbf16>
        %553 = "ttir.matmul"(%549, %551, %552) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %554 = ttir.empty() : tensor<13x32xbf16>
        %555 = "ttir.reshape"(%553, %554) <{shape = [13 : i32, 32 : i32]}> : (tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %556 = ttir.empty() : tensor<1x32xbf16>
        %557 = "ttir.reshape"(%arg11, %556) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %558 = ttir.empty() : tensor<13x32xbf16>
        %559 = "ttir.broadcast"(%557, %558) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %560 = ttir.empty() : tensor<13x32xbf16>
        %561 = "ttir.add"(%555, %559, %560) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %562 = ttir.empty() : tensor<13x32xbf16>
        %563 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%561, %562, %563) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %564 = ttir.empty() : tensor<13x4xsi32>
        %565 = "ttir.slice_static"(%indices, %564) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %566 = ttir.empty() : tensor<13x4x1xsi32>
        %567 = "ttir.reshape"(%565, %566) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %568 = ttir.empty() : tensor<13x4x2xsi32>
        %569 = "ttir.concat"(%541, %567, %568) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %570 = ttir.empty() : tensor<13x4xbf16>
        %571 = "ttir.slice_static"(%values, %570) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %572 = ttir.empty() : tensor<13x4xbf16>
        %573 = "ttir.softmax"(%571, %572) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %574 = ttir.empty() : tensor<13x32xbf16>
        %575 = "ttir.scatter"(%11, %569, %573, %574) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %576 = ttir.empty() : tensor<32x13xbf16>
        %577 = "ttir.permute"(%575, %576) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %578 = ttir.empty() : tensor<32x1x13x1xbf16>
        %579 = "ttir.reshape"(%577, %578) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %580 = ttir.empty() : tensor<32x1x13x1xf32>
        %581 = "ttir.typecast"(%579, %580) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %582 = ttir.empty() : tensor<32x1x13x2880xf32>
        %583 = "ttir.broadcast"(%581, %582) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %584 = ttir.empty() : tensor<32x1x13x2880xf32>
        %585 = "ttir.multiply"(%536, %583, %584) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %586 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %587 = "ttir.typecast"(%585, %586) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %588 = ttir.empty() : tensor<1x13x2880xbf16>
        %589 = "ttir.sum"(%587, %588) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %590 = ttir.empty() : tensor<1x13x2880xbf16>
        %591 = "ttir.add"(%418, %589, %590) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %592 = ttir.empty() : tensor<1x13x2880xf32>
        %593 = "ttir.typecast"(%591, %592) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %594 = ttir.empty() : tensor<1x13x2880xf32>
        %595 = "ttir.pow"(%593, %27, %594) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %596 = ttir.empty() : tensor<1x13xf32>
        %597 = "ttir.sum"(%595, %596) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %598 = ttir.empty() : tensor<1x13xf32>
        %599 = "ttir.multiply"(%597, %4, %598) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %600 = ttir.empty() : tensor<1x13x1xf32>
        %601 = "ttir.reshape"(%599, %600) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %602 = ttir.empty() : tensor<1x13x1xf32>
        %603 = "ttir.add"(%601, %68, %602) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %604 = ttir.empty() : tensor<1x13x1xf32>
        %605 = "ttir.rsqrt"(%603, %604) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %606 = ttir.empty() : tensor<1x13x2880xf32>
        %607 = "ttir.broadcast"(%605, %606) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %608 = ttir.empty() : tensor<1x13x2880xf32>
        %609 = "ttir.multiply"(%593, %607, %608) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %610 = ttir.empty() : tensor<1x13x2880xf32>
        %611 = "ttir.multiply"(%364, %609, %610) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %612 = ttir.empty() : tensor<1x13x2880xbf16>
        %613 = "ttir.typecast"(%611, %612) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %614 = ttir.empty() : tensor<13x2880xbf16>
        %615 = "ttir.reshape"(%613, %614) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %616 = ttir.empty() : tensor<2880x201088xbf16>
        %617 = "ttir.permute"(%arg10, %616) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %618 = ttir.empty() : tensor<13x2880xbf16>
        %619 = "ttir.permute"(%615, %618) <{permutation = array<i64: 0, 1>}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %620 = ttir.empty() : tensor<2880x201088xbf16>
        %621 = "ttir.permute"(%617, %620) <{permutation = array<i64: 0, 1>}> : (tensor<2880x201088xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %622 = ttir.empty() : tensor<13x2880xbf16>
        %623 = "ttir.reshape"(%619, %622) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %624 = ttir.empty() : tensor<2880x201088xbf16>
        %625 = "ttir.reshape"(%621, %624) <{shape = [2880 : i32, 201088 : i32]}> : (tensor<2880x201088xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %626 = ttir.empty() : tensor<13x201088xbf16>
        %627 = "ttir.matmul"(%623, %625, %626) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %628 = ttir.empty() : tensor<13x201088xbf16>
        %629 = "ttir.reshape"(%627, %628) <{shape = [13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %630 = ttir.empty() : tensor<1x13x201088xbf16>
        %631 = "ttir.reshape"(%629, %630) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %354, %356, %358, %244, %629, %631 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = ttir.empty() : tensor<201088x2880xbf16>
        %44 = "ttir.permute"(%arg4, %43) <{permutation = array<i64: 0, 1>}> : (tensor<201088x2880xbf16>, tensor<201088x2880xbf16>) -> tensor<201088x2880xbf16>
        %45 = ttir.empty() : tensor<201088x2880xbf16>
        %46 = "ttir.reshape"(%44, %45) <{shape = [201088 : i32, 2880 : i32]}> : (tensor<201088x2880xbf16>, tensor<201088x2880xbf16>) -> tensor<201088x2880xbf16>
        %47 = ttir.empty() : tensor<13x2880xbf16>
        %48 = "ttir.embedding"(%41, %46, %47) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %49 = ttir.empty() : tensor<13x2880xbf16>
        %50 = "ttir.reshape"(%48, %49) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %51 = ttir.empty() : tensor<13x2880xbf16>
        %52 = "ttir.permute"(%50, %51) <{permutation = array<i64: 0, 1>}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %53 = ttir.empty() : tensor<1x13x2880xbf16>
        %54 = "ttir.reshape"(%52, %53) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %55 = ttir.empty() : tensor<1x13x2880xf32>
        %56 = "ttir.typecast"(%54, %55) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %57 = ttir.empty() : tensor<1x13x2880xf32>
        %58 = "ttir.pow"(%56, %27, %57) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %59 = ttir.empty() : tensor<1x13xf32>
        %60 = "ttir.sum"(%58, %59) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %61 = ttir.empty() : tensor<1x13xf32>
        %62 = "ttir.multiply"(%60, %4, %61) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %63 = ttir.empty() : tensor<1x13x1xf32>
        %64 = "ttir.reshape"(%62, %63) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %65 = ttir.empty() : tensor<1x1x1xf32>
        %66 = "ttir.reshape"(%arg2, %65) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %67 = ttir.empty() : tensor<1x13x1xf32>
        %68 = "ttir.broadcast"(%66, %67) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %69 = ttir.empty() : tensor<1x13x1xf32>
        %70 = "ttir.add"(%64, %68, %69) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %71 = ttir.empty() : tensor<1x13x1xf32>
        %72 = "ttir.rsqrt"(%70, %71) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %73 = ttir.empty() : tensor<1x13x2880xf32>
        %74 = "ttir.broadcast"(%72, %73) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %75 = ttir.empty() : tensor<1x13x2880xf32>
        %76 = "ttir.multiply"(%56, %74, %75) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %77 = ttir.empty() : tensor<1x13x2880xf32>
        %78 = "ttir.multiply"(%37, %76, %77) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %79 = ttir.empty() : tensor<1x13x2880xbf16>
        %80 = "ttir.typecast"(%78, %79) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %81 = ttir.empty() : tensor<13x2880xbf16>
        %82 = "ttir.reshape"(%80, %81) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %83 = ttir.empty() : tensor<2880x4096xbf16>
        %84 = "ttir.permute"(%arg20, %83) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %85 = ttir.empty() : tensor<13x2880xbf16>
        %86 = "ttir.permute"(%82, %85) <{permutation = array<i64: 0, 1>}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %87 = ttir.empty() : tensor<2880x4096xbf16>
        %88 = "ttir.permute"(%84, %87) <{permutation = array<i64: 0, 1>}> : (tensor<2880x4096xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %89 = ttir.empty() : tensor<13x2880xbf16>
        %90 = "ttir.reshape"(%86, %89) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %91 = ttir.empty() : tensor<2880x4096xbf16>
        %92 = "ttir.reshape"(%88, %91) <{shape = [2880 : i32, 4096 : i32]}> : (tensor<2880x4096xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %93 = ttir.empty() : tensor<13x4096xbf16>
        %94 = "ttir.matmul"(%90, %92, %93) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %95 = ttir.empty() : tensor<13x4096xbf16>
        %96 = "ttir.reshape"(%94, %95) <{shape = [13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %97 = ttir.empty() : tensor<1x13x4096xbf16>
        %98 = "ttir.reshape"(%96, %97) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %99 = ttir.empty() : tensor<1x1x4096xbf16>
        %100 = "ttir.reshape"(%arg19, %99) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %101 = ttir.empty() : tensor<1x13x4096xbf16>
        %102 = "ttir.broadcast"(%100, %101) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %103 = ttir.empty() : tensor<1x13x4096xbf16>
        %104 = "ttir.add"(%98, %102, %103) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %105 = ttir.empty() : tensor<1x13x64x64xbf16>
        %106 = "ttir.reshape"(%104, %105) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %107 = ttir.empty() : tensor<1x64x13x64xbf16>
        %108 = "ttir.permute"(%106, %107) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %109 = ttir.empty() : tensor<1x64x13x32xbf16>
        %110 = "ttir.slice_static"(%108, %109) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %111 = ttir.empty() : tensor<1x64x13x32xf32>
        %112 = "ttir.typecast"(%110, %111) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %113 = ttir.empty() : tensor<1x32x1xf32>
        %114 = "ttir.reshape"(%arg7, %113) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %115 = ttir.empty() : tensor<1x32x1xf32>
        %116 = "ttir.permute"(%114, %115) <{permutation = array<i64: 0, 1, 2>}> : (tensor<1x32x1xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %117 = ttir.empty() : tensor<1x1x13xf32>
        %118 = "ttir.permute"(%0, %117) <{permutation = array<i64: 0, 1, 2>}> : (tensor<1x1x13xf32>, tensor<1x1x13xf32>) -> tensor<1x1x13xf32>
        %119 = ttir.empty() : tensor<1x32x1xf32>
        %120 = "ttir.reshape"(%116, %119) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<1x32x1xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %121 = ttir.empty() : tensor<1x1x13xf32>
        %122 = "ttir.reshape"(%118, %121) <{shape = [1 : i32, 1 : i32, 13 : i32]}> : (tensor<1x1x13xf32>, tensor<1x1x13xf32>) -> tensor<1x1x13xf32>
        %123 = ttir.empty() : tensor<1x32x13xf32>
        %124 = "ttir.matmul"(%120, %122, %123) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %125 = ttir.empty() : tensor<1x32x13xf32>
        %126 = "ttir.reshape"(%124, %125) <{shape = [1 : i32, 32 : i32, 13 : i32]}> : (tensor<1x32x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %127 = ttir.empty() : tensor<1x13x32xf32>
        %128 = "ttir.permute"(%126, %127) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %129 = ttir.empty() : tensor<1x13x32xf32>
        %130 = "ttir.cos"(%128, %129) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %131 = ttir.empty() : tensor<1x1x1xf32>
        %132 = "ttir.reshape"(%arg6, %131) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %133 = ttir.empty() : tensor<1x13x32xf32>
        %134 = "ttir.broadcast"(%132, %133) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %135 = ttir.empty() : tensor<1x13x32xf32>
        %136 = "ttir.multiply"(%130, %134, %135) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %137 = ttir.empty() : tensor<1x13x32xbf16>
        %138 = "ttir.typecast"(%136, %137) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %139 = ttir.empty() : tensor<1x1x13x32xbf16>
        %140 = "ttir.reshape"(%138, %139) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %141 = ttir.empty() : tensor<1x1x13x32xf32>
        %142 = "ttir.typecast"(%140, %141) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %143 = ttir.empty() : tensor<1x64x13x32xf32>
        %144 = "ttir.broadcast"(%142, %143) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %145 = ttir.empty() : tensor<1x64x13x32xf32>
        %146 = "ttir.multiply"(%112, %144, %145) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %147 = ttir.empty() : tensor<1x64x13x32xbf16>
        %148 = "ttir.typecast"(%146, %147) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %149 = ttir.empty() : tensor<1x64x13x32xbf16>
        %150 = "ttir.slice_static"(%108, %149) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %151 = ttir.empty() : tensor<1x64x13x32xf32>
        %152 = "ttir.typecast"(%150, %151) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %153 = ttir.empty() : tensor<1x13x32xf32>
        %154 = "ttir.sin"(%128, %153) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %155 = ttir.empty() : tensor<1x13x32xf32>
        %156 = "ttir.multiply"(%154, %134, %155) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %157 = ttir.empty() : tensor<1x13x32xbf16>
        %158 = "ttir.typecast"(%156, %157) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %159 = ttir.empty() : tensor<1x1x13x32xbf16>
        %160 = "ttir.reshape"(%158, %159) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %161 = ttir.empty() : tensor<1x1x13x32xf32>
        %162 = "ttir.typecast"(%160, %161) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %163 = ttir.empty() : tensor<1x64x13x32xf32>
        %164 = "ttir.broadcast"(%162, %163) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %165 = ttir.empty() : tensor<1x64x13x32xf32>
        %166 = "ttir.multiply"(%152, %164, %165) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %167 = ttir.empty() : tensor<1x64x13x32xbf16>
        %168 = "ttir.typecast"(%166, %167) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %169 = ttir.empty() : tensor<1x64x13x32xbf16>
        %170 = "ttir.subtract"(%148, %168, %169) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %171 = ttir.empty() : tensor<1x64x13x32xf32>
        %172 = "ttir.multiply"(%152, %144, %171) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %173 = ttir.empty() : tensor<1x64x13x32xbf16>
        %174 = "ttir.typecast"(%172, %173) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %175 = ttir.empty() : tensor<1x64x13x32xf32>
        %176 = "ttir.multiply"(%112, %164, %175) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %177 = ttir.empty() : tensor<1x64x13x32xbf16>
        %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %179 = ttir.empty() : tensor<1x64x13x32xbf16>
        %180 = "ttir.add"(%174, %178, %179) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %181 = ttir.empty() : tensor<1x64x13x64xbf16>
        %182 = "ttir.concat"(%170, %180, %181) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %183 = ttir.empty() : tensor<64x13x64xbf16>
        %184 = "ttir.reshape"(%182, %183) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %185 = ttir.empty() : tensor<2880x512xbf16>
        %186 = "ttir.permute"(%arg9, %185) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %187 = ttir.empty() : tensor<13x2880xbf16>
        %188 = "ttir.permute"(%82, %187) <{permutation = array<i64: 0, 1>}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %189 = ttir.empty() : tensor<2880x512xbf16>
        %190 = "ttir.permute"(%186, %189) <{permutation = array<i64: 0, 1>}> : (tensor<2880x512xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %191 = ttir.empty() : tensor<13x2880xbf16>
        %192 = "ttir.reshape"(%188, %191) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %193 = ttir.empty() : tensor<2880x512xbf16>
        %194 = "ttir.reshape"(%190, %193) <{shape = [2880 : i32, 512 : i32]}> : (tensor<2880x512xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %195 = ttir.empty() : tensor<13x512xbf16>
        %196 = "ttir.matmul"(%192, %194, %195) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %197 = ttir.empty() : tensor<13x512xbf16>
        %198 = "ttir.reshape"(%196, %197) <{shape = [13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %199 = ttir.empty() : tensor<1x13x512xbf16>
        %200 = "ttir.reshape"(%198, %199) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %201 = ttir.empty() : tensor<1x1x512xbf16>
        %202 = "ttir.reshape"(%arg8, %201) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %203 = ttir.empty() : tensor<1x13x512xbf16>
        %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %205 = ttir.empty() : tensor<1x13x512xbf16>
        %206 = "ttir.add"(%200, %204, %205) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %207 = ttir.empty() : tensor<1x13x8x64xbf16>
        %208 = "ttir.reshape"(%206, %207) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %209 = ttir.empty() : tensor<1x8x13x64xbf16>
        %210 = "ttir.permute"(%208, %209) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %211 = ttir.empty() : tensor<1x8x13x32xbf16>
        %212 = "ttir.slice_static"(%210, %211) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %213 = ttir.empty() : tensor<1x8x13x32xf32>
        %214 = "ttir.typecast"(%212, %213) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %215 = ttir.empty() : tensor<1x8x13x32xf32>
        %216 = "ttir.broadcast"(%142, %215) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %217 = ttir.empty() : tensor<1x8x13x32xf32>
        %218 = "ttir.multiply"(%214, %216, %217) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %219 = ttir.empty() : tensor<1x8x13x32xbf16>
        %220 = "ttir.typecast"(%218, %219) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %221 = ttir.empty() : tensor<1x8x13x32xbf16>
        %222 = "ttir.slice_static"(%210, %221) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %223 = ttir.empty() : tensor<1x8x13x32xf32>
        %224 = "ttir.typecast"(%222, %223) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %225 = ttir.empty() : tensor<1x8x13x32xf32>
        %226 = "ttir.broadcast"(%162, %225) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %227 = ttir.empty() : tensor<1x8x13x32xf32>
        %228 = "ttir.multiply"(%224, %226, %227) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %229 = ttir.empty() : tensor<1x8x13x32xbf16>
        %230 = "ttir.typecast"(%228, %229) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %231 = ttir.empty() : tensor<1x8x13x32xbf16>
        %232 = "ttir.subtract"(%220, %230, %231) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %233 = ttir.empty() : tensor<1x8x13x32xf32>
        %234 = "ttir.multiply"(%224, %216, %233) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %235 = ttir.empty() : tensor<1x8x13x32xbf16>
        %236 = "ttir.typecast"(%234, %235) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %237 = ttir.empty() : tensor<1x8x13x32xf32>
        %238 = "ttir.multiply"(%214, %226, %237) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %239 = ttir.empty() : tensor<1x8x13x32xbf16>
        %240 = "ttir.typecast"(%238, %239) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %241 = ttir.empty() : tensor<1x8x13x32xbf16>
        %242 = "ttir.add"(%236, %240, %241) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %243 = ttir.empty() : tensor<1x8x13x64xbf16>
        %244 = "ttir.concat"(%232, %242, %243) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %245 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %246 = "ttir.reshape"(%244, %245) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %247 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %248 = "ttir.broadcast"(%246, %247) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %249 = ttir.empty() : tensor<1x64x13x64xbf16>
        %250 = "ttir.reshape"(%248, %249) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %251 = ttir.empty() : tensor<1x64x64x13xbf16>
        %252 = "ttir.permute"(%250, %251) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %253 = ttir.empty() : tensor<64x64x13xbf16>
        %254 = "ttir.reshape"(%252, %253) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %255 = ttir.empty() : tensor<64x13x64xbf16>
        %256 = "ttir.permute"(%184, %255) <{permutation = array<i64: 0, 1, 2>}> : (tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %257 = ttir.empty() : tensor<64x64x13xbf16>
        %258 = "ttir.permute"(%254, %257) <{permutation = array<i64: 0, 1, 2>}> : (tensor<64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %259 = ttir.empty() : tensor<64x13x64xbf16>
        %260 = "ttir.reshape"(%256, %259) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %261 = ttir.empty() : tensor<64x64x13xbf16>
        %262 = "ttir.reshape"(%258, %261) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %263 = ttir.empty() : tensor<64x13x13xbf16>
        %264 = "ttir.matmul"(%260, %262, %263) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %265 = ttir.empty() : tensor<64x13x13xbf16>
        %266 = "ttir.reshape"(%264, %265) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %267 = ttir.empty() : tensor<1x64x13x13xbf16>
        %268 = "ttir.reshape"(%266, %267) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %269 = ttir.empty() : tensor<1x64x13x13xf32>
        %270 = "ttir.typecast"(%268, %269) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %271 = ttir.empty() : tensor<1x1x1x1xf32>
        %272 = "ttir.reshape"(%arg18, %271) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %273 = ttir.empty() : tensor<1x64x13x13xf32>
        %274 = "ttir.broadcast"(%272, %273) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %275 = ttir.empty() : tensor<1x64x13x13xf32>
        %276 = "ttir.multiply"(%270, %274, %275) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %277 = ttir.empty() : tensor<1x64x13x13xbf16>
        %278 = "ttir.typecast"(%276, %277) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %279 = ttir.empty() : tensor<1x13xsi32>
        %280 = "ttir.reshape"(%1, %279) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %281 = ttir.empty() : tensor<13x13xsi32>
        %282 = "ttir.broadcast"(%280, %281) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %283 = ttir.empty() : tensor<1xsi32>
        %284 = "ttir.reshape"(%arg17, %283) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %285 = ttir.empty() : tensor<13xsi32>
        %286 = "ttir.broadcast"(%284, %285) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %287 = ttir.empty() : tensor<13xsi32>
        %288 = "ttir.subtract"(%1, %286, %287) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %289 = ttir.empty() : tensor<13x1xsi32>
        %290 = "ttir.reshape"(%288, %289) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %291 = ttir.empty() : tensor<13x13xsi32>
        %292 = "ttir.broadcast"(%290, %291) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %293 = ttir.empty() : tensor<13x13xbf16>
        %294 = "ttir.gt"(%282, %292, %293) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %295 = ttir.empty() : tensor<13x13xui8>
        %296 = "ttir.typecast"(%294, %295) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %297 = ttir.empty() : tensor<13x13xui8>
        %298 = "ttir.bitwise_and"(%296, %23, %297) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %299 = ttir.empty() : tensor<13x13xbf16>
        %300 = "ttir.ne"(%298, %31, %299) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %301 = ttir.empty() : tensor<13x13xui8>
        %302 = "ttir.typecast"(%300, %301) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %303 = ttir.empty() : tensor<13x1xsi32>
        %304 = "ttir.reshape"(%1, %303) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %305 = ttir.empty() : tensor<13x13xsi32>
        %306 = "ttir.broadcast"(%304, %305) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %307 = ttir.empty() : tensor<13x13xbf16>
        %308 = "ttir.le"(%282, %306, %307) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %309 = ttir.empty() : tensor<13x13xui8>
        %310 = "ttir.typecast"(%308, %309) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %311 = ttir.empty() : tensor<13x13xui8>
        %312 = "ttir.bitwise_and"(%302, %310, %311) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %313 = ttir.empty() : tensor<13x13xbf16>
        %314 = "ttir.ne"(%312, %31, %313) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %315 = ttir.empty() : tensor<1x1x13x13xbf16>
        %316 = "ttir.reshape"(%314, %315) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %317 = ttir.empty() : tensor<1x1x1x1xbf16>
        %318 = "ttir.reshape"(%arg16, %317) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %319 = ttir.empty() : tensor<1x1x13x13xbf16>
        %320 = "ttir.broadcast"(%318, %319) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %321 = ttir.empty() : tensor<1x1x13x13xbf16>
        %322 = "ttir.where"(%316, %19, %320, %321) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %323 = ttir.empty() : tensor<1x64x13x13xbf16>
        %324 = "ttir.broadcast"(%322, %323) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %325 = ttir.empty() : tensor<1x64x13x13xbf16>
        %326 = "ttir.add"(%278, %324, %325) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %327 = ttir.empty() : tensor<1x64x1x1xbf16>
        %328 = "ttir.reshape"(%arg15, %327) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %329 = ttir.empty() : tensor<1x64x13x1xbf16>
        %330 = "ttir.broadcast"(%328, %329) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %331 = ttir.empty() : tensor<1x64x13x14xbf16>
        %332 = "ttir.concat"(%326, %330, %331) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %333 = ttir.empty() : tensor<2880x512xbf16>
        %334 = "ttir.permute"(%arg1, %333) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %335 = ttir.empty() : tensor<13x2880xbf16>
        %336 = "ttir.permute"(%82, %335) <{permutation = array<i64: 0, 1>}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %337 = ttir.empty() : tensor<2880x512xbf16>
        %338 = "ttir.permute"(%334, %337) <{permutation = array<i64: 0, 1>}> : (tensor<2880x512xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %339 = ttir.empty() : tensor<13x2880xbf16>
        %340 = "ttir.reshape"(%336, %339) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %341 = ttir.empty() : tensor<2880x512xbf16>
        %342 = "ttir.reshape"(%338, %341) <{shape = [2880 : i32, 512 : i32]}> : (tensor<2880x512xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %343 = ttir.empty() : tensor<13x512xbf16>
        %344 = "ttir.matmul"(%340, %342, %343) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %345 = ttir.empty() : tensor<13x512xbf16>
        %346 = "ttir.reshape"(%344, %345) <{shape = [13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %347 = ttir.empty() : tensor<1x13x512xbf16>
        %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %349 = ttir.empty() : tensor<1x1x512xbf16>
        %350 = "ttir.reshape"(%arg0, %349) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %351 = ttir.empty() : tensor<1x13x512xbf16>
        %352 = "ttir.broadcast"(%350, %351) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %353 = ttir.empty() : tensor<1x13x512xbf16>
        %354 = "ttir.add"(%348, %352, %353) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %355 = ttir.empty() : tensor<1x13x8x64xbf16>
        %356 = "ttir.reshape"(%354, %355) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %357 = ttir.empty() : tensor<1x8x13x64xbf16>
        %358 = "ttir.permute"(%356, %357) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %359 = ttir.empty() : tensor<2880xf32>
        %360 = "ttir.typecast"(%arg30, %359) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %361 = ttir.empty() : tensor<1x1x2880xf32>
        %362 = "ttir.reshape"(%360, %361) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %363 = ttir.empty() : tensor<1x13x2880xf32>
        %364 = "ttir.broadcast"(%362, %363) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %365 = ttir.empty() : tensor<1x64x13x14xbf16>
        %366 = "ttir.softmax"(%332, %365) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %367 = ttir.empty() : tensor<1x64x13x13xbf16>
        %368 = "ttir.slice_static"(%366, %367) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %369 = ttir.empty() : tensor<64x13x13xbf16>
        %370 = "ttir.reshape"(%368, %369) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %371 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %372 = "ttir.reshape"(%358, %371) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %373 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %374 = "ttir.broadcast"(%372, %373) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %375 = ttir.empty() : tensor<64x13x64xbf16>
        %376 = "ttir.reshape"(%374, %375) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %377 = ttir.empty() : tensor<64x13x13xbf16>
        %378 = "ttir.permute"(%370, %377) <{permutation = array<i64: 0, 1, 2>}> : (tensor<64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %379 = ttir.empty() : tensor<64x13x64xbf16>
        %380 = "ttir.permute"(%376, %379) <{permutation = array<i64: 0, 1, 2>}> : (tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %381 = ttir.empty() : tensor<64x13x13xbf16>
        %382 = "ttir.reshape"(%378, %381) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %383 = ttir.empty() : tensor<64x13x64xbf16>
        %384 = "ttir.reshape"(%380, %383) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %385 = ttir.empty() : tensor<64x13x64xbf16>
        %386 = "ttir.matmul"(%382, %384, %385) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %387 = ttir.empty() : tensor<64x13x64xbf16>
        %388 = "ttir.reshape"(%386, %387) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %389 = ttir.empty() : tensor<1x64x13x64xbf16>
        %390 = "ttir.reshape"(%388, %389) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %391 = ttir.empty() : tensor<13x4096xbf16>
        %392 = ttir.empty() : tensor<1x13x4096xbf16>
        %393 = "ttir.concatenate_heads"(%390, %392) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %394 = "ttir.reshape"(%393, %391) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %395 = ttir.empty() : tensor<4096x2880xbf16>
        %396 = "ttir.permute"(%arg14, %395) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %397 = ttir.empty() : tensor<13x4096xbf16>
        %398 = "ttir.permute"(%394, %397) <{permutation = array<i64: 0, 1>}> : (tensor<13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %399 = ttir.empty() : tensor<4096x2880xbf16>
        %400 = "ttir.permute"(%396, %399) <{permutation = array<i64: 0, 1>}> : (tensor<4096x2880xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %401 = ttir.empty() : tensor<13x4096xbf16>
        %402 = "ttir.reshape"(%398, %401) <{shape = [13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %403 = ttir.empty() : tensor<4096x2880xbf16>
        %404 = "ttir.reshape"(%400, %403) <{shape = [4096 : i32, 2880 : i32]}> : (tensor<4096x2880xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %405 = ttir.empty() : tensor<13x2880xbf16>
        %406 = "ttir.matmul"(%402, %404, %405) <{transpose_a = false, transpose_b = false}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %407 = ttir.empty() : tensor<13x2880xbf16>
        %408 = "ttir.reshape"(%406, %407) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %409 = ttir.empty() : tensor<1x13x2880xbf16>
        %410 = "ttir.reshape"(%408, %409) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %411 = ttir.empty() : tensor<1x1x2880xbf16>
        %412 = "ttir.reshape"(%arg13, %411) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %413 = ttir.empty() : tensor<1x13x2880xbf16>
        %414 = "ttir.broadcast"(%412, %413) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %415 = ttir.empty() : tensor<1x13x2880xbf16>
        %416 = "ttir.add"(%410, %414, %415) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %417 = ttir.empty() : tensor<1x13x2880xbf16>
        %418 = "ttir.add"(%54, %416, %417) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %419 = ttir.empty() : tensor<1x1x1xbf16>
        %420 = "ttir.reshape"(%arg29, %419) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %421 = ttir.empty() : tensor<32x13x2880xbf16>
        %422 = "ttir.broadcast"(%420, %421) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %423 = ttir.empty() : tensor<2880xf32>
        %424 = "ttir.typecast"(%arg21, %423) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %425 = ttir.empty() : tensor<1x1x2880xf32>
        %426 = "ttir.reshape"(%424, %425) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %427 = ttir.empty() : tensor<1x13x2880xf32>
        %428 = "ttir.broadcast"(%426, %427) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %429 = ttir.empty() : tensor<1x13x2880xf32>
        %430 = "ttir.typecast"(%418, %429) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %431 = ttir.empty() : tensor<1x13x2880xf32>
        %432 = "ttir.pow"(%430, %27, %431) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %433 = ttir.empty() : tensor<1x13xf32>
        %434 = "ttir.sum"(%432, %433) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %435 = ttir.empty() : tensor<1x13xf32>
        %436 = "ttir.multiply"(%434, %4, %435) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %437 = ttir.empty() : tensor<1x13x1xf32>
        %438 = "ttir.reshape"(%436, %437) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %439 = ttir.empty() : tensor<1x13x1xf32>
        %440 = "ttir.add"(%438, %68, %439) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %441 = ttir.empty() : tensor<1x13x1xf32>
        %442 = "ttir.rsqrt"(%440, %441) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %443 = ttir.empty() : tensor<1x13x2880xf32>
        %444 = "ttir.broadcast"(%442, %443) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %445 = ttir.empty() : tensor<1x13x2880xf32>
        %446 = "ttir.multiply"(%430, %444, %445) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %447 = ttir.empty() : tensor<1x13x2880xf32>
        %448 = "ttir.multiply"(%428, %446, %447) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %449 = ttir.empty() : tensor<1x13x2880xbf16>
        %450 = "ttir.typecast"(%448, %449) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %451 = ttir.empty() : tensor<13x2880xbf16>
        %452 = "ttir.reshape"(%450, %451) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %453 = ttir.empty() : tensor<416x2880xbf16>
        %454 = "ttir.concat"(%452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %452, %453) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %455 = ttir.empty() : tensor<32x13x2880xbf16>
        %456 = "ttir.reshape"(%454, %455) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %457 = ttir.empty() : tensor<32x13x2880xbf16>
        %458 = "ttir.permute"(%456, %457) <{permutation = array<i64: 0, 1, 2>}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %459 = ttir.empty() : tensor<32x2880x5760xbf16>
        %460 = "ttir.permute"(%arg28, %459) <{permutation = array<i64: 0, 1, 2>}> : (tensor<32x2880x5760xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x2880x5760xbf16>
        %461 = ttir.empty() : tensor<32x13x2880xbf16>
        %462 = "ttir.reshape"(%458, %461) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %463 = ttir.empty() : tensor<32x2880x5760xbf16>
        %464 = "ttir.reshape"(%460, %463) <{shape = [32 : i32, 2880 : i32, 5760 : i32]}> : (tensor<32x2880x5760xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x2880x5760xbf16>
        %465 = ttir.empty() : tensor<32x13x5760xbf16>
        %466 = "ttir.matmul"(%462, %464, %465) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %467 = ttir.empty() : tensor<32x13x5760xbf16>
        %468 = "ttir.reshape"(%466, %467) <{shape = [32 : i32, 13 : i32, 5760 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %469 = ttir.empty() : tensor<32x1x5760xbf16>
        %470 = "ttir.reshape"(%arg27, %469) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %471 = ttir.empty() : tensor<32x13x5760xbf16>
        %472 = "ttir.broadcast"(%470, %471) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %473 = ttir.empty() : tensor<32x13x5760xbf16>
        %474 = "ttir.add"(%468, %472, %473) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %475 = ttir.empty() : tensor<32x13x2880xbf16>
        %476 = "ttir.slice_static"(%474, %475) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %477 = ttir.empty() : tensor<1x1x1xbf16>
        %478 = "ttir.reshape"(%arg25, %477) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %479 = ttir.empty() : tensor<32x13x2880xbf16>
        %480 = "ttir.broadcast"(%478, %479) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %481 = ttir.empty() : tensor<32x13x2880xbf16>
        %482 = "ttir.clamp_tensor"(%476, %422, %480, %481) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %483 = ttir.empty() : tensor<32x13x2880xbf16>
        %484 = "ttir.add"(%482, %15, %483) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %485 = ttir.empty() : tensor<32x13x2880xf32>
        %486 = "ttir.typecast"(%484, %485) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %487 = ttir.empty() : tensor<1x1x1xbf16>
        %488 = "ttir.reshape"(%arg26, %487) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %489 = ttir.empty() : tensor<32x13x2880xbf16>
        %490 = "ttir.broadcast"(%488, %489) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %491 = ttir.empty() : tensor<32x13x2880xbf16>
        %492 = "ttir.slice_static"(%474, %491) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %493 = ttir.empty() : tensor<32x13x2880xbf16>
        %494 = "ttir.clamp_tensor"(%492, %490, %480, %493) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %495 = ttir.empty() : tensor<32x13x2880xf32>
        %496 = "ttir.typecast"(%494, %495) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %497 = ttir.empty() : tensor<1x1x1xf32>
        %498 = "ttir.reshape"(%arg24, %497) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %499 = ttir.empty() : tensor<32x13x2880xf32>
        %500 = "ttir.broadcast"(%498, %499) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %501 = ttir.empty() : tensor<32x13x2880xf32>
        %502 = "ttir.multiply"(%496, %500, %501) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %503 = ttir.empty() : tensor<32x13x2880xbf16>
        %504 = "ttir.typecast"(%502, %503) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %505 = ttir.empty() : tensor<32x13x2880xbf16>
        %506 = "ttir.sigmoid"(%504, %505) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %507 = ttir.empty() : tensor<32x13x2880xf32>
        %508 = "ttir.typecast"(%506, %507) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %509 = ttir.empty() : tensor<32x13x2880xf32>
        %510 = "ttir.multiply"(%496, %508, %509) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %511 = ttir.empty() : tensor<32x13x2880xf32>
        %512 = "ttir.multiply"(%486, %510, %511) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %513 = ttir.empty() : tensor<32x13x2880xbf16>
        %514 = "ttir.typecast"(%512, %513) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %515 = ttir.empty() : tensor<32x13x2880xbf16>
        %516 = "ttir.permute"(%514, %515) <{permutation = array<i64: 0, 1, 2>}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %517 = ttir.empty() : tensor<32x2880x2880xbf16>
        %518 = "ttir.permute"(%arg23, %517) <{permutation = array<i64: 0, 1, 2>}> : (tensor<32x2880x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x2880x2880xbf16>
        %519 = ttir.empty() : tensor<32x13x2880xbf16>
        %520 = "ttir.reshape"(%516, %519) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %521 = ttir.empty() : tensor<32x2880x2880xbf16>
        %522 = "ttir.reshape"(%518, %521) <{shape = [32 : i32, 2880 : i32, 2880 : i32]}> : (tensor<32x2880x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x2880x2880xbf16>
        %523 = ttir.empty() : tensor<32x13x2880xbf16>
        %524 = "ttir.matmul"(%520, %522, %523) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %525 = ttir.empty() : tensor<32x13x2880xbf16>
        %526 = "ttir.reshape"(%524, %525) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %527 = ttir.empty() : tensor<32x1x2880xbf16>
        %528 = "ttir.reshape"(%arg22, %527) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %529 = ttir.empty() : tensor<32x13x2880xbf16>
        %530 = "ttir.broadcast"(%528, %529) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %531 = ttir.empty() : tensor<32x13x2880xbf16>
        %532 = "ttir.add"(%526, %530, %531) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %533 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %534 = "ttir.reshape"(%532, %533) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %535 = ttir.empty() : tensor<32x1x13x2880xf32>
        %536 = "ttir.typecast"(%534, %535) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %537 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %538 = ttir.empty() : tensor<13x1x1xsi32>
        %539 = "ttir.reshape"(%537, %538) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %540 = ttir.empty() : tensor<13x4x1xsi32>
        %541 = "ttir.broadcast"(%539, %540) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %542 = ttir.empty() : tensor<2880x32xbf16>
        %543 = "ttir.permute"(%arg12, %542) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %544 = ttir.empty() : tensor<13x2880xbf16>
        %545 = "ttir.permute"(%452, %544) <{permutation = array<i64: 0, 1>}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %546 = ttir.empty() : tensor<2880x32xbf16>
        %547 = "ttir.permute"(%543, %546) <{permutation = array<i64: 0, 1>}> : (tensor<2880x32xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %548 = ttir.empty() : tensor<13x2880xbf16>
        %549 = "ttir.reshape"(%545, %548) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %550 = ttir.empty() : tensor<2880x32xbf16>
        %551 = "ttir.reshape"(%547, %550) <{shape = [2880 : i32, 32 : i32]}> : (tensor<2880x32xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %552 = ttir.empty() : tensor<13x32xbf16>
        %553 = "ttir.matmul"(%549, %551, %552) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %554 = ttir.empty() : tensor<13x32xbf16>
        %555 = "ttir.reshape"(%553, %554) <{shape = [13 : i32, 32 : i32]}> : (tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %556 = ttir.empty() : tensor<1x32xbf16>
        %557 = "ttir.reshape"(%arg11, %556) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %558 = ttir.empty() : tensor<13x32xbf16>
        %559 = "ttir.broadcast"(%557, %558) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %560 = ttir.empty() : tensor<13x32xbf16>
        %561 = "ttir.add"(%555, %559, %560) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %562 = ttir.empty() : tensor<13x32xbf16>
        %563 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%561, %562, %563) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %564 = ttir.empty() : tensor<13x4xsi32>
        %565 = "ttir.slice_static"(%indices, %564) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %566 = ttir.empty() : tensor<13x4x1xsi32>
        %567 = "ttir.reshape"(%565, %566) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %568 = ttir.empty() : tensor<13x4x2xsi32>
        %569 = "ttir.concat"(%541, %567, %568) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %570 = ttir.empty() : tensor<13x4xbf16>
        %571 = "ttir.slice_static"(%values, %570) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %572 = ttir.empty() : tensor<13x4xbf16>
        %573 = "ttir.softmax"(%571, %572) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %574 = ttir.empty() : tensor<13x32xbf16>
        %575 = "ttir.scatter"(%11, %569, %573, %574) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %576 = ttir.empty() : tensor<32x13xbf16>
        %577 = "ttir.permute"(%575, %576) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %578 = ttir.empty() : tensor<32x1x13x1xbf16>
        %579 = "ttir.reshape"(%577, %578) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %580 = ttir.empty() : tensor<32x1x13x1xf32>
        %581 = "ttir.typecast"(%579, %580) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %582 = ttir.empty() : tensor<32x1x13x2880xf32>
        %583 = "ttir.broadcast"(%581, %582) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %584 = ttir.empty() : tensor<32x1x13x2880xf32>
        %585 = "ttir.multiply"(%536, %583, %584) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %586 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %587 = "ttir.typecast"(%585, %586) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %588 = ttir.empty() : tensor<1x13x2880xbf16>
        %589 = "ttir.sum"(%587, %588) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %590 = ttir.empty() : tensor<1x13x2880xbf16>
        %591 = "ttir.add"(%418, %589, %590) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %592 = ttir.empty() : tensor<1x13x2880xf32>
        %593 = "ttir.typecast"(%591, %592) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %594 = ttir.empty() : tensor<1x13x2880xf32>
        %595 = "ttir.pow"(%593, %27, %594) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %596 = ttir.empty() : tensor<1x13xf32>
        %597 = "ttir.sum"(%595, %596) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %598 = ttir.empty() : tensor<1x13xf32>
        %599 = "ttir.multiply"(%597, %4, %598) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %600 = ttir.empty() : tensor<1x13x1xf32>
        %601 = "ttir.reshape"(%599, %600) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %602 = ttir.empty() : tensor<1x13x1xf32>
        %603 = "ttir.add"(%601, %68, %602) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %604 = ttir.empty() : tensor<1x13x1xf32>
        %605 = "ttir.rsqrt"(%603, %604) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %606 = ttir.empty() : tensor<1x13x2880xf32>
        %607 = "ttir.broadcast"(%605, %606) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %608 = ttir.empty() : tensor<1x13x2880xf32>
        %609 = "ttir.multiply"(%593, %607, %608) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %610 = ttir.empty() : tensor<1x13x2880xf32>
        %611 = "ttir.multiply"(%364, %609, %610) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %612 = ttir.empty() : tensor<1x13x2880xbf16>
        %613 = "ttir.typecast"(%611, %612) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %614 = ttir.empty() : tensor<13x2880xbf16>
        %615 = "ttir.reshape"(%613, %614) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %616 = ttir.empty() : tensor<2880x201088xbf16>
        %617 = "ttir.permute"(%arg10, %616) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %618 = ttir.empty() : tensor<13x2880xbf16>
        %619 = "ttir.permute"(%615, %618) <{permutation = array<i64: 0, 1>}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %620 = ttir.empty() : tensor<2880x201088xbf16>
        %621 = "ttir.permute"(%617, %620) <{permutation = array<i64: 0, 1>}> : (tensor<2880x201088xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %622 = ttir.empty() : tensor<13x2880xbf16>
        %623 = "ttir.reshape"(%619, %622) <{shape = [13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %624 = ttir.empty() : tensor<2880x201088xbf16>
        %625 = "ttir.reshape"(%621, %624) <{shape = [2880 : i32, 201088 : i32]}> : (tensor<2880x201088xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %626 = ttir.empty() : tensor<13x201088xbf16>
        %627 = "ttir.matmul"(%623, %625, %626) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %628 = ttir.empty() : tensor<13x201088xbf16>
        %629 = "ttir.reshape"(%627, %628) <{shape = [13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %630 = ttir.empty() : tensor<1x13x201088xbf16>
        %631 = "ttir.reshape"(%629, %630) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %354, %356, %358, %244, %629, %631 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.embedding"(%41, %arg4, %42) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<2880x4096xbf16>
        %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %76 = ttir.empty() : tensor<13x4096xbf16>
        %77 = "ttir.matmul"(%73, %75, %76) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %78 = ttir.empty() : tensor<1x13x4096xbf16>
        %79 = "ttir.reshape"(%77, %78) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %80 = ttir.empty() : tensor<1x1x4096xbf16>
        %81 = "ttir.reshape"(%arg19, %80) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %82 = ttir.empty() : tensor<1x13x4096xbf16>
        %83 = "ttir.broadcast"(%81, %82) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %84 = ttir.empty() : tensor<1x13x4096xbf16>
        %85 = "ttir.add"(%79, %83, %84) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %86 = ttir.empty() : tensor<1x13x64x64xbf16>
        %87 = "ttir.reshape"(%85, %86) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %88 = ttir.empty() : tensor<1x64x13x64xbf16>
        %89 = "ttir.permute"(%87, %88) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %90 = ttir.empty() : tensor<1x64x13x32xbf16>
        %91 = "ttir.slice_static"(%89, %90) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %92 = ttir.empty() : tensor<1x64x13x32xf32>
        %93 = "ttir.typecast"(%91, %92) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %94 = ttir.empty() : tensor<1x32x1xf32>
        %95 = "ttir.reshape"(%arg7, %94) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %96 = ttir.empty() : tensor<1x32x13xf32>
        %97 = "ttir.matmul"(%95, %0, %96) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.permute"(%97, %98) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x13x32xf32>
        %101 = "ttir.cos"(%99, %100) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %102 = ttir.empty() : tensor<1x1x1xf32>
        %103 = "ttir.reshape"(%arg6, %102) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.broadcast"(%103, %104) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xf32>
        %107 = "ttir.multiply"(%101, %105, %106) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %108 = ttir.empty() : tensor<1x13x32xbf16>
        %109 = "ttir.typecast"(%107, %108) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xbf16>
        %111 = "ttir.reshape"(%109, %110) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %112 = ttir.empty() : tensor<1x1x13x32xf32>
        %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.broadcast"(%113, %114) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xf32>
        %117 = "ttir.multiply"(%93, %115, %116) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.typecast"(%117, %118) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xbf16>
        %121 = "ttir.slice_static"(%89, %120) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %122 = ttir.empty() : tensor<1x64x13x32xf32>
        %123 = "ttir.typecast"(%121, %122) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.sin"(%99, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xf32>
        %127 = "ttir.multiply"(%125, %105, %126) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %128 = ttir.empty() : tensor<1x13x32xbf16>
        %129 = "ttir.typecast"(%127, %128) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xbf16>
        %131 = "ttir.reshape"(%129, %130) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %132 = ttir.empty() : tensor<1x1x13x32xf32>
        %133 = "ttir.typecast"(%131, %132) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.broadcast"(%133, %134) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xf32>
        %137 = "ttir.multiply"(%123, %135, %136) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.typecast"(%137, %138) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xbf16>
        %141 = "ttir.subtract"(%119, %139, %140) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %142 = ttir.empty() : tensor<1x64x13x32xf32>
        %143 = "ttir.multiply"(%123, %115, %142) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %144 = ttir.empty() : tensor<1x64x13x32xbf16>
        %145 = "ttir.typecast"(%143, %144) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %146 = ttir.empty() : tensor<1x64x13x32xf32>
        %147 = "ttir.multiply"(%93, %135, %146) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.typecast"(%147, %148) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x32xbf16>
        %151 = "ttir.add"(%145, %149, %150) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %152 = ttir.empty() : tensor<1x64x13x64xbf16>
        %153 = "ttir.concat"(%141, %151, %152) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %154 = ttir.empty() : tensor<64x13x64xbf16>
        %155 = "ttir.reshape"(%153, %154) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %156 = ttir.empty() : tensor<2880x512xbf16>
        %157 = "ttir.permute"(%arg9, %156) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %158 = ttir.empty() : tensor<13x512xbf16>
        %159 = "ttir.matmul"(%73, %157, %158) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %160 = ttir.empty() : tensor<1x13x512xbf16>
        %161 = "ttir.reshape"(%159, %160) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %162 = ttir.empty() : tensor<1x1x512xbf16>
        %163 = "ttir.reshape"(%arg8, %162) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %164 = ttir.empty() : tensor<1x13x512xbf16>
        %165 = "ttir.broadcast"(%163, %164) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %166 = ttir.empty() : tensor<1x13x512xbf16>
        %167 = "ttir.add"(%161, %165, %166) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %168 = ttir.empty() : tensor<1x13x8x64xbf16>
        %169 = "ttir.reshape"(%167, %168) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %170 = ttir.empty() : tensor<1x8x13x64xbf16>
        %171 = "ttir.permute"(%169, %170) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %172 = ttir.empty() : tensor<1x8x13x32xbf16>
        %173 = "ttir.slice_static"(%171, %172) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %174 = ttir.empty() : tensor<1x8x13x32xf32>
        %175 = "ttir.typecast"(%173, %174) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %176 = ttir.empty() : tensor<1x8x13x32xf32>
        %177 = "ttir.broadcast"(%113, %176) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %178 = ttir.empty() : tensor<1x8x13x32xf32>
        %179 = "ttir.multiply"(%175, %177, %178) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %180 = ttir.empty() : tensor<1x8x13x32xbf16>
        %181 = "ttir.typecast"(%179, %180) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %182 = ttir.empty() : tensor<1x8x13x32xbf16>
        %183 = "ttir.slice_static"(%171, %182) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %184 = ttir.empty() : tensor<1x8x13x32xf32>
        %185 = "ttir.typecast"(%183, %184) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %186 = ttir.empty() : tensor<1x8x13x32xf32>
        %187 = "ttir.broadcast"(%133, %186) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %188 = ttir.empty() : tensor<1x8x13x32xf32>
        %189 = "ttir.multiply"(%185, %187, %188) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %190 = ttir.empty() : tensor<1x8x13x32xbf16>
        %191 = "ttir.typecast"(%189, %190) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %192 = ttir.empty() : tensor<1x8x13x32xbf16>
        %193 = "ttir.subtract"(%181, %191, %192) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %194 = ttir.empty() : tensor<1x8x13x32xf32>
        %195 = "ttir.multiply"(%185, %177, %194) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %196 = ttir.empty() : tensor<1x8x13x32xbf16>
        %197 = "ttir.typecast"(%195, %196) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %198 = ttir.empty() : tensor<1x8x13x32xf32>
        %199 = "ttir.multiply"(%175, %187, %198) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %200 = ttir.empty() : tensor<1x8x13x32xbf16>
        %201 = "ttir.typecast"(%199, %200) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %202 = ttir.empty() : tensor<1x8x13x32xbf16>
        %203 = "ttir.add"(%197, %201, %202) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %204 = ttir.empty() : tensor<1x8x13x64xbf16>
        %205 = "ttir.concat"(%193, %203, %204) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %206 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %208 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %210 = ttir.empty() : tensor<1x64x13x64xbf16>
        %211 = "ttir.reshape"(%209, %210) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %212 = ttir.empty() : tensor<1x64x64x13xbf16>
        %213 = "ttir.permute"(%211, %212) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %214 = ttir.empty() : tensor<64x64x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %216 = ttir.empty() : tensor<64x13x13xbf16>
        %217 = "ttir.matmul"(%155, %215, %216) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %218 = ttir.empty() : tensor<1x64x13x13xbf16>
        %219 = "ttir.reshape"(%217, %218) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.typecast"(%219, %220) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x1x1x1xf32>
        %223 = "ttir.reshape"(%arg18, %222) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xf32>
        %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %226 = ttir.empty() : tensor<1x64x13x13xf32>
        %227 = "ttir.multiply"(%221, %225, %226) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %228 = ttir.empty() : tensor<1x64x13x13xbf16>
        %229 = "ttir.typecast"(%227, %228) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %230 = ttir.empty() : tensor<1x13xsi32>
        %231 = "ttir.reshape"(%1, %230) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %232 = ttir.empty() : tensor<13x13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %234 = ttir.empty() : tensor<1xsi32>
        %235 = "ttir.reshape"(%arg17, %234) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %236 = ttir.empty() : tensor<13xsi32>
        %237 = "ttir.broadcast"(%235, %236) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %238 = ttir.empty() : tensor<13xsi32>
        %239 = "ttir.subtract"(%1, %237, %238) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %240 = ttir.empty() : tensor<13x1xsi32>
        %241 = "ttir.reshape"(%239, %240) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %242 = ttir.empty() : tensor<13x13xsi32>
        %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %244 = ttir.empty() : tensor<13x13xbf16>
        %245 = "ttir.gt"(%233, %243, %244) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %246 = ttir.empty() : tensor<13x13xui8>
        %247 = "ttir.typecast"(%245, %246) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.bitwise_and"(%247, %23, %248) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x13xbf16>
        %251 = "ttir.ne"(%249, %31, %250) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %252 = ttir.empty() : tensor<13x13xui8>
        %253 = "ttir.typecast"(%251, %252) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %254 = ttir.empty() : tensor<13x1xsi32>
        %255 = "ttir.reshape"(%1, %254) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %256 = ttir.empty() : tensor<13x13xsi32>
        %257 = "ttir.broadcast"(%255, %256) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %258 = ttir.empty() : tensor<13x13xbf16>
        %259 = "ttir.le"(%233, %257, %258) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %260 = ttir.empty() : tensor<13x13xui8>
        %261 = "ttir.typecast"(%259, %260) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %262 = ttir.empty() : tensor<13x13xui8>
        %263 = "ttir.bitwise_and"(%253, %261, %262) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %264 = ttir.empty() : tensor<13x13xbf16>
        %265 = "ttir.ne"(%263, %31, %264) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.reshape"(%265, %266) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x1x1xbf16>
        %269 = "ttir.reshape"(%arg16, %268) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %270 = ttir.empty() : tensor<1x1x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %272 = ttir.empty() : tensor<1x1x13x13xbf16>
        %273 = "ttir.where"(%267, %19, %271, %272) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x13x13xbf16>
        %275 = "ttir.broadcast"(%273, %274) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %276 = ttir.empty() : tensor<1x64x13x13xbf16>
        %277 = "ttir.add"(%229, %275, %276) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %278 = ttir.empty() : tensor<1x64x1x1xbf16>
        %279 = "ttir.reshape"(%arg15, %278) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %280 = ttir.empty() : tensor<1x64x13x1xbf16>
        %281 = "ttir.broadcast"(%279, %280) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %282 = ttir.empty() : tensor<1x64x13x14xbf16>
        %283 = "ttir.concat"(%277, %281, %282) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %284 = ttir.empty() : tensor<2880x512xbf16>
        %285 = "ttir.permute"(%arg1, %284) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %286 = ttir.empty() : tensor<13x512xbf16>
        %287 = "ttir.matmul"(%73, %285, %286) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %288 = ttir.empty() : tensor<1x13x512xbf16>
        %289 = "ttir.reshape"(%287, %288) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %290 = ttir.empty() : tensor<1x1x512xbf16>
        %291 = "ttir.reshape"(%arg0, %290) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %292 = ttir.empty() : tensor<1x13x512xbf16>
        %293 = "ttir.broadcast"(%291, %292) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %294 = ttir.empty() : tensor<1x13x512xbf16>
        %295 = "ttir.add"(%289, %293, %294) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %296 = ttir.empty() : tensor<1x13x8x64xbf16>
        %297 = "ttir.reshape"(%295, %296) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %298 = ttir.empty() : tensor<1x8x13x64xbf16>
        %299 = "ttir.permute"(%297, %298) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %300 = ttir.empty() : tensor<2880xf32>
        %301 = "ttir.typecast"(%arg30, %300) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %302 = ttir.empty() : tensor<1x1x2880xf32>
        %303 = "ttir.reshape"(%301, %302) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %304 = ttir.empty() : tensor<1x13x2880xf32>
        %305 = "ttir.broadcast"(%303, %304) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %306 = ttir.empty() : tensor<1x64x13x14xbf16>
        %307 = "ttir.softmax"(%283, %306) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %308 = ttir.empty() : tensor<1x64x13x13xbf16>
        %309 = "ttir.slice_static"(%307, %308) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %310 = ttir.empty() : tensor<64x13x13xbf16>
        %311 = "ttir.reshape"(%309, %310) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %312 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %313 = "ttir.reshape"(%299, %312) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %314 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %315 = "ttir.broadcast"(%313, %314) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %316 = ttir.empty() : tensor<64x13x64xbf16>
        %317 = "ttir.reshape"(%315, %316) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %318 = ttir.empty() : tensor<64x13x64xbf16>
        %319 = "ttir.matmul"(%311, %317, %318) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %320 = ttir.empty() : tensor<1x64x13x64xbf16>
        %321 = "ttir.reshape"(%319, %320) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %322 = ttir.empty() : tensor<13x4096xbf16>
        %323 = ttir.empty() : tensor<1x13x4096xbf16>
        %324 = "ttir.concatenate_heads"(%321, %323) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %325 = "ttir.reshape"(%324, %322) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %326 = ttir.empty() : tensor<4096x2880xbf16>
        %327 = "ttir.permute"(%arg14, %326) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %328 = ttir.empty() : tensor<13x2880xbf16>
        %329 = "ttir.matmul"(%325, %327, %328) <{transpose_a = false, transpose_b = false}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %330 = ttir.empty() : tensor<1x13x2880xbf16>
        %331 = "ttir.reshape"(%329, %330) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %332 = ttir.empty() : tensor<1x1x2880xbf16>
        %333 = "ttir.reshape"(%arg13, %332) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %334 = ttir.empty() : tensor<1x13x2880xbf16>
        %335 = "ttir.broadcast"(%333, %334) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %336 = ttir.empty() : tensor<1x13x2880xbf16>
        %337 = "ttir.add"(%331, %335, %336) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %338 = ttir.empty() : tensor<1x13x2880xbf16>
        %339 = "ttir.add"(%45, %337, %338) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %340 = ttir.empty() : tensor<1x1x1xbf16>
        %341 = "ttir.reshape"(%arg29, %340) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %342 = ttir.empty() : tensor<32x13x2880xbf16>
        %343 = "ttir.broadcast"(%341, %342) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %344 = ttir.empty() : tensor<2880xf32>
        %345 = "ttir.typecast"(%arg21, %344) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %346 = ttir.empty() : tensor<1x1x2880xf32>
        %347 = "ttir.reshape"(%345, %346) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %348 = ttir.empty() : tensor<1x13x2880xf32>
        %349 = "ttir.broadcast"(%347, %348) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %350 = ttir.empty() : tensor<1x13x2880xf32>
        %351 = "ttir.typecast"(%339, %350) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %352 = ttir.empty() : tensor<1x13x2880xf32>
        %353 = "ttir.pow"(%351, %27, %352) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %354 = ttir.empty() : tensor<1x13xf32>
        %355 = "ttir.sum"(%353, %354) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %356 = ttir.empty() : tensor<1x13xf32>
        %357 = "ttir.multiply"(%355, %4, %356) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %358 = ttir.empty() : tensor<1x13x1xf32>
        %359 = "ttir.reshape"(%357, %358) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %360 = ttir.empty() : tensor<1x13x1xf32>
        %361 = "ttir.add"(%359, %59, %360) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %362 = ttir.empty() : tensor<1x13x1xf32>
        %363 = "ttir.rsqrt"(%361, %362) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %364 = ttir.empty() : tensor<1x13x2880xf32>
        %365 = "ttir.broadcast"(%363, %364) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %366 = ttir.empty() : tensor<1x13x2880xf32>
        %367 = "ttir.multiply"(%351, %365, %366) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %368 = ttir.empty() : tensor<1x13x2880xf32>
        %369 = "ttir.multiply"(%349, %367, %368) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %370 = ttir.empty() : tensor<1x13x2880xbf16>
        %371 = "ttir.typecast"(%369, %370) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %372 = ttir.empty() : tensor<13x2880xbf16>
        %373 = "ttir.reshape"(%371, %372) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %374 = ttir.empty() : tensor<416x2880xbf16>
        %375 = "ttir.concat"(%373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %374) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %376 = ttir.empty() : tensor<32x13x2880xbf16>
        %377 = "ttir.reshape"(%375, %376) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %378 = ttir.empty() : tensor<32x13x5760xbf16>
        %379 = "ttir.matmul"(%377, %arg28, %378) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %380 = ttir.empty() : tensor<32x1x5760xbf16>
        %381 = "ttir.reshape"(%arg27, %380) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %382 = ttir.empty() : tensor<32x13x5760xbf16>
        %383 = "ttir.broadcast"(%381, %382) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %384 = ttir.empty() : tensor<32x13x5760xbf16>
        %385 = "ttir.add"(%379, %383, %384) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.slice_static"(%385, %386) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<1x1x1xbf16>
        %389 = "ttir.reshape"(%arg25, %388) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %390 = ttir.empty() : tensor<32x13x2880xbf16>
        %391 = "ttir.broadcast"(%389, %390) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xbf16>
        %393 = "ttir.clamp_tensor"(%387, %343, %391, %392) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %394 = ttir.empty() : tensor<32x13x2880xbf16>
        %395 = "ttir.add"(%393, %15, %394) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xf32>
        %397 = "ttir.typecast"(%395, %396) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %398 = ttir.empty() : tensor<1x1x1xbf16>
        %399 = "ttir.reshape"(%arg26, %398) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %400 = ttir.empty() : tensor<32x13x2880xbf16>
        %401 = "ttir.broadcast"(%399, %400) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %402 = ttir.empty() : tensor<32x13x2880xbf16>
        %403 = "ttir.slice_static"(%385, %402) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %404 = ttir.empty() : tensor<32x13x2880xbf16>
        %405 = "ttir.clamp_tensor"(%403, %401, %391, %404) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %406 = ttir.empty() : tensor<32x13x2880xf32>
        %407 = "ttir.typecast"(%405, %406) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %408 = ttir.empty() : tensor<1x1x1xf32>
        %409 = "ttir.reshape"(%arg24, %408) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %410 = ttir.empty() : tensor<32x13x2880xf32>
        %411 = "ttir.broadcast"(%409, %410) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.multiply"(%407, %411, %412) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<32x13x2880xbf16>
        %415 = "ttir.typecast"(%413, %414) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.sigmoid"(%415, %416) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xf32>
        %419 = "ttir.typecast"(%417, %418) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %420 = ttir.empty() : tensor<32x13x2880xf32>
        %421 = "ttir.multiply"(%407, %419, %420) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %422 = ttir.empty() : tensor<32x13x2880xf32>
        %423 = "ttir.multiply"(%397, %421, %422) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %424 = ttir.empty() : tensor<32x13x2880xbf16>
        %425 = "ttir.typecast"(%423, %424) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %426 = ttir.empty() : tensor<32x13x2880xbf16>
        %427 = "ttir.matmul"(%425, %arg23, %426) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %428 = ttir.empty() : tensor<32x1x2880xbf16>
        %429 = "ttir.reshape"(%arg22, %428) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %430 = ttir.empty() : tensor<32x13x2880xbf16>
        %431 = "ttir.broadcast"(%429, %430) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %432 = ttir.empty() : tensor<32x13x2880xbf16>
        %433 = "ttir.add"(%427, %431, %432) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %434 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %435 = "ttir.reshape"(%433, %434) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %436 = ttir.empty() : tensor<32x1x13x2880xf32>
        %437 = "ttir.typecast"(%435, %436) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %438 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %439 = ttir.empty() : tensor<13x1x1xsi32>
        %440 = "ttir.reshape"(%438, %439) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %441 = ttir.empty() : tensor<13x4x1xsi32>
        %442 = "ttir.broadcast"(%440, %441) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %443 = ttir.empty() : tensor<2880x32xbf16>
        %444 = "ttir.permute"(%arg12, %443) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %445 = ttir.empty() : tensor<13x32xbf16>
        %446 = "ttir.matmul"(%373, %444, %445) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %447 = ttir.empty() : tensor<1x32xbf16>
        %448 = "ttir.reshape"(%arg11, %447) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %449 = ttir.empty() : tensor<13x32xbf16>
        %450 = "ttir.broadcast"(%448, %449) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %451 = ttir.empty() : tensor<13x32xbf16>
        %452 = "ttir.add"(%446, %450, %451) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %453 = ttir.empty() : tensor<13x32xbf16>
        %454 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%452, %453, %454) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %455 = ttir.empty() : tensor<13x4xsi32>
        %456 = "ttir.slice_static"(%indices, %455) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %457 = ttir.empty() : tensor<13x4x1xsi32>
        %458 = "ttir.reshape"(%456, %457) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %459 = ttir.empty() : tensor<13x4x2xsi32>
        %460 = "ttir.concat"(%442, %458, %459) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %461 = ttir.empty() : tensor<13x4xbf16>
        %462 = "ttir.slice_static"(%values, %461) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %463 = ttir.empty() : tensor<13x4xbf16>
        %464 = "ttir.softmax"(%462, %463) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %465 = ttir.empty() : tensor<13x32xbf16>
        %466 = "ttir.scatter"(%11, %460, %464, %465) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %467 = ttir.empty() : tensor<32x13xbf16>
        %468 = "ttir.permute"(%466, %467) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %469 = ttir.empty() : tensor<32x1x13x1xbf16>
        %470 = "ttir.reshape"(%468, %469) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %471 = ttir.empty() : tensor<32x1x13x1xf32>
        %472 = "ttir.typecast"(%470, %471) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %473 = ttir.empty() : tensor<32x1x13x2880xf32>
        %474 = "ttir.broadcast"(%472, %473) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %475 = ttir.empty() : tensor<32x1x13x2880xf32>
        %476 = "ttir.multiply"(%437, %474, %475) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %477 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %478 = "ttir.typecast"(%476, %477) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %479 = ttir.empty() : tensor<1x13x2880xbf16>
        %480 = "ttir.sum"(%478, %479) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %481 = ttir.empty() : tensor<1x13x2880xbf16>
        %482 = "ttir.add"(%339, %480, %481) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %483 = ttir.empty() : tensor<1x13x2880xf32>
        %484 = "ttir.typecast"(%482, %483) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %485 = ttir.empty() : tensor<1x13x2880xf32>
        %486 = "ttir.pow"(%484, %27, %485) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %487 = ttir.empty() : tensor<1x13xf32>
        %488 = "ttir.sum"(%486, %487) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %489 = ttir.empty() : tensor<1x13xf32>
        %490 = "ttir.multiply"(%488, %4, %489) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %491 = ttir.empty() : tensor<1x13x1xf32>
        %492 = "ttir.reshape"(%490, %491) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %493 = ttir.empty() : tensor<1x13x1xf32>
        %494 = "ttir.add"(%492, %59, %493) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %495 = ttir.empty() : tensor<1x13x1xf32>
        %496 = "ttir.rsqrt"(%494, %495) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %497 = ttir.empty() : tensor<1x13x2880xf32>
        %498 = "ttir.broadcast"(%496, %497) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %499 = ttir.empty() : tensor<1x13x2880xf32>
        %500 = "ttir.multiply"(%484, %498, %499) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %501 = ttir.empty() : tensor<1x13x2880xf32>
        %502 = "ttir.multiply"(%305, %500, %501) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %503 = ttir.empty() : tensor<1x13x2880xbf16>
        %504 = "ttir.typecast"(%502, %503) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %505 = ttir.empty() : tensor<13x2880xbf16>
        %506 = "ttir.reshape"(%504, %505) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %507 = ttir.empty() : tensor<2880x201088xbf16>
        %508 = "ttir.permute"(%arg10, %507) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %509 = ttir.empty() : tensor<13x201088xbf16>
        %510 = "ttir.matmul"(%506, %508, %509) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %511 = ttir.empty() : tensor<1x13x201088xbf16>
        %512 = "ttir.reshape"(%510, %511) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %295, %297, %299, %205, %510, %512 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.embedding"(%41, %arg4, %42) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<2880x4096xbf16>
        %75 = "ttir.permute"(%arg20, %74) <{permutation = array<i64: 1, 0>}> : (tensor<4096x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<2880x4096xbf16>
        %76 = ttir.empty() : tensor<13x4096xbf16>
        %77 = "ttir.matmul"(%73, %75, %76) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %78 = ttir.empty() : tensor<1x13x4096xbf16>
        %79 = "ttir.reshape"(%77, %78) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %80 = ttir.empty() : tensor<1x1x4096xbf16>
        %81 = "ttir.reshape"(%arg19, %80) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %82 = ttir.empty() : tensor<1x13x4096xbf16>
        %83 = "ttir.broadcast"(%81, %82) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %84 = ttir.empty() : tensor<1x13x4096xbf16>
        %85 = "ttir.add"(%79, %83, %84) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %86 = ttir.empty() : tensor<1x13x64x64xbf16>
        %87 = "ttir.reshape"(%85, %86) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %88 = ttir.empty() : tensor<1x64x13x64xbf16>
        %89 = "ttir.permute"(%87, %88) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %90 = ttir.empty() : tensor<1x64x13x32xbf16>
        %91 = "ttir.slice_static"(%89, %90) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %92 = ttir.empty() : tensor<1x64x13x32xf32>
        %93 = "ttir.typecast"(%91, %92) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %94 = ttir.empty() : tensor<1x32x1xf32>
        %95 = "ttir.reshape"(%arg7, %94) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %96 = ttir.empty() : tensor<1x32x13xf32>
        %97 = "ttir.matmul"(%95, %0, %96) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.permute"(%97, %98) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x13x32xf32>
        %101 = "ttir.cos"(%99, %100) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %102 = ttir.empty() : tensor<1x1x1xf32>
        %103 = "ttir.reshape"(%arg6, %102) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.broadcast"(%103, %104) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xf32>
        %107 = "ttir.multiply"(%101, %105, %106) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %108 = ttir.empty() : tensor<1x13x32xbf16>
        %109 = "ttir.typecast"(%107, %108) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xbf16>
        %111 = "ttir.reshape"(%109, %110) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %112 = ttir.empty() : tensor<1x1x13x32xf32>
        %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.broadcast"(%113, %114) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xf32>
        %117 = "ttir.multiply"(%93, %115, %116) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.typecast"(%117, %118) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xbf16>
        %121 = "ttir.slice_static"(%89, %120) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %122 = ttir.empty() : tensor<1x64x13x32xf32>
        %123 = "ttir.typecast"(%121, %122) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.sin"(%99, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xf32>
        %127 = "ttir.multiply"(%125, %105, %126) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %128 = ttir.empty() : tensor<1x13x32xbf16>
        %129 = "ttir.typecast"(%127, %128) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xbf16>
        %131 = "ttir.reshape"(%129, %130) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %132 = ttir.empty() : tensor<1x1x13x32xf32>
        %133 = "ttir.typecast"(%131, %132) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.broadcast"(%133, %134) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xf32>
        %137 = "ttir.multiply"(%123, %135, %136) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.typecast"(%137, %138) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xbf16>
        %141 = "ttir.subtract"(%119, %139, %140) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %142 = ttir.empty() : tensor<1x64x13x32xf32>
        %143 = "ttir.multiply"(%123, %115, %142) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %144 = ttir.empty() : tensor<1x64x13x32xbf16>
        %145 = "ttir.typecast"(%143, %144) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %146 = ttir.empty() : tensor<1x64x13x32xf32>
        %147 = "ttir.multiply"(%93, %135, %146) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.typecast"(%147, %148) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x32xbf16>
        %151 = "ttir.add"(%145, %149, %150) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %152 = ttir.empty() : tensor<1x64x13x64xbf16>
        %153 = "ttir.concat"(%141, %151, %152) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %154 = ttir.empty() : tensor<64x13x64xbf16>
        %155 = "ttir.reshape"(%153, %154) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %156 = ttir.empty() : tensor<2880x512xbf16>
        %157 = "ttir.permute"(%arg9, %156) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %158 = ttir.empty() : tensor<13x512xbf16>
        %159 = "ttir.matmul"(%73, %157, %158) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %160 = ttir.empty() : tensor<1x13x512xbf16>
        %161 = "ttir.reshape"(%159, %160) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %162 = ttir.empty() : tensor<1x1x512xbf16>
        %163 = "ttir.reshape"(%arg8, %162) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %164 = ttir.empty() : tensor<1x13x512xbf16>
        %165 = "ttir.broadcast"(%163, %164) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %166 = ttir.empty() : tensor<1x13x512xbf16>
        %167 = "ttir.add"(%161, %165, %166) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %168 = ttir.empty() : tensor<1x13x8x64xbf16>
        %169 = "ttir.reshape"(%167, %168) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %170 = ttir.empty() : tensor<1x8x13x64xbf16>
        %171 = "ttir.permute"(%169, %170) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %172 = ttir.empty() : tensor<1x8x13x32xbf16>
        %173 = "ttir.slice_static"(%171, %172) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %174 = ttir.empty() : tensor<1x8x13x32xf32>
        %175 = "ttir.typecast"(%173, %174) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %176 = ttir.empty() : tensor<1x8x13x32xf32>
        %177 = "ttir.broadcast"(%113, %176) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %178 = ttir.empty() : tensor<1x8x13x32xf32>
        %179 = "ttir.multiply"(%175, %177, %178) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %180 = ttir.empty() : tensor<1x8x13x32xbf16>
        %181 = "ttir.typecast"(%179, %180) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %182 = ttir.empty() : tensor<1x8x13x32xbf16>
        %183 = "ttir.slice_static"(%171, %182) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %184 = ttir.empty() : tensor<1x8x13x32xf32>
        %185 = "ttir.typecast"(%183, %184) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %186 = ttir.empty() : tensor<1x8x13x32xf32>
        %187 = "ttir.broadcast"(%133, %186) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %188 = ttir.empty() : tensor<1x8x13x32xf32>
        %189 = "ttir.multiply"(%185, %187, %188) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %190 = ttir.empty() : tensor<1x8x13x32xbf16>
        %191 = "ttir.typecast"(%189, %190) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %192 = ttir.empty() : tensor<1x8x13x32xbf16>
        %193 = "ttir.subtract"(%181, %191, %192) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %194 = ttir.empty() : tensor<1x8x13x32xf32>
        %195 = "ttir.multiply"(%185, %177, %194) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %196 = ttir.empty() : tensor<1x8x13x32xbf16>
        %197 = "ttir.typecast"(%195, %196) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %198 = ttir.empty() : tensor<1x8x13x32xf32>
        %199 = "ttir.multiply"(%175, %187, %198) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %200 = ttir.empty() : tensor<1x8x13x32xbf16>
        %201 = "ttir.typecast"(%199, %200) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %202 = ttir.empty() : tensor<1x8x13x32xbf16>
        %203 = "ttir.add"(%197, %201, %202) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %204 = ttir.empty() : tensor<1x8x13x64xbf16>
        %205 = "ttir.concat"(%193, %203, %204) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %206 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %208 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %210 = ttir.empty() : tensor<1x64x13x64xbf16>
        %211 = "ttir.reshape"(%209, %210) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %212 = ttir.empty() : tensor<1x64x64x13xbf16>
        %213 = "ttir.permute"(%211, %212) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %214 = ttir.empty() : tensor<64x64x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %216 = ttir.empty() : tensor<64x13x13xbf16>
        %217 = "ttir.matmul"(%155, %215, %216) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %218 = ttir.empty() : tensor<1x64x13x13xbf16>
        %219 = "ttir.reshape"(%217, %218) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.typecast"(%219, %220) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x1x1x1xf32>
        %223 = "ttir.reshape"(%arg18, %222) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xf32>
        %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %226 = ttir.empty() : tensor<1x64x13x13xf32>
        %227 = "ttir.multiply"(%221, %225, %226) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %228 = ttir.empty() : tensor<1x64x13x13xbf16>
        %229 = "ttir.typecast"(%227, %228) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %230 = ttir.empty() : tensor<1x13xsi32>
        %231 = "ttir.reshape"(%1, %230) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %232 = ttir.empty() : tensor<13x13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %234 = ttir.empty() : tensor<1xsi32>
        %235 = "ttir.reshape"(%arg17, %234) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %236 = ttir.empty() : tensor<13xsi32>
        %237 = "ttir.broadcast"(%235, %236) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %238 = ttir.empty() : tensor<13xsi32>
        %239 = "ttir.subtract"(%1, %237, %238) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %240 = ttir.empty() : tensor<13x1xsi32>
        %241 = "ttir.reshape"(%239, %240) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %242 = ttir.empty() : tensor<13x13xsi32>
        %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %244 = ttir.empty() : tensor<13x13xbf16>
        %245 = "ttir.gt"(%233, %243, %244) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %246 = ttir.empty() : tensor<13x13xui8>
        %247 = "ttir.typecast"(%245, %246) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.bitwise_and"(%247, %23, %248) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x13xbf16>
        %251 = "ttir.ne"(%249, %31, %250) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %252 = ttir.empty() : tensor<13x13xui8>
        %253 = "ttir.typecast"(%251, %252) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %254 = ttir.empty() : tensor<13x1xsi32>
        %255 = "ttir.reshape"(%1, %254) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %256 = ttir.empty() : tensor<13x13xsi32>
        %257 = "ttir.broadcast"(%255, %256) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %258 = ttir.empty() : tensor<13x13xbf16>
        %259 = "ttir.le"(%233, %257, %258) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %260 = ttir.empty() : tensor<13x13xui8>
        %261 = "ttir.typecast"(%259, %260) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %262 = ttir.empty() : tensor<13x13xui8>
        %263 = "ttir.bitwise_and"(%253, %261, %262) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %264 = ttir.empty() : tensor<13x13xbf16>
        %265 = "ttir.ne"(%263, %31, %264) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.reshape"(%265, %266) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x1x1xbf16>
        %269 = "ttir.reshape"(%arg16, %268) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %270 = ttir.empty() : tensor<1x1x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %272 = ttir.empty() : tensor<1x1x13x13xbf16>
        %273 = "ttir.where"(%267, %19, %271, %272) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x13x13xbf16>
        %275 = "ttir.broadcast"(%273, %274) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %276 = ttir.empty() : tensor<1x64x13x13xbf16>
        %277 = "ttir.add"(%229, %275, %276) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %278 = ttir.empty() : tensor<1x64x1x1xbf16>
        %279 = "ttir.reshape"(%arg15, %278) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %280 = ttir.empty() : tensor<1x64x13x1xbf16>
        %281 = "ttir.broadcast"(%279, %280) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %282 = ttir.empty() : tensor<1x64x13x14xbf16>
        %283 = "ttir.concat"(%277, %281, %282) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %284 = ttir.empty() : tensor<2880x512xbf16>
        %285 = "ttir.permute"(%arg1, %284) <{permutation = array<i64: 1, 0>}> : (tensor<512x2880xbf16>, tensor<2880x512xbf16>) -> tensor<2880x512xbf16>
        %286 = ttir.empty() : tensor<13x512xbf16>
        %287 = "ttir.matmul"(%73, %285, %286) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %288 = ttir.empty() : tensor<1x13x512xbf16>
        %289 = "ttir.reshape"(%287, %288) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %290 = ttir.empty() : tensor<1x1x512xbf16>
        %291 = "ttir.reshape"(%arg0, %290) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %292 = ttir.empty() : tensor<1x13x512xbf16>
        %293 = "ttir.broadcast"(%291, %292) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %294 = ttir.empty() : tensor<1x13x512xbf16>
        %295 = "ttir.add"(%289, %293, %294) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %296 = ttir.empty() : tensor<1x13x8x64xbf16>
        %297 = "ttir.reshape"(%295, %296) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %298 = ttir.empty() : tensor<1x8x13x64xbf16>
        %299 = "ttir.permute"(%297, %298) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %300 = ttir.empty() : tensor<2880xf32>
        %301 = "ttir.typecast"(%arg30, %300) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %302 = ttir.empty() : tensor<1x1x2880xf32>
        %303 = "ttir.reshape"(%301, %302) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %304 = ttir.empty() : tensor<1x13x2880xf32>
        %305 = "ttir.broadcast"(%303, %304) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %306 = ttir.empty() : tensor<1x64x13x14xbf16>
        %307 = "ttir.softmax"(%283, %306) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %308 = ttir.empty() : tensor<1x64x13x13xbf16>
        %309 = "ttir.slice_static"(%307, %308) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %310 = ttir.empty() : tensor<64x13x13xbf16>
        %311 = "ttir.reshape"(%309, %310) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %312 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %313 = "ttir.reshape"(%299, %312) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %314 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %315 = "ttir.broadcast"(%313, %314) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %316 = ttir.empty() : tensor<64x13x64xbf16>
        %317 = "ttir.reshape"(%315, %316) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %318 = ttir.empty() : tensor<64x13x64xbf16>
        %319 = "ttir.matmul"(%311, %317, %318) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %320 = ttir.empty() : tensor<1x64x13x64xbf16>
        %321 = "ttir.reshape"(%319, %320) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %322 = ttir.empty() : tensor<13x4096xbf16>
        %323 = ttir.empty() : tensor<1x13x4096xbf16>
        %324 = "ttir.concatenate_heads"(%321, %323) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %325 = "ttir.reshape"(%324, %322) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %326 = ttir.empty() : tensor<4096x2880xbf16>
        %327 = "ttir.permute"(%arg14, %326) <{permutation = array<i64: 1, 0>}> : (tensor<2880x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<4096x2880xbf16>
        %328 = ttir.empty() : tensor<13x2880xbf16>
        %329 = "ttir.matmul"(%325, %327, %328) <{transpose_a = false, transpose_b = false}> : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %330 = ttir.empty() : tensor<1x13x2880xbf16>
        %331 = "ttir.reshape"(%329, %330) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %332 = ttir.empty() : tensor<1x1x2880xbf16>
        %333 = "ttir.reshape"(%arg13, %332) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %334 = ttir.empty() : tensor<1x13x2880xbf16>
        %335 = "ttir.broadcast"(%333, %334) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %336 = ttir.empty() : tensor<1x13x2880xbf16>
        %337 = "ttir.add"(%331, %335, %336) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %338 = ttir.empty() : tensor<1x13x2880xbf16>
        %339 = "ttir.add"(%45, %337, %338) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %340 = ttir.empty() : tensor<1x1x1xbf16>
        %341 = "ttir.reshape"(%arg29, %340) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %342 = ttir.empty() : tensor<32x13x2880xbf16>
        %343 = "ttir.broadcast"(%341, %342) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %344 = ttir.empty() : tensor<2880xf32>
        %345 = "ttir.typecast"(%arg21, %344) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %346 = ttir.empty() : tensor<1x1x2880xf32>
        %347 = "ttir.reshape"(%345, %346) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %348 = ttir.empty() : tensor<1x13x2880xf32>
        %349 = "ttir.broadcast"(%347, %348) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %350 = ttir.empty() : tensor<1x13x2880xf32>
        %351 = "ttir.typecast"(%339, %350) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %352 = ttir.empty() : tensor<1x13x2880xf32>
        %353 = "ttir.pow"(%351, %27, %352) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %354 = ttir.empty() : tensor<1x13xf32>
        %355 = "ttir.sum"(%353, %354) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %356 = ttir.empty() : tensor<1x13xf32>
        %357 = "ttir.multiply"(%355, %4, %356) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %358 = ttir.empty() : tensor<1x13x1xf32>
        %359 = "ttir.reshape"(%357, %358) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %360 = ttir.empty() : tensor<1x13x1xf32>
        %361 = "ttir.add"(%359, %59, %360) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %362 = ttir.empty() : tensor<1x13x1xf32>
        %363 = "ttir.rsqrt"(%361, %362) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %364 = ttir.empty() : tensor<1x13x2880xf32>
        %365 = "ttir.broadcast"(%363, %364) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %366 = ttir.empty() : tensor<1x13x2880xf32>
        %367 = "ttir.multiply"(%351, %365, %366) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %368 = ttir.empty() : tensor<1x13x2880xf32>
        %369 = "ttir.multiply"(%349, %367, %368) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %370 = ttir.empty() : tensor<1x13x2880xbf16>
        %371 = "ttir.typecast"(%369, %370) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %372 = ttir.empty() : tensor<13x2880xbf16>
        %373 = "ttir.reshape"(%371, %372) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %374 = ttir.empty() : tensor<416x2880xbf16>
        %375 = "ttir.concat"(%373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %373, %374) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %376 = ttir.empty() : tensor<32x13x2880xbf16>
        %377 = "ttir.reshape"(%375, %376) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %378 = ttir.empty() : tensor<32x13x5760xbf16>
        %379 = "ttir.matmul"(%377, %arg28, %378) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %380 = ttir.empty() : tensor<32x1x5760xbf16>
        %381 = "ttir.reshape"(%arg27, %380) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %382 = ttir.empty() : tensor<32x13x5760xbf16>
        %383 = "ttir.broadcast"(%381, %382) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %384 = ttir.empty() : tensor<32x13x5760xbf16>
        %385 = "ttir.add"(%379, %383, %384) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.slice_static"(%385, %386) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<1x1x1xbf16>
        %389 = "ttir.reshape"(%arg25, %388) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %390 = ttir.empty() : tensor<32x13x2880xbf16>
        %391 = "ttir.broadcast"(%389, %390) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xbf16>
        %393 = "ttir.clamp_tensor"(%387, %343, %391, %392) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %394 = ttir.empty() : tensor<32x13x2880xbf16>
        %395 = "ttir.add"(%393, %15, %394) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xf32>
        %397 = "ttir.typecast"(%395, %396) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %398 = ttir.empty() : tensor<1x1x1xbf16>
        %399 = "ttir.reshape"(%arg26, %398) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %400 = ttir.empty() : tensor<32x13x2880xbf16>
        %401 = "ttir.broadcast"(%399, %400) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %402 = ttir.empty() : tensor<32x13x2880xbf16>
        %403 = "ttir.slice_static"(%385, %402) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %404 = ttir.empty() : tensor<32x13x2880xbf16>
        %405 = "ttir.clamp_tensor"(%403, %401, %391, %404) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %406 = ttir.empty() : tensor<32x13x2880xf32>
        %407 = "ttir.typecast"(%405, %406) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %408 = ttir.empty() : tensor<1x1x1xf32>
        %409 = "ttir.reshape"(%arg24, %408) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %410 = ttir.empty() : tensor<32x13x2880xf32>
        %411 = "ttir.broadcast"(%409, %410) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.multiply"(%407, %411, %412) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<32x13x2880xbf16>
        %415 = "ttir.typecast"(%413, %414) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.sigmoid"(%415, %416) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xf32>
        %419 = "ttir.typecast"(%417, %418) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %420 = ttir.empty() : tensor<32x13x2880xf32>
        %421 = "ttir.multiply"(%407, %419, %420) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %422 = ttir.empty() : tensor<32x13x2880xf32>
        %423 = "ttir.multiply"(%397, %421, %422) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %424 = ttir.empty() : tensor<32x13x2880xbf16>
        %425 = "ttir.typecast"(%423, %424) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %426 = ttir.empty() : tensor<32x13x2880xbf16>
        %427 = "ttir.matmul"(%425, %arg23, %426) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %428 = ttir.empty() : tensor<32x1x2880xbf16>
        %429 = "ttir.reshape"(%arg22, %428) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %430 = ttir.empty() : tensor<32x13x2880xbf16>
        %431 = "ttir.broadcast"(%429, %430) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %432 = ttir.empty() : tensor<32x13x2880xbf16>
        %433 = "ttir.add"(%427, %431, %432) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %434 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %435 = "ttir.reshape"(%433, %434) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %436 = ttir.empty() : tensor<32x1x13x2880xf32>
        %437 = "ttir.typecast"(%435, %436) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %438 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %439 = ttir.empty() : tensor<13x1x1xsi32>
        %440 = "ttir.reshape"(%438, %439) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %441 = ttir.empty() : tensor<13x4x1xsi32>
        %442 = "ttir.broadcast"(%440, %441) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %443 = ttir.empty() : tensor<2880x32xbf16>
        %444 = "ttir.permute"(%arg12, %443) <{permutation = array<i64: 1, 0>}> : (tensor<32x2880xbf16>, tensor<2880x32xbf16>) -> tensor<2880x32xbf16>
        %445 = ttir.empty() : tensor<13x32xbf16>
        %446 = "ttir.matmul"(%373, %444, %445) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %447 = ttir.empty() : tensor<1x32xbf16>
        %448 = "ttir.reshape"(%arg11, %447) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %449 = ttir.empty() : tensor<13x32xbf16>
        %450 = "ttir.broadcast"(%448, %449) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %451 = ttir.empty() : tensor<13x32xbf16>
        %452 = "ttir.add"(%446, %450, %451) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %453 = ttir.empty() : tensor<13x32xbf16>
        %454 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%452, %453, %454) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %455 = ttir.empty() : tensor<13x4xsi32>
        %456 = "ttir.slice_static"(%indices, %455) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %457 = ttir.empty() : tensor<13x4x1xsi32>
        %458 = "ttir.reshape"(%456, %457) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %459 = ttir.empty() : tensor<13x4x2xsi32>
        %460 = "ttir.concat"(%442, %458, %459) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %461 = ttir.empty() : tensor<13x4xbf16>
        %462 = "ttir.slice_static"(%values, %461) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %463 = ttir.empty() : tensor<13x4xbf16>
        %464 = "ttir.softmax"(%462, %463) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %465 = ttir.empty() : tensor<13x32xbf16>
        %466 = "ttir.scatter"(%11, %460, %464, %465) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %467 = ttir.empty() : tensor<32x13xbf16>
        %468 = "ttir.permute"(%466, %467) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %469 = ttir.empty() : tensor<32x1x13x1xbf16>
        %470 = "ttir.reshape"(%468, %469) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %471 = ttir.empty() : tensor<32x1x13x1xf32>
        %472 = "ttir.typecast"(%470, %471) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %473 = ttir.empty() : tensor<32x1x13x2880xf32>
        %474 = "ttir.broadcast"(%472, %473) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %475 = ttir.empty() : tensor<32x1x13x2880xf32>
        %476 = "ttir.multiply"(%437, %474, %475) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %477 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %478 = "ttir.typecast"(%476, %477) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %479 = ttir.empty() : tensor<1x13x2880xbf16>
        %480 = "ttir.sum"(%478, %479) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %481 = ttir.empty() : tensor<1x13x2880xbf16>
        %482 = "ttir.add"(%339, %480, %481) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %483 = ttir.empty() : tensor<1x13x2880xf32>
        %484 = "ttir.typecast"(%482, %483) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %485 = ttir.empty() : tensor<1x13x2880xf32>
        %486 = "ttir.pow"(%484, %27, %485) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %487 = ttir.empty() : tensor<1x13xf32>
        %488 = "ttir.sum"(%486, %487) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %489 = ttir.empty() : tensor<1x13xf32>
        %490 = "ttir.multiply"(%488, %4, %489) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %491 = ttir.empty() : tensor<1x13x1xf32>
        %492 = "ttir.reshape"(%490, %491) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %493 = ttir.empty() : tensor<1x13x1xf32>
        %494 = "ttir.add"(%492, %59, %493) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %495 = ttir.empty() : tensor<1x13x1xf32>
        %496 = "ttir.rsqrt"(%494, %495) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %497 = ttir.empty() : tensor<1x13x2880xf32>
        %498 = "ttir.broadcast"(%496, %497) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %499 = ttir.empty() : tensor<1x13x2880xf32>
        %500 = "ttir.multiply"(%484, %498, %499) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %501 = ttir.empty() : tensor<1x13x2880xf32>
        %502 = "ttir.multiply"(%305, %500, %501) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %503 = ttir.empty() : tensor<1x13x2880xbf16>
        %504 = "ttir.typecast"(%502, %503) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %505 = ttir.empty() : tensor<13x2880xbf16>
        %506 = "ttir.reshape"(%504, %505) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %507 = ttir.empty() : tensor<2880x201088xbf16>
        %508 = "ttir.permute"(%arg10, %507) <{permutation = array<i64: 1, 0>}> : (tensor<201088x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<2880x201088xbf16>
        %509 = ttir.empty() : tensor<13x201088xbf16>
        %510 = "ttir.matmul"(%506, %508, %509) <{transpose_a = false, transpose_b = false}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %511 = ttir.empty() : tensor<1x13x201088xbf16>
        %512 = "ttir.reshape"(%510, %511) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %295, %297, %299, %205, %510, %512 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.embedding"(%41, %arg4, %42) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<13x4096xbf16>
        %75 = "ttir.matmul"(%73, %arg20, %74) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<4096x2880xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %76 = ttir.empty() : tensor<1x13x4096xbf16>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %78 = ttir.empty() : tensor<1x1x4096xbf16>
        %79 = "ttir.reshape"(%arg19, %78) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %80 = ttir.empty() : tensor<1x13x4096xbf16>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %82 = ttir.empty() : tensor<1x13x4096xbf16>
        %83 = "ttir.add"(%77, %81, %82) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %84 = ttir.empty() : tensor<1x13x64x64xbf16>
        %85 = "ttir.reshape"(%83, %84) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %86 = ttir.empty() : tensor<1x64x13x64xbf16>
        %87 = "ttir.permute"(%85, %86) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %88 = ttir.empty() : tensor<1x64x13x32xbf16>
        %89 = "ttir.slice_static"(%87, %88) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %90 = ttir.empty() : tensor<1x64x13x32xf32>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %92 = ttir.empty() : tensor<1x32x1xf32>
        %93 = "ttir.reshape"(%arg7, %92) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %94 = ttir.empty() : tensor<1x32x13xf32>
        %95 = "ttir.matmul"(%93, %0, %94) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%91, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%87, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%91, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<13x512xbf16>
        %155 = "ttir.matmul"(%73, %arg9, %154) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %156 = ttir.empty() : tensor<1x13x512xbf16>
        %157 = "ttir.reshape"(%155, %156) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %158 = ttir.empty() : tensor<1x1x512xbf16>
        %159 = "ttir.reshape"(%arg8, %158) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %160 = ttir.empty() : tensor<1x13x512xbf16>
        %161 = "ttir.broadcast"(%159, %160) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %162 = ttir.empty() : tensor<1x13x512xbf16>
        %163 = "ttir.add"(%157, %161, %162) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %164 = ttir.empty() : tensor<1x13x8x64xbf16>
        %165 = "ttir.reshape"(%163, %164) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %166 = ttir.empty() : tensor<1x8x13x64xbf16>
        %167 = "ttir.permute"(%165, %166) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %168 = ttir.empty() : tensor<1x8x13x32xbf16>
        %169 = "ttir.slice_static"(%167, %168) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %170 = ttir.empty() : tensor<1x8x13x32xf32>
        %171 = "ttir.typecast"(%169, %170) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %172 = ttir.empty() : tensor<1x8x13x32xf32>
        %173 = "ttir.broadcast"(%111, %172) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %174 = ttir.empty() : tensor<1x8x13x32xf32>
        %175 = "ttir.multiply"(%171, %173, %174) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %176 = ttir.empty() : tensor<1x8x13x32xbf16>
        %177 = "ttir.typecast"(%175, %176) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %178 = ttir.empty() : tensor<1x8x13x32xbf16>
        %179 = "ttir.slice_static"(%167, %178) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %180 = ttir.empty() : tensor<1x8x13x32xf32>
        %181 = "ttir.typecast"(%179, %180) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %182 = ttir.empty() : tensor<1x8x13x32xf32>
        %183 = "ttir.broadcast"(%131, %182) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %184 = ttir.empty() : tensor<1x8x13x32xf32>
        %185 = "ttir.multiply"(%181, %183, %184) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %186 = ttir.empty() : tensor<1x8x13x32xbf16>
        %187 = "ttir.typecast"(%185, %186) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %188 = ttir.empty() : tensor<1x8x13x32xbf16>
        %189 = "ttir.subtract"(%177, %187, %188) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %190 = ttir.empty() : tensor<1x8x13x32xf32>
        %191 = "ttir.multiply"(%181, %173, %190) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %192 = ttir.empty() : tensor<1x8x13x32xbf16>
        %193 = "ttir.typecast"(%191, %192) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %194 = ttir.empty() : tensor<1x8x13x32xf32>
        %195 = "ttir.multiply"(%171, %183, %194) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %196 = ttir.empty() : tensor<1x8x13x32xbf16>
        %197 = "ttir.typecast"(%195, %196) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %198 = ttir.empty() : tensor<1x8x13x32xbf16>
        %199 = "ttir.add"(%193, %197, %198) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %200 = ttir.empty() : tensor<1x8x13x64xbf16>
        %201 = "ttir.concat"(%189, %199, %200) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %202 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %203 = "ttir.reshape"(%201, %202) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %204 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %205 = "ttir.broadcast"(%203, %204) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %206 = ttir.empty() : tensor<1x64x13x64xbf16>
        %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %208 = ttir.empty() : tensor<1x64x64x13xbf16>
        %209 = "ttir.permute"(%207, %208) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %210 = ttir.empty() : tensor<64x64x13xbf16>
        %211 = "ttir.reshape"(%209, %210) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %212 = ttir.empty() : tensor<64x13x13xbf16>
        %213 = "ttir.matmul"(%153, %211, %212) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<13x512xbf16>
        %281 = "ttir.matmul"(%73, %arg1, %280) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %282 = ttir.empty() : tensor<1x13x512xbf16>
        %283 = "ttir.reshape"(%281, %282) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %284 = ttir.empty() : tensor<1x1x512xbf16>
        %285 = "ttir.reshape"(%arg0, %284) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %286 = ttir.empty() : tensor<1x13x512xbf16>
        %287 = "ttir.broadcast"(%285, %286) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %288 = ttir.empty() : tensor<1x13x512xbf16>
        %289 = "ttir.add"(%283, %287, %288) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %290 = ttir.empty() : tensor<1x13x8x64xbf16>
        %291 = "ttir.reshape"(%289, %290) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %292 = ttir.empty() : tensor<1x8x13x64xbf16>
        %293 = "ttir.permute"(%291, %292) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %294 = ttir.empty() : tensor<2880xf32>
        %295 = "ttir.typecast"(%arg30, %294) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %296 = ttir.empty() : tensor<1x1x2880xf32>
        %297 = "ttir.reshape"(%295, %296) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %298 = ttir.empty() : tensor<1x13x2880xf32>
        %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %300 = ttir.empty() : tensor<1x64x13x14xbf16>
        %301 = "ttir.softmax"(%279, %300) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %302 = ttir.empty() : tensor<1x64x13x13xbf16>
        %303 = "ttir.slice_static"(%301, %302) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %304 = ttir.empty() : tensor<64x13x13xbf16>
        %305 = "ttir.reshape"(%303, %304) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %306 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %307 = "ttir.reshape"(%293, %306) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %308 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %309 = "ttir.broadcast"(%307, %308) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %310 = ttir.empty() : tensor<64x13x64xbf16>
        %311 = "ttir.reshape"(%309, %310) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %312 = ttir.empty() : tensor<64x13x64xbf16>
        %313 = "ttir.matmul"(%305, %311, %312) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %314 = ttir.empty() : tensor<1x64x13x64xbf16>
        %315 = "ttir.reshape"(%313, %314) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %316 = ttir.empty() : tensor<13x4096xbf16>
        %317 = ttir.empty() : tensor<1x13x4096xbf16>
        %318 = "ttir.concatenate_heads"(%315, %317) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %319 = "ttir.reshape"(%318, %316) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %320 = ttir.empty() : tensor<13x2880xbf16>
        %321 = "ttir.matmul"(%319, %arg14, %320) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16>, tensor<2880x4096xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %322 = ttir.empty() : tensor<1x13x2880xbf16>
        %323 = "ttir.reshape"(%321, %322) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %324 = ttir.empty() : tensor<1x1x2880xbf16>
        %325 = "ttir.reshape"(%arg13, %324) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %326 = ttir.empty() : tensor<1x13x2880xbf16>
        %327 = "ttir.broadcast"(%325, %326) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %328 = ttir.empty() : tensor<1x13x2880xbf16>
        %329 = "ttir.add"(%323, %327, %328) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %330 = ttir.empty() : tensor<1x13x2880xbf16>
        %331 = "ttir.add"(%45, %329, %330) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %332 = ttir.empty() : tensor<1x1x1xbf16>
        %333 = "ttir.reshape"(%arg29, %332) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %334 = ttir.empty() : tensor<32x13x2880xbf16>
        %335 = "ttir.broadcast"(%333, %334) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %336 = ttir.empty() : tensor<2880xf32>
        %337 = "ttir.typecast"(%arg21, %336) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %338 = ttir.empty() : tensor<1x1x2880xf32>
        %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %340 = ttir.empty() : tensor<1x13x2880xf32>
        %341 = "ttir.broadcast"(%339, %340) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %342 = ttir.empty() : tensor<1x13x2880xf32>
        %343 = "ttir.typecast"(%331, %342) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %344 = ttir.empty() : tensor<1x13x2880xf32>
        %345 = "ttir.pow"(%343, %27, %344) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %346 = ttir.empty() : tensor<1x13xf32>
        %347 = "ttir.sum"(%345, %346) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %348 = ttir.empty() : tensor<1x13xf32>
        %349 = "ttir.multiply"(%347, %4, %348) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %350 = ttir.empty() : tensor<1x13x1xf32>
        %351 = "ttir.reshape"(%349, %350) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %352 = ttir.empty() : tensor<1x13x1xf32>
        %353 = "ttir.add"(%351, %59, %352) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %354 = ttir.empty() : tensor<1x13x1xf32>
        %355 = "ttir.rsqrt"(%353, %354) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %356 = ttir.empty() : tensor<1x13x2880xf32>
        %357 = "ttir.broadcast"(%355, %356) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %358 = ttir.empty() : tensor<1x13x2880xf32>
        %359 = "ttir.multiply"(%343, %357, %358) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %360 = ttir.empty() : tensor<1x13x2880xf32>
        %361 = "ttir.multiply"(%341, %359, %360) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %362 = ttir.empty() : tensor<1x13x2880xbf16>
        %363 = "ttir.typecast"(%361, %362) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %364 = ttir.empty() : tensor<13x2880xbf16>
        %365 = "ttir.reshape"(%363, %364) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %366 = ttir.empty() : tensor<416x2880xbf16>
        %367 = "ttir.concat"(%365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %366) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %368 = ttir.empty() : tensor<32x13x2880xbf16>
        %369 = "ttir.reshape"(%367, %368) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %370 = ttir.empty() : tensor<32x13x5760xbf16>
        %371 = "ttir.matmul"(%369, %arg28, %370) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %372 = ttir.empty() : tensor<32x1x5760xbf16>
        %373 = "ttir.reshape"(%arg27, %372) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %374 = ttir.empty() : tensor<32x13x5760xbf16>
        %375 = "ttir.broadcast"(%373, %374) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %376 = ttir.empty() : tensor<32x13x5760xbf16>
        %377 = "ttir.add"(%371, %375, %376) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %378 = ttir.empty() : tensor<32x13x2880xbf16>
        %379 = "ttir.slice_static"(%377, %378) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %380 = ttir.empty() : tensor<1x1x1xbf16>
        %381 = "ttir.reshape"(%arg25, %380) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %382 = ttir.empty() : tensor<32x13x2880xbf16>
        %383 = "ttir.broadcast"(%381, %382) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %384 = ttir.empty() : tensor<32x13x2880xbf16>
        %385 = "ttir.clamp_tensor"(%379, %335, %383, %384) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.add"(%385, %15, %386) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<32x13x2880xf32>
        %389 = "ttir.typecast"(%387, %388) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %390 = ttir.empty() : tensor<1x1x1xbf16>
        %391 = "ttir.reshape"(%arg26, %390) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xbf16>
        %393 = "ttir.broadcast"(%391, %392) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %394 = ttir.empty() : tensor<32x13x2880xbf16>
        %395 = "ttir.slice_static"(%377, %394) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xbf16>
        %397 = "ttir.clamp_tensor"(%395, %393, %383, %396) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %398 = ttir.empty() : tensor<32x13x2880xf32>
        %399 = "ttir.typecast"(%397, %398) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %400 = ttir.empty() : tensor<1x1x1xf32>
        %401 = "ttir.reshape"(%arg24, %400) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %402 = ttir.empty() : tensor<32x13x2880xf32>
        %403 = "ttir.broadcast"(%401, %402) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %404 = ttir.empty() : tensor<32x13x2880xf32>
        %405 = "ttir.multiply"(%399, %403, %404) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.typecast"(%405, %406) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.sigmoid"(%407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xf32>
        %411 = "ttir.typecast"(%409, %410) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.multiply"(%399, %411, %412) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<32x13x2880xf32>
        %415 = "ttir.multiply"(%389, %413, %414) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.typecast"(%415, %416) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xbf16>
        %419 = "ttir.matmul"(%417, %arg23, %418) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %420 = ttir.empty() : tensor<32x1x2880xbf16>
        %421 = "ttir.reshape"(%arg22, %420) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xbf16>
        %423 = "ttir.broadcast"(%421, %422) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %424 = ttir.empty() : tensor<32x13x2880xbf16>
        %425 = "ttir.add"(%419, %423, %424) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %426 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %427 = "ttir.reshape"(%425, %426) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %428 = ttir.empty() : tensor<32x1x13x2880xf32>
        %429 = "ttir.typecast"(%427, %428) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %430 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %431 = ttir.empty() : tensor<13x1x1xsi32>
        %432 = "ttir.reshape"(%430, %431) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %433 = ttir.empty() : tensor<13x4x1xsi32>
        %434 = "ttir.broadcast"(%432, %433) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %435 = ttir.empty() : tensor<13x32xbf16>
        %436 = "ttir.matmul"(%365, %arg12, %435) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<32x2880xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %437 = ttir.empty() : tensor<1x32xbf16>
        %438 = "ttir.reshape"(%arg11, %437) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %439 = ttir.empty() : tensor<13x32xbf16>
        %440 = "ttir.broadcast"(%438, %439) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %441 = ttir.empty() : tensor<13x32xbf16>
        %442 = "ttir.add"(%436, %440, %441) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %443 = ttir.empty() : tensor<13x32xbf16>
        %444 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%442, %443, %444) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %445 = ttir.empty() : tensor<13x4xsi32>
        %446 = "ttir.slice_static"(%indices, %445) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %447 = ttir.empty() : tensor<13x4x1xsi32>
        %448 = "ttir.reshape"(%446, %447) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %449 = ttir.empty() : tensor<13x4x2xsi32>
        %450 = "ttir.concat"(%434, %448, %449) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %451 = ttir.empty() : tensor<13x4xbf16>
        %452 = "ttir.slice_static"(%values, %451) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %453 = ttir.empty() : tensor<13x4xbf16>
        %454 = "ttir.softmax"(%452, %453) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %455 = ttir.empty() : tensor<13x32xbf16>
        %456 = "ttir.scatter"(%11, %450, %454, %455) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %457 = ttir.empty() : tensor<32x13xbf16>
        %458 = "ttir.permute"(%456, %457) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %459 = ttir.empty() : tensor<32x1x13x1xbf16>
        %460 = "ttir.reshape"(%458, %459) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %461 = ttir.empty() : tensor<32x1x13x1xf32>
        %462 = "ttir.typecast"(%460, %461) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %463 = ttir.empty() : tensor<32x1x13x2880xf32>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %465 = ttir.empty() : tensor<32x1x13x2880xf32>
        %466 = "ttir.multiply"(%429, %464, %465) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %467 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %468 = "ttir.typecast"(%466, %467) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %469 = ttir.empty() : tensor<1x13x2880xbf16>
        %470 = "ttir.sum"(%468, %469) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %471 = ttir.empty() : tensor<1x13x2880xbf16>
        %472 = "ttir.add"(%331, %470, %471) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %473 = ttir.empty() : tensor<1x13x2880xf32>
        %474 = "ttir.typecast"(%472, %473) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %475 = ttir.empty() : tensor<1x13x2880xf32>
        %476 = "ttir.pow"(%474, %27, %475) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %477 = ttir.empty() : tensor<1x13xf32>
        %478 = "ttir.sum"(%476, %477) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %479 = ttir.empty() : tensor<1x13xf32>
        %480 = "ttir.multiply"(%478, %4, %479) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %481 = ttir.empty() : tensor<1x13x1xf32>
        %482 = "ttir.reshape"(%480, %481) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %483 = ttir.empty() : tensor<1x13x1xf32>
        %484 = "ttir.add"(%482, %59, %483) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %485 = ttir.empty() : tensor<1x13x1xf32>
        %486 = "ttir.rsqrt"(%484, %485) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %487 = ttir.empty() : tensor<1x13x2880xf32>
        %488 = "ttir.broadcast"(%486, %487) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %489 = ttir.empty() : tensor<1x13x2880xf32>
        %490 = "ttir.multiply"(%474, %488, %489) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %491 = ttir.empty() : tensor<1x13x2880xf32>
        %492 = "ttir.multiply"(%299, %490, %491) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %493 = ttir.empty() : tensor<1x13x2880xbf16>
        %494 = "ttir.typecast"(%492, %493) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %495 = ttir.empty() : tensor<13x2880xbf16>
        %496 = "ttir.reshape"(%494, %495) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %497 = ttir.empty() : tensor<13x201088xbf16>
        %498 = "ttir.matmul"(%496, %arg10, %497) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<201088x2880xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %499 = ttir.empty() : tensor<1x13x201088xbf16>
        %500 = "ttir.reshape"(%498, %499) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %289, %291, %293, %201, %498, %500 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.embedding"(%41, %arg4, %42) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<13x4096xbf16>
        %75 = "ttir.matmul"(%73, %arg20, %74) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<4096x2880xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %76 = ttir.empty() : tensor<1x13x4096xbf16>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %78 = ttir.empty() : tensor<1x1x4096xbf16>
        %79 = "ttir.reshape"(%arg19, %78) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %80 = ttir.empty() : tensor<1x13x4096xbf16>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %82 = ttir.empty() : tensor<1x13x4096xbf16>
        %83 = "ttir.add"(%77, %81, %82) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %84 = ttir.empty() : tensor<1x13x64x64xbf16>
        %85 = "ttir.reshape"(%83, %84) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %86 = ttir.empty() : tensor<1x64x13x64xbf16>
        %87 = "ttir.permute"(%85, %86) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %88 = ttir.empty() : tensor<1x64x13x32xbf16>
        %89 = "ttir.slice_static"(%87, %88) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %90 = ttir.empty() : tensor<1x64x13x32xf32>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %92 = ttir.empty() : tensor<1x32x1xf32>
        %93 = "ttir.reshape"(%arg7, %92) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %94 = ttir.empty() : tensor<1x32x13xf32>
        %95 = "ttir.matmul"(%93, %0, %94) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%91, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%87, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%91, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<13x512xbf16>
        %155 = "ttir.matmul"(%73, %arg9, %154) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %156 = ttir.empty() : tensor<1x13x512xbf16>
        %157 = "ttir.reshape"(%155, %156) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %158 = ttir.empty() : tensor<1x1x512xbf16>
        %159 = "ttir.reshape"(%arg8, %158) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %160 = ttir.empty() : tensor<1x13x512xbf16>
        %161 = "ttir.broadcast"(%159, %160) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %162 = ttir.empty() : tensor<1x13x512xbf16>
        %163 = "ttir.add"(%157, %161, %162) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %164 = ttir.empty() : tensor<1x13x8x64xbf16>
        %165 = "ttir.reshape"(%163, %164) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %166 = ttir.empty() : tensor<1x8x13x64xbf16>
        %167 = "ttir.permute"(%165, %166) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %168 = ttir.empty() : tensor<1x8x13x32xbf16>
        %169 = "ttir.slice_static"(%167, %168) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %170 = ttir.empty() : tensor<1x8x13x32xf32>
        %171 = "ttir.typecast"(%169, %170) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %172 = ttir.empty() : tensor<1x8x13x32xf32>
        %173 = "ttir.broadcast"(%111, %172) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %174 = ttir.empty() : tensor<1x8x13x32xf32>
        %175 = "ttir.multiply"(%171, %173, %174) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %176 = ttir.empty() : tensor<1x8x13x32xbf16>
        %177 = "ttir.typecast"(%175, %176) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %178 = ttir.empty() : tensor<1x8x13x32xbf16>
        %179 = "ttir.slice_static"(%167, %178) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %180 = ttir.empty() : tensor<1x8x13x32xf32>
        %181 = "ttir.typecast"(%179, %180) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %182 = ttir.empty() : tensor<1x8x13x32xf32>
        %183 = "ttir.broadcast"(%131, %182) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %184 = ttir.empty() : tensor<1x8x13x32xf32>
        %185 = "ttir.multiply"(%181, %183, %184) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %186 = ttir.empty() : tensor<1x8x13x32xbf16>
        %187 = "ttir.typecast"(%185, %186) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %188 = ttir.empty() : tensor<1x8x13x32xbf16>
        %189 = "ttir.subtract"(%177, %187, %188) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %190 = ttir.empty() : tensor<1x8x13x32xf32>
        %191 = "ttir.multiply"(%181, %173, %190) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %192 = ttir.empty() : tensor<1x8x13x32xbf16>
        %193 = "ttir.typecast"(%191, %192) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %194 = ttir.empty() : tensor<1x8x13x32xf32>
        %195 = "ttir.multiply"(%171, %183, %194) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %196 = ttir.empty() : tensor<1x8x13x32xbf16>
        %197 = "ttir.typecast"(%195, %196) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %198 = ttir.empty() : tensor<1x8x13x32xbf16>
        %199 = "ttir.add"(%193, %197, %198) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %200 = ttir.empty() : tensor<1x8x13x64xbf16>
        %201 = "ttir.concat"(%189, %199, %200) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %202 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %203 = "ttir.reshape"(%201, %202) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %204 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %205 = "ttir.broadcast"(%203, %204) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %206 = ttir.empty() : tensor<1x64x13x64xbf16>
        %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %208 = ttir.empty() : tensor<1x64x64x13xbf16>
        %209 = "ttir.permute"(%207, %208) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %210 = ttir.empty() : tensor<64x64x13xbf16>
        %211 = "ttir.reshape"(%209, %210) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %212 = ttir.empty() : tensor<64x13x13xbf16>
        %213 = "ttir.matmul"(%153, %211, %212) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<13x512xbf16>
        %281 = "ttir.matmul"(%73, %arg1, %280) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %282 = ttir.empty() : tensor<1x13x512xbf16>
        %283 = "ttir.reshape"(%281, %282) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %284 = ttir.empty() : tensor<1x1x512xbf16>
        %285 = "ttir.reshape"(%arg0, %284) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %286 = ttir.empty() : tensor<1x13x512xbf16>
        %287 = "ttir.broadcast"(%285, %286) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %288 = ttir.empty() : tensor<1x13x512xbf16>
        %289 = "ttir.add"(%283, %287, %288) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %290 = ttir.empty() : tensor<1x13x8x64xbf16>
        %291 = "ttir.reshape"(%289, %290) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %292 = ttir.empty() : tensor<1x8x13x64xbf16>
        %293 = "ttir.permute"(%291, %292) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %294 = ttir.empty() : tensor<2880xf32>
        %295 = "ttir.typecast"(%arg30, %294) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %296 = ttir.empty() : tensor<1x1x2880xf32>
        %297 = "ttir.reshape"(%295, %296) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %298 = ttir.empty() : tensor<1x13x2880xf32>
        %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %300 = ttir.empty() : tensor<1x64x13x14xbf16>
        %301 = "ttir.softmax"(%279, %300) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %302 = ttir.empty() : tensor<1x64x13x13xbf16>
        %303 = "ttir.slice_static"(%301, %302) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %304 = ttir.empty() : tensor<64x13x13xbf16>
        %305 = "ttir.reshape"(%303, %304) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %306 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %307 = "ttir.reshape"(%293, %306) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %308 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %309 = "ttir.broadcast"(%307, %308) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %310 = ttir.empty() : tensor<64x13x64xbf16>
        %311 = "ttir.reshape"(%309, %310) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %312 = ttir.empty() : tensor<64x13x64xbf16>
        %313 = "ttir.matmul"(%305, %311, %312) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %314 = ttir.empty() : tensor<1x64x13x64xbf16>
        %315 = "ttir.reshape"(%313, %314) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %316 = ttir.empty() : tensor<13x4096xbf16>
        %317 = ttir.empty() : tensor<1x13x4096xbf16>
        %318 = "ttir.concatenate_heads"(%315, %317) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %319 = "ttir.reshape"(%318, %316) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %320 = ttir.empty() : tensor<13x2880xbf16>
        %321 = "ttir.matmul"(%319, %arg14, %320) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16>, tensor<2880x4096xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %322 = ttir.empty() : tensor<1x13x2880xbf16>
        %323 = "ttir.reshape"(%321, %322) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %324 = ttir.empty() : tensor<1x1x2880xbf16>
        %325 = "ttir.reshape"(%arg13, %324) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %326 = ttir.empty() : tensor<1x13x2880xbf16>
        %327 = "ttir.broadcast"(%325, %326) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %328 = ttir.empty() : tensor<1x13x2880xbf16>
        %329 = "ttir.add"(%323, %327, %328) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %330 = ttir.empty() : tensor<1x13x2880xbf16>
        %331 = "ttir.add"(%45, %329, %330) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %332 = ttir.empty() : tensor<1x1x1xbf16>
        %333 = "ttir.reshape"(%arg29, %332) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %334 = ttir.empty() : tensor<32x13x2880xbf16>
        %335 = "ttir.broadcast"(%333, %334) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %336 = ttir.empty() : tensor<2880xf32>
        %337 = "ttir.typecast"(%arg21, %336) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %338 = ttir.empty() : tensor<1x1x2880xf32>
        %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %340 = ttir.empty() : tensor<1x13x2880xf32>
        %341 = "ttir.broadcast"(%339, %340) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %342 = ttir.empty() : tensor<1x13x2880xf32>
        %343 = "ttir.typecast"(%331, %342) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %344 = ttir.empty() : tensor<1x13x2880xf32>
        %345 = "ttir.pow"(%343, %27, %344) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %346 = ttir.empty() : tensor<1x13xf32>
        %347 = "ttir.sum"(%345, %346) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %348 = ttir.empty() : tensor<1x13xf32>
        %349 = "ttir.multiply"(%347, %4, %348) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %350 = ttir.empty() : tensor<1x13x1xf32>
        %351 = "ttir.reshape"(%349, %350) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %352 = ttir.empty() : tensor<1x13x1xf32>
        %353 = "ttir.add"(%351, %59, %352) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %354 = ttir.empty() : tensor<1x13x1xf32>
        %355 = "ttir.rsqrt"(%353, %354) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %356 = ttir.empty() : tensor<1x13x2880xf32>
        %357 = "ttir.broadcast"(%355, %356) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %358 = ttir.empty() : tensor<1x13x2880xf32>
        %359 = "ttir.multiply"(%343, %357, %358) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %360 = ttir.empty() : tensor<1x13x2880xf32>
        %361 = "ttir.multiply"(%341, %359, %360) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %362 = ttir.empty() : tensor<1x13x2880xbf16>
        %363 = "ttir.typecast"(%361, %362) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %364 = ttir.empty() : tensor<13x2880xbf16>
        %365 = "ttir.reshape"(%363, %364) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %366 = ttir.empty() : tensor<416x2880xbf16>
        %367 = "ttir.concat"(%365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %366) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %368 = ttir.empty() : tensor<32x13x2880xbf16>
        %369 = "ttir.reshape"(%367, %368) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %370 = ttir.empty() : tensor<32x13x5760xbf16>
        %371 = "ttir.matmul"(%369, %arg28, %370) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %372 = ttir.empty() : tensor<32x1x5760xbf16>
        %373 = "ttir.reshape"(%arg27, %372) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %374 = ttir.empty() : tensor<32x13x5760xbf16>
        %375 = "ttir.broadcast"(%373, %374) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %376 = ttir.empty() : tensor<32x13x5760xbf16>
        %377 = "ttir.add"(%371, %375, %376) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %378 = ttir.empty() : tensor<32x13x2880xbf16>
        %379 = "ttir.slice_static"(%377, %378) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %380 = ttir.empty() : tensor<1x1x1xbf16>
        %381 = "ttir.reshape"(%arg25, %380) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %382 = ttir.empty() : tensor<32x13x2880xbf16>
        %383 = "ttir.broadcast"(%381, %382) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %384 = ttir.empty() : tensor<32x13x2880xbf16>
        %385 = "ttir.clamp_tensor"(%379, %335, %383, %384) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.add"(%385, %15, %386) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<32x13x2880xf32>
        %389 = "ttir.typecast"(%387, %388) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %390 = ttir.empty() : tensor<1x1x1xbf16>
        %391 = "ttir.reshape"(%arg26, %390) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xbf16>
        %393 = "ttir.broadcast"(%391, %392) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %394 = ttir.empty() : tensor<32x13x2880xbf16>
        %395 = "ttir.slice_static"(%377, %394) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xbf16>
        %397 = "ttir.clamp_tensor"(%395, %393, %383, %396) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %398 = ttir.empty() : tensor<32x13x2880xf32>
        %399 = "ttir.typecast"(%397, %398) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %400 = ttir.empty() : tensor<1x1x1xf32>
        %401 = "ttir.reshape"(%arg24, %400) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %402 = ttir.empty() : tensor<32x13x2880xf32>
        %403 = "ttir.broadcast"(%401, %402) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %404 = ttir.empty() : tensor<32x13x2880xf32>
        %405 = "ttir.multiply"(%399, %403, %404) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.typecast"(%405, %406) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.sigmoid"(%407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xf32>
        %411 = "ttir.typecast"(%409, %410) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.multiply"(%399, %411, %412) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<32x13x2880xf32>
        %415 = "ttir.multiply"(%389, %413, %414) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.typecast"(%415, %416) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xbf16>
        %419 = "ttir.matmul"(%417, %arg23, %418) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %420 = ttir.empty() : tensor<32x1x2880xbf16>
        %421 = "ttir.reshape"(%arg22, %420) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xbf16>
        %423 = "ttir.broadcast"(%421, %422) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %424 = ttir.empty() : tensor<32x13x2880xbf16>
        %425 = "ttir.add"(%419, %423, %424) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %426 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %427 = "ttir.reshape"(%425, %426) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %428 = ttir.empty() : tensor<32x1x13x2880xf32>
        %429 = "ttir.typecast"(%427, %428) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %430 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %431 = ttir.empty() : tensor<13x1x1xsi32>
        %432 = "ttir.reshape"(%430, %431) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %433 = ttir.empty() : tensor<13x4x1xsi32>
        %434 = "ttir.broadcast"(%432, %433) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %435 = ttir.empty() : tensor<13x32xbf16>
        %436 = "ttir.matmul"(%365, %arg12, %435) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<32x2880xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %437 = ttir.empty() : tensor<1x32xbf16>
        %438 = "ttir.reshape"(%arg11, %437) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %439 = ttir.empty() : tensor<13x32xbf16>
        %440 = "ttir.broadcast"(%438, %439) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %441 = ttir.empty() : tensor<13x32xbf16>
        %442 = "ttir.add"(%436, %440, %441) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %443 = ttir.empty() : tensor<13x32xbf16>
        %444 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%442, %443, %444) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %445 = ttir.empty() : tensor<13x4xsi32>
        %446 = "ttir.slice_static"(%indices, %445) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %447 = ttir.empty() : tensor<13x4x1xsi32>
        %448 = "ttir.reshape"(%446, %447) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %449 = ttir.empty() : tensor<13x4x2xsi32>
        %450 = "ttir.concat"(%434, %448, %449) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %451 = ttir.empty() : tensor<13x4xbf16>
        %452 = "ttir.slice_static"(%values, %451) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %453 = ttir.empty() : tensor<13x4xbf16>
        %454 = "ttir.softmax"(%452, %453) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %455 = ttir.empty() : tensor<13x32xbf16>
        %456 = "ttir.scatter"(%11, %450, %454, %455) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %457 = ttir.empty() : tensor<32x13xbf16>
        %458 = "ttir.permute"(%456, %457) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %459 = ttir.empty() : tensor<32x1x13x1xbf16>
        %460 = "ttir.reshape"(%458, %459) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %461 = ttir.empty() : tensor<32x1x13x1xf32>
        %462 = "ttir.typecast"(%460, %461) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %463 = ttir.empty() : tensor<32x1x13x2880xf32>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %465 = ttir.empty() : tensor<32x1x13x2880xf32>
        %466 = "ttir.multiply"(%429, %464, %465) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %467 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %468 = "ttir.typecast"(%466, %467) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %469 = ttir.empty() : tensor<1x13x2880xbf16>
        %470 = "ttir.sum"(%468, %469) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %471 = ttir.empty() : tensor<1x13x2880xbf16>
        %472 = "ttir.add"(%331, %470, %471) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %473 = ttir.empty() : tensor<1x13x2880xf32>
        %474 = "ttir.typecast"(%472, %473) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %475 = ttir.empty() : tensor<1x13x2880xf32>
        %476 = "ttir.pow"(%474, %27, %475) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %477 = ttir.empty() : tensor<1x13xf32>
        %478 = "ttir.sum"(%476, %477) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %479 = ttir.empty() : tensor<1x13xf32>
        %480 = "ttir.multiply"(%478, %4, %479) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %481 = ttir.empty() : tensor<1x13x1xf32>
        %482 = "ttir.reshape"(%480, %481) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %483 = ttir.empty() : tensor<1x13x1xf32>
        %484 = "ttir.add"(%482, %59, %483) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %485 = ttir.empty() : tensor<1x13x1xf32>
        %486 = "ttir.rsqrt"(%484, %485) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %487 = ttir.empty() : tensor<1x13x2880xf32>
        %488 = "ttir.broadcast"(%486, %487) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %489 = ttir.empty() : tensor<1x13x2880xf32>
        %490 = "ttir.multiply"(%474, %488, %489) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %491 = ttir.empty() : tensor<1x13x2880xf32>
        %492 = "ttir.multiply"(%299, %490, %491) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %493 = ttir.empty() : tensor<1x13x2880xbf16>
        %494 = "ttir.typecast"(%492, %493) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %495 = ttir.empty() : tensor<13x2880xbf16>
        %496 = "ttir.reshape"(%494, %495) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %497 = ttir.empty() : tensor<13x201088xbf16>
        %498 = "ttir.matmul"(%496, %arg10, %497) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<201088x2880xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %499 = ttir.empty() : tensor<1x13x201088xbf16>
        %500 = "ttir.reshape"(%498, %499) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %289, %291, %293, %201, %498, %500 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.embedding"(%41, %arg4, %42) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<13x4096xbf16>
        %75 = "ttir.matmul"(%73, %arg20, %74) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<4096x2880xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %76 = ttir.empty() : tensor<1x13x4096xbf16>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %78 = ttir.empty() : tensor<1x1x4096xbf16>
        %79 = "ttir.reshape"(%arg19, %78) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %80 = ttir.empty() : tensor<1x13x4096xbf16>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %82 = ttir.empty() : tensor<1x13x4096xbf16>
        %83 = "ttir.add"(%77, %81, %82) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %84 = ttir.empty() : tensor<1x13x64x64xbf16>
        %85 = "ttir.reshape"(%83, %84) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %86 = ttir.empty() : tensor<1x64x13x64xbf16>
        %87 = "ttir.permute"(%85, %86) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %88 = ttir.empty() : tensor<1x64x13x32xbf16>
        %89 = "ttir.slice_static"(%87, %88) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %90 = ttir.empty() : tensor<1x64x13x32xf32>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %92 = ttir.empty() : tensor<1x32x1xf32>
        %93 = "ttir.reshape"(%arg7, %92) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %94 = ttir.empty() : tensor<1x32x13xf32>
        %95 = "ttir.matmul"(%93, %0, %94) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%91, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%87, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%91, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<13x512xbf16>
        %155 = "ttir.matmul"(%73, %arg9, %154) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %156 = ttir.empty() : tensor<1x13x512xbf16>
        %157 = "ttir.reshape"(%155, %156) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %158 = ttir.empty() : tensor<1x1x512xbf16>
        %159 = "ttir.reshape"(%arg8, %158) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %160 = ttir.empty() : tensor<1x13x512xbf16>
        %161 = "ttir.broadcast"(%159, %160) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %162 = ttir.empty() : tensor<1x13x512xbf16>
        %163 = "ttir.add"(%157, %161, %162) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %164 = ttir.empty() : tensor<1x13x8x64xbf16>
        %165 = "ttir.reshape"(%163, %164) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %166 = ttir.empty() : tensor<1x8x13x64xbf16>
        %167 = "ttir.permute"(%165, %166) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %168 = ttir.empty() : tensor<1x8x13x32xbf16>
        %169 = "ttir.slice_static"(%167, %168) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %170 = ttir.empty() : tensor<1x8x13x32xf32>
        %171 = "ttir.typecast"(%169, %170) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %172 = ttir.empty() : tensor<1x8x13x32xf32>
        %173 = "ttir.broadcast"(%111, %172) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %174 = ttir.empty() : tensor<1x8x13x32xf32>
        %175 = "ttir.multiply"(%171, %173, %174) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %176 = ttir.empty() : tensor<1x8x13x32xbf16>
        %177 = "ttir.typecast"(%175, %176) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %178 = ttir.empty() : tensor<1x8x13x32xbf16>
        %179 = "ttir.slice_static"(%167, %178) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %180 = ttir.empty() : tensor<1x8x13x32xf32>
        %181 = "ttir.typecast"(%179, %180) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %182 = ttir.empty() : tensor<1x8x13x32xf32>
        %183 = "ttir.broadcast"(%131, %182) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %184 = ttir.empty() : tensor<1x8x13x32xf32>
        %185 = "ttir.multiply"(%181, %183, %184) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %186 = ttir.empty() : tensor<1x8x13x32xbf16>
        %187 = "ttir.typecast"(%185, %186) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %188 = ttir.empty() : tensor<1x8x13x32xbf16>
        %189 = "ttir.subtract"(%177, %187, %188) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %190 = ttir.empty() : tensor<1x8x13x32xf32>
        %191 = "ttir.multiply"(%181, %173, %190) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %192 = ttir.empty() : tensor<1x8x13x32xbf16>
        %193 = "ttir.typecast"(%191, %192) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %194 = ttir.empty() : tensor<1x8x13x32xf32>
        %195 = "ttir.multiply"(%171, %183, %194) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %196 = ttir.empty() : tensor<1x8x13x32xbf16>
        %197 = "ttir.typecast"(%195, %196) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %198 = ttir.empty() : tensor<1x8x13x32xbf16>
        %199 = "ttir.add"(%193, %197, %198) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %200 = ttir.empty() : tensor<1x8x13x64xbf16>
        %201 = "ttir.concat"(%189, %199, %200) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %202 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %203 = "ttir.reshape"(%201, %202) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %204 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %205 = "ttir.broadcast"(%203, %204) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %206 = ttir.empty() : tensor<1x64x13x64xbf16>
        %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %208 = ttir.empty() : tensor<1x64x64x13xbf16>
        %209 = "ttir.permute"(%207, %208) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %210 = ttir.empty() : tensor<64x64x13xbf16>
        %211 = "ttir.reshape"(%209, %210) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %212 = ttir.empty() : tensor<64x13x13xbf16>
        %213 = "ttir.matmul"(%153, %211, %212) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<13x512xbf16>
        %281 = "ttir.matmul"(%73, %arg1, %280) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %282 = ttir.empty() : tensor<1x13x512xbf16>
        %283 = "ttir.reshape"(%281, %282) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %284 = ttir.empty() : tensor<1x1x512xbf16>
        %285 = "ttir.reshape"(%arg0, %284) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %286 = ttir.empty() : tensor<1x13x512xbf16>
        %287 = "ttir.broadcast"(%285, %286) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %288 = ttir.empty() : tensor<1x13x512xbf16>
        %289 = "ttir.add"(%283, %287, %288) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %290 = ttir.empty() : tensor<1x13x8x64xbf16>
        %291 = "ttir.reshape"(%289, %290) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %292 = ttir.empty() : tensor<1x8x13x64xbf16>
        %293 = "ttir.permute"(%291, %292) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %294 = ttir.empty() : tensor<2880xf32>
        %295 = "ttir.typecast"(%arg30, %294) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %296 = ttir.empty() : tensor<1x1x2880xf32>
        %297 = "ttir.reshape"(%295, %296) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %298 = ttir.empty() : tensor<1x13x2880xf32>
        %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %300 = ttir.empty() : tensor<1x64x13x14xbf16>
        %301 = "ttir.softmax"(%279, %300) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %302 = ttir.empty() : tensor<1x64x13x13xbf16>
        %303 = "ttir.slice_static"(%301, %302) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %304 = ttir.empty() : tensor<64x13x13xbf16>
        %305 = "ttir.reshape"(%303, %304) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %306 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %307 = "ttir.reshape"(%293, %306) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %308 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %309 = "ttir.broadcast"(%307, %308) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %310 = ttir.empty() : tensor<64x13x64xbf16>
        %311 = "ttir.reshape"(%309, %310) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %312 = ttir.empty() : tensor<64x13x64xbf16>
        %313 = "ttir.matmul"(%305, %311, %312) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %314 = ttir.empty() : tensor<1x64x13x64xbf16>
        %315 = "ttir.reshape"(%313, %314) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %316 = ttir.empty() : tensor<13x4096xbf16>
        %317 = ttir.empty() : tensor<1x13x4096xbf16>
        %318 = "ttir.concatenate_heads"(%315, %317) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %319 = "ttir.reshape"(%318, %316) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %320 = ttir.empty() : tensor<13x2880xbf16>
        %321 = "ttir.matmul"(%319, %arg14, %320) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16>, tensor<2880x4096xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %322 = ttir.empty() : tensor<1x13x2880xbf16>
        %323 = "ttir.reshape"(%321, %322) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %324 = ttir.empty() : tensor<1x1x2880xbf16>
        %325 = "ttir.reshape"(%arg13, %324) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %326 = ttir.empty() : tensor<1x13x2880xbf16>
        %327 = "ttir.broadcast"(%325, %326) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %328 = ttir.empty() : tensor<1x13x2880xbf16>
        %329 = "ttir.add"(%323, %327, %328) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %330 = ttir.empty() : tensor<1x13x2880xbf16>
        %331 = "ttir.add"(%45, %329, %330) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %332 = ttir.empty() : tensor<1x1x1xbf16>
        %333 = "ttir.reshape"(%arg29, %332) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %334 = ttir.empty() : tensor<32x13x2880xbf16>
        %335 = "ttir.broadcast"(%333, %334) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %336 = ttir.empty() : tensor<2880xf32>
        %337 = "ttir.typecast"(%arg21, %336) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %338 = ttir.empty() : tensor<1x1x2880xf32>
        %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %340 = ttir.empty() : tensor<1x13x2880xf32>
        %341 = "ttir.broadcast"(%339, %340) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %342 = ttir.empty() : tensor<1x13x2880xf32>
        %343 = "ttir.typecast"(%331, %342) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %344 = ttir.empty() : tensor<1x13x2880xf32>
        %345 = "ttir.pow"(%343, %27, %344) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %346 = ttir.empty() : tensor<1x13xf32>
        %347 = "ttir.sum"(%345, %346) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %348 = ttir.empty() : tensor<1x13xf32>
        %349 = "ttir.multiply"(%347, %4, %348) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %350 = ttir.empty() : tensor<1x13x1xf32>
        %351 = "ttir.reshape"(%349, %350) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %352 = ttir.empty() : tensor<1x13x1xf32>
        %353 = "ttir.add"(%351, %59, %352) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %354 = ttir.empty() : tensor<1x13x1xf32>
        %355 = "ttir.rsqrt"(%353, %354) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %356 = ttir.empty() : tensor<1x13x2880xf32>
        %357 = "ttir.broadcast"(%355, %356) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %358 = ttir.empty() : tensor<1x13x2880xf32>
        %359 = "ttir.multiply"(%343, %357, %358) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %360 = ttir.empty() : tensor<1x13x2880xf32>
        %361 = "ttir.multiply"(%341, %359, %360) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %362 = ttir.empty() : tensor<1x13x2880xbf16>
        %363 = "ttir.typecast"(%361, %362) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %364 = ttir.empty() : tensor<13x2880xbf16>
        %365 = "ttir.reshape"(%363, %364) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %366 = ttir.empty() : tensor<416x2880xbf16>
        %367 = "ttir.concat"(%365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %366) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %368 = ttir.empty() : tensor<32x13x2880xbf16>
        %369 = "ttir.reshape"(%367, %368) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %370 = ttir.empty() : tensor<32x13x5760xbf16>
        %371 = "ttir.matmul"(%369, %arg28, %370) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %372 = ttir.empty() : tensor<32x1x5760xbf16>
        %373 = "ttir.reshape"(%arg27, %372) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %374 = ttir.empty() : tensor<32x13x5760xbf16>
        %375 = "ttir.broadcast"(%373, %374) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %376 = ttir.empty() : tensor<32x13x5760xbf16>
        %377 = "ttir.add"(%371, %375, %376) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %378 = ttir.empty() : tensor<32x13x2880xbf16>
        %379 = "ttir.slice_static"(%377, %378) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %380 = ttir.empty() : tensor<1x1x1xbf16>
        %381 = "ttir.reshape"(%arg25, %380) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %382 = ttir.empty() : tensor<32x13x2880xbf16>
        %383 = "ttir.broadcast"(%381, %382) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %384 = ttir.empty() : tensor<32x13x2880xbf16>
        %385 = "ttir.clamp_tensor"(%379, %335, %383, %384) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.add"(%385, %15, %386) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<32x13x2880xf32>
        %389 = "ttir.typecast"(%387, %388) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %390 = ttir.empty() : tensor<1x1x1xbf16>
        %391 = "ttir.reshape"(%arg26, %390) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xbf16>
        %393 = "ttir.broadcast"(%391, %392) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %394 = ttir.empty() : tensor<32x13x2880xbf16>
        %395 = "ttir.slice_static"(%377, %394) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xbf16>
        %397 = "ttir.clamp_tensor"(%395, %393, %383, %396) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %398 = ttir.empty() : tensor<32x13x2880xf32>
        %399 = "ttir.typecast"(%397, %398) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %400 = ttir.empty() : tensor<1x1x1xf32>
        %401 = "ttir.reshape"(%arg24, %400) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %402 = ttir.empty() : tensor<32x13x2880xf32>
        %403 = "ttir.broadcast"(%401, %402) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %404 = ttir.empty() : tensor<32x13x2880xf32>
        %405 = "ttir.multiply"(%399, %403, %404) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.typecast"(%405, %406) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.sigmoid"(%407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xf32>
        %411 = "ttir.typecast"(%409, %410) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.multiply"(%399, %411, %412) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<32x13x2880xf32>
        %415 = "ttir.multiply"(%389, %413, %414) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.typecast"(%415, %416) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xbf16>
        %419 = "ttir.matmul"(%417, %arg23, %418) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %420 = ttir.empty() : tensor<32x1x2880xbf16>
        %421 = "ttir.reshape"(%arg22, %420) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xbf16>
        %423 = "ttir.broadcast"(%421, %422) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %424 = ttir.empty() : tensor<32x13x2880xbf16>
        %425 = "ttir.add"(%419, %423, %424) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %426 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %427 = "ttir.reshape"(%425, %426) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %428 = ttir.empty() : tensor<32x1x13x2880xf32>
        %429 = "ttir.typecast"(%427, %428) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %430 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %431 = ttir.empty() : tensor<13x1x1xsi32>
        %432 = "ttir.reshape"(%430, %431) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %433 = ttir.empty() : tensor<13x4x1xsi32>
        %434 = "ttir.broadcast"(%432, %433) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %435 = ttir.empty() : tensor<13x32xbf16>
        %436 = "ttir.matmul"(%365, %arg12, %435) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<32x2880xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %437 = ttir.empty() : tensor<1x32xbf16>
        %438 = "ttir.reshape"(%arg11, %437) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %439 = ttir.empty() : tensor<13x32xbf16>
        %440 = "ttir.broadcast"(%438, %439) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %441 = ttir.empty() : tensor<13x32xbf16>
        %442 = "ttir.add"(%436, %440, %441) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %443 = ttir.empty() : tensor<13x32xbf16>
        %444 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%442, %443, %444) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %445 = ttir.empty() : tensor<13x4xsi32>
        %446 = "ttir.slice_static"(%indices, %445) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %447 = ttir.empty() : tensor<13x4x1xsi32>
        %448 = "ttir.reshape"(%446, %447) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %449 = ttir.empty() : tensor<13x4x2xsi32>
        %450 = "ttir.concat"(%434, %448, %449) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %451 = ttir.empty() : tensor<13x4xbf16>
        %452 = "ttir.slice_static"(%values, %451) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %453 = ttir.empty() : tensor<13x4xbf16>
        %454 = "ttir.softmax"(%452, %453) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %455 = ttir.empty() : tensor<13x32xbf16>
        %456 = "ttir.scatter"(%11, %450, %454, %455) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %457 = ttir.empty() : tensor<32x13xbf16>
        %458 = "ttir.permute"(%456, %457) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %459 = ttir.empty() : tensor<32x1x13x1xbf16>
        %460 = "ttir.reshape"(%458, %459) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %461 = ttir.empty() : tensor<32x1x13x1xf32>
        %462 = "ttir.typecast"(%460, %461) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %463 = ttir.empty() : tensor<32x1x13x2880xf32>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %465 = ttir.empty() : tensor<32x1x13x2880xf32>
        %466 = "ttir.multiply"(%429, %464, %465) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %467 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %468 = "ttir.typecast"(%466, %467) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %469 = ttir.empty() : tensor<1x13x2880xbf16>
        %470 = "ttir.sum"(%468, %469) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %471 = ttir.empty() : tensor<1x13x2880xbf16>
        %472 = "ttir.add"(%331, %470, %471) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %473 = ttir.empty() : tensor<1x13x2880xf32>
        %474 = "ttir.typecast"(%472, %473) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %475 = ttir.empty() : tensor<1x13x2880xf32>
        %476 = "ttir.pow"(%474, %27, %475) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %477 = ttir.empty() : tensor<1x13xf32>
        %478 = "ttir.sum"(%476, %477) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %479 = ttir.empty() : tensor<1x13xf32>
        %480 = "ttir.multiply"(%478, %4, %479) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %481 = ttir.empty() : tensor<1x13x1xf32>
        %482 = "ttir.reshape"(%480, %481) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %483 = ttir.empty() : tensor<1x13x1xf32>
        %484 = "ttir.add"(%482, %59, %483) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %485 = ttir.empty() : tensor<1x13x1xf32>
        %486 = "ttir.rsqrt"(%484, %485) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %487 = ttir.empty() : tensor<1x13x2880xf32>
        %488 = "ttir.broadcast"(%486, %487) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %489 = ttir.empty() : tensor<1x13x2880xf32>
        %490 = "ttir.multiply"(%474, %488, %489) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %491 = ttir.empty() : tensor<1x13x2880xf32>
        %492 = "ttir.multiply"(%299, %490, %491) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %493 = ttir.empty() : tensor<1x13x2880xbf16>
        %494 = "ttir.typecast"(%492, %493) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %495 = ttir.empty() : tensor<13x2880xbf16>
        %496 = "ttir.reshape"(%494, %495) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %497 = ttir.empty() : tensor<13x201088xbf16>
        %498 = "ttir.matmul"(%496, %arg10, %497) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<201088x2880xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %499 = ttir.empty() : tensor<1x13x201088xbf16>
        %500 = "ttir.reshape"(%498, %499) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %289, %291, %293, %201, %498, %500 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFlattenSlidingWindow (ttir-flatten-sliding-window) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.embedding"(%41, %arg4, %42) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<13x4096xbf16>
        %75 = "ttir.matmul"(%73, %arg20, %74) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<4096x2880xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %76 = ttir.empty() : tensor<1x13x4096xbf16>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %78 = ttir.empty() : tensor<1x1x4096xbf16>
        %79 = "ttir.reshape"(%arg19, %78) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %80 = ttir.empty() : tensor<1x13x4096xbf16>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %82 = ttir.empty() : tensor<1x13x4096xbf16>
        %83 = "ttir.add"(%77, %81, %82) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %84 = ttir.empty() : tensor<1x13x64x64xbf16>
        %85 = "ttir.reshape"(%83, %84) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %86 = ttir.empty() : tensor<1x64x13x64xbf16>
        %87 = "ttir.permute"(%85, %86) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %88 = ttir.empty() : tensor<1x64x13x32xbf16>
        %89 = "ttir.slice_static"(%87, %88) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %90 = ttir.empty() : tensor<1x64x13x32xf32>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %92 = ttir.empty() : tensor<1x32x1xf32>
        %93 = "ttir.reshape"(%arg7, %92) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %94 = ttir.empty() : tensor<1x32x13xf32>
        %95 = "ttir.matmul"(%93, %0, %94) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%91, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%87, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%91, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<13x512xbf16>
        %155 = "ttir.matmul"(%73, %arg9, %154) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %156 = ttir.empty() : tensor<1x13x512xbf16>
        %157 = "ttir.reshape"(%155, %156) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %158 = ttir.empty() : tensor<1x1x512xbf16>
        %159 = "ttir.reshape"(%arg8, %158) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %160 = ttir.empty() : tensor<1x13x512xbf16>
        %161 = "ttir.broadcast"(%159, %160) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %162 = ttir.empty() : tensor<1x13x512xbf16>
        %163 = "ttir.add"(%157, %161, %162) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %164 = ttir.empty() : tensor<1x13x8x64xbf16>
        %165 = "ttir.reshape"(%163, %164) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %166 = ttir.empty() : tensor<1x8x13x64xbf16>
        %167 = "ttir.permute"(%165, %166) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %168 = ttir.empty() : tensor<1x8x13x32xbf16>
        %169 = "ttir.slice_static"(%167, %168) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %170 = ttir.empty() : tensor<1x8x13x32xf32>
        %171 = "ttir.typecast"(%169, %170) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %172 = ttir.empty() : tensor<1x8x13x32xf32>
        %173 = "ttir.broadcast"(%111, %172) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %174 = ttir.empty() : tensor<1x8x13x32xf32>
        %175 = "ttir.multiply"(%171, %173, %174) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %176 = ttir.empty() : tensor<1x8x13x32xbf16>
        %177 = "ttir.typecast"(%175, %176) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %178 = ttir.empty() : tensor<1x8x13x32xbf16>
        %179 = "ttir.slice_static"(%167, %178) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %180 = ttir.empty() : tensor<1x8x13x32xf32>
        %181 = "ttir.typecast"(%179, %180) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %182 = ttir.empty() : tensor<1x8x13x32xf32>
        %183 = "ttir.broadcast"(%131, %182) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %184 = ttir.empty() : tensor<1x8x13x32xf32>
        %185 = "ttir.multiply"(%181, %183, %184) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %186 = ttir.empty() : tensor<1x8x13x32xbf16>
        %187 = "ttir.typecast"(%185, %186) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %188 = ttir.empty() : tensor<1x8x13x32xbf16>
        %189 = "ttir.subtract"(%177, %187, %188) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %190 = ttir.empty() : tensor<1x8x13x32xf32>
        %191 = "ttir.multiply"(%181, %173, %190) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %192 = ttir.empty() : tensor<1x8x13x32xbf16>
        %193 = "ttir.typecast"(%191, %192) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %194 = ttir.empty() : tensor<1x8x13x32xf32>
        %195 = "ttir.multiply"(%171, %183, %194) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %196 = ttir.empty() : tensor<1x8x13x32xbf16>
        %197 = "ttir.typecast"(%195, %196) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %198 = ttir.empty() : tensor<1x8x13x32xbf16>
        %199 = "ttir.add"(%193, %197, %198) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %200 = ttir.empty() : tensor<1x8x13x64xbf16>
        %201 = "ttir.concat"(%189, %199, %200) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %202 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %203 = "ttir.reshape"(%201, %202) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %204 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %205 = "ttir.broadcast"(%203, %204) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %206 = ttir.empty() : tensor<1x64x13x64xbf16>
        %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %208 = ttir.empty() : tensor<1x64x64x13xbf16>
        %209 = "ttir.permute"(%207, %208) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %210 = ttir.empty() : tensor<64x64x13xbf16>
        %211 = "ttir.reshape"(%209, %210) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %212 = ttir.empty() : tensor<64x13x13xbf16>
        %213 = "ttir.matmul"(%153, %211, %212) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<13x512xbf16>
        %281 = "ttir.matmul"(%73, %arg1, %280) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %282 = ttir.empty() : tensor<1x13x512xbf16>
        %283 = "ttir.reshape"(%281, %282) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %284 = ttir.empty() : tensor<1x1x512xbf16>
        %285 = "ttir.reshape"(%arg0, %284) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %286 = ttir.empty() : tensor<1x13x512xbf16>
        %287 = "ttir.broadcast"(%285, %286) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %288 = ttir.empty() : tensor<1x13x512xbf16>
        %289 = "ttir.add"(%283, %287, %288) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %290 = ttir.empty() : tensor<1x13x8x64xbf16>
        %291 = "ttir.reshape"(%289, %290) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %292 = ttir.empty() : tensor<1x8x13x64xbf16>
        %293 = "ttir.permute"(%291, %292) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %294 = ttir.empty() : tensor<2880xf32>
        %295 = "ttir.typecast"(%arg30, %294) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %296 = ttir.empty() : tensor<1x1x2880xf32>
        %297 = "ttir.reshape"(%295, %296) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %298 = ttir.empty() : tensor<1x13x2880xf32>
        %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %300 = ttir.empty() : tensor<1x64x13x14xbf16>
        %301 = "ttir.softmax"(%279, %300) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %302 = ttir.empty() : tensor<1x64x13x13xbf16>
        %303 = "ttir.slice_static"(%301, %302) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %304 = ttir.empty() : tensor<64x13x13xbf16>
        %305 = "ttir.reshape"(%303, %304) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %306 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %307 = "ttir.reshape"(%293, %306) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %308 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %309 = "ttir.broadcast"(%307, %308) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %310 = ttir.empty() : tensor<64x13x64xbf16>
        %311 = "ttir.reshape"(%309, %310) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %312 = ttir.empty() : tensor<64x13x64xbf16>
        %313 = "ttir.matmul"(%305, %311, %312) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %314 = ttir.empty() : tensor<1x64x13x64xbf16>
        %315 = "ttir.reshape"(%313, %314) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %316 = ttir.empty() : tensor<13x4096xbf16>
        %317 = ttir.empty() : tensor<1x13x4096xbf16>
        %318 = "ttir.concatenate_heads"(%315, %317) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %319 = "ttir.reshape"(%318, %316) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %320 = ttir.empty() : tensor<13x2880xbf16>
        %321 = "ttir.matmul"(%319, %arg14, %320) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16>, tensor<2880x4096xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %322 = ttir.empty() : tensor<1x13x2880xbf16>
        %323 = "ttir.reshape"(%321, %322) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %324 = ttir.empty() : tensor<1x1x2880xbf16>
        %325 = "ttir.reshape"(%arg13, %324) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %326 = ttir.empty() : tensor<1x13x2880xbf16>
        %327 = "ttir.broadcast"(%325, %326) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %328 = ttir.empty() : tensor<1x13x2880xbf16>
        %329 = "ttir.add"(%323, %327, %328) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %330 = ttir.empty() : tensor<1x13x2880xbf16>
        %331 = "ttir.add"(%45, %329, %330) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %332 = ttir.empty() : tensor<1x1x1xbf16>
        %333 = "ttir.reshape"(%arg29, %332) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %334 = ttir.empty() : tensor<32x13x2880xbf16>
        %335 = "ttir.broadcast"(%333, %334) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %336 = ttir.empty() : tensor<2880xf32>
        %337 = "ttir.typecast"(%arg21, %336) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %338 = ttir.empty() : tensor<1x1x2880xf32>
        %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %340 = ttir.empty() : tensor<1x13x2880xf32>
        %341 = "ttir.broadcast"(%339, %340) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %342 = ttir.empty() : tensor<1x13x2880xf32>
        %343 = "ttir.typecast"(%331, %342) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %344 = ttir.empty() : tensor<1x13x2880xf32>
        %345 = "ttir.pow"(%343, %27, %344) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %346 = ttir.empty() : tensor<1x13xf32>
        %347 = "ttir.sum"(%345, %346) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %348 = ttir.empty() : tensor<1x13xf32>
        %349 = "ttir.multiply"(%347, %4, %348) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %350 = ttir.empty() : tensor<1x13x1xf32>
        %351 = "ttir.reshape"(%349, %350) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %352 = ttir.empty() : tensor<1x13x1xf32>
        %353 = "ttir.add"(%351, %59, %352) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %354 = ttir.empty() : tensor<1x13x1xf32>
        %355 = "ttir.rsqrt"(%353, %354) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %356 = ttir.empty() : tensor<1x13x2880xf32>
        %357 = "ttir.broadcast"(%355, %356) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %358 = ttir.empty() : tensor<1x13x2880xf32>
        %359 = "ttir.multiply"(%343, %357, %358) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %360 = ttir.empty() : tensor<1x13x2880xf32>
        %361 = "ttir.multiply"(%341, %359, %360) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %362 = ttir.empty() : tensor<1x13x2880xbf16>
        %363 = "ttir.typecast"(%361, %362) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %364 = ttir.empty() : tensor<13x2880xbf16>
        %365 = "ttir.reshape"(%363, %364) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %366 = ttir.empty() : tensor<416x2880xbf16>
        %367 = "ttir.concat"(%365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %366) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %368 = ttir.empty() : tensor<32x13x2880xbf16>
        %369 = "ttir.reshape"(%367, %368) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %370 = ttir.empty() : tensor<32x13x5760xbf16>
        %371 = "ttir.matmul"(%369, %arg28, %370) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %372 = ttir.empty() : tensor<32x1x5760xbf16>
        %373 = "ttir.reshape"(%arg27, %372) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %374 = ttir.empty() : tensor<32x13x5760xbf16>
        %375 = "ttir.broadcast"(%373, %374) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %376 = ttir.empty() : tensor<32x13x5760xbf16>
        %377 = "ttir.add"(%371, %375, %376) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %378 = ttir.empty() : tensor<32x13x2880xbf16>
        %379 = "ttir.slice_static"(%377, %378) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %380 = ttir.empty() : tensor<1x1x1xbf16>
        %381 = "ttir.reshape"(%arg25, %380) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %382 = ttir.empty() : tensor<32x13x2880xbf16>
        %383 = "ttir.broadcast"(%381, %382) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %384 = ttir.empty() : tensor<32x13x2880xbf16>
        %385 = "ttir.clamp_tensor"(%379, %335, %383, %384) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.add"(%385, %15, %386) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<32x13x2880xf32>
        %389 = "ttir.typecast"(%387, %388) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %390 = ttir.empty() : tensor<1x1x1xbf16>
        %391 = "ttir.reshape"(%arg26, %390) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xbf16>
        %393 = "ttir.broadcast"(%391, %392) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %394 = ttir.empty() : tensor<32x13x2880xbf16>
        %395 = "ttir.slice_static"(%377, %394) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xbf16>
        %397 = "ttir.clamp_tensor"(%395, %393, %383, %396) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %398 = ttir.empty() : tensor<32x13x2880xf32>
        %399 = "ttir.typecast"(%397, %398) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %400 = ttir.empty() : tensor<1x1x1xf32>
        %401 = "ttir.reshape"(%arg24, %400) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %402 = ttir.empty() : tensor<32x13x2880xf32>
        %403 = "ttir.broadcast"(%401, %402) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %404 = ttir.empty() : tensor<32x13x2880xf32>
        %405 = "ttir.multiply"(%399, %403, %404) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.typecast"(%405, %406) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.sigmoid"(%407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xf32>
        %411 = "ttir.typecast"(%409, %410) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.multiply"(%399, %411, %412) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<32x13x2880xf32>
        %415 = "ttir.multiply"(%389, %413, %414) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.typecast"(%415, %416) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xbf16>
        %419 = "ttir.matmul"(%417, %arg23, %418) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %420 = ttir.empty() : tensor<32x1x2880xbf16>
        %421 = "ttir.reshape"(%arg22, %420) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xbf16>
        %423 = "ttir.broadcast"(%421, %422) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %424 = ttir.empty() : tensor<32x13x2880xbf16>
        %425 = "ttir.add"(%419, %423, %424) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %426 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %427 = "ttir.reshape"(%425, %426) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %428 = ttir.empty() : tensor<32x1x13x2880xf32>
        %429 = "ttir.typecast"(%427, %428) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %430 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %431 = ttir.empty() : tensor<13x1x1xsi32>
        %432 = "ttir.reshape"(%430, %431) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %433 = ttir.empty() : tensor<13x4x1xsi32>
        %434 = "ttir.broadcast"(%432, %433) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %435 = ttir.empty() : tensor<13x32xbf16>
        %436 = "ttir.matmul"(%365, %arg12, %435) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<32x2880xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %437 = ttir.empty() : tensor<1x32xbf16>
        %438 = "ttir.reshape"(%arg11, %437) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %439 = ttir.empty() : tensor<13x32xbf16>
        %440 = "ttir.broadcast"(%438, %439) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %441 = ttir.empty() : tensor<13x32xbf16>
        %442 = "ttir.add"(%436, %440, %441) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %443 = ttir.empty() : tensor<13x32xbf16>
        %444 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%442, %443, %444) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %445 = ttir.empty() : tensor<13x4xsi32>
        %446 = "ttir.slice_static"(%indices, %445) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %447 = ttir.empty() : tensor<13x4x1xsi32>
        %448 = "ttir.reshape"(%446, %447) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %449 = ttir.empty() : tensor<13x4x2xsi32>
        %450 = "ttir.concat"(%434, %448, %449) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %451 = ttir.empty() : tensor<13x4xbf16>
        %452 = "ttir.slice_static"(%values, %451) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %453 = ttir.empty() : tensor<13x4xbf16>
        %454 = "ttir.softmax"(%452, %453) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %455 = ttir.empty() : tensor<13x32xbf16>
        %456 = "ttir.scatter"(%11, %450, %454, %455) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %457 = ttir.empty() : tensor<32x13xbf16>
        %458 = "ttir.permute"(%456, %457) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %459 = ttir.empty() : tensor<32x1x13x1xbf16>
        %460 = "ttir.reshape"(%458, %459) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %461 = ttir.empty() : tensor<32x1x13x1xf32>
        %462 = "ttir.typecast"(%460, %461) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %463 = ttir.empty() : tensor<32x1x13x2880xf32>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %465 = ttir.empty() : tensor<32x1x13x2880xf32>
        %466 = "ttir.multiply"(%429, %464, %465) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %467 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %468 = "ttir.typecast"(%466, %467) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %469 = ttir.empty() : tensor<1x13x2880xbf16>
        %470 = "ttir.sum"(%468, %469) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %471 = ttir.empty() : tensor<1x13x2880xbf16>
        %472 = "ttir.add"(%331, %470, %471) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %473 = ttir.empty() : tensor<1x13x2880xf32>
        %474 = "ttir.typecast"(%472, %473) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %475 = ttir.empty() : tensor<1x13x2880xf32>
        %476 = "ttir.pow"(%474, %27, %475) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %477 = ttir.empty() : tensor<1x13xf32>
        %478 = "ttir.sum"(%476, %477) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %479 = ttir.empty() : tensor<1x13xf32>
        %480 = "ttir.multiply"(%478, %4, %479) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %481 = ttir.empty() : tensor<1x13x1xf32>
        %482 = "ttir.reshape"(%480, %481) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %483 = ttir.empty() : tensor<1x13x1xf32>
        %484 = "ttir.add"(%482, %59, %483) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %485 = ttir.empty() : tensor<1x13x1xf32>
        %486 = "ttir.rsqrt"(%484, %485) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %487 = ttir.empty() : tensor<1x13x2880xf32>
        %488 = "ttir.broadcast"(%486, %487) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %489 = ttir.empty() : tensor<1x13x2880xf32>
        %490 = "ttir.multiply"(%474, %488, %489) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %491 = ttir.empty() : tensor<1x13x2880xf32>
        %492 = "ttir.multiply"(%299, %490, %491) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %493 = ttir.empty() : tensor<1x13x2880xbf16>
        %494 = "ttir.typecast"(%492, %493) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %495 = ttir.empty() : tensor<13x2880xbf16>
        %496 = "ttir.reshape"(%494, %495) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %497 = ttir.empty() : tensor<13x201088xbf16>
        %498 = "ttir.matmul"(%496, %arg10, %497) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<201088x2880xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %499 = ttir.empty() : tensor<1x13x201088xbf16>
        %500 = "ttir.reshape"(%498, %499) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %289, %291, %293, %201, %498, %500 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRExplicateTMs (ttir-explicate-tms) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.embedding"(%41, %arg4, %42) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<13x4096xbf16>
        %75 = "ttir.matmul"(%73, %arg20, %74) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<4096x2880xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %76 = ttir.empty() : tensor<1x13x4096xbf16>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %78 = ttir.empty() : tensor<1x1x4096xbf16>
        %79 = "ttir.reshape"(%arg19, %78) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %80 = ttir.empty() : tensor<1x13x4096xbf16>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %82 = ttir.empty() : tensor<1x13x4096xbf16>
        %83 = "ttir.add"(%77, %81, %82) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %84 = ttir.empty() : tensor<1x13x64x64xbf16>
        %85 = "ttir.reshape"(%83, %84) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %86 = ttir.empty() : tensor<1x64x13x64xbf16>
        %87 = "ttir.permute"(%85, %86) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %88 = ttir.empty() : tensor<1x64x13x32xbf16>
        %89 = "ttir.slice_static"(%87, %88) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %90 = ttir.empty() : tensor<1x64x13x32xf32>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %92 = ttir.empty() : tensor<1x32x1xf32>
        %93 = "ttir.reshape"(%arg7, %92) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %94 = ttir.empty() : tensor<1x32x13xf32>
        %95 = "ttir.matmul"(%93, %0, %94) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%91, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%87, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%91, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<13x512xbf16>
        %155 = "ttir.matmul"(%73, %arg9, %154) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %156 = ttir.empty() : tensor<1x13x512xbf16>
        %157 = "ttir.reshape"(%155, %156) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %158 = ttir.empty() : tensor<1x1x512xbf16>
        %159 = "ttir.reshape"(%arg8, %158) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %160 = ttir.empty() : tensor<1x13x512xbf16>
        %161 = "ttir.broadcast"(%159, %160) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %162 = ttir.empty() : tensor<1x13x512xbf16>
        %163 = "ttir.add"(%157, %161, %162) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %164 = ttir.empty() : tensor<1x13x8x64xbf16>
        %165 = "ttir.reshape"(%163, %164) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %166 = ttir.empty() : tensor<1x8x13x64xbf16>
        %167 = "ttir.permute"(%165, %166) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %168 = ttir.empty() : tensor<1x8x13x32xbf16>
        %169 = "ttir.slice_static"(%167, %168) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %170 = ttir.empty() : tensor<1x8x13x32xf32>
        %171 = "ttir.typecast"(%169, %170) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %172 = ttir.empty() : tensor<1x8x13x32xf32>
        %173 = "ttir.broadcast"(%111, %172) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %174 = ttir.empty() : tensor<1x8x13x32xf32>
        %175 = "ttir.multiply"(%171, %173, %174) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %176 = ttir.empty() : tensor<1x8x13x32xbf16>
        %177 = "ttir.typecast"(%175, %176) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %178 = ttir.empty() : tensor<1x8x13x32xbf16>
        %179 = "ttir.slice_static"(%167, %178) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %180 = ttir.empty() : tensor<1x8x13x32xf32>
        %181 = "ttir.typecast"(%179, %180) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %182 = ttir.empty() : tensor<1x8x13x32xf32>
        %183 = "ttir.broadcast"(%131, %182) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %184 = ttir.empty() : tensor<1x8x13x32xf32>
        %185 = "ttir.multiply"(%181, %183, %184) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %186 = ttir.empty() : tensor<1x8x13x32xbf16>
        %187 = "ttir.typecast"(%185, %186) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %188 = ttir.empty() : tensor<1x8x13x32xbf16>
        %189 = "ttir.subtract"(%177, %187, %188) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %190 = ttir.empty() : tensor<1x8x13x32xf32>
        %191 = "ttir.multiply"(%181, %173, %190) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %192 = ttir.empty() : tensor<1x8x13x32xbf16>
        %193 = "ttir.typecast"(%191, %192) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %194 = ttir.empty() : tensor<1x8x13x32xf32>
        %195 = "ttir.multiply"(%171, %183, %194) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %196 = ttir.empty() : tensor<1x8x13x32xbf16>
        %197 = "ttir.typecast"(%195, %196) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %198 = ttir.empty() : tensor<1x8x13x32xbf16>
        %199 = "ttir.add"(%193, %197, %198) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %200 = ttir.empty() : tensor<1x8x13x64xbf16>
        %201 = "ttir.concat"(%189, %199, %200) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %202 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %203 = "ttir.reshape"(%201, %202) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %204 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %205 = "ttir.broadcast"(%203, %204) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %206 = ttir.empty() : tensor<1x64x13x64xbf16>
        %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %208 = ttir.empty() : tensor<1x64x64x13xbf16>
        %209 = "ttir.permute"(%207, %208) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %210 = ttir.empty() : tensor<64x64x13xbf16>
        %211 = "ttir.reshape"(%209, %210) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %212 = ttir.empty() : tensor<64x13x13xbf16>
        %213 = "ttir.matmul"(%153, %211, %212) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<13x512xbf16>
        %281 = "ttir.matmul"(%73, %arg1, %280) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %282 = ttir.empty() : tensor<1x13x512xbf16>
        %283 = "ttir.reshape"(%281, %282) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %284 = ttir.empty() : tensor<1x1x512xbf16>
        %285 = "ttir.reshape"(%arg0, %284) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %286 = ttir.empty() : tensor<1x13x512xbf16>
        %287 = "ttir.broadcast"(%285, %286) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %288 = ttir.empty() : tensor<1x13x512xbf16>
        %289 = "ttir.add"(%283, %287, %288) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %290 = ttir.empty() : tensor<1x13x8x64xbf16>
        %291 = "ttir.reshape"(%289, %290) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %292 = ttir.empty() : tensor<1x8x13x64xbf16>
        %293 = "ttir.permute"(%291, %292) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %294 = ttir.empty() : tensor<2880xf32>
        %295 = "ttir.typecast"(%arg30, %294) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %296 = ttir.empty() : tensor<1x1x2880xf32>
        %297 = "ttir.reshape"(%295, %296) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %298 = ttir.empty() : tensor<1x13x2880xf32>
        %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %300 = ttir.empty() : tensor<1x64x13x14xbf16>
        %301 = "ttir.softmax"(%279, %300) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %302 = ttir.empty() : tensor<1x64x13x13xbf16>
        %303 = "ttir.slice_static"(%301, %302) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %304 = ttir.empty() : tensor<64x13x13xbf16>
        %305 = "ttir.reshape"(%303, %304) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %306 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %307 = "ttir.reshape"(%293, %306) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %308 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %309 = "ttir.broadcast"(%307, %308) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %310 = ttir.empty() : tensor<64x13x64xbf16>
        %311 = "ttir.reshape"(%309, %310) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %312 = ttir.empty() : tensor<64x13x64xbf16>
        %313 = "ttir.matmul"(%305, %311, %312) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %314 = ttir.empty() : tensor<1x64x13x64xbf16>
        %315 = "ttir.reshape"(%313, %314) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %316 = ttir.empty() : tensor<13x4096xbf16>
        %317 = ttir.empty() : tensor<1x13x4096xbf16>
        %318 = "ttir.concatenate_heads"(%315, %317) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %319 = "ttir.reshape"(%318, %316) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %320 = ttir.empty() : tensor<13x2880xbf16>
        %321 = "ttir.matmul"(%319, %arg14, %320) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16>, tensor<2880x4096xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %322 = ttir.empty() : tensor<1x13x2880xbf16>
        %323 = "ttir.reshape"(%321, %322) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %324 = ttir.empty() : tensor<1x1x2880xbf16>
        %325 = "ttir.reshape"(%arg13, %324) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %326 = ttir.empty() : tensor<1x13x2880xbf16>
        %327 = "ttir.broadcast"(%325, %326) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %328 = ttir.empty() : tensor<1x13x2880xbf16>
        %329 = "ttir.add"(%323, %327, %328) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %330 = ttir.empty() : tensor<1x13x2880xbf16>
        %331 = "ttir.add"(%45, %329, %330) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %332 = ttir.empty() : tensor<1x1x1xbf16>
        %333 = "ttir.reshape"(%arg29, %332) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %334 = ttir.empty() : tensor<32x13x2880xbf16>
        %335 = "ttir.broadcast"(%333, %334) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %336 = ttir.empty() : tensor<2880xf32>
        %337 = "ttir.typecast"(%arg21, %336) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %338 = ttir.empty() : tensor<1x1x2880xf32>
        %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %340 = ttir.empty() : tensor<1x13x2880xf32>
        %341 = "ttir.broadcast"(%339, %340) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %342 = ttir.empty() : tensor<1x13x2880xf32>
        %343 = "ttir.typecast"(%331, %342) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %344 = ttir.empty() : tensor<1x13x2880xf32>
        %345 = "ttir.pow"(%343, %27, %344) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %346 = ttir.empty() : tensor<1x13xf32>
        %347 = "ttir.sum"(%345, %346) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %348 = ttir.empty() : tensor<1x13xf32>
        %349 = "ttir.multiply"(%347, %4, %348) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %350 = ttir.empty() : tensor<1x13x1xf32>
        %351 = "ttir.reshape"(%349, %350) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %352 = ttir.empty() : tensor<1x13x1xf32>
        %353 = "ttir.add"(%351, %59, %352) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %354 = ttir.empty() : tensor<1x13x1xf32>
        %355 = "ttir.rsqrt"(%353, %354) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %356 = ttir.empty() : tensor<1x13x2880xf32>
        %357 = "ttir.broadcast"(%355, %356) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %358 = ttir.empty() : tensor<1x13x2880xf32>
        %359 = "ttir.multiply"(%343, %357, %358) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %360 = ttir.empty() : tensor<1x13x2880xf32>
        %361 = "ttir.multiply"(%341, %359, %360) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %362 = ttir.empty() : tensor<1x13x2880xbf16>
        %363 = "ttir.typecast"(%361, %362) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %364 = ttir.empty() : tensor<13x2880xbf16>
        %365 = "ttir.reshape"(%363, %364) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %366 = ttir.empty() : tensor<416x2880xbf16>
        %367 = "ttir.concat"(%365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %366) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %368 = ttir.empty() : tensor<32x13x2880xbf16>
        %369 = "ttir.reshape"(%367, %368) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %370 = ttir.empty() : tensor<32x13x5760xbf16>
        %371 = "ttir.matmul"(%369, %arg28, %370) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %372 = ttir.empty() : tensor<32x1x5760xbf16>
        %373 = "ttir.reshape"(%arg27, %372) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %374 = ttir.empty() : tensor<32x13x5760xbf16>
        %375 = "ttir.broadcast"(%373, %374) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %376 = ttir.empty() : tensor<32x13x5760xbf16>
        %377 = "ttir.add"(%371, %375, %376) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %378 = ttir.empty() : tensor<32x13x2880xbf16>
        %379 = "ttir.slice_static"(%377, %378) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %380 = ttir.empty() : tensor<1x1x1xbf16>
        %381 = "ttir.reshape"(%arg25, %380) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %382 = ttir.empty() : tensor<32x13x2880xbf16>
        %383 = "ttir.broadcast"(%381, %382) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %384 = ttir.empty() : tensor<32x13x2880xbf16>
        %385 = "ttir.clamp_tensor"(%379, %335, %383, %384) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.add"(%385, %15, %386) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<32x13x2880xf32>
        %389 = "ttir.typecast"(%387, %388) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %390 = ttir.empty() : tensor<1x1x1xbf16>
        %391 = "ttir.reshape"(%arg26, %390) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xbf16>
        %393 = "ttir.broadcast"(%391, %392) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %394 = ttir.empty() : tensor<32x13x2880xbf16>
        %395 = "ttir.slice_static"(%377, %394) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xbf16>
        %397 = "ttir.clamp_tensor"(%395, %393, %383, %396) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %398 = ttir.empty() : tensor<32x13x2880xf32>
        %399 = "ttir.typecast"(%397, %398) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %400 = ttir.empty() : tensor<1x1x1xf32>
        %401 = "ttir.reshape"(%arg24, %400) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %402 = ttir.empty() : tensor<32x13x2880xf32>
        %403 = "ttir.broadcast"(%401, %402) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %404 = ttir.empty() : tensor<32x13x2880xf32>
        %405 = "ttir.multiply"(%399, %403, %404) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.typecast"(%405, %406) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.sigmoid"(%407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xf32>
        %411 = "ttir.typecast"(%409, %410) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.multiply"(%399, %411, %412) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<32x13x2880xf32>
        %415 = "ttir.multiply"(%389, %413, %414) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.typecast"(%415, %416) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xbf16>
        %419 = "ttir.matmul"(%417, %arg23, %418) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %420 = ttir.empty() : tensor<32x1x2880xbf16>
        %421 = "ttir.reshape"(%arg22, %420) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xbf16>
        %423 = "ttir.broadcast"(%421, %422) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %424 = ttir.empty() : tensor<32x13x2880xbf16>
        %425 = "ttir.add"(%419, %423, %424) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %426 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %427 = "ttir.reshape"(%425, %426) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %428 = ttir.empty() : tensor<32x1x13x2880xf32>
        %429 = "ttir.typecast"(%427, %428) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %430 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %431 = ttir.empty() : tensor<13x1x1xsi32>
        %432 = "ttir.reshape"(%430, %431) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %433 = ttir.empty() : tensor<13x4x1xsi32>
        %434 = "ttir.broadcast"(%432, %433) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %435 = ttir.empty() : tensor<13x32xbf16>
        %436 = "ttir.matmul"(%365, %arg12, %435) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<32x2880xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %437 = ttir.empty() : tensor<1x32xbf16>
        %438 = "ttir.reshape"(%arg11, %437) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %439 = ttir.empty() : tensor<13x32xbf16>
        %440 = "ttir.broadcast"(%438, %439) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %441 = ttir.empty() : tensor<13x32xbf16>
        %442 = "ttir.add"(%436, %440, %441) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %443 = ttir.empty() : tensor<13x32xbf16>
        %444 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%442, %443, %444) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %445 = ttir.empty() : tensor<13x4xsi32>
        %446 = "ttir.slice_static"(%indices, %445) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %447 = ttir.empty() : tensor<13x4x1xsi32>
        %448 = "ttir.reshape"(%446, %447) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %449 = ttir.empty() : tensor<13x4x2xsi32>
        %450 = "ttir.concat"(%434, %448, %449) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %451 = ttir.empty() : tensor<13x4xbf16>
        %452 = "ttir.slice_static"(%values, %451) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %453 = ttir.empty() : tensor<13x4xbf16>
        %454 = "ttir.softmax"(%452, %453) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %455 = ttir.empty() : tensor<13x32xbf16>
        %456 = "ttir.scatter"(%11, %450, %454, %455) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %457 = ttir.empty() : tensor<32x13xbf16>
        %458 = "ttir.permute"(%456, %457) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %459 = ttir.empty() : tensor<32x1x13x1xbf16>
        %460 = "ttir.reshape"(%458, %459) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %461 = ttir.empty() : tensor<32x1x13x1xf32>
        %462 = "ttir.typecast"(%460, %461) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %463 = ttir.empty() : tensor<32x1x13x2880xf32>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %465 = ttir.empty() : tensor<32x1x13x2880xf32>
        %466 = "ttir.multiply"(%429, %464, %465) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %467 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %468 = "ttir.typecast"(%466, %467) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %469 = ttir.empty() : tensor<1x13x2880xbf16>
        %470 = "ttir.sum"(%468, %469) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %471 = ttir.empty() : tensor<1x13x2880xbf16>
        %472 = "ttir.add"(%331, %470, %471) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %473 = ttir.empty() : tensor<1x13x2880xf32>
        %474 = "ttir.typecast"(%472, %473) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %475 = ttir.empty() : tensor<1x13x2880xf32>
        %476 = "ttir.pow"(%474, %27, %475) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %477 = ttir.empty() : tensor<1x13xf32>
        %478 = "ttir.sum"(%476, %477) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %479 = ttir.empty() : tensor<1x13xf32>
        %480 = "ttir.multiply"(%478, %4, %479) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %481 = ttir.empty() : tensor<1x13x1xf32>
        %482 = "ttir.reshape"(%480, %481) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %483 = ttir.empty() : tensor<1x13x1xf32>
        %484 = "ttir.add"(%482, %59, %483) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %485 = ttir.empty() : tensor<1x13x1xf32>
        %486 = "ttir.rsqrt"(%484, %485) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %487 = ttir.empty() : tensor<1x13x2880xf32>
        %488 = "ttir.broadcast"(%486, %487) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %489 = ttir.empty() : tensor<1x13x2880xf32>
        %490 = "ttir.multiply"(%474, %488, %489) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %491 = ttir.empty() : tensor<1x13x2880xf32>
        %492 = "ttir.multiply"(%299, %490, %491) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %493 = ttir.empty() : tensor<1x13x2880xbf16>
        %494 = "ttir.typecast"(%492, %493) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %495 = ttir.empty() : tensor<13x2880xbf16>
        %496 = "ttir.reshape"(%494, %495) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %497 = ttir.empty() : tensor<13x201088xbf16>
        %498 = "ttir.matmul"(%496, %arg10, %497) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<201088x2880xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %499 = ttir.empty() : tensor<1x13x201088xbf16>
        %500 = "ttir.reshape"(%498, %499) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %289, %291, %293, %201, %498, %500 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIREraseInverseOps (ttir-erase-inverse-ops) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %22 = ttir.empty() : tensor<13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1xui8>) -> tensor<1x1xui8>
        %30 = ttir.empty() : tensor<13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 13, 13>}> : (tensor<1x1xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<13xsi32>
        %39 = "ttir.reshape"(%arg3, %38) <{shape = [13 : i32]}> : (tensor<1x13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.typecast"(%39, %40) <{conservative_folding = false}> : (tensor<13xsi32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.embedding"(%41, %arg4, %42) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<1x13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %56 = ttir.empty() : tensor<1x1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %58 = ttir.empty() : tensor<1x13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %60 = ttir.empty() : tensor<1x13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %62 = ttir.empty() : tensor<1x13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %64 = ttir.empty() : tensor<1x13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %66 = ttir.empty() : tensor<1x13x2880xf32>
        %67 = "ttir.multiply"(%47, %65, %66) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %68 = ttir.empty() : tensor<1x13x2880xf32>
        %69 = "ttir.multiply"(%37, %67, %68) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %70 = ttir.empty() : tensor<1x13x2880xbf16>
        %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.reshape"(%71, %72) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<13x4096xbf16>
        %75 = "ttir.matmul"(%73, %arg20, %74) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<4096x2880xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %76 = ttir.empty() : tensor<1x13x4096xbf16>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 13 : i32, 4096 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %78 = ttir.empty() : tensor<1x1x4096xbf16>
        %79 = "ttir.reshape"(%arg19, %78) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xbf16>, tensor<1x1x4096xbf16>) -> tensor<1x1x4096xbf16>
        %80 = ttir.empty() : tensor<1x13x4096xbf16>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %82 = ttir.empty() : tensor<1x13x4096xbf16>
        %83 = "ttir.add"(%77, %81, %82) : (tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %84 = ttir.empty() : tensor<1x13x64x64xbf16>
        %85 = "ttir.reshape"(%83, %84) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<1x13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %86 = ttir.empty() : tensor<1x64x13x64xbf16>
        %87 = "ttir.permute"(%85, %86) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %88 = ttir.empty() : tensor<1x64x13x32xbf16>
        %89 = "ttir.slice_static"(%87, %88) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %90 = ttir.empty() : tensor<1x64x13x32xf32>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %92 = ttir.empty() : tensor<1x32x1xf32>
        %93 = "ttir.reshape"(%arg7, %92) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %94 = ttir.empty() : tensor<1x32x13xf32>
        %95 = "ttir.matmul"(%93, %0, %94) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %96 = ttir.empty() : tensor<1x13x32xf32>
        %97 = "ttir.permute"(%95, %96) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.cos"(%97, %98) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x1x1xf32>
        %101 = "ttir.reshape"(%arg6, %100) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %102 = ttir.empty() : tensor<1x13x32xf32>
        %103 = "ttir.broadcast"(%101, %102) <{broadcast_dimensions = array<i64: 1, 13, 32>}> : (tensor<1x1x1xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %104 = ttir.empty() : tensor<1x13x32xf32>
        %105 = "ttir.multiply"(%99, %103, %104) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xbf16>
        %107 = "ttir.typecast"(%105, %106) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %108 = ttir.empty() : tensor<1x1x13x32xbf16>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x64x13x32xf32>
        %113 = "ttir.broadcast"(%111, %112) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %114 = ttir.empty() : tensor<1x64x13x32xf32>
        %115 = "ttir.multiply"(%91, %113, %114) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %116 = ttir.empty() : tensor<1x64x13x32xbf16>
        %117 = "ttir.typecast"(%115, %116) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %118 = ttir.empty() : tensor<1x64x13x32xbf16>
        %119 = "ttir.slice_static"(%87, %118) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %120 = ttir.empty() : tensor<1x64x13x32xf32>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %122 = ttir.empty() : tensor<1x13x32xf32>
        %123 = "ttir.sin"(%97, %122) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %124 = ttir.empty() : tensor<1x13x32xf32>
        %125 = "ttir.multiply"(%123, %103, %124) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %126 = ttir.empty() : tensor<1x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<1x13x32xf32>, tensor<1x13x32xbf16>) -> tensor<1x13x32xbf16>
        %128 = ttir.empty() : tensor<1x1x13x32xbf16>
        %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x64x13x32xf32>
        %133 = "ttir.broadcast"(%131, %132) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %134 = ttir.empty() : tensor<1x64x13x32xf32>
        %135 = "ttir.multiply"(%121, %133, %134) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %136 = ttir.empty() : tensor<1x64x13x32xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %138 = ttir.empty() : tensor<1x64x13x32xbf16>
        %139 = "ttir.subtract"(%117, %137, %138) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %140 = ttir.empty() : tensor<1x64x13x32xf32>
        %141 = "ttir.multiply"(%121, %113, %140) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %142 = ttir.empty() : tensor<1x64x13x32xbf16>
        %143 = "ttir.typecast"(%141, %142) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %144 = ttir.empty() : tensor<1x64x13x32xf32>
        %145 = "ttir.multiply"(%91, %133, %144) : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %146 = ttir.empty() : tensor<1x64x13x32xbf16>
        %147 = "ttir.typecast"(%145, %146) <{conservative_folding = false}> : (tensor<1x64x13x32xf32>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %148 = ttir.empty() : tensor<1x64x13x32xbf16>
        %149 = "ttir.add"(%143, %147, %148) : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %150 = ttir.empty() : tensor<1x64x13x64xbf16>
        %151 = "ttir.concat"(%139, %149, %150) <{dim = 3 : si32}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %152 = ttir.empty() : tensor<64x13x64xbf16>
        %153 = "ttir.reshape"(%151, %152) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %154 = ttir.empty() : tensor<13x512xbf16>
        %155 = "ttir.matmul"(%73, %arg9, %154) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %156 = ttir.empty() : tensor<1x13x512xbf16>
        %157 = "ttir.reshape"(%155, %156) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %158 = ttir.empty() : tensor<1x1x512xbf16>
        %159 = "ttir.reshape"(%arg8, %158) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %160 = ttir.empty() : tensor<1x13x512xbf16>
        %161 = "ttir.broadcast"(%159, %160) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %162 = ttir.empty() : tensor<1x13x512xbf16>
        %163 = "ttir.add"(%157, %161, %162) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %164 = ttir.empty() : tensor<1x13x8x64xbf16>
        %165 = "ttir.reshape"(%163, %164) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %166 = ttir.empty() : tensor<1x8x13x64xbf16>
        %167 = "ttir.permute"(%165, %166) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %168 = ttir.empty() : tensor<1x8x13x32xbf16>
        %169 = "ttir.slice_static"(%167, %168) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %170 = ttir.empty() : tensor<1x8x13x32xf32>
        %171 = "ttir.typecast"(%169, %170) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %172 = ttir.empty() : tensor<1x8x13x32xf32>
        %173 = "ttir.broadcast"(%111, %172) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %174 = ttir.empty() : tensor<1x8x13x32xf32>
        %175 = "ttir.multiply"(%171, %173, %174) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %176 = ttir.empty() : tensor<1x8x13x32xbf16>
        %177 = "ttir.typecast"(%175, %176) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %178 = ttir.empty() : tensor<1x8x13x32xbf16>
        %179 = "ttir.slice_static"(%167, %178) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %180 = ttir.empty() : tensor<1x8x13x32xf32>
        %181 = "ttir.typecast"(%179, %180) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %182 = ttir.empty() : tensor<1x8x13x32xf32>
        %183 = "ttir.broadcast"(%131, %182) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %184 = ttir.empty() : tensor<1x8x13x32xf32>
        %185 = "ttir.multiply"(%181, %183, %184) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %186 = ttir.empty() : tensor<1x8x13x32xbf16>
        %187 = "ttir.typecast"(%185, %186) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %188 = ttir.empty() : tensor<1x8x13x32xbf16>
        %189 = "ttir.subtract"(%177, %187, %188) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %190 = ttir.empty() : tensor<1x8x13x32xf32>
        %191 = "ttir.multiply"(%181, %173, %190) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %192 = ttir.empty() : tensor<1x8x13x32xbf16>
        %193 = "ttir.typecast"(%191, %192) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %194 = ttir.empty() : tensor<1x8x13x32xf32>
        %195 = "ttir.multiply"(%171, %183, %194) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %196 = ttir.empty() : tensor<1x8x13x32xbf16>
        %197 = "ttir.typecast"(%195, %196) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %198 = ttir.empty() : tensor<1x8x13x32xbf16>
        %199 = "ttir.add"(%193, %197, %198) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %200 = ttir.empty() : tensor<1x8x13x64xbf16>
        %201 = "ttir.concat"(%189, %199, %200) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %202 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %203 = "ttir.reshape"(%201, %202) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %204 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %205 = "ttir.broadcast"(%203, %204) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %206 = ttir.empty() : tensor<1x64x13x64xbf16>
        %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %208 = ttir.empty() : tensor<1x64x64x13xbf16>
        %209 = "ttir.permute"(%207, %208) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %210 = ttir.empty() : tensor<64x64x13xbf16>
        %211 = "ttir.reshape"(%209, %210) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %212 = ttir.empty() : tensor<64x13x13xbf16>
        %213 = "ttir.matmul"(%153, %211, %212) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %214 = ttir.empty() : tensor<1x64x13x13xbf16>
        %215 = "ttir.reshape"(%213, %214) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %216 = ttir.empty() : tensor<1x64x13x13xf32>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %218 = ttir.empty() : tensor<1x1x1x1xf32>
        %219 = "ttir.reshape"(%arg18, %218) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %220 = ttir.empty() : tensor<1x64x13x13xf32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.multiply"(%217, %221, %222) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x64x13x13xbf16>
        %225 = "ttir.typecast"(%223, %224) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %226 = ttir.empty() : tensor<1x13xsi32>
        %227 = "ttir.reshape"(%1, %226) <{shape = [1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x13xsi32>) -> tensor<1x13xsi32>
        %228 = ttir.empty() : tensor<13x13xsi32>
        %229 = "ttir.broadcast"(%227, %228) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x13xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %230 = ttir.empty() : tensor<1xsi32>
        %231 = "ttir.reshape"(%arg17, %230) <{shape = [1 : i32]}> : (tensor<si32>, tensor<1xsi32>) -> tensor<1xsi32>
        %232 = ttir.empty() : tensor<13xsi32>
        %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 13>}> : (tensor<1xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %234 = ttir.empty() : tensor<13xsi32>
        %235 = "ttir.subtract"(%1, %233, %234) : (tensor<13xsi32>, tensor<13xsi32>, tensor<13xsi32>) -> tensor<13xsi32>
        %236 = ttir.empty() : tensor<13x1xsi32>
        %237 = "ttir.reshape"(%235, %236) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %238 = ttir.empty() : tensor<13x13xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %240 = ttir.empty() : tensor<13x13xbf16>
        %241 = "ttir.gt"(%229, %239, %240) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %242 = ttir.empty() : tensor<13x13xui8>
        %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %244 = ttir.empty() : tensor<13x13xui8>
        %245 = "ttir.bitwise_and"(%243, %23, %244) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %246 = ttir.empty() : tensor<13x13xbf16>
        %247 = "ttir.ne"(%245, %31, %246) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %248 = ttir.empty() : tensor<13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %250 = ttir.empty() : tensor<13x1xsi32>
        %251 = "ttir.reshape"(%1, %250) <{shape = [13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1xsi32>) -> tensor<13x1xsi32>
        %252 = ttir.empty() : tensor<13x13xsi32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 13>}> : (tensor<13x1xsi32>, tensor<13x13xsi32>) -> tensor<13x13xsi32>
        %254 = ttir.empty() : tensor<13x13xbf16>
        %255 = "ttir.le"(%229, %253, %254) : (tensor<13x13xsi32>, tensor<13x13xsi32>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %256 = ttir.empty() : tensor<13x13xui8>
        %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<13x13xbf16>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %258 = ttir.empty() : tensor<13x13xui8>
        %259 = "ttir.bitwise_and"(%249, %257, %258) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
        %260 = ttir.empty() : tensor<13x13xbf16>
        %261 = "ttir.ne"(%259, %31, %260) : (tensor<13x13xui8>, tensor<13x13xui8>, tensor<13x13xbf16>) -> tensor<13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xbf16>
        %263 = "ttir.reshape"(%261, %262) <{shape = [1 : i32, 1 : i32, 13 : i32, 13 : i32]}> : (tensor<13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %264 = ttir.empty() : tensor<1x1x1x1xbf16>
        %265 = "ttir.reshape"(%arg16, %264) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.broadcast"(%265, %266) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x13x13xbf16>
        %269 = "ttir.where"(%263, %19, %267, %268) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %270 = ttir.empty() : tensor<1x64x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %272 = ttir.empty() : tensor<1x64x13x13xbf16>
        %273 = "ttir.add"(%225, %271, %272) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x1x1xbf16>
        %275 = "ttir.reshape"(%arg15, %274) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %276 = ttir.empty() : tensor<1x64x13x1xbf16>
        %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %278 = ttir.empty() : tensor<1x64x13x14xbf16>
        %279 = "ttir.concat"(%273, %277, %278) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %280 = ttir.empty() : tensor<13x512xbf16>
        %281 = "ttir.matmul"(%73, %arg1, %280) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %282 = ttir.empty() : tensor<1x13x512xbf16>
        %283 = "ttir.reshape"(%281, %282) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %284 = ttir.empty() : tensor<1x1x512xbf16>
        %285 = "ttir.reshape"(%arg0, %284) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %286 = ttir.empty() : tensor<1x13x512xbf16>
        %287 = "ttir.broadcast"(%285, %286) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %288 = ttir.empty() : tensor<1x13x512xbf16>
        %289 = "ttir.add"(%283, %287, %288) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %290 = ttir.empty() : tensor<1x13x8x64xbf16>
        %291 = "ttir.reshape"(%289, %290) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %292 = ttir.empty() : tensor<1x8x13x64xbf16>
        %293 = "ttir.permute"(%291, %292) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %294 = ttir.empty() : tensor<2880xf32>
        %295 = "ttir.typecast"(%arg30, %294) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %296 = ttir.empty() : tensor<1x1x2880xf32>
        %297 = "ttir.reshape"(%295, %296) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %298 = ttir.empty() : tensor<1x13x2880xf32>
        %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %300 = ttir.empty() : tensor<1x64x13x14xbf16>
        %301 = "ttir.softmax"(%279, %300) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %302 = ttir.empty() : tensor<1x64x13x13xbf16>
        %303 = "ttir.slice_static"(%301, %302) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %304 = ttir.empty() : tensor<64x13x13xbf16>
        %305 = "ttir.reshape"(%303, %304) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %306 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %307 = "ttir.reshape"(%293, %306) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %308 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %309 = "ttir.broadcast"(%307, %308) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %310 = ttir.empty() : tensor<64x13x64xbf16>
        %311 = "ttir.reshape"(%309, %310) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %312 = ttir.empty() : tensor<64x13x64xbf16>
        %313 = "ttir.matmul"(%305, %311, %312) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %314 = ttir.empty() : tensor<1x64x13x64xbf16>
        %315 = "ttir.reshape"(%313, %314) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %316 = ttir.empty() : tensor<13x4096xbf16>
        %317 = ttir.empty() : tensor<1x13x4096xbf16>
        %318 = "ttir.concatenate_heads"(%315, %317) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %319 = "ttir.reshape"(%318, %316) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %320 = ttir.empty() : tensor<13x2880xbf16>
        %321 = "ttir.matmul"(%319, %arg14, %320) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16>, tensor<2880x4096xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %322 = ttir.empty() : tensor<1x13x2880xbf16>
        %323 = "ttir.reshape"(%321, %322) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %324 = ttir.empty() : tensor<1x1x2880xbf16>
        %325 = "ttir.reshape"(%arg13, %324) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %326 = ttir.empty() : tensor<1x13x2880xbf16>
        %327 = "ttir.broadcast"(%325, %326) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %328 = ttir.empty() : tensor<1x13x2880xbf16>
        %329 = "ttir.add"(%323, %327, %328) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %330 = ttir.empty() : tensor<1x13x2880xbf16>
        %331 = "ttir.add"(%45, %329, %330) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %332 = ttir.empty() : tensor<1x1x1xbf16>
        %333 = "ttir.reshape"(%arg29, %332) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %334 = ttir.empty() : tensor<32x13x2880xbf16>
        %335 = "ttir.broadcast"(%333, %334) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %336 = ttir.empty() : tensor<2880xf32>
        %337 = "ttir.typecast"(%arg21, %336) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %338 = ttir.empty() : tensor<1x1x2880xf32>
        %339 = "ttir.reshape"(%337, %338) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x1x2880xf32>) -> tensor<1x1x2880xf32>
        %340 = ttir.empty() : tensor<1x13x2880xf32>
        %341 = "ttir.broadcast"(%339, %340) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %342 = ttir.empty() : tensor<1x13x2880xf32>
        %343 = "ttir.typecast"(%331, %342) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %344 = ttir.empty() : tensor<1x13x2880xf32>
        %345 = "ttir.pow"(%343, %27, %344) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %346 = ttir.empty() : tensor<1x13xf32>
        %347 = "ttir.sum"(%345, %346) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %348 = ttir.empty() : tensor<1x13xf32>
        %349 = "ttir.multiply"(%347, %4, %348) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %350 = ttir.empty() : tensor<1x13x1xf32>
        %351 = "ttir.reshape"(%349, %350) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %352 = ttir.empty() : tensor<1x13x1xf32>
        %353 = "ttir.add"(%351, %59, %352) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %354 = ttir.empty() : tensor<1x13x1xf32>
        %355 = "ttir.rsqrt"(%353, %354) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %356 = ttir.empty() : tensor<1x13x2880xf32>
        %357 = "ttir.broadcast"(%355, %356) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %358 = ttir.empty() : tensor<1x13x2880xf32>
        %359 = "ttir.multiply"(%343, %357, %358) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %360 = ttir.empty() : tensor<1x13x2880xf32>
        %361 = "ttir.multiply"(%341, %359, %360) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %362 = ttir.empty() : tensor<1x13x2880xbf16>
        %363 = "ttir.typecast"(%361, %362) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %364 = ttir.empty() : tensor<13x2880xbf16>
        %365 = "ttir.reshape"(%363, %364) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %366 = ttir.empty() : tensor<416x2880xbf16>
        %367 = "ttir.concat"(%365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %365, %366) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %368 = ttir.empty() : tensor<32x13x2880xbf16>
        %369 = "ttir.reshape"(%367, %368) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %370 = ttir.empty() : tensor<32x13x5760xbf16>
        %371 = "ttir.matmul"(%369, %arg28, %370) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %372 = ttir.empty() : tensor<32x1x5760xbf16>
        %373 = "ttir.reshape"(%arg27, %372) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %374 = ttir.empty() : tensor<32x13x5760xbf16>
        %375 = "ttir.broadcast"(%373, %374) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %376 = ttir.empty() : tensor<32x13x5760xbf16>
        %377 = "ttir.add"(%371, %375, %376) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %378 = ttir.empty() : tensor<32x13x2880xbf16>
        %379 = "ttir.slice_static"(%377, %378) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %380 = ttir.empty() : tensor<1x1x1xbf16>
        %381 = "ttir.reshape"(%arg25, %380) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %382 = ttir.empty() : tensor<32x13x2880xbf16>
        %383 = "ttir.broadcast"(%381, %382) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %384 = ttir.empty() : tensor<32x13x2880xbf16>
        %385 = "ttir.clamp_tensor"(%379, %335, %383, %384) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.add"(%385, %15, %386) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<32x13x2880xf32>
        %389 = "ttir.typecast"(%387, %388) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %390 = ttir.empty() : tensor<1x1x1xbf16>
        %391 = "ttir.reshape"(%arg26, %390) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xbf16>
        %393 = "ttir.broadcast"(%391, %392) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %394 = ttir.empty() : tensor<32x13x2880xbf16>
        %395 = "ttir.slice_static"(%377, %394) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xbf16>
        %397 = "ttir.clamp_tensor"(%395, %393, %383, %396) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %398 = ttir.empty() : tensor<32x13x2880xf32>
        %399 = "ttir.typecast"(%397, %398) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %400 = ttir.empty() : tensor<1x1x1xf32>
        %401 = "ttir.reshape"(%arg24, %400) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %402 = ttir.empty() : tensor<32x13x2880xf32>
        %403 = "ttir.broadcast"(%401, %402) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %404 = ttir.empty() : tensor<32x13x2880xf32>
        %405 = "ttir.multiply"(%399, %403, %404) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %406 = ttir.empty() : tensor<32x13x2880xbf16>
        %407 = "ttir.typecast"(%405, %406) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %408 = ttir.empty() : tensor<32x13x2880xbf16>
        %409 = "ttir.sigmoid"(%407, %408) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %410 = ttir.empty() : tensor<32x13x2880xf32>
        %411 = "ttir.typecast"(%409, %410) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %412 = ttir.empty() : tensor<32x13x2880xf32>
        %413 = "ttir.multiply"(%399, %411, %412) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %414 = ttir.empty() : tensor<32x13x2880xf32>
        %415 = "ttir.multiply"(%389, %413, %414) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %416 = ttir.empty() : tensor<32x13x2880xbf16>
        %417 = "ttir.typecast"(%415, %416) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %418 = ttir.empty() : tensor<32x13x2880xbf16>
        %419 = "ttir.matmul"(%417, %arg23, %418) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %420 = ttir.empty() : tensor<32x1x2880xbf16>
        %421 = "ttir.reshape"(%arg22, %420) <{shape = [32 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x2880xbf16>) -> tensor<32x1x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xbf16>
        %423 = "ttir.broadcast"(%421, %422) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %424 = ttir.empty() : tensor<32x13x2880xbf16>
        %425 = "ttir.add"(%419, %423, %424) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %426 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %427 = "ttir.reshape"(%425, %426) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %428 = ttir.empty() : tensor<32x1x13x2880xf32>
        %429 = "ttir.typecast"(%427, %428) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %430 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %431 = ttir.empty() : tensor<13x1x1xsi32>
        %432 = "ttir.reshape"(%430, %431) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %433 = ttir.empty() : tensor<13x4x1xsi32>
        %434 = "ttir.broadcast"(%432, %433) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %435 = ttir.empty() : tensor<13x32xbf16>
        %436 = "ttir.matmul"(%365, %arg12, %435) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<32x2880xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %437 = ttir.empty() : tensor<1x32xbf16>
        %438 = "ttir.reshape"(%arg11, %437) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %439 = ttir.empty() : tensor<13x32xbf16>
        %440 = "ttir.broadcast"(%438, %439) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %441 = ttir.empty() : tensor<13x32xbf16>
        %442 = "ttir.add"(%436, %440, %441) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %443 = ttir.empty() : tensor<13x32xbf16>
        %444 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%442, %443, %444) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %445 = ttir.empty() : tensor<13x4xsi32>
        %446 = "ttir.slice_static"(%indices, %445) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %447 = ttir.empty() : tensor<13x4x1xsi32>
        %448 = "ttir.reshape"(%446, %447) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %449 = ttir.empty() : tensor<13x4x2xsi32>
        %450 = "ttir.concat"(%434, %448, %449) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %451 = ttir.empty() : tensor<13x4xbf16>
        %452 = "ttir.slice_static"(%values, %451) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %453 = ttir.empty() : tensor<13x4xbf16>
        %454 = "ttir.softmax"(%452, %453) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %455 = ttir.empty() : tensor<13x32xbf16>
        %456 = "ttir.scatter"(%11, %450, %454, %455) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %457 = ttir.empty() : tensor<32x13xbf16>
        %458 = "ttir.permute"(%456, %457) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xbf16>, tensor<32x13xbf16>) -> tensor<32x13xbf16>
        %459 = ttir.empty() : tensor<32x1x13x1xbf16>
        %460 = "ttir.reshape"(%458, %459) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xbf16>, tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xbf16>
        %461 = ttir.empty() : tensor<32x1x13x1xf32>
        %462 = "ttir.typecast"(%460, %461) <{conservative_folding = false}> : (tensor<32x1x13x1xbf16>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %463 = ttir.empty() : tensor<32x1x13x2880xf32>
        %464 = "ttir.broadcast"(%462, %463) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %465 = ttir.empty() : tensor<32x1x13x2880xf32>
        %466 = "ttir.multiply"(%429, %464, %465) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %467 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %468 = "ttir.typecast"(%466, %467) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %469 = ttir.empty() : tensor<1x13x2880xbf16>
        %470 = "ttir.sum"(%468, %469) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %471 = ttir.empty() : tensor<1x13x2880xbf16>
        %472 = "ttir.add"(%331, %470, %471) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %473 = ttir.empty() : tensor<1x13x2880xf32>
        %474 = "ttir.typecast"(%472, %473) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %475 = ttir.empty() : tensor<1x13x2880xf32>
        %476 = "ttir.pow"(%474, %27, %475) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %477 = ttir.empty() : tensor<1x13xf32>
        %478 = "ttir.sum"(%476, %477) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %479 = ttir.empty() : tensor<1x13xf32>
        %480 = "ttir.multiply"(%478, %4, %479) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %481 = ttir.empty() : tensor<1x13x1xf32>
        %482 = "ttir.reshape"(%480, %481) <{shape = [1 : i32, 13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %483 = ttir.empty() : tensor<1x13x1xf32>
        %484 = "ttir.add"(%482, %59, %483) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %485 = ttir.empty() : tensor<1x13x1xf32>
        %486 = "ttir.rsqrt"(%484, %485) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
        %487 = ttir.empty() : tensor<1x13x2880xf32>
        %488 = "ttir.broadcast"(%486, %487) <{broadcast_dimensions = array<i64: 1, 1, 2880>}> : (tensor<1x13x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %489 = ttir.empty() : tensor<1x13x2880xf32>
        %490 = "ttir.multiply"(%474, %488, %489) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %491 = ttir.empty() : tensor<1x13x2880xf32>
        %492 = "ttir.multiply"(%299, %490, %491) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %493 = ttir.empty() : tensor<1x13x2880xbf16>
        %494 = "ttir.typecast"(%492, %493) <{conservative_folding = false}> : (tensor<1x13x2880xf32>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %495 = ttir.empty() : tensor<13x2880xbf16>
        %496 = "ttir.reshape"(%494, %495) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %497 = ttir.empty() : tensor<13x201088xbf16>
        %498 = "ttir.matmul"(%496, %arg10, %497) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<201088x2880xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %499 = ttir.empty() : tensor<1x13x201088xbf16>
        %500 = "ttir.reshape"(%498, %499) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %289, %291, %293, %201, %498, %500 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIREraseInverseOps (ttir-erase-inverse-ops) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1x1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1x1x1xui8>) -> tensor<1x1x1x1xui8>
        %22 = ttir.empty() : tensor<1x1x13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1x1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1x1x1xui8>) -> tensor<1x1x1x1xui8>
        %30 = ttir.empty() : tensor<1x1x13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %36 = ttir.empty() : tensor<13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %38 = ttir.empty() : tensor<1x13xui32>
        %39 = "ttir.typecast"(%arg3, %38) <{conservative_folding = false}> : (tensor<1x13xsi32>, tensor<1x13xui32>) -> tensor<1x13xui32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.reshape"(%39, %40) <{shape = [13 : i32]}> : (tensor<1x13xui32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.embedding"(%41, %arg4, %42) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %56 = ttir.empty() : tensor<1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %58 = ttir.empty() : tensor<13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %60 = ttir.empty() : tensor<13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<13x1xf32>, tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %62 = ttir.empty() : tensor<13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %64 = ttir.empty() : tensor<13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 2880>}> : (tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %66 = ttir.empty() : tensor<13x2880xf32>
        %67 = "ttir.reshape"(%47, %66) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %68 = ttir.empty() : tensor<13x2880xf32>
        %69 = "ttir.multiply"(%67, %65, %68) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %70 = ttir.empty() : tensor<13x2880xf32>
        %71 = "ttir.multiply"(%37, %69, %70) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.typecast"(%71, %72) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<13x4096xbf16>
        %75 = "ttir.matmul"(%73, %arg20, %74) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<4096x2880xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %76 = ttir.empty() : tensor<1x1x64x64xbf16>
        %77 = "ttir.reshape"(%arg19, %76) <{shape = [1 : i32, 1 : i32, 64 : i32, 64 : i32]}> : (tensor<4096xbf16>, tensor<1x1x64x64xbf16>) -> tensor<1x1x64x64xbf16>
        %78 = ttir.empty() : tensor<1x64x1x64xbf16>
        %79 = "ttir.permute"(%77, %78) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x64x64xbf16>, tensor<1x64x1x64xbf16>) -> tensor<1x64x1x64xbf16>
        %80 = ttir.empty() : tensor<1x64x13x64xbf16>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %82 = ttir.empty() : tensor<1x13x64x64xbf16>
        %83 = "ttir.reshape"(%75, %82) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %84 = ttir.empty() : tensor<1x64x13x64xbf16>
        %85 = "ttir.permute"(%83, %84) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %86 = ttir.empty() : tensor<1x64x13x64xbf16>
        %87 = "ttir.add"(%85, %81, %86) : (tensor<1x64x13x64xbf16>, tensor<1x64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %88 = ttir.empty() : tensor<1x64x13x32xbf16>
        %89 = "ttir.slice_static"(%87, %88) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %90 = ttir.empty() : tensor<1x64x13x32xf32>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %92 = ttir.empty() : tensor<64x13x32xf32>
        %93 = "ttir.reshape"(%91, %92) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %94 = ttir.empty() : tensor<1x32x1xf32>
        %95 = "ttir.reshape"(%arg7, %94) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %96 = ttir.empty() : tensor<1x32x13xf32>
        %97 = "ttir.matmul"(%95, %0, %96) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.permute"(%97, %98) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x13x32xf32>
        %101 = "ttir.cos"(%99, %100) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %102 = ttir.empty() : tensor<1x1x13x32xf32>
        %103 = "ttir.reshape"(%101, %102) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %104 = ttir.empty() : tensor<1x1x1x1xf32>
        %105 = "ttir.reshape"(%arg6, %104) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %106 = ttir.empty() : tensor<1x1x13x32xf32>
        %107 = "ttir.broadcast"(%105, %106) <{broadcast_dimensions = array<i64: 1, 1, 13, 32>}> : (tensor<1x1x1x1xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %108 = ttir.empty() : tensor<1x1x13x32xf32>
        %109 = "ttir.multiply"(%103, %107, %108) : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %110 = ttir.empty() : tensor<1x1x13x32xbf16>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %112 = ttir.empty() : tensor<1x1x13x32xf32>
        %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %114 = ttir.empty() : tensor<1x13x32xf32>
        %115 = "ttir.reshape"(%113, %114) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %116 = ttir.empty() : tensor<64x13x32xf32>
        %117 = "ttir.broadcast"(%115, %116) <{broadcast_dimensions = array<i64: 64, 1, 1>}> : (tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %118 = ttir.empty() : tensor<64x13x32xf32>
        %119 = "ttir.multiply"(%93, %117, %118) : (tensor<64x13x32xf32>, tensor<64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %120 = ttir.empty() : tensor<64x13x32xbf16>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %122 = ttir.empty() : tensor<1x64x13x32xbf16>
        %123 = "ttir.slice_static"(%87, %122) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %124 = ttir.empty() : tensor<1x64x13x32xf32>
        %125 = "ttir.typecast"(%123, %124) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %126 = ttir.empty() : tensor<64x13x32xf32>
        %127 = "ttir.reshape"(%125, %126) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %128 = ttir.empty() : tensor<1x13x32xf32>
        %129 = "ttir.sin"(%99, %128) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.reshape"(%129, %130) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x1x13x32xf32>
        %133 = "ttir.multiply"(%131, %107, %132) : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %134 = ttir.empty() : tensor<1x1x13x32xbf16>
        %135 = "ttir.typecast"(%133, %134) <{conservative_folding = false}> : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %136 = ttir.empty() : tensor<1x1x13x32xf32>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %138 = ttir.empty() : tensor<1x13x32xf32>
        %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %140 = ttir.empty() : tensor<64x13x32xf32>
        %141 = "ttir.broadcast"(%139, %140) <{broadcast_dimensions = array<i64: 64, 1, 1>}> : (tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %142 = ttir.empty() : tensor<64x13x32xf32>
        %143 = "ttir.multiply"(%127, %141, %142) : (tensor<64x13x32xf32>, tensor<64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %144 = ttir.empty() : tensor<64x13x32xbf16>
        %145 = "ttir.typecast"(%143, %144) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %146 = ttir.empty() : tensor<64x13x32xbf16>
        %147 = "ttir.subtract"(%121, %145, %146) : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %148 = ttir.empty() : tensor<64x13x32xf32>
        %149 = "ttir.multiply"(%127, %117, %148) : (tensor<64x13x32xf32>, tensor<64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %150 = ttir.empty() : tensor<64x13x32xbf16>
        %151 = "ttir.typecast"(%149, %150) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %152 = ttir.empty() : tensor<64x13x32xf32>
        %153 = "ttir.multiply"(%93, %141, %152) : (tensor<64x13x32xf32>, tensor<64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %154 = ttir.empty() : tensor<64x13x32xbf16>
        %155 = "ttir.typecast"(%153, %154) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %156 = ttir.empty() : tensor<64x13x32xbf16>
        %157 = "ttir.add"(%151, %155, %156) : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %158 = ttir.empty() : tensor<64x13x64xbf16>
        %159 = "ttir.concat"(%147, %157, %158) <{dim = 2 : si32}> : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %160 = ttir.empty() : tensor<13x512xbf16>
        %161 = "ttir.matmul"(%73, %arg9, %160) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %162 = ttir.empty() : tensor<1x1x8x64xbf16>
        %163 = "ttir.reshape"(%arg8, %162) <{shape = [1 : i32, 1 : i32, 8 : i32, 64 : i32]}> : (tensor<512xbf16>, tensor<1x1x8x64xbf16>) -> tensor<1x1x8x64xbf16>
        %164 = ttir.empty() : tensor<1x8x1x64xbf16>
        %165 = "ttir.permute"(%163, %164) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x8x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
        %166 = ttir.empty() : tensor<1x8x13x64xbf16>
        %167 = "ttir.broadcast"(%165, %166) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x8x1x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %168 = ttir.empty() : tensor<1x13x8x64xbf16>
        %169 = "ttir.reshape"(%161, %168) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %170 = ttir.empty() : tensor<1x8x13x64xbf16>
        %171 = "ttir.permute"(%169, %170) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %172 = ttir.empty() : tensor<1x8x13x64xbf16>
        %173 = "ttir.add"(%171, %167, %172) : (tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %174 = ttir.empty() : tensor<1x8x13x32xbf16>
        %175 = "ttir.slice_static"(%173, %174) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %176 = ttir.empty() : tensor<1x8x13x32xf32>
        %177 = "ttir.typecast"(%175, %176) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %178 = ttir.empty() : tensor<1x8x13x32xf32>
        %179 = "ttir.broadcast"(%113, %178) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %180 = ttir.empty() : tensor<1x8x13x32xf32>
        %181 = "ttir.multiply"(%177, %179, %180) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %182 = ttir.empty() : tensor<1x8x13x32xbf16>
        %183 = "ttir.typecast"(%181, %182) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %184 = ttir.empty() : tensor<1x8x13x32xbf16>
        %185 = "ttir.slice_static"(%173, %184) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %186 = ttir.empty() : tensor<1x8x13x32xf32>
        %187 = "ttir.typecast"(%185, %186) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %188 = ttir.empty() : tensor<1x8x13x32xf32>
        %189 = "ttir.broadcast"(%137, %188) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %190 = ttir.empty() : tensor<1x8x13x32xf32>
        %191 = "ttir.multiply"(%187, %189, %190) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %192 = ttir.empty() : tensor<1x8x13x32xbf16>
        %193 = "ttir.typecast"(%191, %192) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %194 = ttir.empty() : tensor<1x8x13x32xbf16>
        %195 = "ttir.subtract"(%183, %193, %194) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %196 = ttir.empty() : tensor<1x8x13x32xf32>
        %197 = "ttir.multiply"(%187, %179, %196) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %198 = ttir.empty() : tensor<1x8x13x32xbf16>
        %199 = "ttir.typecast"(%197, %198) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %200 = ttir.empty() : tensor<1x8x13x32xf32>
        %201 = "ttir.multiply"(%177, %189, %200) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %202 = ttir.empty() : tensor<1x8x13x32xbf16>
        %203 = "ttir.typecast"(%201, %202) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %204 = ttir.empty() : tensor<1x8x13x32xbf16>
        %205 = "ttir.add"(%199, %203, %204) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %206 = ttir.empty() : tensor<1x8x13x64xbf16>
        %207 = "ttir.concat"(%195, %205, %206) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %208 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %209 = "ttir.reshape"(%207, %208) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %210 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %211 = "ttir.broadcast"(%209, %210) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %212 = ttir.empty() : tensor<1x64x13x64xbf16>
        %213 = "ttir.reshape"(%211, %212) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %214 = ttir.empty() : tensor<1x64x64x13xbf16>
        %215 = "ttir.permute"(%213, %214) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %216 = ttir.empty() : tensor<64x64x13xbf16>
        %217 = "ttir.reshape"(%215, %216) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %218 = ttir.empty() : tensor<64x13x13xbf16>
        %219 = "ttir.matmul"(%159, %217, %218) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %220 = ttir.empty() : tensor<64x13x13xf32>
        %221 = "ttir.typecast"(%219, %220) <{conservative_folding = false}> : (tensor<64x13x13xbf16>, tensor<64x13x13xf32>) -> tensor<64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.reshape"(%221, %222) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x1x1x1xf32>
        %225 = "ttir.reshape"(%arg18, %224) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %226 = ttir.empty() : tensor<1x64x13x13xf32>
        %227 = "ttir.broadcast"(%225, %226) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %228 = ttir.empty() : tensor<1x64x13x13xf32>
        %229 = "ttir.multiply"(%223, %227, %228) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %230 = ttir.empty() : tensor<1x64x13x13xbf16>
        %231 = "ttir.typecast"(%229, %230) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %232 = ttir.empty() : tensor<1x1x1x13xsi32>
        %233 = "ttir.reshape"(%1, %232) <{shape = [1 : i32, 1 : i32, 1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x1x1x13xsi32>) -> tensor<1x1x1x13xsi32>
        %234 = ttir.empty() : tensor<1x1x13x13xsi32>
        %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x1x1x13xsi32>, tensor<1x1x13x13xsi32>) -> tensor<1x1x13x13xsi32>
        %236 = ttir.empty() : tensor<1x1x1x1xsi32>
        %237 = "ttir.reshape"(%arg17, %236) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<si32>, tensor<1x1x1x1xsi32>) -> tensor<1x1x1x1xsi32>
        %238 = ttir.empty() : tensor<1x1x13x1xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x1x1x1xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %240 = ttir.empty() : tensor<1x1x13x1xsi32>
        %241 = "ttir.reshape"(%1, %240) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %242 = ttir.empty() : tensor<1x1x13x1xsi32>
        %243 = "ttir.subtract"(%241, %239, %242) : (tensor<1x1x13x1xsi32>, tensor<1x1x13x1xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %244 = ttir.empty() : tensor<1x1x13x13xsi32>
        %245 = "ttir.broadcast"(%243, %244) <{broadcast_dimensions = array<i64: 1, 1, 1, 13>}> : (tensor<1x1x13x1xsi32>, tensor<1x1x13x13xsi32>) -> tensor<1x1x13x13xsi32>
        %246 = ttir.empty() : tensor<1x1x13x13xbf16>
        %247 = "ttir.gt"(%235, %245, %246) : (tensor<1x1x13x13xsi32>, tensor<1x1x13x13xsi32>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %248 = ttir.empty() : tensor<1x1x13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %250 = ttir.empty() : tensor<1x1x13x13xui8>
        %251 = "ttir.bitwise_and"(%249, %23, %250) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %252 = ttir.empty() : tensor<1x1x13x13xbf16>
        %253 = "ttir.ne"(%251, %31, %252) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %254 = ttir.empty() : tensor<1x1x13x13xui8>
        %255 = "ttir.typecast"(%253, %254) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %256 = ttir.empty() : tensor<1x1x13x1xsi32>
        %257 = "ttir.reshape"(%1, %256) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %258 = ttir.empty() : tensor<1x1x13x13xsi32>
        %259 = "ttir.broadcast"(%257, %258) <{broadcast_dimensions = array<i64: 1, 1, 1, 13>}> : (tensor<1x1x13x1xsi32>, tensor<1x1x13x13xsi32>) -> tensor<1x1x13x13xsi32>
        %260 = ttir.empty() : tensor<1x1x13x13xbf16>
        %261 = "ttir.le"(%235, %259, %260) : (tensor<1x1x13x13xsi32>, tensor<1x1x13x13xsi32>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xui8>
        %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %264 = ttir.empty() : tensor<1x1x13x13xui8>
        %265 = "ttir.bitwise_and"(%255, %263, %264) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.ne"(%265, %31, %266) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x1x1xbf16>
        %269 = "ttir.reshape"(%arg16, %268) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %270 = ttir.empty() : tensor<1x1x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %272 = ttir.empty() : tensor<1x1x13x13xbf16>
        %273 = "ttir.where"(%267, %19, %271, %272) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x13x13xbf16>
        %275 = "ttir.broadcast"(%273, %274) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %276 = ttir.empty() : tensor<1x64x13x13xbf16>
        %277 = "ttir.add"(%231, %275, %276) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %278 = ttir.empty() : tensor<1x64x1x1xbf16>
        %279 = "ttir.reshape"(%arg15, %278) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %280 = ttir.empty() : tensor<1x64x13x1xbf16>
        %281 = "ttir.broadcast"(%279, %280) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %282 = ttir.empty() : tensor<1x64x13x14xbf16>
        %283 = "ttir.concat"(%277, %281, %282) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %284 = ttir.empty() : tensor<13x512xbf16>
        %285 = "ttir.matmul"(%73, %arg1, %284) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %286 = ttir.empty() : tensor<1x13x512xbf16>
        %287 = "ttir.reshape"(%285, %286) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %288 = ttir.empty() : tensor<1x1x512xbf16>
        %289 = "ttir.reshape"(%arg0, %288) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %290 = ttir.empty() : tensor<1x13x512xbf16>
        %291 = "ttir.broadcast"(%289, %290) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %292 = ttir.empty() : tensor<1x13x512xbf16>
        %293 = "ttir.add"(%287, %291, %292) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %294 = ttir.empty() : tensor<1x13x8x64xbf16>
        %295 = "ttir.reshape"(%293, %294) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %296 = ttir.empty() : tensor<1x8x13x64xbf16>
        %297 = "ttir.permute"(%295, %296) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %298 = ttir.empty() : tensor<2880xf32>
        %299 = "ttir.typecast"(%arg30, %298) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %300 = ttir.empty() : tensor<1x2880xf32>
        %301 = "ttir.reshape"(%299, %300) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %302 = ttir.empty() : tensor<13x2880xf32>
        %303 = "ttir.broadcast"(%301, %302) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %304 = ttir.empty() : tensor<1x64x13x14xbf16>
        %305 = "ttir.softmax"(%283, %304) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %306 = ttir.empty() : tensor<1x64x13x13xbf16>
        %307 = "ttir.slice_static"(%305, %306) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %308 = ttir.empty() : tensor<64x13x13xbf16>
        %309 = "ttir.reshape"(%307, %308) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %310 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %311 = "ttir.reshape"(%297, %310) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %312 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %313 = "ttir.broadcast"(%311, %312) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %314 = ttir.empty() : tensor<64x13x64xbf16>
        %315 = "ttir.reshape"(%313, %314) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %316 = ttir.empty() : tensor<64x13x64xbf16>
        %317 = "ttir.matmul"(%309, %315, %316) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %318 = ttir.empty() : tensor<1x64x13x64xbf16>
        %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %320 = ttir.empty() : tensor<13x4096xbf16>
        %321 = ttir.empty() : tensor<1x13x4096xbf16>
        %322 = "ttir.concatenate_heads"(%319, %321) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %323 = "ttir.reshape"(%322, %320) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %324 = ttir.empty() : tensor<13x2880xbf16>
        %325 = "ttir.matmul"(%323, %arg14, %324) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16>, tensor<2880x4096xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %326 = ttir.empty() : tensor<1x13x2880xbf16>
        %327 = "ttir.reshape"(%325, %326) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %328 = ttir.empty() : tensor<1x1x2880xbf16>
        %329 = "ttir.reshape"(%arg13, %328) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %330 = ttir.empty() : tensor<1x13x2880xbf16>
        %331 = "ttir.broadcast"(%329, %330) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %332 = ttir.empty() : tensor<1x13x2880xbf16>
        %333 = "ttir.add"(%327, %331, %332) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %334 = ttir.empty() : tensor<1x13x2880xbf16>
        %335 = "ttir.add"(%45, %333, %334) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %336 = ttir.empty() : tensor<1x1x1xbf16>
        %337 = "ttir.reshape"(%arg29, %336) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %338 = ttir.empty() : tensor<32x13x2880xbf16>
        %339 = "ttir.broadcast"(%337, %338) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %340 = ttir.empty() : tensor<2880xf32>
        %341 = "ttir.typecast"(%arg21, %340) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %342 = ttir.empty() : tensor<1x2880xf32>
        %343 = "ttir.reshape"(%341, %342) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %344 = ttir.empty() : tensor<13x2880xf32>
        %345 = "ttir.broadcast"(%343, %344) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %346 = ttir.empty() : tensor<1x13x2880xf32>
        %347 = "ttir.typecast"(%335, %346) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %348 = ttir.empty() : tensor<1x13x2880xf32>
        %349 = "ttir.pow"(%347, %27, %348) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %350 = ttir.empty() : tensor<1x13xf32>
        %351 = "ttir.sum"(%349, %350) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %352 = ttir.empty() : tensor<1x13xf32>
        %353 = "ttir.multiply"(%351, %4, %352) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %354 = ttir.empty() : tensor<13x1xf32>
        %355 = "ttir.reshape"(%353, %354) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %356 = ttir.empty() : tensor<13x1xf32>
        %357 = "ttir.add"(%355, %59, %356) : (tensor<13x1xf32>, tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %358 = ttir.empty() : tensor<13x1xf32>
        %359 = "ttir.rsqrt"(%357, %358) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %360 = ttir.empty() : tensor<13x2880xf32>
        %361 = "ttir.broadcast"(%359, %360) <{broadcast_dimensions = array<i64: 1, 2880>}> : (tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %362 = ttir.empty() : tensor<13x2880xf32>
        %363 = "ttir.reshape"(%347, %362) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %364 = ttir.empty() : tensor<13x2880xf32>
        %365 = "ttir.multiply"(%363, %361, %364) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %366 = ttir.empty() : tensor<13x2880xf32>
        %367 = "ttir.multiply"(%345, %365, %366) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %368 = ttir.empty() : tensor<13x2880xbf16>
        %369 = "ttir.typecast"(%367, %368) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %370 = ttir.empty() : tensor<416x2880xbf16>
        %371 = "ttir.concat"(%369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %370) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %372 = ttir.empty() : tensor<32x13x2880xbf16>
        %373 = "ttir.reshape"(%371, %372) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %374 = ttir.empty() : tensor<32x13x5760xbf16>
        %375 = "ttir.matmul"(%373, %arg28, %374) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %376 = ttir.empty() : tensor<32x1x5760xbf16>
        %377 = "ttir.reshape"(%arg27, %376) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %378 = ttir.empty() : tensor<32x13x5760xbf16>
        %379 = "ttir.broadcast"(%377, %378) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %380 = ttir.empty() : tensor<32x13x5760xbf16>
        %381 = "ttir.add"(%375, %379, %380) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %382 = ttir.empty() : tensor<32x13x2880xbf16>
        %383 = "ttir.slice_static"(%381, %382) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %384 = ttir.empty() : tensor<1x1x1xbf16>
        %385 = "ttir.reshape"(%arg25, %384) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.broadcast"(%385, %386) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<32x13x2880xbf16>
        %389 = "ttir.clamp_tensor"(%383, %339, %387, %388) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %390 = ttir.empty() : tensor<32x13x2880xbf16>
        %391 = "ttir.add"(%389, %15, %390) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xf32>
        %393 = "ttir.typecast"(%391, %392) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %394 = ttir.empty() : tensor<1x1x1xbf16>
        %395 = "ttir.reshape"(%arg26, %394) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xbf16>
        %397 = "ttir.broadcast"(%395, %396) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %398 = ttir.empty() : tensor<32x13x2880xbf16>
        %399 = "ttir.slice_static"(%381, %398) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %400 = ttir.empty() : tensor<32x13x2880xbf16>
        %401 = "ttir.clamp_tensor"(%399, %397, %387, %400) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %402 = ttir.empty() : tensor<32x13x2880xf32>
        %403 = "ttir.typecast"(%401, %402) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %404 = ttir.empty() : tensor<1x1x1xf32>
        %405 = "ttir.reshape"(%arg24, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %406 = ttir.empty() : tensor<32x13x2880xf32>
        %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %408 = ttir.empty() : tensor<32x13x2880xf32>
        %409 = "ttir.multiply"(%403, %407, %408) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %410 = ttir.empty() : tensor<32x13x2880xbf16>
        %411 = "ttir.typecast"(%409, %410) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %412 = ttir.empty() : tensor<32x13x2880xbf16>
        %413 = "ttir.sigmoid"(%411, %412) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %414 = ttir.empty() : tensor<32x13x2880xf32>
        %415 = "ttir.typecast"(%413, %414) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %416 = ttir.empty() : tensor<32x13x2880xf32>
        %417 = "ttir.multiply"(%403, %415, %416) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %418 = ttir.empty() : tensor<32x13x2880xf32>
        %419 = "ttir.multiply"(%393, %417, %418) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %420 = ttir.empty() : tensor<32x13x2880xbf16>
        %421 = "ttir.typecast"(%419, %420) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xbf16>
        %423 = "ttir.matmul"(%421, %arg23, %422) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %424 = ttir.empty() : tensor<32x1x1x2880xbf16>
        %425 = "ttir.reshape"(%arg22, %424) <{shape = [32 : i32, 1 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x1x2880xbf16>) -> tensor<32x1x1x2880xbf16>
        %426 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<32x1x1x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %428 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %429 = "ttir.reshape"(%423, %428) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %430 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %431 = "ttir.add"(%429, %427, %430) : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %432 = ttir.empty() : tensor<32x1x13x2880xf32>
        %433 = "ttir.typecast"(%431, %432) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %434 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %435 = ttir.empty() : tensor<13x1x1xsi32>
        %436 = "ttir.reshape"(%434, %435) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %437 = ttir.empty() : tensor<13x4x1xsi32>
        %438 = "ttir.broadcast"(%436, %437) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %439 = ttir.empty() : tensor<13x32xbf16>
        %440 = "ttir.matmul"(%369, %arg12, %439) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<32x2880xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %441 = ttir.empty() : tensor<1x32xbf16>
        %442 = "ttir.reshape"(%arg11, %441) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %443 = ttir.empty() : tensor<13x32xbf16>
        %444 = "ttir.broadcast"(%442, %443) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %445 = ttir.empty() : tensor<13x32xbf16>
        %446 = "ttir.add"(%440, %444, %445) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %447 = ttir.empty() : tensor<13x32xbf16>
        %448 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%446, %447, %448) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %449 = ttir.empty() : tensor<13x4xsi32>
        %450 = "ttir.slice_static"(%indices, %449) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %451 = ttir.empty() : tensor<13x4x1xsi32>
        %452 = "ttir.reshape"(%450, %451) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %453 = ttir.empty() : tensor<13x4x2xsi32>
        %454 = "ttir.concat"(%438, %452, %453) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %455 = ttir.empty() : tensor<13x4xbf16>
        %456 = "ttir.slice_static"(%values, %455) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %457 = ttir.empty() : tensor<13x4xbf16>
        %458 = "ttir.softmax"(%456, %457) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %459 = ttir.empty() : tensor<13x32xbf16>
        %460 = "ttir.scatter"(%11, %454, %458, %459) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %461 = ttir.empty() : tensor<13x32xf32>
        %462 = "ttir.typecast"(%460, %461) <{conservative_folding = false}> : (tensor<13x32xbf16>, tensor<13x32xf32>) -> tensor<13x32xf32>
        %463 = ttir.empty() : tensor<32x13xf32>
        %464 = "ttir.permute"(%462, %463) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xf32>, tensor<32x13xf32>) -> tensor<32x13xf32>
        %465 = ttir.empty() : tensor<32x1x13x1xf32>
        %466 = "ttir.reshape"(%464, %465) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xf32>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %467 = ttir.empty() : tensor<32x1x13x2880xf32>
        %468 = "ttir.broadcast"(%466, %467) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %469 = ttir.empty() : tensor<32x1x13x2880xf32>
        %470 = "ttir.multiply"(%433, %468, %469) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %471 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %472 = "ttir.typecast"(%470, %471) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %473 = ttir.empty() : tensor<1x13x2880xbf16>
        %474 = "ttir.sum"(%472, %473) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %475 = ttir.empty() : tensor<1x13x2880xbf16>
        %476 = "ttir.add"(%335, %474, %475) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %477 = ttir.empty() : tensor<1x13x2880xf32>
        %478 = "ttir.typecast"(%476, %477) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %479 = ttir.empty() : tensor<1x13x2880xf32>
        %480 = "ttir.pow"(%478, %27, %479) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %481 = ttir.empty() : tensor<1x13xf32>
        %482 = "ttir.sum"(%480, %481) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %483 = ttir.empty() : tensor<1x13xf32>
        %484 = "ttir.multiply"(%482, %4, %483) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %485 = ttir.empty() : tensor<13x1xf32>
        %486 = "ttir.reshape"(%484, %485) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %487 = ttir.empty() : tensor<13x1xf32>
        %488 = "ttir.add"(%486, %59, %487) : (tensor<13x1xf32>, tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %489 = ttir.empty() : tensor<13x1xf32>
        %490 = "ttir.rsqrt"(%488, %489) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %491 = ttir.empty() : tensor<13x2880xf32>
        %492 = "ttir.broadcast"(%490, %491) <{broadcast_dimensions = array<i64: 1, 2880>}> : (tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %493 = ttir.empty() : tensor<13x2880xf32>
        %494 = "ttir.reshape"(%478, %493) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %495 = ttir.empty() : tensor<13x2880xf32>
        %496 = "ttir.multiply"(%494, %492, %495) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %497 = ttir.empty() : tensor<13x2880xf32>
        %498 = "ttir.multiply"(%303, %496, %497) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %499 = ttir.empty() : tensor<13x2880xbf16>
        %500 = "ttir.typecast"(%498, %499) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %501 = ttir.empty() : tensor<13x201088xbf16>
        %502 = "ttir.matmul"(%500, %arg10, %501) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<201088x2880xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %503 = ttir.empty() : tensor<1x13x201088xbf16>
        %504 = "ttir.reshape"(%502, %503) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %293, %295, %297, %207, %502, %504 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFusing (ttir-fusing) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1x1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1x1x1xui8>) -> tensor<1x1x1x1xui8>
        %22 = ttir.empty() : tensor<1x1x13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1x1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1x1x1xui8>) -> tensor<1x1x1x1xui8>
        %30 = ttir.empty() : tensor<1x1x13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %36 = ttir.empty() : tensor<13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %38 = ttir.empty() : tensor<1x13xui32>
        %39 = "ttir.typecast"(%arg3, %38) <{conservative_folding = false}> : (tensor<1x13xsi32>, tensor<1x13xui32>) -> tensor<1x13xui32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.reshape"(%39, %40) <{shape = [13 : i32]}> : (tensor<1x13xui32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.embedding"(%41, %arg4, %42) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %56 = ttir.empty() : tensor<1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %58 = ttir.empty() : tensor<13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %60 = ttir.empty() : tensor<13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<13x1xf32>, tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %62 = ttir.empty() : tensor<13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %64 = ttir.empty() : tensor<13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 2880>}> : (tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %66 = ttir.empty() : tensor<13x2880xf32>
        %67 = "ttir.reshape"(%47, %66) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %68 = ttir.empty() : tensor<13x2880xf32>
        %69 = "ttir.multiply"(%67, %65, %68) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %70 = ttir.empty() : tensor<13x2880xf32>
        %71 = "ttir.multiply"(%37, %69, %70) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.typecast"(%71, %72) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<13x4096xbf16>
        %75 = "ttir.matmul"(%73, %arg20, %74) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<4096x2880xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %76 = ttir.empty() : tensor<1x1x64x64xbf16>
        %77 = "ttir.reshape"(%arg19, %76) <{shape = [1 : i32, 1 : i32, 64 : i32, 64 : i32]}> : (tensor<4096xbf16>, tensor<1x1x64x64xbf16>) -> tensor<1x1x64x64xbf16>
        %78 = ttir.empty() : tensor<1x64x1x64xbf16>
        %79 = "ttir.permute"(%77, %78) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x64x64xbf16>, tensor<1x64x1x64xbf16>) -> tensor<1x64x1x64xbf16>
        %80 = ttir.empty() : tensor<1x64x13x64xbf16>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %82 = ttir.empty() : tensor<1x13x64x64xbf16>
        %83 = "ttir.reshape"(%75, %82) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %84 = ttir.empty() : tensor<1x64x13x64xbf16>
        %85 = "ttir.permute"(%83, %84) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %86 = ttir.empty() : tensor<1x64x13x64xbf16>
        %87 = "ttir.add"(%85, %81, %86) : (tensor<1x64x13x64xbf16>, tensor<1x64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %88 = ttir.empty() : tensor<1x64x13x32xbf16>
        %89 = "ttir.slice_static"(%87, %88) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %90 = ttir.empty() : tensor<1x64x13x32xf32>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %92 = ttir.empty() : tensor<64x13x32xf32>
        %93 = "ttir.reshape"(%91, %92) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %94 = ttir.empty() : tensor<1x32x1xf32>
        %95 = "ttir.reshape"(%arg7, %94) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %96 = ttir.empty() : tensor<1x32x13xf32>
        %97 = "ttir.matmul"(%95, %0, %96) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.permute"(%97, %98) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x13x32xf32>
        %101 = "ttir.cos"(%99, %100) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %102 = ttir.empty() : tensor<1x1x13x32xf32>
        %103 = "ttir.reshape"(%101, %102) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %104 = ttir.empty() : tensor<1x1x1x1xf32>
        %105 = "ttir.reshape"(%arg6, %104) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %106 = ttir.empty() : tensor<1x1x13x32xf32>
        %107 = "ttir.broadcast"(%105, %106) <{broadcast_dimensions = array<i64: 1, 1, 13, 32>}> : (tensor<1x1x1x1xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %108 = ttir.empty() : tensor<1x1x13x32xf32>
        %109 = "ttir.multiply"(%103, %107, %108) : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %110 = ttir.empty() : tensor<1x1x13x32xbf16>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %112 = ttir.empty() : tensor<1x1x13x32xf32>
        %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %114 = ttir.empty() : tensor<1x13x32xf32>
        %115 = "ttir.reshape"(%113, %114) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %116 = ttir.empty() : tensor<64x13x32xf32>
        %117 = "ttir.broadcast"(%115, %116) <{broadcast_dimensions = array<i64: 64, 1, 1>}> : (tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %118 = ttir.empty() : tensor<64x13x32xf32>
        %119 = "ttir.multiply"(%93, %117, %118) : (tensor<64x13x32xf32>, tensor<64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %120 = ttir.empty() : tensor<64x13x32xbf16>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %122 = ttir.empty() : tensor<1x64x13x32xbf16>
        %123 = "ttir.slice_static"(%87, %122) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %124 = ttir.empty() : tensor<1x64x13x32xf32>
        %125 = "ttir.typecast"(%123, %124) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %126 = ttir.empty() : tensor<64x13x32xf32>
        %127 = "ttir.reshape"(%125, %126) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %128 = ttir.empty() : tensor<1x13x32xf32>
        %129 = "ttir.sin"(%99, %128) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.reshape"(%129, %130) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x1x13x32xf32>
        %133 = "ttir.multiply"(%131, %107, %132) : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %134 = ttir.empty() : tensor<1x1x13x32xbf16>
        %135 = "ttir.typecast"(%133, %134) <{conservative_folding = false}> : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %136 = ttir.empty() : tensor<1x1x13x32xf32>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %138 = ttir.empty() : tensor<1x13x32xf32>
        %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %140 = ttir.empty() : tensor<64x13x32xf32>
        %141 = "ttir.broadcast"(%139, %140) <{broadcast_dimensions = array<i64: 64, 1, 1>}> : (tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %142 = ttir.empty() : tensor<64x13x32xf32>
        %143 = "ttir.multiply"(%127, %141, %142) : (tensor<64x13x32xf32>, tensor<64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %144 = ttir.empty() : tensor<64x13x32xbf16>
        %145 = "ttir.typecast"(%143, %144) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %146 = ttir.empty() : tensor<64x13x32xbf16>
        %147 = "ttir.subtract"(%121, %145, %146) : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %148 = ttir.empty() : tensor<64x13x32xf32>
        %149 = "ttir.multiply"(%127, %117, %148) : (tensor<64x13x32xf32>, tensor<64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %150 = ttir.empty() : tensor<64x13x32xbf16>
        %151 = "ttir.typecast"(%149, %150) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %152 = ttir.empty() : tensor<64x13x32xf32>
        %153 = "ttir.multiply"(%93, %141, %152) : (tensor<64x13x32xf32>, tensor<64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %154 = ttir.empty() : tensor<64x13x32xbf16>
        %155 = "ttir.typecast"(%153, %154) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %156 = ttir.empty() : tensor<64x13x32xbf16>
        %157 = "ttir.add"(%151, %155, %156) : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %158 = ttir.empty() : tensor<64x13x64xbf16>
        %159 = "ttir.concat"(%147, %157, %158) <{dim = 2 : si32}> : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %160 = ttir.empty() : tensor<13x512xbf16>
        %161 = "ttir.matmul"(%73, %arg9, %160) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %162 = ttir.empty() : tensor<1x1x8x64xbf16>
        %163 = "ttir.reshape"(%arg8, %162) <{shape = [1 : i32, 1 : i32, 8 : i32, 64 : i32]}> : (tensor<512xbf16>, tensor<1x1x8x64xbf16>) -> tensor<1x1x8x64xbf16>
        %164 = ttir.empty() : tensor<1x8x1x64xbf16>
        %165 = "ttir.permute"(%163, %164) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x8x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
        %166 = ttir.empty() : tensor<1x8x13x64xbf16>
        %167 = "ttir.broadcast"(%165, %166) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x8x1x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %168 = ttir.empty() : tensor<1x13x8x64xbf16>
        %169 = "ttir.reshape"(%161, %168) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %170 = ttir.empty() : tensor<1x8x13x64xbf16>
        %171 = "ttir.permute"(%169, %170) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %172 = ttir.empty() : tensor<1x8x13x64xbf16>
        %173 = "ttir.add"(%171, %167, %172) : (tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %174 = ttir.empty() : tensor<1x8x13x32xbf16>
        %175 = "ttir.slice_static"(%173, %174) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %176 = ttir.empty() : tensor<1x8x13x32xf32>
        %177 = "ttir.typecast"(%175, %176) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %178 = ttir.empty() : tensor<1x8x13x32xf32>
        %179 = "ttir.broadcast"(%113, %178) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %180 = ttir.empty() : tensor<1x8x13x32xf32>
        %181 = "ttir.multiply"(%177, %179, %180) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %182 = ttir.empty() : tensor<1x8x13x32xbf16>
        %183 = "ttir.typecast"(%181, %182) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %184 = ttir.empty() : tensor<1x8x13x32xbf16>
        %185 = "ttir.slice_static"(%173, %184) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %186 = ttir.empty() : tensor<1x8x13x32xf32>
        %187 = "ttir.typecast"(%185, %186) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %188 = ttir.empty() : tensor<1x8x13x32xf32>
        %189 = "ttir.broadcast"(%137, %188) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %190 = ttir.empty() : tensor<1x8x13x32xf32>
        %191 = "ttir.multiply"(%187, %189, %190) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %192 = ttir.empty() : tensor<1x8x13x32xbf16>
        %193 = "ttir.typecast"(%191, %192) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %194 = ttir.empty() : tensor<1x8x13x32xbf16>
        %195 = "ttir.subtract"(%183, %193, %194) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %196 = ttir.empty() : tensor<1x8x13x32xf32>
        %197 = "ttir.multiply"(%187, %179, %196) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %198 = ttir.empty() : tensor<1x8x13x32xbf16>
        %199 = "ttir.typecast"(%197, %198) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %200 = ttir.empty() : tensor<1x8x13x32xf32>
        %201 = "ttir.multiply"(%177, %189, %200) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %202 = ttir.empty() : tensor<1x8x13x32xbf16>
        %203 = "ttir.typecast"(%201, %202) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %204 = ttir.empty() : tensor<1x8x13x32xbf16>
        %205 = "ttir.add"(%199, %203, %204) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %206 = ttir.empty() : tensor<1x8x13x64xbf16>
        %207 = "ttir.concat"(%195, %205, %206) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %208 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %209 = "ttir.reshape"(%207, %208) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %210 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %211 = "ttir.broadcast"(%209, %210) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %212 = ttir.empty() : tensor<1x64x13x64xbf16>
        %213 = "ttir.reshape"(%211, %212) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %214 = ttir.empty() : tensor<1x64x64x13xbf16>
        %215 = "ttir.permute"(%213, %214) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %216 = ttir.empty() : tensor<64x64x13xbf16>
        %217 = "ttir.reshape"(%215, %216) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %218 = ttir.empty() : tensor<64x13x13xbf16>
        %219 = "ttir.matmul"(%159, %217, %218) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %220 = ttir.empty() : tensor<64x13x13xf32>
        %221 = "ttir.typecast"(%219, %220) <{conservative_folding = false}> : (tensor<64x13x13xbf16>, tensor<64x13x13xf32>) -> tensor<64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.reshape"(%221, %222) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x1x1x1xf32>
        %225 = "ttir.reshape"(%arg18, %224) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %226 = ttir.empty() : tensor<1x64x13x13xf32>
        %227 = "ttir.broadcast"(%225, %226) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %228 = ttir.empty() : tensor<1x64x13x13xf32>
        %229 = "ttir.multiply"(%223, %227, %228) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %230 = ttir.empty() : tensor<1x64x13x13xbf16>
        %231 = "ttir.typecast"(%229, %230) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %232 = ttir.empty() : tensor<1x1x1x13xsi32>
        %233 = "ttir.reshape"(%1, %232) <{shape = [1 : i32, 1 : i32, 1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x1x1x13xsi32>) -> tensor<1x1x1x13xsi32>
        %234 = ttir.empty() : tensor<1x1x13x13xsi32>
        %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x1x1x13xsi32>, tensor<1x1x13x13xsi32>) -> tensor<1x1x13x13xsi32>
        %236 = ttir.empty() : tensor<1x1x1x1xsi32>
        %237 = "ttir.reshape"(%arg17, %236) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<si32>, tensor<1x1x1x1xsi32>) -> tensor<1x1x1x1xsi32>
        %238 = ttir.empty() : tensor<1x1x13x1xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x1x1x1xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %240 = ttir.empty() : tensor<1x1x13x1xsi32>
        %241 = "ttir.reshape"(%1, %240) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %242 = ttir.empty() : tensor<1x1x13x1xsi32>
        %243 = "ttir.subtract"(%241, %239, %242) : (tensor<1x1x13x1xsi32>, tensor<1x1x13x1xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %244 = ttir.empty() : tensor<1x1x13x13xsi32>
        %245 = "ttir.broadcast"(%243, %244) <{broadcast_dimensions = array<i64: 1, 1, 1, 13>}> : (tensor<1x1x13x1xsi32>, tensor<1x1x13x13xsi32>) -> tensor<1x1x13x13xsi32>
        %246 = ttir.empty() : tensor<1x1x13x13xbf16>
        %247 = "ttir.gt"(%235, %245, %246) : (tensor<1x1x13x13xsi32>, tensor<1x1x13x13xsi32>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %248 = ttir.empty() : tensor<1x1x13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %250 = ttir.empty() : tensor<1x1x13x13xui8>
        %251 = "ttir.bitwise_and"(%249, %23, %250) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %252 = ttir.empty() : tensor<1x1x13x13xbf16>
        %253 = "ttir.ne"(%251, %31, %252) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %254 = ttir.empty() : tensor<1x1x13x13xui8>
        %255 = "ttir.typecast"(%253, %254) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %256 = ttir.empty() : tensor<1x1x13x1xsi32>
        %257 = "ttir.reshape"(%1, %256) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %258 = ttir.empty() : tensor<1x1x13x13xsi32>
        %259 = "ttir.broadcast"(%257, %258) <{broadcast_dimensions = array<i64: 1, 1, 1, 13>}> : (tensor<1x1x13x1xsi32>, tensor<1x1x13x13xsi32>) -> tensor<1x1x13x13xsi32>
        %260 = ttir.empty() : tensor<1x1x13x13xbf16>
        %261 = "ttir.le"(%235, %259, %260) : (tensor<1x1x13x13xsi32>, tensor<1x1x13x13xsi32>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xui8>
        %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %264 = ttir.empty() : tensor<1x1x13x13xui8>
        %265 = "ttir.bitwise_and"(%255, %263, %264) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.ne"(%265, %31, %266) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x1x1xbf16>
        %269 = "ttir.reshape"(%arg16, %268) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %270 = ttir.empty() : tensor<1x1x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %272 = ttir.empty() : tensor<1x1x13x13xbf16>
        %273 = "ttir.where"(%267, %19, %271, %272) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x13x13xbf16>
        %275 = "ttir.broadcast"(%273, %274) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %276 = ttir.empty() : tensor<1x64x13x13xbf16>
        %277 = "ttir.add"(%231, %275, %276) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %278 = ttir.empty() : tensor<1x64x1x1xbf16>
        %279 = "ttir.reshape"(%arg15, %278) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %280 = ttir.empty() : tensor<1x64x13x1xbf16>
        %281 = "ttir.broadcast"(%279, %280) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %282 = ttir.empty() : tensor<1x64x13x14xbf16>
        %283 = "ttir.concat"(%277, %281, %282) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %284 = ttir.empty() : tensor<13x512xbf16>
        %285 = "ttir.matmul"(%73, %arg1, %284) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %286 = ttir.empty() : tensor<1x13x512xbf16>
        %287 = "ttir.reshape"(%285, %286) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %288 = ttir.empty() : tensor<1x1x512xbf16>
        %289 = "ttir.reshape"(%arg0, %288) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %290 = ttir.empty() : tensor<1x13x512xbf16>
        %291 = "ttir.broadcast"(%289, %290) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %292 = ttir.empty() : tensor<1x13x512xbf16>
        %293 = "ttir.add"(%287, %291, %292) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %294 = ttir.empty() : tensor<1x13x8x64xbf16>
        %295 = "ttir.reshape"(%293, %294) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %296 = ttir.empty() : tensor<1x8x13x64xbf16>
        %297 = "ttir.permute"(%295, %296) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %298 = ttir.empty() : tensor<2880xf32>
        %299 = "ttir.typecast"(%arg30, %298) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %300 = ttir.empty() : tensor<1x2880xf32>
        %301 = "ttir.reshape"(%299, %300) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %302 = ttir.empty() : tensor<13x2880xf32>
        %303 = "ttir.broadcast"(%301, %302) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %304 = ttir.empty() : tensor<1x64x13x14xbf16>
        %305 = "ttir.softmax"(%283, %304) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %306 = ttir.empty() : tensor<1x64x13x13xbf16>
        %307 = "ttir.slice_static"(%305, %306) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %308 = ttir.empty() : tensor<64x13x13xbf16>
        %309 = "ttir.reshape"(%307, %308) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %310 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %311 = "ttir.reshape"(%297, %310) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %312 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %313 = "ttir.broadcast"(%311, %312) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %314 = ttir.empty() : tensor<64x13x64xbf16>
        %315 = "ttir.reshape"(%313, %314) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %316 = ttir.empty() : tensor<64x13x64xbf16>
        %317 = "ttir.matmul"(%309, %315, %316) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %318 = ttir.empty() : tensor<1x64x13x64xbf16>
        %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %320 = ttir.empty() : tensor<13x4096xbf16>
        %321 = ttir.empty() : tensor<1x13x4096xbf16>
        %322 = "ttir.concatenate_heads"(%319, %321) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %323 = "ttir.reshape"(%322, %320) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %324 = ttir.empty() : tensor<13x2880xbf16>
        %325 = "ttir.matmul"(%323, %arg14, %324) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16>, tensor<2880x4096xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %326 = ttir.empty() : tensor<1x13x2880xbf16>
        %327 = "ttir.reshape"(%325, %326) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %328 = ttir.empty() : tensor<1x1x2880xbf16>
        %329 = "ttir.reshape"(%arg13, %328) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %330 = ttir.empty() : tensor<1x13x2880xbf16>
        %331 = "ttir.broadcast"(%329, %330) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %332 = ttir.empty() : tensor<1x13x2880xbf16>
        %333 = "ttir.add"(%327, %331, %332) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %334 = ttir.empty() : tensor<1x13x2880xbf16>
        %335 = "ttir.add"(%45, %333, %334) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %336 = ttir.empty() : tensor<1x1x1xbf16>
        %337 = "ttir.reshape"(%arg29, %336) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %338 = ttir.empty() : tensor<32x13x2880xbf16>
        %339 = "ttir.broadcast"(%337, %338) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %340 = ttir.empty() : tensor<2880xf32>
        %341 = "ttir.typecast"(%arg21, %340) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %342 = ttir.empty() : tensor<1x2880xf32>
        %343 = "ttir.reshape"(%341, %342) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %344 = ttir.empty() : tensor<13x2880xf32>
        %345 = "ttir.broadcast"(%343, %344) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %346 = ttir.empty() : tensor<1x13x2880xf32>
        %347 = "ttir.typecast"(%335, %346) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %348 = ttir.empty() : tensor<1x13x2880xf32>
        %349 = "ttir.pow"(%347, %27, %348) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %350 = ttir.empty() : tensor<1x13xf32>
        %351 = "ttir.sum"(%349, %350) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %352 = ttir.empty() : tensor<1x13xf32>
        %353 = "ttir.multiply"(%351, %4, %352) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %354 = ttir.empty() : tensor<13x1xf32>
        %355 = "ttir.reshape"(%353, %354) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %356 = ttir.empty() : tensor<13x1xf32>
        %357 = "ttir.add"(%355, %59, %356) : (tensor<13x1xf32>, tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %358 = ttir.empty() : tensor<13x1xf32>
        %359 = "ttir.rsqrt"(%357, %358) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %360 = ttir.empty() : tensor<13x2880xf32>
        %361 = "ttir.broadcast"(%359, %360) <{broadcast_dimensions = array<i64: 1, 2880>}> : (tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %362 = ttir.empty() : tensor<13x2880xf32>
        %363 = "ttir.reshape"(%347, %362) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %364 = ttir.empty() : tensor<13x2880xf32>
        %365 = "ttir.multiply"(%363, %361, %364) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %366 = ttir.empty() : tensor<13x2880xf32>
        %367 = "ttir.multiply"(%345, %365, %366) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %368 = ttir.empty() : tensor<13x2880xbf16>
        %369 = "ttir.typecast"(%367, %368) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %370 = ttir.empty() : tensor<416x2880xbf16>
        %371 = "ttir.concat"(%369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %370) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %372 = ttir.empty() : tensor<32x13x2880xbf16>
        %373 = "ttir.reshape"(%371, %372) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %374 = ttir.empty() : tensor<32x13x5760xbf16>
        %375 = "ttir.matmul"(%373, %arg28, %374) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %376 = ttir.empty() : tensor<32x1x5760xbf16>
        %377 = "ttir.reshape"(%arg27, %376) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %378 = ttir.empty() : tensor<32x13x5760xbf16>
        %379 = "ttir.broadcast"(%377, %378) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %380 = ttir.empty() : tensor<32x13x5760xbf16>
        %381 = "ttir.add"(%375, %379, %380) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %382 = ttir.empty() : tensor<32x13x2880xbf16>
        %383 = "ttir.slice_static"(%381, %382) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %384 = ttir.empty() : tensor<1x1x1xbf16>
        %385 = "ttir.reshape"(%arg25, %384) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.broadcast"(%385, %386) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<32x13x2880xbf16>
        %389 = "ttir.clamp_tensor"(%383, %339, %387, %388) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %390 = ttir.empty() : tensor<32x13x2880xbf16>
        %391 = "ttir.add"(%389, %15, %390) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xf32>
        %393 = "ttir.typecast"(%391, %392) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %394 = ttir.empty() : tensor<1x1x1xbf16>
        %395 = "ttir.reshape"(%arg26, %394) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xbf16>
        %397 = "ttir.broadcast"(%395, %396) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %398 = ttir.empty() : tensor<32x13x2880xbf16>
        %399 = "ttir.slice_static"(%381, %398) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %400 = ttir.empty() : tensor<32x13x2880xbf16>
        %401 = "ttir.clamp_tensor"(%399, %397, %387, %400) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %402 = ttir.empty() : tensor<32x13x2880xf32>
        %403 = "ttir.typecast"(%401, %402) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %404 = ttir.empty() : tensor<1x1x1xf32>
        %405 = "ttir.reshape"(%arg24, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %406 = ttir.empty() : tensor<32x13x2880xf32>
        %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %408 = ttir.empty() : tensor<32x13x2880xf32>
        %409 = "ttir.multiply"(%403, %407, %408) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %410 = ttir.empty() : tensor<32x13x2880xbf16>
        %411 = "ttir.typecast"(%409, %410) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %412 = ttir.empty() : tensor<32x13x2880xbf16>
        %413 = "ttir.sigmoid"(%411, %412) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %414 = ttir.empty() : tensor<32x13x2880xf32>
        %415 = "ttir.typecast"(%413, %414) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %416 = ttir.empty() : tensor<32x13x2880xf32>
        %417 = "ttir.multiply"(%403, %415, %416) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %418 = ttir.empty() : tensor<32x13x2880xf32>
        %419 = "ttir.multiply"(%393, %417, %418) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %420 = ttir.empty() : tensor<32x13x2880xbf16>
        %421 = "ttir.typecast"(%419, %420) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xbf16>
        %423 = "ttir.matmul"(%421, %arg23, %422) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %424 = ttir.empty() : tensor<32x1x1x2880xbf16>
        %425 = "ttir.reshape"(%arg22, %424) <{shape = [32 : i32, 1 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x1x2880xbf16>) -> tensor<32x1x1x2880xbf16>
        %426 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<32x1x1x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %428 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %429 = "ttir.reshape"(%423, %428) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %430 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %431 = "ttir.add"(%429, %427, %430) : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %432 = ttir.empty() : tensor<32x1x13x2880xf32>
        %433 = "ttir.typecast"(%431, %432) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %434 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %435 = ttir.empty() : tensor<13x1x1xsi32>
        %436 = "ttir.reshape"(%434, %435) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %437 = ttir.empty() : tensor<13x4x1xsi32>
        %438 = "ttir.broadcast"(%436, %437) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %439 = ttir.empty() : tensor<13x32xbf16>
        %440 = "ttir.matmul"(%369, %arg12, %439) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<32x2880xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %441 = ttir.empty() : tensor<1x32xbf16>
        %442 = "ttir.reshape"(%arg11, %441) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %443 = ttir.empty() : tensor<13x32xbf16>
        %444 = "ttir.broadcast"(%442, %443) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %445 = ttir.empty() : tensor<13x32xbf16>
        %446 = "ttir.add"(%440, %444, %445) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %447 = ttir.empty() : tensor<13x32xbf16>
        %448 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%446, %447, %448) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %449 = ttir.empty() : tensor<13x4xsi32>
        %450 = "ttir.slice_static"(%indices, %449) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %451 = ttir.empty() : tensor<13x4x1xsi32>
        %452 = "ttir.reshape"(%450, %451) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %453 = ttir.empty() : tensor<13x4x2xsi32>
        %454 = "ttir.concat"(%438, %452, %453) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %455 = ttir.empty() : tensor<13x4xbf16>
        %456 = "ttir.slice_static"(%values, %455) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %457 = ttir.empty() : tensor<13x4xbf16>
        %458 = "ttir.softmax"(%456, %457) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %459 = ttir.empty() : tensor<13x32xbf16>
        %460 = "ttir.scatter"(%11, %454, %458, %459) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %461 = ttir.empty() : tensor<13x32xf32>
        %462 = "ttir.typecast"(%460, %461) <{conservative_folding = false}> : (tensor<13x32xbf16>, tensor<13x32xf32>) -> tensor<13x32xf32>
        %463 = ttir.empty() : tensor<32x13xf32>
        %464 = "ttir.permute"(%462, %463) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xf32>, tensor<32x13xf32>) -> tensor<32x13xf32>
        %465 = ttir.empty() : tensor<32x1x13x1xf32>
        %466 = "ttir.reshape"(%464, %465) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xf32>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %467 = ttir.empty() : tensor<32x1x13x2880xf32>
        %468 = "ttir.broadcast"(%466, %467) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %469 = ttir.empty() : tensor<32x1x13x2880xf32>
        %470 = "ttir.multiply"(%433, %468, %469) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %471 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %472 = "ttir.typecast"(%470, %471) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %473 = ttir.empty() : tensor<1x13x2880xbf16>
        %474 = "ttir.sum"(%472, %473) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %475 = ttir.empty() : tensor<1x13x2880xbf16>
        %476 = "ttir.add"(%335, %474, %475) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %477 = ttir.empty() : tensor<1x13x2880xf32>
        %478 = "ttir.typecast"(%476, %477) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %479 = ttir.empty() : tensor<1x13x2880xf32>
        %480 = "ttir.pow"(%478, %27, %479) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %481 = ttir.empty() : tensor<1x13xf32>
        %482 = "ttir.sum"(%480, %481) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %483 = ttir.empty() : tensor<1x13xf32>
        %484 = "ttir.multiply"(%482, %4, %483) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %485 = ttir.empty() : tensor<13x1xf32>
        %486 = "ttir.reshape"(%484, %485) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %487 = ttir.empty() : tensor<13x1xf32>
        %488 = "ttir.add"(%486, %59, %487) : (tensor<13x1xf32>, tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %489 = ttir.empty() : tensor<13x1xf32>
        %490 = "ttir.rsqrt"(%488, %489) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %491 = ttir.empty() : tensor<13x2880xf32>
        %492 = "ttir.broadcast"(%490, %491) <{broadcast_dimensions = array<i64: 1, 2880>}> : (tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %493 = ttir.empty() : tensor<13x2880xf32>
        %494 = "ttir.reshape"(%478, %493) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %495 = ttir.empty() : tensor<13x2880xf32>
        %496 = "ttir.multiply"(%494, %492, %495) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %497 = ttir.empty() : tensor<13x2880xf32>
        %498 = "ttir.multiply"(%303, %496, %497) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %499 = ttir.empty() : tensor<13x2880xbf16>
        %500 = "ttir.typecast"(%498, %499) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %501 = ttir.empty() : tensor<13x201088xbf16>
        %502 = "ttir.matmul"(%500, %arg10, %501) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<201088x2880xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %503 = ttir.empty() : tensor<1x13x201088xbf16>
        %504 = "ttir.reshape"(%502, %503) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %293, %295, %297, %207, %502, %504 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<32x13x2880xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xbf16>
        %17 = "ttir.reshape"(%7, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %18 = ttir.empty() : tensor<1x1x13x13xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %20 = ttir.empty() : tensor<1x1x1x1xui8>
        %21 = "ttir.reshape"(%5, %20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1x1x1xui8>) -> tensor<1x1x1x1xui8>
        %22 = ttir.empty() : tensor<1x1x13x13xui8>
        %23 = "ttir.broadcast"(%21, %22) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %24 = ttir.empty() : tensor<1x1x1xf32>
        %25 = "ttir.reshape"(%3, %24) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %26 = ttir.empty() : tensor<1x13x2880xf32>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %28 = ttir.empty() : tensor<1x1x1x1xui8>
        %29 = "ttir.reshape"(%2, %28) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1x1x1xui8>) -> tensor<1x1x1x1xui8>
        %30 = ttir.empty() : tensor<1x1x13x13xui8>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %32 = ttir.empty() : tensor<2880xf32>
        %33 = "ttir.typecast"(%arg5, %32) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %34 = ttir.empty() : tensor<1x2880xf32>
        %35 = "ttir.reshape"(%33, %34) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %36 = ttir.empty() : tensor<13x2880xf32>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %38 = ttir.empty() : tensor<1x13xui32>
        %39 = "ttir.typecast"(%arg3, %38) <{conservative_folding = false}> : (tensor<1x13xsi32>, tensor<1x13xui32>) -> tensor<1x13xui32>
        %40 = ttir.empty() : tensor<13xui32>
        %41 = "ttir.reshape"(%39, %40) <{shape = [13 : i32]}> : (tensor<1x13xui32>, tensor<13xui32>) -> tensor<13xui32>
        %42 = ttir.empty() : tensor<13x2880xbf16>
        %43 = "ttir.embedding"(%41, %arg4, %42) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %44 = ttir.empty() : tensor<1x13x2880xbf16>
        %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %46 = ttir.empty() : tensor<1x13x2880xf32>
        %47 = "ttir.typecast"(%45, %46) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %48 = ttir.empty() : tensor<1x13x2880xf32>
        %49 = "ttir.pow"(%47, %27, %48) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %50 = ttir.empty() : tensor<1x13xf32>
        %51 = "ttir.sum"(%49, %50) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %52 = ttir.empty() : tensor<1x13xf32>
        %53 = "ttir.multiply"(%51, %4, %52) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %54 = ttir.empty() : tensor<13x1xf32>
        %55 = "ttir.reshape"(%53, %54) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %56 = ttir.empty() : tensor<1x1xf32>
        %57 = "ttir.reshape"(%arg2, %56) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %58 = ttir.empty() : tensor<13x1xf32>
        %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %60 = ttir.empty() : tensor<13x1xf32>
        %61 = "ttir.add"(%55, %59, %60) : (tensor<13x1xf32>, tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %62 = ttir.empty() : tensor<13x1xf32>
        %63 = "ttir.rsqrt"(%61, %62) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %64 = ttir.empty() : tensor<13x2880xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 2880>}> : (tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %66 = ttir.empty() : tensor<13x2880xf32>
        %67 = "ttir.reshape"(%47, %66) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %68 = ttir.empty() : tensor<13x2880xf32>
        %69 = "ttir.multiply"(%67, %65, %68) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %70 = ttir.empty() : tensor<13x2880xf32>
        %71 = "ttir.multiply"(%37, %69, %70) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %72 = ttir.empty() : tensor<13x2880xbf16>
        %73 = "ttir.typecast"(%71, %72) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %74 = ttir.empty() : tensor<13x4096xbf16>
        %75 = "ttir.matmul"(%73, %arg20, %74) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<4096x2880xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %76 = ttir.empty() : tensor<1x1x64x64xbf16>
        %77 = "ttir.reshape"(%arg19, %76) <{shape = [1 : i32, 1 : i32, 64 : i32, 64 : i32]}> : (tensor<4096xbf16>, tensor<1x1x64x64xbf16>) -> tensor<1x1x64x64xbf16>
        %78 = ttir.empty() : tensor<1x64x1x64xbf16>
        %79 = "ttir.permute"(%77, %78) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x64x64xbf16>, tensor<1x64x1x64xbf16>) -> tensor<1x64x1x64xbf16>
        %80 = ttir.empty() : tensor<1x64x13x64xbf16>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %82 = ttir.empty() : tensor<1x13x64x64xbf16>
        %83 = "ttir.reshape"(%75, %82) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %84 = ttir.empty() : tensor<1x64x13x64xbf16>
        %85 = "ttir.permute"(%83, %84) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %86 = ttir.empty() : tensor<1x64x13x64xbf16>
        %87 = "ttir.add"(%85, %81, %86) : (tensor<1x64x13x64xbf16>, tensor<1x64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %88 = ttir.empty() : tensor<1x64x13x32xbf16>
        %89 = "ttir.slice_static"(%87, %88) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %90 = ttir.empty() : tensor<1x64x13x32xf32>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %92 = ttir.empty() : tensor<64x13x32xf32>
        %93 = "ttir.reshape"(%91, %92) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %94 = ttir.empty() : tensor<1x32x1xf32>
        %95 = "ttir.reshape"(%arg7, %94) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %96 = ttir.empty() : tensor<1x32x13xf32>
        %97 = "ttir.matmul"(%95, %0, %96) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %98 = ttir.empty() : tensor<1x13x32xf32>
        %99 = "ttir.permute"(%97, %98) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %100 = ttir.empty() : tensor<1x13x32xf32>
        %101 = "ttir.cos"(%99, %100) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %102 = ttir.empty() : tensor<1x1x13x32xf32>
        %103 = "ttir.reshape"(%101, %102) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %104 = ttir.empty() : tensor<1x1x1x1xf32>
        %105 = "ttir.reshape"(%arg6, %104) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %106 = ttir.empty() : tensor<1x1x13x32xf32>
        %107 = "ttir.broadcast"(%105, %106) <{broadcast_dimensions = array<i64: 1, 1, 13, 32>}> : (tensor<1x1x1x1xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %108 = ttir.empty() : tensor<1x1x13x32xf32>
        %109 = "ttir.multiply"(%103, %107, %108) : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %110 = ttir.empty() : tensor<1x1x13x32xbf16>
        %111 = "ttir.typecast"(%109, %110) <{conservative_folding = false}> : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %112 = ttir.empty() : tensor<1x1x13x32xf32>
        %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %114 = ttir.empty() : tensor<1x13x32xf32>
        %115 = "ttir.reshape"(%113, %114) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %116 = ttir.empty() : tensor<64x13x32xf32>
        %117 = "ttir.broadcast"(%115, %116) <{broadcast_dimensions = array<i64: 64, 1, 1>}> : (tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %118 = ttir.empty() : tensor<64x13x32xf32>
        %119 = "ttir.multiply"(%93, %117, %118) : (tensor<64x13x32xf32>, tensor<64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %120 = ttir.empty() : tensor<64x13x32xbf16>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %122 = ttir.empty() : tensor<1x64x13x32xbf16>
        %123 = "ttir.slice_static"(%87, %122) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %124 = ttir.empty() : tensor<1x64x13x32xf32>
        %125 = "ttir.typecast"(%123, %124) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %126 = ttir.empty() : tensor<64x13x32xf32>
        %127 = "ttir.reshape"(%125, %126) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %128 = ttir.empty() : tensor<1x13x32xf32>
        %129 = "ttir.sin"(%99, %128) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %130 = ttir.empty() : tensor<1x1x13x32xf32>
        %131 = "ttir.reshape"(%129, %130) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %132 = ttir.empty() : tensor<1x1x13x32xf32>
        %133 = "ttir.multiply"(%131, %107, %132) : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %134 = ttir.empty() : tensor<1x1x13x32xbf16>
        %135 = "ttir.typecast"(%133, %134) <{conservative_folding = false}> : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %136 = ttir.empty() : tensor<1x1x13x32xf32>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %138 = ttir.empty() : tensor<1x13x32xf32>
        %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %140 = ttir.empty() : tensor<64x13x32xf32>
        %141 = "ttir.broadcast"(%139, %140) <{broadcast_dimensions = array<i64: 64, 1, 1>}> : (tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %142 = ttir.empty() : tensor<64x13x32xf32>
        %143 = "ttir.multiply"(%127, %141, %142) : (tensor<64x13x32xf32>, tensor<64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %144 = ttir.empty() : tensor<64x13x32xbf16>
        %145 = "ttir.typecast"(%143, %144) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %146 = ttir.empty() : tensor<64x13x32xbf16>
        %147 = "ttir.subtract"(%121, %145, %146) : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %148 = ttir.empty() : tensor<64x13x32xf32>
        %149 = "ttir.multiply"(%127, %117, %148) : (tensor<64x13x32xf32>, tensor<64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %150 = ttir.empty() : tensor<64x13x32xbf16>
        %151 = "ttir.typecast"(%149, %150) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %152 = ttir.empty() : tensor<64x13x32xf32>
        %153 = "ttir.multiply"(%93, %141, %152) : (tensor<64x13x32xf32>, tensor<64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %154 = ttir.empty() : tensor<64x13x32xbf16>
        %155 = "ttir.typecast"(%153, %154) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %156 = ttir.empty() : tensor<64x13x32xbf16>
        %157 = "ttir.add"(%151, %155, %156) : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %158 = ttir.empty() : tensor<64x13x64xbf16>
        %159 = "ttir.concat"(%147, %157, %158) <{dim = 2 : si32}> : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %160 = ttir.empty() : tensor<13x512xbf16>
        %161 = "ttir.matmul"(%73, %arg9, %160) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %162 = ttir.empty() : tensor<1x1x8x64xbf16>
        %163 = "ttir.reshape"(%arg8, %162) <{shape = [1 : i32, 1 : i32, 8 : i32, 64 : i32]}> : (tensor<512xbf16>, tensor<1x1x8x64xbf16>) -> tensor<1x1x8x64xbf16>
        %164 = ttir.empty() : tensor<1x8x1x64xbf16>
        %165 = "ttir.permute"(%163, %164) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x8x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
        %166 = ttir.empty() : tensor<1x8x13x64xbf16>
        %167 = "ttir.broadcast"(%165, %166) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x8x1x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %168 = ttir.empty() : tensor<1x13x8x64xbf16>
        %169 = "ttir.reshape"(%161, %168) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %170 = ttir.empty() : tensor<1x8x13x64xbf16>
        %171 = "ttir.permute"(%169, %170) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %172 = ttir.empty() : tensor<1x8x13x64xbf16>
        %173 = "ttir.add"(%171, %167, %172) : (tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %174 = ttir.empty() : tensor<1x8x13x32xbf16>
        %175 = "ttir.slice_static"(%173, %174) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %176 = ttir.empty() : tensor<1x8x13x32xf32>
        %177 = "ttir.typecast"(%175, %176) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %178 = ttir.empty() : tensor<1x8x13x32xf32>
        %179 = "ttir.broadcast"(%113, %178) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %180 = ttir.empty() : tensor<1x8x13x32xf32>
        %181 = "ttir.multiply"(%177, %179, %180) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %182 = ttir.empty() : tensor<1x8x13x32xbf16>
        %183 = "ttir.typecast"(%181, %182) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %184 = ttir.empty() : tensor<1x8x13x32xbf16>
        %185 = "ttir.slice_static"(%173, %184) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %186 = ttir.empty() : tensor<1x8x13x32xf32>
        %187 = "ttir.typecast"(%185, %186) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %188 = ttir.empty() : tensor<1x8x13x32xf32>
        %189 = "ttir.broadcast"(%137, %188) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %190 = ttir.empty() : tensor<1x8x13x32xf32>
        %191 = "ttir.multiply"(%187, %189, %190) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %192 = ttir.empty() : tensor<1x8x13x32xbf16>
        %193 = "ttir.typecast"(%191, %192) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %194 = ttir.empty() : tensor<1x8x13x32xbf16>
        %195 = "ttir.subtract"(%183, %193, %194) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %196 = ttir.empty() : tensor<1x8x13x32xf32>
        %197 = "ttir.multiply"(%187, %179, %196) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %198 = ttir.empty() : tensor<1x8x13x32xbf16>
        %199 = "ttir.typecast"(%197, %198) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %200 = ttir.empty() : tensor<1x8x13x32xf32>
        %201 = "ttir.multiply"(%177, %189, %200) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %202 = ttir.empty() : tensor<1x8x13x32xbf16>
        %203 = "ttir.typecast"(%201, %202) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %204 = ttir.empty() : tensor<1x8x13x32xbf16>
        %205 = "ttir.add"(%199, %203, %204) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %206 = ttir.empty() : tensor<1x8x13x64xbf16>
        %207 = "ttir.concat"(%195, %205, %206) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %208 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %209 = "ttir.reshape"(%207, %208) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %210 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %211 = "ttir.broadcast"(%209, %210) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %212 = ttir.empty() : tensor<1x64x13x64xbf16>
        %213 = "ttir.reshape"(%211, %212) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %214 = ttir.empty() : tensor<1x64x64x13xbf16>
        %215 = "ttir.permute"(%213, %214) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %216 = ttir.empty() : tensor<64x64x13xbf16>
        %217 = "ttir.reshape"(%215, %216) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %218 = ttir.empty() : tensor<64x13x13xbf16>
        %219 = "ttir.matmul"(%159, %217, %218) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %220 = ttir.empty() : tensor<64x13x13xf32>
        %221 = "ttir.typecast"(%219, %220) <{conservative_folding = false}> : (tensor<64x13x13xbf16>, tensor<64x13x13xf32>) -> tensor<64x13x13xf32>
        %222 = ttir.empty() : tensor<1x64x13x13xf32>
        %223 = "ttir.reshape"(%221, %222) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %224 = ttir.empty() : tensor<1x1x1x1xf32>
        %225 = "ttir.reshape"(%arg18, %224) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %226 = ttir.empty() : tensor<1x64x13x13xf32>
        %227 = "ttir.broadcast"(%225, %226) <{broadcast_dimensions = array<i64: 1, 64, 13, 13>}> : (tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %228 = ttir.empty() : tensor<1x64x13x13xf32>
        %229 = "ttir.multiply"(%223, %227, %228) : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %230 = ttir.empty() : tensor<1x64x13x13xbf16>
        %231 = "ttir.typecast"(%229, %230) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %232 = ttir.empty() : tensor<1x1x1x13xsi32>
        %233 = "ttir.reshape"(%1, %232) <{shape = [1 : i32, 1 : i32, 1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x1x1x13xsi32>) -> tensor<1x1x1x13xsi32>
        %234 = ttir.empty() : tensor<1x1x13x13xsi32>
        %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x1x1x13xsi32>, tensor<1x1x13x13xsi32>) -> tensor<1x1x13x13xsi32>
        %236 = ttir.empty() : tensor<1x1x1x1xsi32>
        %237 = "ttir.reshape"(%arg17, %236) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<si32>, tensor<1x1x1x1xsi32>) -> tensor<1x1x1x1xsi32>
        %238 = ttir.empty() : tensor<1x1x13x1xsi32>
        %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x1x1x1xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %240 = ttir.empty() : tensor<1x1x13x1xsi32>
        %241 = "ttir.reshape"(%1, %240) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %242 = ttir.empty() : tensor<1x1x13x1xsi32>
        %243 = "ttir.subtract"(%241, %239, %242) : (tensor<1x1x13x1xsi32>, tensor<1x1x13x1xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %244 = ttir.empty() : tensor<1x1x13x13xsi32>
        %245 = "ttir.broadcast"(%243, %244) <{broadcast_dimensions = array<i64: 1, 1, 1, 13>}> : (tensor<1x1x13x1xsi32>, tensor<1x1x13x13xsi32>) -> tensor<1x1x13x13xsi32>
        %246 = ttir.empty() : tensor<1x1x13x13xbf16>
        %247 = "ttir.gt"(%235, %245, %246) : (tensor<1x1x13x13xsi32>, tensor<1x1x13x13xsi32>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %248 = ttir.empty() : tensor<1x1x13x13xui8>
        %249 = "ttir.typecast"(%247, %248) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %250 = ttir.empty() : tensor<1x1x13x13xui8>
        %251 = "ttir.bitwise_and"(%249, %23, %250) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %252 = ttir.empty() : tensor<1x1x13x13xbf16>
        %253 = "ttir.ne"(%251, %31, %252) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %254 = ttir.empty() : tensor<1x1x13x13xui8>
        %255 = "ttir.typecast"(%253, %254) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %256 = ttir.empty() : tensor<1x1x13x1xsi32>
        %257 = "ttir.reshape"(%1, %256) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %258 = ttir.empty() : tensor<1x1x13x13xsi32>
        %259 = "ttir.broadcast"(%257, %258) <{broadcast_dimensions = array<i64: 1, 1, 1, 13>}> : (tensor<1x1x13x1xsi32>, tensor<1x1x13x13xsi32>) -> tensor<1x1x13x13xsi32>
        %260 = ttir.empty() : tensor<1x1x13x13xbf16>
        %261 = "ttir.le"(%235, %259, %260) : (tensor<1x1x13x13xsi32>, tensor<1x1x13x13xsi32>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %262 = ttir.empty() : tensor<1x1x13x13xui8>
        %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %264 = ttir.empty() : tensor<1x1x13x13xui8>
        %265 = "ttir.bitwise_and"(%255, %263, %264) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %266 = ttir.empty() : tensor<1x1x13x13xbf16>
        %267 = "ttir.ne"(%265, %31, %266) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %268 = ttir.empty() : tensor<1x1x1x1xbf16>
        %269 = "ttir.reshape"(%arg16, %268) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %270 = ttir.empty() : tensor<1x1x13x13xbf16>
        %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 1, 1, 13, 13>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %272 = ttir.empty() : tensor<1x1x13x13xbf16>
        %273 = "ttir.where"(%267, %19, %271, %272) : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %274 = ttir.empty() : tensor<1x64x13x13xbf16>
        %275 = "ttir.broadcast"(%273, %274) <{broadcast_dimensions = array<i64: 1, 64, 1, 1>}> : (tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %276 = ttir.empty() : tensor<1x64x13x13xbf16>
        %277 = "ttir.add"(%231, %275, %276) : (tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %278 = ttir.empty() : tensor<1x64x1x1xbf16>
        %279 = "ttir.reshape"(%arg15, %278) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %280 = ttir.empty() : tensor<1x64x13x1xbf16>
        %281 = "ttir.broadcast"(%279, %280) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %282 = ttir.empty() : tensor<1x64x13x14xbf16>
        %283 = "ttir.concat"(%277, %281, %282) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %284 = ttir.empty() : tensor<13x512xbf16>
        %285 = "ttir.matmul"(%73, %arg1, %284) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %286 = ttir.empty() : tensor<1x13x512xbf16>
        %287 = "ttir.reshape"(%285, %286) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %288 = ttir.empty() : tensor<1x1x512xbf16>
        %289 = "ttir.reshape"(%arg0, %288) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %290 = ttir.empty() : tensor<1x13x512xbf16>
        %291 = "ttir.broadcast"(%289, %290) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %292 = ttir.empty() : tensor<1x13x512xbf16>
        %293 = "ttir.add"(%287, %291, %292) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %294 = ttir.empty() : tensor<1x13x8x64xbf16>
        %295 = "ttir.reshape"(%293, %294) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %296 = ttir.empty() : tensor<1x8x13x64xbf16>
        %297 = "ttir.permute"(%295, %296) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %298 = ttir.empty() : tensor<2880xf32>
        %299 = "ttir.typecast"(%arg30, %298) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %300 = ttir.empty() : tensor<1x2880xf32>
        %301 = "ttir.reshape"(%299, %300) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %302 = ttir.empty() : tensor<13x2880xf32>
        %303 = "ttir.broadcast"(%301, %302) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %304 = ttir.empty() : tensor<1x64x13x14xbf16>
        %305 = "ttir.softmax"(%283, %304) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %306 = ttir.empty() : tensor<1x64x13x13xbf16>
        %307 = "ttir.slice_static"(%305, %306) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %308 = ttir.empty() : tensor<64x13x13xbf16>
        %309 = "ttir.reshape"(%307, %308) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %310 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %311 = "ttir.reshape"(%297, %310) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %312 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %313 = "ttir.broadcast"(%311, %312) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %314 = ttir.empty() : tensor<64x13x64xbf16>
        %315 = "ttir.reshape"(%313, %314) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %316 = ttir.empty() : tensor<64x13x64xbf16>
        %317 = "ttir.matmul"(%309, %315, %316) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %318 = ttir.empty() : tensor<1x64x13x64xbf16>
        %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %320 = ttir.empty() : tensor<13x4096xbf16>
        %321 = ttir.empty() : tensor<1x13x4096xbf16>
        %322 = "ttir.concatenate_heads"(%319, %321) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %323 = "ttir.reshape"(%322, %320) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %324 = ttir.empty() : tensor<13x2880xbf16>
        %325 = "ttir.matmul"(%323, %arg14, %324) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16>, tensor<2880x4096xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %326 = ttir.empty() : tensor<1x13x2880xbf16>
        %327 = "ttir.reshape"(%325, %326) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %328 = ttir.empty() : tensor<1x1x2880xbf16>
        %329 = "ttir.reshape"(%arg13, %328) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %330 = ttir.empty() : tensor<1x13x2880xbf16>
        %331 = "ttir.broadcast"(%329, %330) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %332 = ttir.empty() : tensor<1x13x2880xbf16>
        %333 = "ttir.add"(%327, %331, %332) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %334 = ttir.empty() : tensor<1x13x2880xbf16>
        %335 = "ttir.add"(%45, %333, %334) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %336 = ttir.empty() : tensor<1x1x1xbf16>
        %337 = "ttir.reshape"(%arg29, %336) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %338 = ttir.empty() : tensor<32x13x2880xbf16>
        %339 = "ttir.broadcast"(%337, %338) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %340 = ttir.empty() : tensor<2880xf32>
        %341 = "ttir.typecast"(%arg21, %340) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %342 = ttir.empty() : tensor<1x2880xf32>
        %343 = "ttir.reshape"(%341, %342) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %344 = ttir.empty() : tensor<13x2880xf32>
        %345 = "ttir.broadcast"(%343, %344) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %346 = ttir.empty() : tensor<1x13x2880xf32>
        %347 = "ttir.typecast"(%335, %346) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %348 = ttir.empty() : tensor<1x13x2880xf32>
        %349 = "ttir.pow"(%347, %27, %348) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %350 = ttir.empty() : tensor<1x13xf32>
        %351 = "ttir.sum"(%349, %350) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %352 = ttir.empty() : tensor<1x13xf32>
        %353 = "ttir.multiply"(%351, %4, %352) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %354 = ttir.empty() : tensor<13x1xf32>
        %355 = "ttir.reshape"(%353, %354) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %356 = ttir.empty() : tensor<13x1xf32>
        %357 = "ttir.add"(%355, %59, %356) : (tensor<13x1xf32>, tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %358 = ttir.empty() : tensor<13x1xf32>
        %359 = "ttir.rsqrt"(%357, %358) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %360 = ttir.empty() : tensor<13x2880xf32>
        %361 = "ttir.broadcast"(%359, %360) <{broadcast_dimensions = array<i64: 1, 2880>}> : (tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %362 = ttir.empty() : tensor<13x2880xf32>
        %363 = "ttir.reshape"(%347, %362) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %364 = ttir.empty() : tensor<13x2880xf32>
        %365 = "ttir.multiply"(%363, %361, %364) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %366 = ttir.empty() : tensor<13x2880xf32>
        %367 = "ttir.multiply"(%345, %365, %366) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %368 = ttir.empty() : tensor<13x2880xbf16>
        %369 = "ttir.typecast"(%367, %368) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %370 = ttir.empty() : tensor<416x2880xbf16>
        %371 = "ttir.concat"(%369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %369, %370) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %372 = ttir.empty() : tensor<32x13x2880xbf16>
        %373 = "ttir.reshape"(%371, %372) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %374 = ttir.empty() : tensor<32x13x5760xbf16>
        %375 = "ttir.matmul"(%373, %arg28, %374) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %376 = ttir.empty() : tensor<32x1x5760xbf16>
        %377 = "ttir.reshape"(%arg27, %376) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %378 = ttir.empty() : tensor<32x13x5760xbf16>
        %379 = "ttir.broadcast"(%377, %378) <{broadcast_dimensions = array<i64: 1, 13, 1>}> : (tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %380 = ttir.empty() : tensor<32x13x5760xbf16>
        %381 = "ttir.add"(%375, %379, %380) : (tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %382 = ttir.empty() : tensor<32x13x2880xbf16>
        %383 = "ttir.slice_static"(%381, %382) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %384 = ttir.empty() : tensor<1x1x1xbf16>
        %385 = "ttir.reshape"(%arg25, %384) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %386 = ttir.empty() : tensor<32x13x2880xbf16>
        %387 = "ttir.broadcast"(%385, %386) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %388 = ttir.empty() : tensor<32x13x2880xbf16>
        %389 = "ttir.clamp_tensor"(%383, %339, %387, %388) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %390 = ttir.empty() : tensor<32x13x2880xbf16>
        %391 = "ttir.add"(%389, %15, %390) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %392 = ttir.empty() : tensor<32x13x2880xf32>
        %393 = "ttir.typecast"(%391, %392) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %394 = ttir.empty() : tensor<1x1x1xbf16>
        %395 = "ttir.reshape"(%arg26, %394) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %396 = ttir.empty() : tensor<32x13x2880xbf16>
        %397 = "ttir.broadcast"(%395, %396) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %398 = ttir.empty() : tensor<32x13x2880xbf16>
        %399 = "ttir.slice_static"(%381, %398) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %400 = ttir.empty() : tensor<32x13x2880xbf16>
        %401 = "ttir.clamp_tensor"(%399, %397, %387, %400) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %402 = ttir.empty() : tensor<32x13x2880xf32>
        %403 = "ttir.typecast"(%401, %402) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %404 = ttir.empty() : tensor<1x1x1xf32>
        %405 = "ttir.reshape"(%arg24, %404) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %406 = ttir.empty() : tensor<32x13x2880xf32>
        %407 = "ttir.broadcast"(%405, %406) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %408 = ttir.empty() : tensor<32x13x2880xf32>
        %409 = "ttir.multiply"(%403, %407, %408) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %410 = ttir.empty() : tensor<32x13x2880xbf16>
        %411 = "ttir.typecast"(%409, %410) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %412 = ttir.empty() : tensor<32x13x2880xbf16>
        %413 = "ttir.sigmoid"(%411, %412) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %414 = ttir.empty() : tensor<32x13x2880xf32>
        %415 = "ttir.typecast"(%413, %414) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %416 = ttir.empty() : tensor<32x13x2880xf32>
        %417 = "ttir.multiply"(%403, %415, %416) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %418 = ttir.empty() : tensor<32x13x2880xf32>
        %419 = "ttir.multiply"(%393, %417, %418) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %420 = ttir.empty() : tensor<32x13x2880xbf16>
        %421 = "ttir.typecast"(%419, %420) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %422 = ttir.empty() : tensor<32x13x2880xbf16>
        %423 = "ttir.matmul"(%421, %arg23, %422) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %424 = ttir.empty() : tensor<32x1x1x2880xbf16>
        %425 = "ttir.reshape"(%arg22, %424) <{shape = [32 : i32, 1 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x1x2880xbf16>) -> tensor<32x1x1x2880xbf16>
        %426 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %427 = "ttir.broadcast"(%425, %426) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<32x1x1x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %428 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %429 = "ttir.reshape"(%423, %428) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %430 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %431 = "ttir.add"(%429, %427, %430) : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %432 = ttir.empty() : tensor<32x1x13x2880xf32>
        %433 = "ttir.typecast"(%431, %432) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %434 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %435 = ttir.empty() : tensor<13x1x1xsi32>
        %436 = "ttir.reshape"(%434, %435) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %437 = ttir.empty() : tensor<13x4x1xsi32>
        %438 = "ttir.broadcast"(%436, %437) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %439 = ttir.empty() : tensor<13x32xbf16>
        %440 = "ttir.matmul"(%369, %arg12, %439) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<32x2880xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %441 = ttir.empty() : tensor<1x32xbf16>
        %442 = "ttir.reshape"(%arg11, %441) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %443 = ttir.empty() : tensor<13x32xbf16>
        %444 = "ttir.broadcast"(%442, %443) <{broadcast_dimensions = array<i64: 13, 1>}> : (tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %445 = ttir.empty() : tensor<13x32xbf16>
        %446 = "ttir.add"(%440, %444, %445) : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %447 = ttir.empty() : tensor<13x32xbf16>
        %448 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%446, %447, %448) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %449 = ttir.empty() : tensor<13x4xsi32>
        %450 = "ttir.slice_static"(%indices, %449) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %451 = ttir.empty() : tensor<13x4x1xsi32>
        %452 = "ttir.reshape"(%450, %451) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %453 = ttir.empty() : tensor<13x4x2xsi32>
        %454 = "ttir.concat"(%438, %452, %453) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %455 = ttir.empty() : tensor<13x4xbf16>
        %456 = "ttir.slice_static"(%values, %455) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %457 = ttir.empty() : tensor<13x4xbf16>
        %458 = "ttir.softmax"(%456, %457) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %459 = ttir.empty() : tensor<13x32xbf16>
        %460 = "ttir.scatter"(%11, %454, %458, %459) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %461 = ttir.empty() : tensor<13x32xf32>
        %462 = "ttir.typecast"(%460, %461) <{conservative_folding = false}> : (tensor<13x32xbf16>, tensor<13x32xf32>) -> tensor<13x32xf32>
        %463 = ttir.empty() : tensor<32x13xf32>
        %464 = "ttir.permute"(%462, %463) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xf32>, tensor<32x13xf32>) -> tensor<32x13xf32>
        %465 = ttir.empty() : tensor<32x1x13x1xf32>
        %466 = "ttir.reshape"(%464, %465) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xf32>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %467 = ttir.empty() : tensor<32x1x13x2880xf32>
        %468 = "ttir.broadcast"(%466, %467) <{broadcast_dimensions = array<i64: 1, 1, 1, 2880>}> : (tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %469 = ttir.empty() : tensor<32x1x13x2880xf32>
        %470 = "ttir.multiply"(%433, %468, %469) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %471 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %472 = "ttir.typecast"(%470, %471) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %473 = ttir.empty() : tensor<1x13x2880xbf16>
        %474 = "ttir.sum"(%472, %473) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %475 = ttir.empty() : tensor<1x13x2880xbf16>
        %476 = "ttir.add"(%335, %474, %475) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %477 = ttir.empty() : tensor<1x13x2880xf32>
        %478 = "ttir.typecast"(%476, %477) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %479 = ttir.empty() : tensor<1x13x2880xf32>
        %480 = "ttir.pow"(%478, %27, %479) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %481 = ttir.empty() : tensor<1x13xf32>
        %482 = "ttir.sum"(%480, %481) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %483 = ttir.empty() : tensor<1x13xf32>
        %484 = "ttir.multiply"(%482, %4, %483) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %485 = ttir.empty() : tensor<13x1xf32>
        %486 = "ttir.reshape"(%484, %485) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %487 = ttir.empty() : tensor<13x1xf32>
        %488 = "ttir.add"(%486, %59, %487) : (tensor<13x1xf32>, tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %489 = ttir.empty() : tensor<13x1xf32>
        %490 = "ttir.rsqrt"(%488, %489) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %491 = ttir.empty() : tensor<13x2880xf32>
        %492 = "ttir.broadcast"(%490, %491) <{broadcast_dimensions = array<i64: 1, 2880>}> : (tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %493 = ttir.empty() : tensor<13x2880xf32>
        %494 = "ttir.reshape"(%478, %493) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %495 = ttir.empty() : tensor<13x2880xf32>
        %496 = "ttir.multiply"(%494, %492, %495) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %497 = ttir.empty() : tensor<13x2880xf32>
        %498 = "ttir.multiply"(%303, %496, %497) : (tensor<13x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %499 = ttir.empty() : tensor<13x2880xbf16>
        %500 = "ttir.typecast"(%498, %499) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %501 = ttir.empty() : tensor<13x201088xbf16>
        %502 = "ttir.matmul"(%500, %arg10, %501) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<201088x2880xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %503 = ttir.empty() : tensor<1x13x201088xbf16>
        %504 = "ttir.reshape"(%502, %503) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %293, %295, %297, %207, %502, %504 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<1x1x1x1xbf16>
        %15 = "ttir.reshape"(%7, %14) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xui8>
        %17 = "ttir.reshape"(%5, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1x1x1xui8>) -> tensor<1x1x1x1xui8>
        %18 = ttir.empty() : tensor<1x1x1xf32>
        %19 = "ttir.reshape"(%3, %18) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %20 = ttir.empty() : tensor<1x1x1x1xui8>
        %21 = "ttir.reshape"(%2, %20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1x1x1xui8>) -> tensor<1x1x1x1xui8>
        %22 = ttir.empty() : tensor<2880xf32>
        %23 = "ttir.typecast"(%arg5, %22) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %24 = ttir.empty() : tensor<1x2880xf32>
        %25 = "ttir.reshape"(%23, %24) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %26 = ttir.empty() : tensor<1x13xui32>
        %27 = "ttir.typecast"(%arg3, %26) <{conservative_folding = false}> : (tensor<1x13xsi32>, tensor<1x13xui32>) -> tensor<1x13xui32>
        %28 = ttir.empty() : tensor<13xui32>
        %29 = "ttir.reshape"(%27, %28) <{shape = [13 : i32]}> : (tensor<1x13xui32>, tensor<13xui32>) -> tensor<13xui32>
        %30 = ttir.empty() : tensor<13x2880xbf16>
        %31 = "ttir.embedding"(%29, %arg4, %30) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %32 = ttir.empty() : tensor<1x13x2880xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %34 = ttir.empty() : tensor<1x13x2880xf32>
        %35 = "ttir.typecast"(%33, %34) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.pow"(%35, %19, %36) : (tensor<1x13x2880xf32>, tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<1x13xf32>
        %39 = "ttir.sum"(%37, %38) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %40 = ttir.empty() : tensor<1x13xf32>
        %41 = "ttir.multiply"(%39, %4, %40) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %42 = ttir.empty() : tensor<13x1xf32>
        %43 = "ttir.reshape"(%41, %42) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %44 = ttir.empty() : tensor<1x1xf32>
        %45 = "ttir.reshape"(%arg2, %44) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %46 = ttir.empty() : tensor<13x1xf32>
        %47 = "ttir.add"(%43, %45, %46) : (tensor<13x1xf32>, tensor<1x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %48 = ttir.empty() : tensor<13x1xf32>
        %49 = "ttir.rsqrt"(%47, %48) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %50 = ttir.empty() : tensor<13x2880xf32>
        %51 = "ttir.reshape"(%35, %50) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %52 = ttir.empty() : tensor<13x2880xf32>
        %53 = "ttir.multiply"(%51, %49, %52) : (tensor<13x2880xf32>, tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %54 = ttir.empty() : tensor<13x2880xf32>
        %55 = "ttir.multiply"(%25, %53, %54) : (tensor<1x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %56 = ttir.empty() : tensor<13x2880xbf16>
        %57 = "ttir.typecast"(%55, %56) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %58 = ttir.empty() : tensor<13x4096xbf16>
        %59 = "ttir.matmul"(%57, %arg20, %58) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<4096x2880xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %60 = ttir.empty() : tensor<1x1x64x64xbf16>
        %61 = "ttir.reshape"(%arg19, %60) <{shape = [1 : i32, 1 : i32, 64 : i32, 64 : i32]}> : (tensor<4096xbf16>, tensor<1x1x64x64xbf16>) -> tensor<1x1x64x64xbf16>
        %62 = ttir.empty() : tensor<1x64x1x64xbf16>
        %63 = "ttir.permute"(%61, %62) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x64x64xbf16>, tensor<1x64x1x64xbf16>) -> tensor<1x64x1x64xbf16>
        %64 = ttir.empty() : tensor<1x13x64x64xbf16>
        %65 = "ttir.reshape"(%59, %64) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %66 = ttir.empty() : tensor<1x64x13x64xbf16>
        %67 = "ttir.permute"(%65, %66) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %68 = ttir.empty() : tensor<1x64x13x64xbf16>
        %69 = "ttir.add"(%67, %63, %68) : (tensor<1x64x13x64xbf16>, tensor<1x64x1x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %70 = ttir.empty() : tensor<1x64x13x32xbf16>
        %71 = "ttir.slice_static"(%69, %70) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %72 = ttir.empty() : tensor<1x64x13x32xf32>
        %73 = "ttir.typecast"(%71, %72) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %74 = ttir.empty() : tensor<64x13x32xf32>
        %75 = "ttir.reshape"(%73, %74) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %76 = ttir.empty() : tensor<1x32x1xf32>
        %77 = "ttir.reshape"(%arg7, %76) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %78 = ttir.empty() : tensor<1x32x13xf32>
        %79 = "ttir.matmul"(%77, %0, %78) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %80 = ttir.empty() : tensor<1x13x32xf32>
        %81 = "ttir.permute"(%79, %80) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %82 = ttir.empty() : tensor<1x13x32xf32>
        %83 = "ttir.cos"(%81, %82) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %84 = ttir.empty() : tensor<1x1x13x32xf32>
        %85 = "ttir.reshape"(%83, %84) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %86 = ttir.empty() : tensor<1x1x1x1xf32>
        %87 = "ttir.reshape"(%arg6, %86) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %88 = ttir.empty() : tensor<1x1x13x32xf32>
        %89 = "ttir.multiply"(%85, %87, %88) : (tensor<1x1x13x32xf32>, tensor<1x1x1x1xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %90 = ttir.empty() : tensor<1x1x13x32xbf16>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %92 = ttir.empty() : tensor<1x1x13x32xf32>
        %93 = "ttir.typecast"(%91, %92) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %94 = ttir.empty() : tensor<1x13x32xf32>
        %95 = "ttir.reshape"(%93, %94) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %96 = ttir.empty() : tensor<64x13x32xf32>
        %97 = "ttir.multiply"(%75, %95, %96) : (tensor<64x13x32xf32>, tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %98 = ttir.empty() : tensor<64x13x32xbf16>
        %99 = "ttir.typecast"(%97, %98) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %100 = ttir.empty() : tensor<1x64x13x32xbf16>
        %101 = "ttir.slice_static"(%69, %100) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %102 = ttir.empty() : tensor<1x64x13x32xf32>
        %103 = "ttir.typecast"(%101, %102) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %104 = ttir.empty() : tensor<64x13x32xf32>
        %105 = "ttir.reshape"(%103, %104) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xf32>
        %107 = "ttir.sin"(%81, %106) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %108 = ttir.empty() : tensor<1x1x13x32xf32>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.multiply"(%109, %87, %110) : (tensor<1x1x13x32xf32>, tensor<1x1x1x1xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x1x13x32xbf16>
        %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %114 = ttir.empty() : tensor<1x1x13x32xf32>
        %115 = "ttir.typecast"(%113, %114) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %116 = ttir.empty() : tensor<1x13x32xf32>
        %117 = "ttir.reshape"(%115, %116) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %118 = ttir.empty() : tensor<64x13x32xf32>
        %119 = "ttir.multiply"(%105, %117, %118) : (tensor<64x13x32xf32>, tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %120 = ttir.empty() : tensor<64x13x32xbf16>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %122 = ttir.empty() : tensor<64x13x32xbf16>
        %123 = "ttir.subtract"(%99, %121, %122) : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %124 = ttir.empty() : tensor<64x13x32xf32>
        %125 = "ttir.multiply"(%105, %95, %124) : (tensor<64x13x32xf32>, tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %126 = ttir.empty() : tensor<64x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %128 = ttir.empty() : tensor<64x13x32xf32>
        %129 = "ttir.multiply"(%75, %117, %128) : (tensor<64x13x32xf32>, tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %130 = ttir.empty() : tensor<64x13x32xbf16>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %132 = ttir.empty() : tensor<64x13x32xbf16>
        %133 = "ttir.add"(%127, %131, %132) : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %134 = ttir.empty() : tensor<64x13x64xbf16>
        %135 = "ttir.concat"(%123, %133, %134) <{dim = 2 : si32}> : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %136 = ttir.empty() : tensor<13x512xbf16>
        %137 = "ttir.matmul"(%57, %arg9, %136) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %138 = ttir.empty() : tensor<1x1x8x64xbf16>
        %139 = "ttir.reshape"(%arg8, %138) <{shape = [1 : i32, 1 : i32, 8 : i32, 64 : i32]}> : (tensor<512xbf16>, tensor<1x1x8x64xbf16>) -> tensor<1x1x8x64xbf16>
        %140 = ttir.empty() : tensor<1x8x1x64xbf16>
        %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x8x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
        %142 = ttir.empty() : tensor<1x13x8x64xbf16>
        %143 = "ttir.reshape"(%137, %142) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %144 = ttir.empty() : tensor<1x8x13x64xbf16>
        %145 = "ttir.permute"(%143, %144) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %146 = ttir.empty() : tensor<1x8x13x64xbf16>
        %147 = "ttir.add"(%145, %141, %146) : (tensor<1x8x13x64xbf16>, tensor<1x8x1x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %148 = ttir.empty() : tensor<1x8x13x32xbf16>
        %149 = "ttir.slice_static"(%147, %148) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %150 = ttir.empty() : tensor<1x8x13x32xf32>
        %151 = "ttir.typecast"(%149, %150) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %152 = ttir.empty() : tensor<1x8x13x32xf32>
        %153 = "ttir.multiply"(%151, %93, %152) : (tensor<1x8x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %154 = ttir.empty() : tensor<1x8x13x32xbf16>
        %155 = "ttir.typecast"(%153, %154) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %156 = ttir.empty() : tensor<1x8x13x32xbf16>
        %157 = "ttir.slice_static"(%147, %156) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %158 = ttir.empty() : tensor<1x8x13x32xf32>
        %159 = "ttir.typecast"(%157, %158) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %160 = ttir.empty() : tensor<1x8x13x32xf32>
        %161 = "ttir.multiply"(%159, %115, %160) : (tensor<1x8x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %162 = ttir.empty() : tensor<1x8x13x32xbf16>
        %163 = "ttir.typecast"(%161, %162) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %164 = ttir.empty() : tensor<1x8x13x32xbf16>
        %165 = "ttir.subtract"(%155, %163, %164) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %166 = ttir.empty() : tensor<1x8x13x32xf32>
        %167 = "ttir.multiply"(%159, %93, %166) : (tensor<1x8x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %168 = ttir.empty() : tensor<1x8x13x32xbf16>
        %169 = "ttir.typecast"(%167, %168) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %170 = ttir.empty() : tensor<1x8x13x32xf32>
        %171 = "ttir.multiply"(%151, %115, %170) : (tensor<1x8x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %172 = ttir.empty() : tensor<1x8x13x32xbf16>
        %173 = "ttir.typecast"(%171, %172) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %174 = ttir.empty() : tensor<1x8x13x32xbf16>
        %175 = "ttir.add"(%169, %173, %174) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %176 = ttir.empty() : tensor<1x8x13x64xbf16>
        %177 = "ttir.concat"(%165, %175, %176) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %178 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %179 = "ttir.reshape"(%177, %178) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %180 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %181 = "ttir.broadcast"(%179, %180) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %182 = ttir.empty() : tensor<1x64x13x64xbf16>
        %183 = "ttir.reshape"(%181, %182) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %184 = ttir.empty() : tensor<1x64x64x13xbf16>
        %185 = "ttir.permute"(%183, %184) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %186 = ttir.empty() : tensor<64x64x13xbf16>
        %187 = "ttir.reshape"(%185, %186) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %188 = ttir.empty() : tensor<64x13x13xbf16>
        %189 = "ttir.matmul"(%135, %187, %188) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %190 = ttir.empty() : tensor<64x13x13xf32>
        %191 = "ttir.typecast"(%189, %190) <{conservative_folding = false}> : (tensor<64x13x13xbf16>, tensor<64x13x13xf32>) -> tensor<64x13x13xf32>
        %192 = ttir.empty() : tensor<1x64x13x13xf32>
        %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %194 = ttir.empty() : tensor<1x1x1x1xf32>
        %195 = "ttir.reshape"(%arg18, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %196 = ttir.empty() : tensor<1x64x13x13xf32>
        %197 = "ttir.multiply"(%193, %195, %196) : (tensor<1x64x13x13xf32>, tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %198 = ttir.empty() : tensor<1x64x13x13xbf16>
        %199 = "ttir.typecast"(%197, %198) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %200 = ttir.empty() : tensor<1x1x1x13xsi32>
        %201 = "ttir.reshape"(%1, %200) <{shape = [1 : i32, 1 : i32, 1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x1x1x13xsi32>) -> tensor<1x1x1x13xsi32>
        %202 = ttir.empty() : tensor<1x1x1x1xsi32>
        %203 = "ttir.reshape"(%arg17, %202) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<si32>, tensor<1x1x1x1xsi32>) -> tensor<1x1x1x1xsi32>
        %204 = ttir.empty() : tensor<1x1x13x1xsi32>
        %205 = "ttir.reshape"(%1, %204) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %206 = ttir.empty() : tensor<1x1x13x1xsi32>
        %207 = "ttir.subtract"(%205, %203, %206) : (tensor<1x1x13x1xsi32>, tensor<1x1x1x1xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %208 = ttir.empty() : tensor<1x1x13x13xbf16>
        %209 = "ttir.gt"(%201, %207, %208) : (tensor<1x1x1x13xsi32>, tensor<1x1x13x1xsi32>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %210 = ttir.empty() : tensor<1x1x13x13xui8>
        %211 = "ttir.typecast"(%209, %210) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %212 = ttir.empty() : tensor<1x1x13x13xui8>
        %213 = "ttir.bitwise_and"(%211, %17, %212) : (tensor<1x1x13x13xui8>, tensor<1x1x1x1xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %214 = ttir.empty() : tensor<1x1x13x13xbf16>
        %215 = "ttir.ne"(%213, %21, %214) : (tensor<1x1x13x13xui8>, tensor<1x1x1x1xui8>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %216 = ttir.empty() : tensor<1x1x13x13xui8>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %218 = ttir.empty() : tensor<1x1x13x1xsi32>
        %219 = "ttir.reshape"(%1, %218) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %220 = ttir.empty() : tensor<1x1x13x13xbf16>
        %221 = "ttir.le"(%201, %219, %220) : (tensor<1x1x1x13xsi32>, tensor<1x1x13x1xsi32>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %222 = ttir.empty() : tensor<1x1x13x13xui8>
        %223 = "ttir.typecast"(%221, %222) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %224 = ttir.empty() : tensor<1x1x13x13xui8>
        %225 = "ttir.bitwise_and"(%217, %223, %224) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %226 = ttir.empty() : tensor<1x1x13x13xbf16>
        %227 = "ttir.ne"(%225, %21, %226) : (tensor<1x1x13x13xui8>, tensor<1x1x1x1xui8>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %228 = ttir.empty() : tensor<1x1x1x1xbf16>
        %229 = "ttir.reshape"(%arg16, %228) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %230 = ttir.empty() : tensor<1x1x13x13xbf16>
        %231 = "ttir.where"(%227, %15, %229, %230) : (tensor<1x1x13x13xbf16>, tensor<1x1x1x1xbf16>, tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %232 = ttir.empty() : tensor<1x64x13x13xbf16>
        %233 = "ttir.add"(%199, %231, %232) : (tensor<1x64x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %234 = ttir.empty() : tensor<1x64x1x1xbf16>
        %235 = "ttir.reshape"(%arg15, %234) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %236 = ttir.empty() : tensor<1x64x13x1xbf16>
        %237 = "ttir.broadcast"(%235, %236) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %238 = ttir.empty() : tensor<1x64x13x14xbf16>
        %239 = "ttir.concat"(%233, %237, %238) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %240 = ttir.empty() : tensor<13x512xbf16>
        %241 = "ttir.matmul"(%57, %arg1, %240) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %242 = ttir.empty() : tensor<1x13x512xbf16>
        %243 = "ttir.reshape"(%241, %242) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %244 = ttir.empty() : tensor<1x1x512xbf16>
        %245 = "ttir.reshape"(%arg0, %244) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %246 = ttir.empty() : tensor<1x13x512xbf16>
        %247 = "ttir.add"(%243, %245, %246) : (tensor<1x13x512xbf16>, tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %248 = ttir.empty() : tensor<1x13x8x64xbf16>
        %249 = "ttir.reshape"(%247, %248) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %250 = ttir.empty() : tensor<1x8x13x64xbf16>
        %251 = "ttir.permute"(%249, %250) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %252 = ttir.empty() : tensor<2880xf32>
        %253 = "ttir.typecast"(%arg30, %252) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %254 = ttir.empty() : tensor<1x2880xf32>
        %255 = "ttir.reshape"(%253, %254) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %256 = ttir.empty() : tensor<1x64x13x14xbf16>
        %257 = "ttir.softmax"(%239, %256) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %258 = ttir.empty() : tensor<1x64x13x13xbf16>
        %259 = "ttir.slice_static"(%257, %258) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %260 = ttir.empty() : tensor<64x13x13xbf16>
        %261 = "ttir.reshape"(%259, %260) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %262 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %263 = "ttir.reshape"(%251, %262) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %264 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %265 = "ttir.broadcast"(%263, %264) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %266 = ttir.empty() : tensor<64x13x64xbf16>
        %267 = "ttir.reshape"(%265, %266) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %268 = ttir.empty() : tensor<64x13x64xbf16>
        %269 = "ttir.matmul"(%261, %267, %268) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %270 = ttir.empty() : tensor<1x64x13x64xbf16>
        %271 = "ttir.reshape"(%269, %270) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %272 = ttir.empty() : tensor<13x4096xbf16>
        %273 = ttir.empty() : tensor<1x13x4096xbf16>
        %274 = "ttir.concatenate_heads"(%271, %273) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %275 = "ttir.reshape"(%274, %272) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %276 = ttir.empty() : tensor<13x2880xbf16>
        %277 = "ttir.matmul"(%275, %arg14, %276) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16>, tensor<2880x4096xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %278 = ttir.empty() : tensor<1x13x2880xbf16>
        %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %280 = ttir.empty() : tensor<1x1x2880xbf16>
        %281 = "ttir.reshape"(%arg13, %280) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %282 = ttir.empty() : tensor<1x13x2880xbf16>
        %283 = "ttir.add"(%279, %281, %282) : (tensor<1x13x2880xbf16>, tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %284 = ttir.empty() : tensor<1x13x2880xbf16>
        %285 = "ttir.add"(%33, %283, %284) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %286 = ttir.empty() : tensor<1x1x1xbf16>
        %287 = "ttir.reshape"(%arg29, %286) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %288 = ttir.empty() : tensor<32x13x2880xbf16>
        %289 = "ttir.broadcast"(%287, %288) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %290 = ttir.empty() : tensor<2880xf32>
        %291 = "ttir.typecast"(%arg21, %290) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %292 = ttir.empty() : tensor<1x2880xf32>
        %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %294 = ttir.empty() : tensor<1x13x2880xf32>
        %295 = "ttir.typecast"(%285, %294) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %296 = ttir.empty() : tensor<1x13x2880xf32>
        %297 = "ttir.pow"(%295, %19, %296) : (tensor<1x13x2880xf32>, tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %298 = ttir.empty() : tensor<1x13xf32>
        %299 = "ttir.sum"(%297, %298) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %300 = ttir.empty() : tensor<1x13xf32>
        %301 = "ttir.multiply"(%299, %4, %300) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %302 = ttir.empty() : tensor<13x1xf32>
        %303 = "ttir.reshape"(%301, %302) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %304 = ttir.empty() : tensor<13x1xf32>
        %305 = "ttir.add"(%303, %45, %304) : (tensor<13x1xf32>, tensor<1x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %306 = ttir.empty() : tensor<13x1xf32>
        %307 = "ttir.rsqrt"(%305, %306) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %308 = ttir.empty() : tensor<13x2880xf32>
        %309 = "ttir.reshape"(%295, %308) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %310 = ttir.empty() : tensor<13x2880xf32>
        %311 = "ttir.multiply"(%309, %307, %310) : (tensor<13x2880xf32>, tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %312 = ttir.empty() : tensor<13x2880xf32>
        %313 = "ttir.multiply"(%293, %311, %312) : (tensor<1x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %314 = ttir.empty() : tensor<13x2880xbf16>
        %315 = "ttir.typecast"(%313, %314) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %316 = ttir.empty() : tensor<416x2880xbf16>
        %317 = "ttir.concat"(%315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %316) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %318 = ttir.empty() : tensor<32x13x2880xbf16>
        %319 = "ttir.reshape"(%317, %318) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %320 = ttir.empty() : tensor<32x13x5760xbf16>
        %321 = "ttir.matmul"(%319, %arg28, %320) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %322 = ttir.empty() : tensor<32x1x5760xbf16>
        %323 = "ttir.reshape"(%arg27, %322) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %324 = ttir.empty() : tensor<32x13x5760xbf16>
        %325 = "ttir.add"(%321, %323, %324) : (tensor<32x13x5760xbf16>, tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %326 = ttir.empty() : tensor<32x13x2880xbf16>
        %327 = "ttir.slice_static"(%325, %326) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %328 = ttir.empty() : tensor<1x1x1xbf16>
        %329 = "ttir.reshape"(%arg25, %328) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %330 = ttir.empty() : tensor<32x13x2880xbf16>
        %331 = "ttir.broadcast"(%329, %330) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %332 = ttir.empty() : tensor<32x13x2880xbf16>
        %333 = "ttir.clamp_tensor"(%327, %289, %331, %332) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %334 = ttir.empty() : tensor<32x13x2880xbf16>
        %335 = "ttir.add"(%333, %13, %334) : (tensor<32x13x2880xbf16>, tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %336 = ttir.empty() : tensor<32x13x2880xf32>
        %337 = "ttir.typecast"(%335, %336) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %338 = ttir.empty() : tensor<1x1x1xbf16>
        %339 = "ttir.reshape"(%arg26, %338) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %340 = ttir.empty() : tensor<32x13x2880xbf16>
        %341 = "ttir.broadcast"(%339, %340) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %342 = ttir.empty() : tensor<32x13x2880xbf16>
        %343 = "ttir.slice_static"(%325, %342) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %344 = ttir.empty() : tensor<32x13x2880xbf16>
        %345 = "ttir.clamp_tensor"(%343, %341, %331, %344) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %346 = ttir.empty() : tensor<32x13x2880xf32>
        %347 = "ttir.typecast"(%345, %346) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %348 = ttir.empty() : tensor<1x1x1xf32>
        %349 = "ttir.reshape"(%arg24, %348) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %350 = ttir.empty() : tensor<32x13x2880xf32>
        %351 = "ttir.multiply"(%347, %349, %350) : (tensor<32x13x2880xf32>, tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %352 = ttir.empty() : tensor<32x13x2880xbf16>
        %353 = "ttir.typecast"(%351, %352) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %354 = ttir.empty() : tensor<32x13x2880xbf16>
        %355 = "ttir.sigmoid"(%353, %354) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %356 = ttir.empty() : tensor<32x13x2880xf32>
        %357 = "ttir.typecast"(%355, %356) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %358 = ttir.empty() : tensor<32x13x2880xf32>
        %359 = "ttir.multiply"(%347, %357, %358) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %360 = ttir.empty() : tensor<32x13x2880xf32>
        %361 = "ttir.multiply"(%337, %359, %360) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %362 = ttir.empty() : tensor<32x13x2880xbf16>
        %363 = "ttir.typecast"(%361, %362) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %364 = ttir.empty() : tensor<32x13x2880xbf16>
        %365 = "ttir.matmul"(%363, %arg23, %364) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %366 = ttir.empty() : tensor<32x1x1x2880xbf16>
        %367 = "ttir.reshape"(%arg22, %366) <{shape = [32 : i32, 1 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x1x2880xbf16>) -> tensor<32x1x1x2880xbf16>
        %368 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %369 = "ttir.reshape"(%365, %368) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %370 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %371 = "ttir.add"(%369, %367, %370) : (tensor<32x1x13x2880xbf16>, tensor<32x1x1x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %372 = ttir.empty() : tensor<32x1x13x2880xf32>
        %373 = "ttir.typecast"(%371, %372) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %374 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %375 = ttir.empty() : tensor<13x1x1xsi32>
        %376 = "ttir.reshape"(%374, %375) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %377 = ttir.empty() : tensor<13x4x1xsi32>
        %378 = "ttir.broadcast"(%376, %377) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %379 = ttir.empty() : tensor<13x32xbf16>
        %380 = "ttir.matmul"(%315, %arg12, %379) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<32x2880xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %381 = ttir.empty() : tensor<1x32xbf16>
        %382 = "ttir.reshape"(%arg11, %381) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %383 = ttir.empty() : tensor<13x32xbf16>
        %384 = "ttir.add"(%380, %382, %383) : (tensor<13x32xbf16>, tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %385 = ttir.empty() : tensor<13x32xbf16>
        %386 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%384, %385, %386) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %387 = ttir.empty() : tensor<13x4xsi32>
        %388 = "ttir.slice_static"(%indices, %387) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %389 = ttir.empty() : tensor<13x4x1xsi32>
        %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %391 = ttir.empty() : tensor<13x4x2xsi32>
        %392 = "ttir.concat"(%378, %390, %391) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %393 = ttir.empty() : tensor<13x4xbf16>
        %394 = "ttir.slice_static"(%values, %393) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %395 = ttir.empty() : tensor<13x4xbf16>
        %396 = "ttir.softmax"(%394, %395) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %397 = ttir.empty() : tensor<13x32xbf16>
        %398 = "ttir.scatter"(%11, %392, %396, %397) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %399 = ttir.empty() : tensor<13x32xf32>
        %400 = "ttir.typecast"(%398, %399) <{conservative_folding = false}> : (tensor<13x32xbf16>, tensor<13x32xf32>) -> tensor<13x32xf32>
        %401 = ttir.empty() : tensor<32x13xf32>
        %402 = "ttir.permute"(%400, %401) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xf32>, tensor<32x13xf32>) -> tensor<32x13xf32>
        %403 = ttir.empty() : tensor<32x1x13x1xf32>
        %404 = "ttir.reshape"(%402, %403) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xf32>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %405 = ttir.empty() : tensor<32x1x13x2880xf32>
        %406 = "ttir.multiply"(%373, %404, %405) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %407 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %408 = "ttir.typecast"(%406, %407) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %409 = ttir.empty() : tensor<1x13x2880xbf16>
        %410 = "ttir.sum"(%408, %409) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %411 = ttir.empty() : tensor<1x13x2880xbf16>
        %412 = "ttir.add"(%285, %410, %411) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %413 = ttir.empty() : tensor<1x13x2880xf32>
        %414 = "ttir.typecast"(%412, %413) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %415 = ttir.empty() : tensor<1x13x2880xf32>
        %416 = "ttir.pow"(%414, %19, %415) : (tensor<1x13x2880xf32>, tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %417 = ttir.empty() : tensor<1x13xf32>
        %418 = "ttir.sum"(%416, %417) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %419 = ttir.empty() : tensor<1x13xf32>
        %420 = "ttir.multiply"(%418, %4, %419) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %421 = ttir.empty() : tensor<13x1xf32>
        %422 = "ttir.reshape"(%420, %421) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %423 = ttir.empty() : tensor<13x1xf32>
        %424 = "ttir.add"(%422, %45, %423) : (tensor<13x1xf32>, tensor<1x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %425 = ttir.empty() : tensor<13x1xf32>
        %426 = "ttir.rsqrt"(%424, %425) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %427 = ttir.empty() : tensor<13x2880xf32>
        %428 = "ttir.reshape"(%414, %427) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %429 = ttir.empty() : tensor<13x2880xf32>
        %430 = "ttir.multiply"(%428, %426, %429) : (tensor<13x2880xf32>, tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %431 = ttir.empty() : tensor<13x2880xf32>
        %432 = "ttir.multiply"(%255, %430, %431) : (tensor<1x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %433 = ttir.empty() : tensor<13x2880xbf16>
        %434 = "ttir.typecast"(%432, %433) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %435 = ttir.empty() : tensor<13x201088xbf16>
        %436 = "ttir.matmul"(%434, %arg10, %435) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<201088x2880xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %437 = ttir.empty() : tensor<1x13x201088xbf16>
        %438 = "ttir.reshape"(%436, %437) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %247, %249, %251, %177, %436, %438 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRQuantDataTypeConversionPass (ttir-quant-data-type-conversion) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<1x1x1x1xbf16>
        %15 = "ttir.reshape"(%7, %14) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xui8>
        %17 = "ttir.reshape"(%5, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1x1x1xui8>) -> tensor<1x1x1x1xui8>
        %18 = ttir.empty() : tensor<1x1x1xf32>
        %19 = "ttir.reshape"(%3, %18) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %20 = ttir.empty() : tensor<1x1x1x1xui8>
        %21 = "ttir.reshape"(%2, %20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1x1x1xui8>) -> tensor<1x1x1x1xui8>
        %22 = ttir.empty() : tensor<2880xf32>
        %23 = "ttir.typecast"(%arg5, %22) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %24 = ttir.empty() : tensor<1x2880xf32>
        %25 = "ttir.reshape"(%23, %24) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %26 = ttir.empty() : tensor<1x13xui32>
        %27 = "ttir.typecast"(%arg3, %26) <{conservative_folding = false}> : (tensor<1x13xsi32>, tensor<1x13xui32>) -> tensor<1x13xui32>
        %28 = ttir.empty() : tensor<13xui32>
        %29 = "ttir.reshape"(%27, %28) <{shape = [13 : i32]}> : (tensor<1x13xui32>, tensor<13xui32>) -> tensor<13xui32>
        %30 = ttir.empty() : tensor<13x2880xbf16>
        %31 = "ttir.embedding"(%29, %arg4, %30) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %32 = ttir.empty() : tensor<1x13x2880xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %34 = ttir.empty() : tensor<1x13x2880xf32>
        %35 = "ttir.typecast"(%33, %34) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.pow"(%35, %19, %36) : (tensor<1x13x2880xf32>, tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<1x13xf32>
        %39 = "ttir.sum"(%37, %38) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %40 = ttir.empty() : tensor<1x13xf32>
        %41 = "ttir.multiply"(%39, %4, %40) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %42 = ttir.empty() : tensor<13x1xf32>
        %43 = "ttir.reshape"(%41, %42) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %44 = ttir.empty() : tensor<1x1xf32>
        %45 = "ttir.reshape"(%arg2, %44) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %46 = ttir.empty() : tensor<13x1xf32>
        %47 = "ttir.add"(%43, %45, %46) : (tensor<13x1xf32>, tensor<1x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %48 = ttir.empty() : tensor<13x1xf32>
        %49 = "ttir.rsqrt"(%47, %48) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %50 = ttir.empty() : tensor<13x2880xf32>
        %51 = "ttir.reshape"(%35, %50) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %52 = ttir.empty() : tensor<13x2880xf32>
        %53 = "ttir.multiply"(%51, %49, %52) : (tensor<13x2880xf32>, tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %54 = ttir.empty() : tensor<13x2880xf32>
        %55 = "ttir.multiply"(%25, %53, %54) : (tensor<1x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %56 = ttir.empty() : tensor<13x2880xbf16>
        %57 = "ttir.typecast"(%55, %56) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %58 = ttir.empty() : tensor<13x4096xbf16>
        %59 = "ttir.matmul"(%57, %arg20, %58) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<4096x2880xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %60 = ttir.empty() : tensor<1x1x64x64xbf16>
        %61 = "ttir.reshape"(%arg19, %60) <{shape = [1 : i32, 1 : i32, 64 : i32, 64 : i32]}> : (tensor<4096xbf16>, tensor<1x1x64x64xbf16>) -> tensor<1x1x64x64xbf16>
        %62 = ttir.empty() : tensor<1x64x1x64xbf16>
        %63 = "ttir.permute"(%61, %62) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x64x64xbf16>, tensor<1x64x1x64xbf16>) -> tensor<1x64x1x64xbf16>
        %64 = ttir.empty() : tensor<1x13x64x64xbf16>
        %65 = "ttir.reshape"(%59, %64) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %66 = ttir.empty() : tensor<1x64x13x64xbf16>
        %67 = "ttir.permute"(%65, %66) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %68 = ttir.empty() : tensor<1x64x13x64xbf16>
        %69 = "ttir.add"(%67, %63, %68) : (tensor<1x64x13x64xbf16>, tensor<1x64x1x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %70 = ttir.empty() : tensor<1x64x13x32xbf16>
        %71 = "ttir.slice_static"(%69, %70) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %72 = ttir.empty() : tensor<1x64x13x32xf32>
        %73 = "ttir.typecast"(%71, %72) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %74 = ttir.empty() : tensor<64x13x32xf32>
        %75 = "ttir.reshape"(%73, %74) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %76 = ttir.empty() : tensor<1x32x1xf32>
        %77 = "ttir.reshape"(%arg7, %76) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %78 = ttir.empty() : tensor<1x32x13xf32>
        %79 = "ttir.matmul"(%77, %0, %78) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %80 = ttir.empty() : tensor<1x13x32xf32>
        %81 = "ttir.permute"(%79, %80) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %82 = ttir.empty() : tensor<1x13x32xf32>
        %83 = "ttir.cos"(%81, %82) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %84 = ttir.empty() : tensor<1x1x13x32xf32>
        %85 = "ttir.reshape"(%83, %84) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %86 = ttir.empty() : tensor<1x1x1x1xf32>
        %87 = "ttir.reshape"(%arg6, %86) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %88 = ttir.empty() : tensor<1x1x13x32xf32>
        %89 = "ttir.multiply"(%85, %87, %88) : (tensor<1x1x13x32xf32>, tensor<1x1x1x1xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %90 = ttir.empty() : tensor<1x1x13x32xbf16>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %92 = ttir.empty() : tensor<1x1x13x32xf32>
        %93 = "ttir.typecast"(%91, %92) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %94 = ttir.empty() : tensor<1x13x32xf32>
        %95 = "ttir.reshape"(%93, %94) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %96 = ttir.empty() : tensor<64x13x32xf32>
        %97 = "ttir.multiply"(%75, %95, %96) : (tensor<64x13x32xf32>, tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %98 = ttir.empty() : tensor<64x13x32xbf16>
        %99 = "ttir.typecast"(%97, %98) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %100 = ttir.empty() : tensor<1x64x13x32xbf16>
        %101 = "ttir.slice_static"(%69, %100) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %102 = ttir.empty() : tensor<1x64x13x32xf32>
        %103 = "ttir.typecast"(%101, %102) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %104 = ttir.empty() : tensor<64x13x32xf32>
        %105 = "ttir.reshape"(%103, %104) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xf32>
        %107 = "ttir.sin"(%81, %106) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %108 = ttir.empty() : tensor<1x1x13x32xf32>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.multiply"(%109, %87, %110) : (tensor<1x1x13x32xf32>, tensor<1x1x1x1xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x1x13x32xbf16>
        %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %114 = ttir.empty() : tensor<1x1x13x32xf32>
        %115 = "ttir.typecast"(%113, %114) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %116 = ttir.empty() : tensor<1x13x32xf32>
        %117 = "ttir.reshape"(%115, %116) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %118 = ttir.empty() : tensor<64x13x32xf32>
        %119 = "ttir.multiply"(%105, %117, %118) : (tensor<64x13x32xf32>, tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %120 = ttir.empty() : tensor<64x13x32xbf16>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %122 = ttir.empty() : tensor<64x13x32xbf16>
        %123 = "ttir.subtract"(%99, %121, %122) : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %124 = ttir.empty() : tensor<64x13x32xf32>
        %125 = "ttir.multiply"(%105, %95, %124) : (tensor<64x13x32xf32>, tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %126 = ttir.empty() : tensor<64x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %128 = ttir.empty() : tensor<64x13x32xf32>
        %129 = "ttir.multiply"(%75, %117, %128) : (tensor<64x13x32xf32>, tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %130 = ttir.empty() : tensor<64x13x32xbf16>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %132 = ttir.empty() : tensor<64x13x32xbf16>
        %133 = "ttir.add"(%127, %131, %132) : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %134 = ttir.empty() : tensor<64x13x64xbf16>
        %135 = "ttir.concat"(%123, %133, %134) <{dim = 2 : si32}> : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %136 = ttir.empty() : tensor<13x512xbf16>
        %137 = "ttir.matmul"(%57, %arg9, %136) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %138 = ttir.empty() : tensor<1x1x8x64xbf16>
        %139 = "ttir.reshape"(%arg8, %138) <{shape = [1 : i32, 1 : i32, 8 : i32, 64 : i32]}> : (tensor<512xbf16>, tensor<1x1x8x64xbf16>) -> tensor<1x1x8x64xbf16>
        %140 = ttir.empty() : tensor<1x8x1x64xbf16>
        %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x8x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
        %142 = ttir.empty() : tensor<1x13x8x64xbf16>
        %143 = "ttir.reshape"(%137, %142) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %144 = ttir.empty() : tensor<1x8x13x64xbf16>
        %145 = "ttir.permute"(%143, %144) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %146 = ttir.empty() : tensor<1x8x13x64xbf16>
        %147 = "ttir.add"(%145, %141, %146) : (tensor<1x8x13x64xbf16>, tensor<1x8x1x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %148 = ttir.empty() : tensor<1x8x13x32xbf16>
        %149 = "ttir.slice_static"(%147, %148) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %150 = ttir.empty() : tensor<1x8x13x32xf32>
        %151 = "ttir.typecast"(%149, %150) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %152 = ttir.empty() : tensor<1x8x13x32xf32>
        %153 = "ttir.multiply"(%151, %93, %152) : (tensor<1x8x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %154 = ttir.empty() : tensor<1x8x13x32xbf16>
        %155 = "ttir.typecast"(%153, %154) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %156 = ttir.empty() : tensor<1x8x13x32xbf16>
        %157 = "ttir.slice_static"(%147, %156) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %158 = ttir.empty() : tensor<1x8x13x32xf32>
        %159 = "ttir.typecast"(%157, %158) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %160 = ttir.empty() : tensor<1x8x13x32xf32>
        %161 = "ttir.multiply"(%159, %115, %160) : (tensor<1x8x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %162 = ttir.empty() : tensor<1x8x13x32xbf16>
        %163 = "ttir.typecast"(%161, %162) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %164 = ttir.empty() : tensor<1x8x13x32xbf16>
        %165 = "ttir.subtract"(%155, %163, %164) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %166 = ttir.empty() : tensor<1x8x13x32xf32>
        %167 = "ttir.multiply"(%159, %93, %166) : (tensor<1x8x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %168 = ttir.empty() : tensor<1x8x13x32xbf16>
        %169 = "ttir.typecast"(%167, %168) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %170 = ttir.empty() : tensor<1x8x13x32xf32>
        %171 = "ttir.multiply"(%151, %115, %170) : (tensor<1x8x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %172 = ttir.empty() : tensor<1x8x13x32xbf16>
        %173 = "ttir.typecast"(%171, %172) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %174 = ttir.empty() : tensor<1x8x13x32xbf16>
        %175 = "ttir.add"(%169, %173, %174) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %176 = ttir.empty() : tensor<1x8x13x64xbf16>
        %177 = "ttir.concat"(%165, %175, %176) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %178 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %179 = "ttir.reshape"(%177, %178) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %180 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %181 = "ttir.broadcast"(%179, %180) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %182 = ttir.empty() : tensor<1x64x13x64xbf16>
        %183 = "ttir.reshape"(%181, %182) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %184 = ttir.empty() : tensor<1x64x64x13xbf16>
        %185 = "ttir.permute"(%183, %184) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %186 = ttir.empty() : tensor<64x64x13xbf16>
        %187 = "ttir.reshape"(%185, %186) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %188 = ttir.empty() : tensor<64x13x13xbf16>
        %189 = "ttir.matmul"(%135, %187, %188) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %190 = ttir.empty() : tensor<64x13x13xf32>
        %191 = "ttir.typecast"(%189, %190) <{conservative_folding = false}> : (tensor<64x13x13xbf16>, tensor<64x13x13xf32>) -> tensor<64x13x13xf32>
        %192 = ttir.empty() : tensor<1x64x13x13xf32>
        %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %194 = ttir.empty() : tensor<1x1x1x1xf32>
        %195 = "ttir.reshape"(%arg18, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %196 = ttir.empty() : tensor<1x64x13x13xf32>
        %197 = "ttir.multiply"(%193, %195, %196) : (tensor<1x64x13x13xf32>, tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %198 = ttir.empty() : tensor<1x64x13x13xbf16>
        %199 = "ttir.typecast"(%197, %198) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %200 = ttir.empty() : tensor<1x1x1x13xsi32>
        %201 = "ttir.reshape"(%1, %200) <{shape = [1 : i32, 1 : i32, 1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x1x1x13xsi32>) -> tensor<1x1x1x13xsi32>
        %202 = ttir.empty() : tensor<1x1x1x1xsi32>
        %203 = "ttir.reshape"(%arg17, %202) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<si32>, tensor<1x1x1x1xsi32>) -> tensor<1x1x1x1xsi32>
        %204 = ttir.empty() : tensor<1x1x13x1xsi32>
        %205 = "ttir.reshape"(%1, %204) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %206 = ttir.empty() : tensor<1x1x13x1xsi32>
        %207 = "ttir.subtract"(%205, %203, %206) : (tensor<1x1x13x1xsi32>, tensor<1x1x1x1xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %208 = ttir.empty() : tensor<1x1x13x13xbf16>
        %209 = "ttir.gt"(%201, %207, %208) : (tensor<1x1x1x13xsi32>, tensor<1x1x13x1xsi32>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %210 = ttir.empty() : tensor<1x1x13x13xui8>
        %211 = "ttir.typecast"(%209, %210) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %212 = ttir.empty() : tensor<1x1x13x13xui8>
        %213 = "ttir.bitwise_and"(%211, %17, %212) : (tensor<1x1x13x13xui8>, tensor<1x1x1x1xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %214 = ttir.empty() : tensor<1x1x13x13xbf16>
        %215 = "ttir.ne"(%213, %21, %214) : (tensor<1x1x13x13xui8>, tensor<1x1x1x1xui8>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %216 = ttir.empty() : tensor<1x1x13x13xui8>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %218 = ttir.empty() : tensor<1x1x13x1xsi32>
        %219 = "ttir.reshape"(%1, %218) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %220 = ttir.empty() : tensor<1x1x13x13xbf16>
        %221 = "ttir.le"(%201, %219, %220) : (tensor<1x1x1x13xsi32>, tensor<1x1x13x1xsi32>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %222 = ttir.empty() : tensor<1x1x13x13xui8>
        %223 = "ttir.typecast"(%221, %222) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %224 = ttir.empty() : tensor<1x1x13x13xui8>
        %225 = "ttir.bitwise_and"(%217, %223, %224) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %226 = ttir.empty() : tensor<1x1x13x13xbf16>
        %227 = "ttir.ne"(%225, %21, %226) : (tensor<1x1x13x13xui8>, tensor<1x1x1x1xui8>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %228 = ttir.empty() : tensor<1x1x1x1xbf16>
        %229 = "ttir.reshape"(%arg16, %228) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %230 = ttir.empty() : tensor<1x1x13x13xbf16>
        %231 = "ttir.where"(%227, %15, %229, %230) : (tensor<1x1x13x13xbf16>, tensor<1x1x1x1xbf16>, tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %232 = ttir.empty() : tensor<1x64x13x13xbf16>
        %233 = "ttir.add"(%199, %231, %232) : (tensor<1x64x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %234 = ttir.empty() : tensor<1x64x1x1xbf16>
        %235 = "ttir.reshape"(%arg15, %234) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %236 = ttir.empty() : tensor<1x64x13x1xbf16>
        %237 = "ttir.broadcast"(%235, %236) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %238 = ttir.empty() : tensor<1x64x13x14xbf16>
        %239 = "ttir.concat"(%233, %237, %238) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %240 = ttir.empty() : tensor<13x512xbf16>
        %241 = "ttir.matmul"(%57, %arg1, %240) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %242 = ttir.empty() : tensor<1x13x512xbf16>
        %243 = "ttir.reshape"(%241, %242) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %244 = ttir.empty() : tensor<1x1x512xbf16>
        %245 = "ttir.reshape"(%arg0, %244) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %246 = ttir.empty() : tensor<1x13x512xbf16>
        %247 = "ttir.add"(%243, %245, %246) : (tensor<1x13x512xbf16>, tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %248 = ttir.empty() : tensor<1x13x8x64xbf16>
        %249 = "ttir.reshape"(%247, %248) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %250 = ttir.empty() : tensor<1x8x13x64xbf16>
        %251 = "ttir.permute"(%249, %250) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %252 = ttir.empty() : tensor<2880xf32>
        %253 = "ttir.typecast"(%arg30, %252) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %254 = ttir.empty() : tensor<1x2880xf32>
        %255 = "ttir.reshape"(%253, %254) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %256 = ttir.empty() : tensor<1x64x13x14xbf16>
        %257 = "ttir.softmax"(%239, %256) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %258 = ttir.empty() : tensor<1x64x13x13xbf16>
        %259 = "ttir.slice_static"(%257, %258) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %260 = ttir.empty() : tensor<64x13x13xbf16>
        %261 = "ttir.reshape"(%259, %260) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %262 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %263 = "ttir.reshape"(%251, %262) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %264 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %265 = "ttir.broadcast"(%263, %264) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %266 = ttir.empty() : tensor<64x13x64xbf16>
        %267 = "ttir.reshape"(%265, %266) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %268 = ttir.empty() : tensor<64x13x64xbf16>
        %269 = "ttir.matmul"(%261, %267, %268) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %270 = ttir.empty() : tensor<1x64x13x64xbf16>
        %271 = "ttir.reshape"(%269, %270) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %272 = ttir.empty() : tensor<13x4096xbf16>
        %273 = ttir.empty() : tensor<1x13x4096xbf16>
        %274 = "ttir.concatenate_heads"(%271, %273) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %275 = "ttir.reshape"(%274, %272) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %276 = ttir.empty() : tensor<13x2880xbf16>
        %277 = "ttir.matmul"(%275, %arg14, %276) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16>, tensor<2880x4096xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %278 = ttir.empty() : tensor<1x13x2880xbf16>
        %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %280 = ttir.empty() : tensor<1x1x2880xbf16>
        %281 = "ttir.reshape"(%arg13, %280) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %282 = ttir.empty() : tensor<1x13x2880xbf16>
        %283 = "ttir.add"(%279, %281, %282) : (tensor<1x13x2880xbf16>, tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %284 = ttir.empty() : tensor<1x13x2880xbf16>
        %285 = "ttir.add"(%33, %283, %284) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %286 = ttir.empty() : tensor<1x1x1xbf16>
        %287 = "ttir.reshape"(%arg29, %286) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %288 = ttir.empty() : tensor<32x13x2880xbf16>
        %289 = "ttir.broadcast"(%287, %288) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %290 = ttir.empty() : tensor<2880xf32>
        %291 = "ttir.typecast"(%arg21, %290) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %292 = ttir.empty() : tensor<1x2880xf32>
        %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %294 = ttir.empty() : tensor<1x13x2880xf32>
        %295 = "ttir.typecast"(%285, %294) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %296 = ttir.empty() : tensor<1x13x2880xf32>
        %297 = "ttir.pow"(%295, %19, %296) : (tensor<1x13x2880xf32>, tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %298 = ttir.empty() : tensor<1x13xf32>
        %299 = "ttir.sum"(%297, %298) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %300 = ttir.empty() : tensor<1x13xf32>
        %301 = "ttir.multiply"(%299, %4, %300) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %302 = ttir.empty() : tensor<13x1xf32>
        %303 = "ttir.reshape"(%301, %302) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %304 = ttir.empty() : tensor<13x1xf32>
        %305 = "ttir.add"(%303, %45, %304) : (tensor<13x1xf32>, tensor<1x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %306 = ttir.empty() : tensor<13x1xf32>
        %307 = "ttir.rsqrt"(%305, %306) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %308 = ttir.empty() : tensor<13x2880xf32>
        %309 = "ttir.reshape"(%295, %308) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %310 = ttir.empty() : tensor<13x2880xf32>
        %311 = "ttir.multiply"(%309, %307, %310) : (tensor<13x2880xf32>, tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %312 = ttir.empty() : tensor<13x2880xf32>
        %313 = "ttir.multiply"(%293, %311, %312) : (tensor<1x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %314 = ttir.empty() : tensor<13x2880xbf16>
        %315 = "ttir.typecast"(%313, %314) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %316 = ttir.empty() : tensor<416x2880xbf16>
        %317 = "ttir.concat"(%315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %316) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %318 = ttir.empty() : tensor<32x13x2880xbf16>
        %319 = "ttir.reshape"(%317, %318) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %320 = ttir.empty() : tensor<32x13x5760xbf16>
        %321 = "ttir.matmul"(%319, %arg28, %320) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %322 = ttir.empty() : tensor<32x1x5760xbf16>
        %323 = "ttir.reshape"(%arg27, %322) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %324 = ttir.empty() : tensor<32x13x5760xbf16>
        %325 = "ttir.add"(%321, %323, %324) : (tensor<32x13x5760xbf16>, tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %326 = ttir.empty() : tensor<32x13x2880xbf16>
        %327 = "ttir.slice_static"(%325, %326) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %328 = ttir.empty() : tensor<1x1x1xbf16>
        %329 = "ttir.reshape"(%arg25, %328) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %330 = ttir.empty() : tensor<32x13x2880xbf16>
        %331 = "ttir.broadcast"(%329, %330) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %332 = ttir.empty() : tensor<32x13x2880xbf16>
        %333 = "ttir.clamp_tensor"(%327, %289, %331, %332) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %334 = ttir.empty() : tensor<32x13x2880xbf16>
        %335 = "ttir.add"(%333, %13, %334) : (tensor<32x13x2880xbf16>, tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %336 = ttir.empty() : tensor<32x13x2880xf32>
        %337 = "ttir.typecast"(%335, %336) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %338 = ttir.empty() : tensor<1x1x1xbf16>
        %339 = "ttir.reshape"(%arg26, %338) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %340 = ttir.empty() : tensor<32x13x2880xbf16>
        %341 = "ttir.broadcast"(%339, %340) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %342 = ttir.empty() : tensor<32x13x2880xbf16>
        %343 = "ttir.slice_static"(%325, %342) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %344 = ttir.empty() : tensor<32x13x2880xbf16>
        %345 = "ttir.clamp_tensor"(%343, %341, %331, %344) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %346 = ttir.empty() : tensor<32x13x2880xf32>
        %347 = "ttir.typecast"(%345, %346) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %348 = ttir.empty() : tensor<1x1x1xf32>
        %349 = "ttir.reshape"(%arg24, %348) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %350 = ttir.empty() : tensor<32x13x2880xf32>
        %351 = "ttir.multiply"(%347, %349, %350) : (tensor<32x13x2880xf32>, tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %352 = ttir.empty() : tensor<32x13x2880xbf16>
        %353 = "ttir.typecast"(%351, %352) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %354 = ttir.empty() : tensor<32x13x2880xbf16>
        %355 = "ttir.sigmoid"(%353, %354) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %356 = ttir.empty() : tensor<32x13x2880xf32>
        %357 = "ttir.typecast"(%355, %356) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %358 = ttir.empty() : tensor<32x13x2880xf32>
        %359 = "ttir.multiply"(%347, %357, %358) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %360 = ttir.empty() : tensor<32x13x2880xf32>
        %361 = "ttir.multiply"(%337, %359, %360) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %362 = ttir.empty() : tensor<32x13x2880xbf16>
        %363 = "ttir.typecast"(%361, %362) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %364 = ttir.empty() : tensor<32x13x2880xbf16>
        %365 = "ttir.matmul"(%363, %arg23, %364) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %366 = ttir.empty() : tensor<32x1x1x2880xbf16>
        %367 = "ttir.reshape"(%arg22, %366) <{shape = [32 : i32, 1 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x1x2880xbf16>) -> tensor<32x1x1x2880xbf16>
        %368 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %369 = "ttir.reshape"(%365, %368) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %370 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %371 = "ttir.add"(%369, %367, %370) : (tensor<32x1x13x2880xbf16>, tensor<32x1x1x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %372 = ttir.empty() : tensor<32x1x13x2880xf32>
        %373 = "ttir.typecast"(%371, %372) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %374 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %375 = ttir.empty() : tensor<13x1x1xsi32>
        %376 = "ttir.reshape"(%374, %375) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %377 = ttir.empty() : tensor<13x4x1xsi32>
        %378 = "ttir.broadcast"(%376, %377) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %379 = ttir.empty() : tensor<13x32xbf16>
        %380 = "ttir.matmul"(%315, %arg12, %379) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<32x2880xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %381 = ttir.empty() : tensor<1x32xbf16>
        %382 = "ttir.reshape"(%arg11, %381) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %383 = ttir.empty() : tensor<13x32xbf16>
        %384 = "ttir.add"(%380, %382, %383) : (tensor<13x32xbf16>, tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %385 = ttir.empty() : tensor<13x32xbf16>
        %386 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%384, %385, %386) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %387 = ttir.empty() : tensor<13x4xsi32>
        %388 = "ttir.slice_static"(%indices, %387) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %389 = ttir.empty() : tensor<13x4x1xsi32>
        %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %391 = ttir.empty() : tensor<13x4x2xsi32>
        %392 = "ttir.concat"(%378, %390, %391) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %393 = ttir.empty() : tensor<13x4xbf16>
        %394 = "ttir.slice_static"(%values, %393) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %395 = ttir.empty() : tensor<13x4xbf16>
        %396 = "ttir.softmax"(%394, %395) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %397 = ttir.empty() : tensor<13x32xbf16>
        %398 = "ttir.scatter"(%11, %392, %396, %397) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %399 = ttir.empty() : tensor<13x32xf32>
        %400 = "ttir.typecast"(%398, %399) <{conservative_folding = false}> : (tensor<13x32xbf16>, tensor<13x32xf32>) -> tensor<13x32xf32>
        %401 = ttir.empty() : tensor<32x13xf32>
        %402 = "ttir.permute"(%400, %401) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xf32>, tensor<32x13xf32>) -> tensor<32x13xf32>
        %403 = ttir.empty() : tensor<32x1x13x1xf32>
        %404 = "ttir.reshape"(%402, %403) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xf32>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %405 = ttir.empty() : tensor<32x1x13x2880xf32>
        %406 = "ttir.multiply"(%373, %404, %405) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %407 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %408 = "ttir.typecast"(%406, %407) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %409 = ttir.empty() : tensor<1x13x2880xbf16>
        %410 = "ttir.sum"(%408, %409) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %411 = ttir.empty() : tensor<1x13x2880xbf16>
        %412 = "ttir.add"(%285, %410, %411) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %413 = ttir.empty() : tensor<1x13x2880xf32>
        %414 = "ttir.typecast"(%412, %413) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %415 = ttir.empty() : tensor<1x13x2880xf32>
        %416 = "ttir.pow"(%414, %19, %415) : (tensor<1x13x2880xf32>, tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %417 = ttir.empty() : tensor<1x13xf32>
        %418 = "ttir.sum"(%416, %417) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %419 = ttir.empty() : tensor<1x13xf32>
        %420 = "ttir.multiply"(%418, %4, %419) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %421 = ttir.empty() : tensor<13x1xf32>
        %422 = "ttir.reshape"(%420, %421) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %423 = ttir.empty() : tensor<13x1xf32>
        %424 = "ttir.add"(%422, %45, %423) : (tensor<13x1xf32>, tensor<1x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %425 = ttir.empty() : tensor<13x1xf32>
        %426 = "ttir.rsqrt"(%424, %425) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %427 = ttir.empty() : tensor<13x2880xf32>
        %428 = "ttir.reshape"(%414, %427) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %429 = ttir.empty() : tensor<13x2880xf32>
        %430 = "ttir.multiply"(%428, %426, %429) : (tensor<13x2880xf32>, tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %431 = ttir.empty() : tensor<13x2880xf32>
        %432 = "ttir.multiply"(%255, %430, %431) : (tensor<1x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %433 = ttir.empty() : tensor<13x2880xbf16>
        %434 = "ttir.typecast"(%432, %433) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %435 = ttir.empty() : tensor<13x201088xbf16>
        %436 = "ttir.matmul"(%434, %arg10, %435) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<201088x2880xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %437 = ttir.empty() : tensor<1x13x201088xbf16>
        %438 = "ttir.reshape"(%436, %437) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %247, %249, %251, %177, %436, %438 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTNNLayout (ttnn-layout) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %8 = ttir.empty() : tensor<1x1xbf16>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1xbf16>) -> tensor<1x1xbf16>
        %10 = ttir.empty() : tensor<13x32xbf16>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %12 = ttir.empty() : tensor<1x1x1xbf16>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %14 = ttir.empty() : tensor<1x1x1x1xbf16>
        %15 = "ttir.reshape"(%7, %14) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %16 = ttir.empty() : tensor<1x1x1x1xui8>
        %17 = "ttir.reshape"(%5, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1x1x1xui8>) -> tensor<1x1x1x1xui8>
        %18 = ttir.empty() : tensor<1x1x1xf32>
        %19 = "ttir.reshape"(%3, %18) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %20 = ttir.empty() : tensor<1x1x1x1xui8>
        %21 = "ttir.reshape"(%2, %20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8>, tensor<1x1x1x1xui8>) -> tensor<1x1x1x1xui8>
        %22 = ttir.empty() : tensor<2880xf32>
        %23 = "ttir.typecast"(%arg5, %22) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %24 = ttir.empty() : tensor<1x2880xf32>
        %25 = "ttir.reshape"(%23, %24) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %26 = ttir.empty() : tensor<1x13xui32>
        %27 = "ttir.typecast"(%arg3, %26) <{conservative_folding = false}> : (tensor<1x13xsi32>, tensor<1x13xui32>) -> tensor<1x13xui32>
        %28 = ttir.empty() : tensor<13xui32>
        %29 = "ttir.reshape"(%27, %28) <{shape = [13 : i32]}> : (tensor<1x13xui32>, tensor<13xui32>) -> tensor<13xui32>
        %30 = ttir.empty() : tensor<13x2880xbf16>
        %31 = "ttir.embedding"(%29, %arg4, %30) : (tensor<13xui32>, tensor<201088x2880xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %32 = ttir.empty() : tensor<1x13x2880xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %34 = ttir.empty() : tensor<1x13x2880xf32>
        %35 = "ttir.typecast"(%33, %34) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %36 = ttir.empty() : tensor<1x13x2880xf32>
        %37 = "ttir.pow"(%35, %19, %36) : (tensor<1x13x2880xf32>, tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %38 = ttir.empty() : tensor<1x13xf32>
        %39 = "ttir.sum"(%37, %38) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %40 = ttir.empty() : tensor<1x13xf32>
        %41 = "ttir.multiply"(%39, %4, %40) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %42 = ttir.empty() : tensor<13x1xf32>
        %43 = "ttir.reshape"(%41, %42) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %44 = ttir.empty() : tensor<1x1xf32>
        %45 = "ttir.reshape"(%arg2, %44) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %46 = ttir.empty() : tensor<13x1xf32>
        %47 = "ttir.add"(%43, %45, %46) : (tensor<13x1xf32>, tensor<1x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %48 = ttir.empty() : tensor<13x1xf32>
        %49 = "ttir.rsqrt"(%47, %48) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %50 = ttir.empty() : tensor<13x2880xf32>
        %51 = "ttir.reshape"(%35, %50) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %52 = ttir.empty() : tensor<13x2880xf32>
        %53 = "ttir.multiply"(%51, %49, %52) : (tensor<13x2880xf32>, tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %54 = ttir.empty() : tensor<13x2880xf32>
        %55 = "ttir.multiply"(%25, %53, %54) : (tensor<1x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %56 = ttir.empty() : tensor<13x2880xbf16>
        %57 = "ttir.typecast"(%55, %56) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %58 = ttir.empty() : tensor<13x4096xbf16>
        %59 = "ttir.matmul"(%57, %arg20, %58) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<4096x2880xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %60 = ttir.empty() : tensor<1x1x64x64xbf16>
        %61 = "ttir.reshape"(%arg19, %60) <{shape = [1 : i32, 1 : i32, 64 : i32, 64 : i32]}> : (tensor<4096xbf16>, tensor<1x1x64x64xbf16>) -> tensor<1x1x64x64xbf16>
        %62 = ttir.empty() : tensor<1x64x1x64xbf16>
        %63 = "ttir.permute"(%61, %62) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x64x64xbf16>, tensor<1x64x1x64xbf16>) -> tensor<1x64x1x64xbf16>
        %64 = ttir.empty() : tensor<1x13x64x64xbf16>
        %65 = "ttir.reshape"(%59, %64) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<13x4096xbf16>, tensor<1x13x64x64xbf16>) -> tensor<1x13x64x64xbf16>
        %66 = ttir.empty() : tensor<1x64x13x64xbf16>
        %67 = "ttir.permute"(%65, %66) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %68 = ttir.empty() : tensor<1x64x13x64xbf16>
        %69 = "ttir.add"(%67, %63, %68) : (tensor<1x64x13x64xbf16>, tensor<1x64x1x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %70 = ttir.empty() : tensor<1x64x13x32xbf16>
        %71 = "ttir.slice_static"(%69, %70) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %72 = ttir.empty() : tensor<1x64x13x32xf32>
        %73 = "ttir.typecast"(%71, %72) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %74 = ttir.empty() : tensor<64x13x32xf32>
        %75 = "ttir.reshape"(%73, %74) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %76 = ttir.empty() : tensor<1x32x1xf32>
        %77 = "ttir.reshape"(%arg7, %76) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32>, tensor<1x32x1xf32>) -> tensor<1x32x1xf32>
        %78 = ttir.empty() : tensor<1x32x13xf32>
        %79 = "ttir.matmul"(%77, %0, %78) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>, tensor<1x32x13xf32>) -> tensor<1x32x13xf32>
        %80 = ttir.empty() : tensor<1x13x32xf32>
        %81 = "ttir.permute"(%79, %80) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %82 = ttir.empty() : tensor<1x13x32xf32>
        %83 = "ttir.cos"(%81, %82) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %84 = ttir.empty() : tensor<1x1x13x32xf32>
        %85 = "ttir.reshape"(%83, %84) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %86 = ttir.empty() : tensor<1x1x1x1xf32>
        %87 = "ttir.reshape"(%arg6, %86) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %88 = ttir.empty() : tensor<1x1x13x32xf32>
        %89 = "ttir.multiply"(%85, %87, %88) : (tensor<1x1x13x32xf32>, tensor<1x1x1x1xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %90 = ttir.empty() : tensor<1x1x13x32xbf16>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %92 = ttir.empty() : tensor<1x1x13x32xf32>
        %93 = "ttir.typecast"(%91, %92) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %94 = ttir.empty() : tensor<1x13x32xf32>
        %95 = "ttir.reshape"(%93, %94) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %96 = ttir.empty() : tensor<64x13x32xf32>
        %97 = "ttir.multiply"(%75, %95, %96) : (tensor<64x13x32xf32>, tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %98 = ttir.empty() : tensor<64x13x32xbf16>
        %99 = "ttir.typecast"(%97, %98) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %100 = ttir.empty() : tensor<1x64x13x32xbf16>
        %101 = "ttir.slice_static"(%69, %100) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xbf16>
        %102 = ttir.empty() : tensor<1x64x13x32xf32>
        %103 = "ttir.typecast"(%101, %102) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xf32>
        %104 = ttir.empty() : tensor<64x13x32xf32>
        %105 = "ttir.reshape"(%103, %104) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %106 = ttir.empty() : tensor<1x13x32xf32>
        %107 = "ttir.sin"(%81, %106) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %108 = ttir.empty() : tensor<1x1x13x32xf32>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %110 = ttir.empty() : tensor<1x1x13x32xf32>
        %111 = "ttir.multiply"(%109, %87, %110) : (tensor<1x1x13x32xf32>, tensor<1x1x1x1xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %112 = ttir.empty() : tensor<1x1x13x32xbf16>
        %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
        %114 = ttir.empty() : tensor<1x1x13x32xf32>
        %115 = "ttir.typecast"(%113, %114) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
        %116 = ttir.empty() : tensor<1x13x32xf32>
        %117 = "ttir.reshape"(%115, %116) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
        %118 = ttir.empty() : tensor<64x13x32xf32>
        %119 = "ttir.multiply"(%105, %117, %118) : (tensor<64x13x32xf32>, tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %120 = ttir.empty() : tensor<64x13x32xbf16>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %122 = ttir.empty() : tensor<64x13x32xbf16>
        %123 = "ttir.subtract"(%99, %121, %122) : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %124 = ttir.empty() : tensor<64x13x32xf32>
        %125 = "ttir.multiply"(%105, %95, %124) : (tensor<64x13x32xf32>, tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %126 = ttir.empty() : tensor<64x13x32xbf16>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %128 = ttir.empty() : tensor<64x13x32xf32>
        %129 = "ttir.multiply"(%75, %117, %128) : (tensor<64x13x32xf32>, tensor<1x13x32xf32>, tensor<64x13x32xf32>) -> tensor<64x13x32xf32>
        %130 = ttir.empty() : tensor<64x13x32xbf16>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<64x13x32xf32>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %132 = ttir.empty() : tensor<64x13x32xbf16>
        %133 = "ttir.add"(%127, %131, %132) : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x32xbf16>) -> tensor<64x13x32xbf16>
        %134 = ttir.empty() : tensor<64x13x64xbf16>
        %135 = "ttir.concat"(%123, %133, %134) <{dim = 2 : si32}> : (tensor<64x13x32xbf16>, tensor<64x13x32xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %136 = ttir.empty() : tensor<13x512xbf16>
        %137 = "ttir.matmul"(%57, %arg9, %136) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %138 = ttir.empty() : tensor<1x1x8x64xbf16>
        %139 = "ttir.reshape"(%arg8, %138) <{shape = [1 : i32, 1 : i32, 8 : i32, 64 : i32]}> : (tensor<512xbf16>, tensor<1x1x8x64xbf16>) -> tensor<1x1x8x64xbf16>
        %140 = ttir.empty() : tensor<1x8x1x64xbf16>
        %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x8x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
        %142 = ttir.empty() : tensor<1x13x8x64xbf16>
        %143 = "ttir.reshape"(%137, %142) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %144 = ttir.empty() : tensor<1x8x13x64xbf16>
        %145 = "ttir.permute"(%143, %144) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %146 = ttir.empty() : tensor<1x8x13x64xbf16>
        %147 = "ttir.add"(%145, %141, %146) : (tensor<1x8x13x64xbf16>, tensor<1x8x1x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %148 = ttir.empty() : tensor<1x8x13x32xbf16>
        %149 = "ttir.slice_static"(%147, %148) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %150 = ttir.empty() : tensor<1x8x13x32xf32>
        %151 = "ttir.typecast"(%149, %150) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %152 = ttir.empty() : tensor<1x8x13x32xf32>
        %153 = "ttir.multiply"(%151, %93, %152) : (tensor<1x8x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %154 = ttir.empty() : tensor<1x8x13x32xbf16>
        %155 = "ttir.typecast"(%153, %154) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %156 = ttir.empty() : tensor<1x8x13x32xbf16>
        %157 = "ttir.slice_static"(%147, %156) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %158 = ttir.empty() : tensor<1x8x13x32xf32>
        %159 = "ttir.typecast"(%157, %158) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %160 = ttir.empty() : tensor<1x8x13x32xf32>
        %161 = "ttir.multiply"(%159, %115, %160) : (tensor<1x8x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %162 = ttir.empty() : tensor<1x8x13x32xbf16>
        %163 = "ttir.typecast"(%161, %162) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %164 = ttir.empty() : tensor<1x8x13x32xbf16>
        %165 = "ttir.subtract"(%155, %163, %164) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %166 = ttir.empty() : tensor<1x8x13x32xf32>
        %167 = "ttir.multiply"(%159, %93, %166) : (tensor<1x8x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %168 = ttir.empty() : tensor<1x8x13x32xbf16>
        %169 = "ttir.typecast"(%167, %168) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %170 = ttir.empty() : tensor<1x8x13x32xf32>
        %171 = "ttir.multiply"(%151, %115, %170) : (tensor<1x8x13x32xf32>, tensor<1x1x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
        %172 = ttir.empty() : tensor<1x8x13x32xbf16>
        %173 = "ttir.typecast"(%171, %172) <{conservative_folding = false}> : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %174 = ttir.empty() : tensor<1x8x13x32xbf16>
        %175 = "ttir.add"(%169, %173, %174) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
        %176 = ttir.empty() : tensor<1x8x13x64xbf16>
        %177 = "ttir.concat"(%165, %175, %176) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %178 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %179 = "ttir.reshape"(%177, %178) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %180 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %181 = "ttir.broadcast"(%179, %180) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %182 = ttir.empty() : tensor<1x64x13x64xbf16>
        %183 = "ttir.reshape"(%181, %182) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %184 = ttir.empty() : tensor<1x64x64x13xbf16>
        %185 = "ttir.permute"(%183, %184) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16>, tensor<1x64x64x13xbf16>) -> tensor<1x64x64x13xbf16>
        %186 = ttir.empty() : tensor<64x64x13xbf16>
        %187 = "ttir.reshape"(%185, %186) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16>, tensor<64x64x13xbf16>) -> tensor<64x64x13xbf16>
        %188 = ttir.empty() : tensor<64x13x13xbf16>
        %189 = "ttir.matmul"(%135, %187, %188) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %190 = ttir.empty() : tensor<64x13x13xf32>
        %191 = "ttir.typecast"(%189, %190) <{conservative_folding = false}> : (tensor<64x13x13xbf16>, tensor<64x13x13xf32>) -> tensor<64x13x13xf32>
        %192 = ttir.empty() : tensor<1x64x13x13xf32>
        %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %194 = ttir.empty() : tensor<1x1x1x1xf32>
        %195 = "ttir.reshape"(%arg18, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %196 = ttir.empty() : tensor<1x64x13x13xf32>
        %197 = "ttir.multiply"(%193, %195, %196) : (tensor<1x64x13x13xf32>, tensor<1x1x1x1xf32>, tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xf32>
        %198 = ttir.empty() : tensor<1x64x13x13xbf16>
        %199 = "ttir.typecast"(%197, %198) <{conservative_folding = false}> : (tensor<1x64x13x13xf32>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %200 = ttir.empty() : tensor<1x1x1x13xsi32>
        %201 = "ttir.reshape"(%1, %200) <{shape = [1 : i32, 1 : i32, 1 : i32, 13 : i32]}> : (tensor<13xsi32>, tensor<1x1x1x13xsi32>) -> tensor<1x1x1x13xsi32>
        %202 = ttir.empty() : tensor<1x1x1x1xsi32>
        %203 = "ttir.reshape"(%arg17, %202) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<si32>, tensor<1x1x1x1xsi32>) -> tensor<1x1x1x1xsi32>
        %204 = ttir.empty() : tensor<1x1x13x1xsi32>
        %205 = "ttir.reshape"(%1, %204) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %206 = ttir.empty() : tensor<1x1x13x1xsi32>
        %207 = "ttir.subtract"(%205, %203, %206) : (tensor<1x1x13x1xsi32>, tensor<1x1x1x1xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %208 = ttir.empty() : tensor<1x1x13x13xbf16>
        %209 = "ttir.gt"(%201, %207, %208) : (tensor<1x1x1x13xsi32>, tensor<1x1x13x1xsi32>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %210 = ttir.empty() : tensor<1x1x13x13xui8>
        %211 = "ttir.typecast"(%209, %210) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %212 = ttir.empty() : tensor<1x1x13x13xui8>
        %213 = "ttir.bitwise_and"(%211, %17, %212) : (tensor<1x1x13x13xui8>, tensor<1x1x1x1xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %214 = ttir.empty() : tensor<1x1x13x13xbf16>
        %215 = "ttir.ne"(%213, %21, %214) : (tensor<1x1x13x13xui8>, tensor<1x1x1x1xui8>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %216 = ttir.empty() : tensor<1x1x13x13xui8>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %218 = ttir.empty() : tensor<1x1x13x1xsi32>
        %219 = "ttir.reshape"(%1, %218) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<1x1x13x1xsi32>) -> tensor<1x1x13x1xsi32>
        %220 = ttir.empty() : tensor<1x1x13x13xbf16>
        %221 = "ttir.le"(%201, %219, %220) : (tensor<1x1x1x13xsi32>, tensor<1x1x13x1xsi32>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %222 = ttir.empty() : tensor<1x1x13x13xui8>
        %223 = "ttir.typecast"(%221, %222) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %224 = ttir.empty() : tensor<1x1x13x13xui8>
        %225 = "ttir.bitwise_and"(%217, %223, %224) : (tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>, tensor<1x1x13x13xui8>) -> tensor<1x1x13x13xui8>
        %226 = ttir.empty() : tensor<1x1x13x13xbf16>
        %227 = "ttir.ne"(%225, %21, %226) : (tensor<1x1x13x13xui8>, tensor<1x1x1x1xui8>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %228 = ttir.empty() : tensor<1x1x1x1xbf16>
        %229 = "ttir.reshape"(%arg16, %228) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %230 = ttir.empty() : tensor<1x1x13x13xbf16>
        %231 = "ttir.where"(%227, %15, %229, %230) : (tensor<1x1x13x13xbf16>, tensor<1x1x1x1xbf16>, tensor<1x1x1x1xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
        %232 = ttir.empty() : tensor<1x64x13x13xbf16>
        %233 = "ttir.add"(%199, %231, %232) : (tensor<1x64x13x13xbf16>, tensor<1x1x13x13xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %234 = ttir.empty() : tensor<1x64x1x1xbf16>
        %235 = "ttir.reshape"(%arg15, %234) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %236 = ttir.empty() : tensor<1x64x13x1xbf16>
        %237 = "ttir.broadcast"(%235, %236) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x1xbf16>
        %238 = ttir.empty() : tensor<1x64x13x14xbf16>
        %239 = "ttir.concat"(%233, %237, %238) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %240 = ttir.empty() : tensor<13x512xbf16>
        %241 = "ttir.matmul"(%57, %arg1, %240) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<512x2880xbf16>, tensor<13x512xbf16>) -> tensor<13x512xbf16>
        %242 = ttir.empty() : tensor<1x13x512xbf16>
        %243 = "ttir.reshape"(%241, %242) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %244 = ttir.empty() : tensor<1x1x512xbf16>
        %245 = "ttir.reshape"(%arg0, %244) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16>
        %246 = ttir.empty() : tensor<1x13x512xbf16>
        %247 = "ttir.add"(%243, %245, %246) : (tensor<1x13x512xbf16>, tensor<1x1x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
        %248 = ttir.empty() : tensor<1x13x8x64xbf16>
        %249 = "ttir.reshape"(%247, %248) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>) -> tensor<1x13x8x64xbf16>
        %250 = ttir.empty() : tensor<1x8x13x64xbf16>
        %251 = "ttir.permute"(%249, %250) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
        %252 = ttir.empty() : tensor<2880xf32>
        %253 = "ttir.typecast"(%arg30, %252) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %254 = ttir.empty() : tensor<1x2880xf32>
        %255 = "ttir.reshape"(%253, %254) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %256 = ttir.empty() : tensor<1x64x13x14xbf16>
        %257 = "ttir.softmax"(%239, %256) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x14xbf16>) -> tensor<1x64x13x14xbf16>
        %258 = ttir.empty() : tensor<1x64x13x13xbf16>
        %259 = "ttir.slice_static"(%257, %258) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16>, tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
        %260 = ttir.empty() : tensor<64x13x13xbf16>
        %261 = "ttir.reshape"(%259, %260) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16>, tensor<64x13x13xbf16>) -> tensor<64x13x13xbf16>
        %262 = ttir.empty() : tensor<1x8x1x13x64xbf16>
        %263 = "ttir.reshape"(%251, %262) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16>, tensor<1x8x1x13x64xbf16>) -> tensor<1x8x1x13x64xbf16>
        %264 = ttir.empty() : tensor<1x8x8x13x64xbf16>
        %265 = "ttir.broadcast"(%263, %264) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16>, tensor<1x8x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
        %266 = ttir.empty() : tensor<64x13x64xbf16>
        %267 = "ttir.reshape"(%265, %266) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %268 = ttir.empty() : tensor<64x13x64xbf16>
        %269 = "ttir.matmul"(%261, %267, %268) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
        %270 = ttir.empty() : tensor<1x64x13x64xbf16>
        %271 = "ttir.reshape"(%269, %270) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16>, tensor<1x64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
        %272 = ttir.empty() : tensor<13x4096xbf16>
        %273 = ttir.empty() : tensor<1x13x4096xbf16>
        %274 = "ttir.concatenate_heads"(%271, %273) : (tensor<1x64x13x64xbf16>, tensor<1x13x4096xbf16>) -> tensor<1x13x4096xbf16>
        %275 = "ttir.reshape"(%274, %272) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16>, tensor<13x4096xbf16>) -> tensor<13x4096xbf16>
        %276 = ttir.empty() : tensor<13x2880xbf16>
        %277 = "ttir.matmul"(%275, %arg14, %276) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16>, tensor<2880x4096xbf16>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %278 = ttir.empty() : tensor<1x13x2880xbf16>
        %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %280 = ttir.empty() : tensor<1x1x2880xbf16>
        %281 = "ttir.reshape"(%arg13, %280) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16>, tensor<1x1x2880xbf16>) -> tensor<1x1x2880xbf16>
        %282 = ttir.empty() : tensor<1x13x2880xbf16>
        %283 = "ttir.add"(%279, %281, %282) : (tensor<1x13x2880xbf16>, tensor<1x1x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %284 = ttir.empty() : tensor<1x13x2880xbf16>
        %285 = "ttir.add"(%33, %283, %284) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %286 = ttir.empty() : tensor<1x1x1xbf16>
        %287 = "ttir.reshape"(%arg29, %286) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %288 = ttir.empty() : tensor<32x13x2880xbf16>
        %289 = "ttir.broadcast"(%287, %288) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %290 = ttir.empty() : tensor<2880xf32>
        %291 = "ttir.typecast"(%arg21, %290) <{conservative_folding = false}> : (tensor<2880xbf16>, tensor<2880xf32>) -> tensor<2880xf32>
        %292 = ttir.empty() : tensor<1x2880xf32>
        %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32>, tensor<1x2880xf32>) -> tensor<1x2880xf32>
        %294 = ttir.empty() : tensor<1x13x2880xf32>
        %295 = "ttir.typecast"(%285, %294) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %296 = ttir.empty() : tensor<1x13x2880xf32>
        %297 = "ttir.pow"(%295, %19, %296) : (tensor<1x13x2880xf32>, tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %298 = ttir.empty() : tensor<1x13xf32>
        %299 = "ttir.sum"(%297, %298) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %300 = ttir.empty() : tensor<1x13xf32>
        %301 = "ttir.multiply"(%299, %4, %300) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %302 = ttir.empty() : tensor<13x1xf32>
        %303 = "ttir.reshape"(%301, %302) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %304 = ttir.empty() : tensor<13x1xf32>
        %305 = "ttir.add"(%303, %45, %304) : (tensor<13x1xf32>, tensor<1x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %306 = ttir.empty() : tensor<13x1xf32>
        %307 = "ttir.rsqrt"(%305, %306) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %308 = ttir.empty() : tensor<13x2880xf32>
        %309 = "ttir.reshape"(%295, %308) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %310 = ttir.empty() : tensor<13x2880xf32>
        %311 = "ttir.multiply"(%309, %307, %310) : (tensor<13x2880xf32>, tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %312 = ttir.empty() : tensor<13x2880xf32>
        %313 = "ttir.multiply"(%293, %311, %312) : (tensor<1x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %314 = ttir.empty() : tensor<13x2880xbf16>
        %315 = "ttir.typecast"(%313, %314) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %316 = ttir.empty() : tensor<416x2880xbf16>
        %317 = "ttir.concat"(%315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %316) <{dim = 0 : si32}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<416x2880xbf16>) -> tensor<416x2880xbf16>
        %318 = ttir.empty() : tensor<32x13x2880xbf16>
        %319 = "ttir.reshape"(%317, %318) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %320 = ttir.empty() : tensor<32x13x5760xbf16>
        %321 = "ttir.matmul"(%319, %arg28, %320) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %322 = ttir.empty() : tensor<32x1x5760xbf16>
        %323 = "ttir.reshape"(%arg27, %322) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16>, tensor<32x1x5760xbf16>) -> tensor<32x1x5760xbf16>
        %324 = ttir.empty() : tensor<32x13x5760xbf16>
        %325 = "ttir.add"(%321, %323, %324) : (tensor<32x13x5760xbf16>, tensor<32x1x5760xbf16>, tensor<32x13x5760xbf16>) -> tensor<32x13x5760xbf16>
        %326 = ttir.empty() : tensor<32x13x2880xbf16>
        %327 = "ttir.slice_static"(%325, %326) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %328 = ttir.empty() : tensor<1x1x1xbf16>
        %329 = "ttir.reshape"(%arg25, %328) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %330 = ttir.empty() : tensor<32x13x2880xbf16>
        %331 = "ttir.broadcast"(%329, %330) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %332 = ttir.empty() : tensor<32x13x2880xbf16>
        %333 = "ttir.clamp_tensor"(%327, %289, %331, %332) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %334 = ttir.empty() : tensor<32x13x2880xbf16>
        %335 = "ttir.add"(%333, %13, %334) : (tensor<32x13x2880xbf16>, tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %336 = ttir.empty() : tensor<32x13x2880xf32>
        %337 = "ttir.typecast"(%335, %336) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %338 = ttir.empty() : tensor<1x1x1xbf16>
        %339 = "ttir.reshape"(%arg26, %338) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16>
        %340 = ttir.empty() : tensor<32x13x2880xbf16>
        %341 = "ttir.broadcast"(%339, %340) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %342 = ttir.empty() : tensor<32x13x2880xbf16>
        %343 = "ttir.slice_static"(%325, %342) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %344 = ttir.empty() : tensor<32x13x2880xbf16>
        %345 = "ttir.clamp_tensor"(%343, %341, %331, %344) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %346 = ttir.empty() : tensor<32x13x2880xf32>
        %347 = "ttir.typecast"(%345, %346) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %348 = ttir.empty() : tensor<1x1x1xf32>
        %349 = "ttir.reshape"(%arg24, %348) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %350 = ttir.empty() : tensor<32x13x2880xf32>
        %351 = "ttir.multiply"(%347, %349, %350) : (tensor<32x13x2880xf32>, tensor<1x1x1xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %352 = ttir.empty() : tensor<32x13x2880xbf16>
        %353 = "ttir.typecast"(%351, %352) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %354 = ttir.empty() : tensor<32x13x2880xbf16>
        %355 = "ttir.sigmoid"(%353, %354) : (tensor<32x13x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %356 = ttir.empty() : tensor<32x13x2880xf32>
        %357 = "ttir.typecast"(%355, %356) <{conservative_folding = false}> : (tensor<32x13x2880xbf16>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %358 = ttir.empty() : tensor<32x13x2880xf32>
        %359 = "ttir.multiply"(%347, %357, %358) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %360 = ttir.empty() : tensor<32x13x2880xf32>
        %361 = "ttir.multiply"(%337, %359, %360) : (tensor<32x13x2880xf32>, tensor<32x13x2880xf32>, tensor<32x13x2880xf32>) -> tensor<32x13x2880xf32>
        %362 = ttir.empty() : tensor<32x13x2880xbf16>
        %363 = "ttir.typecast"(%361, %362) <{conservative_folding = false}> : (tensor<32x13x2880xf32>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %364 = ttir.empty() : tensor<32x13x2880xbf16>
        %365 = "ttir.matmul"(%363, %arg23, %364) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x13x2880xbf16>) -> tensor<32x13x2880xbf16>
        %366 = ttir.empty() : tensor<32x1x1x2880xbf16>
        %367 = "ttir.reshape"(%arg22, %366) <{shape = [32 : i32, 1 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16>, tensor<32x1x1x2880xbf16>) -> tensor<32x1x1x2880xbf16>
        %368 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %369 = "ttir.reshape"(%365, %368) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %370 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %371 = "ttir.add"(%369, %367, %370) : (tensor<32x1x13x2880xbf16>, tensor<32x1x1x2880xbf16>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %372 = ttir.empty() : tensor<32x1x13x2880xf32>
        %373 = "ttir.typecast"(%371, %372) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %374 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32>
        %375 = ttir.empty() : tensor<13x1x1xsi32>
        %376 = "ttir.reshape"(%374, %375) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32>, tensor<13x1x1xsi32>) -> tensor<13x1x1xsi32>
        %377 = ttir.empty() : tensor<13x4x1xsi32>
        %378 = "ttir.broadcast"(%376, %377) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %379 = ttir.empty() : tensor<13x32xbf16>
        %380 = "ttir.matmul"(%315, %arg12, %379) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<32x2880xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %381 = ttir.empty() : tensor<1x32xbf16>
        %382 = "ttir.reshape"(%arg11, %381) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16>, tensor<1x32xbf16>) -> tensor<1x32xbf16>
        %383 = ttir.empty() : tensor<13x32xbf16>
        %384 = "ttir.add"(%380, %382, %383) : (tensor<13x32xbf16>, tensor<1x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %385 = ttir.empty() : tensor<13x32xbf16>
        %386 = ttir.empty() : tensor<13x32xsi32>
        %values, %indices = "ttir.sort"(%384, %385, %386) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16>, tensor<13x32xbf16>, tensor<13x32xsi32>) -> (tensor<13x32xbf16>, tensor<13x32xsi32>)
        %387 = ttir.empty() : tensor<13x4xsi32>
        %388 = "ttir.slice_static"(%indices, %387) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32>, tensor<13x4xsi32>) -> tensor<13x4xsi32>
        %389 = ttir.empty() : tensor<13x4x1xsi32>
        %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32>, tensor<13x4x1xsi32>) -> tensor<13x4x1xsi32>
        %391 = ttir.empty() : tensor<13x4x2xsi32>
        %392 = "ttir.concat"(%378, %390, %391) <{dim = 2 : si32}> : (tensor<13x4x1xsi32>, tensor<13x4x1xsi32>, tensor<13x4x2xsi32>) -> tensor<13x4x2xsi32>
        %393 = ttir.empty() : tensor<13x4xbf16>
        %394 = "ttir.slice_static"(%values, %393) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %395 = ttir.empty() : tensor<13x4xbf16>
        %396 = "ttir.softmax"(%394, %395) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
        %397 = ttir.empty() : tensor<13x32xbf16>
        %398 = "ttir.scatter"(%11, %392, %396, %397) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16>, tensor<13x4x2xsi32>, tensor<13x4xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
        %399 = ttir.empty() : tensor<13x32xf32>
        %400 = "ttir.typecast"(%398, %399) <{conservative_folding = false}> : (tensor<13x32xbf16>, tensor<13x32xf32>) -> tensor<13x32xf32>
        %401 = ttir.empty() : tensor<32x13xf32>
        %402 = "ttir.permute"(%400, %401) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xf32>, tensor<32x13xf32>) -> tensor<32x13xf32>
        %403 = ttir.empty() : tensor<32x1x13x1xf32>
        %404 = "ttir.reshape"(%402, %403) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xf32>, tensor<32x1x13x1xf32>) -> tensor<32x1x13x1xf32>
        %405 = ttir.empty() : tensor<32x1x13x2880xf32>
        %406 = "ttir.multiply"(%373, %404, %405) : (tensor<32x1x13x2880xf32>, tensor<32x1x13x1xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
        %407 = ttir.empty() : tensor<32x1x13x2880xbf16>
        %408 = "ttir.typecast"(%406, %407) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32>, tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
        %409 = ttir.empty() : tensor<1x13x2880xbf16>
        %410 = "ttir.sum"(%408, %409) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %411 = ttir.empty() : tensor<1x13x2880xbf16>
        %412 = "ttir.add"(%285, %410, %411) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
        %413 = ttir.empty() : tensor<1x13x2880xf32>
        %414 = "ttir.typecast"(%412, %413) <{conservative_folding = false}> : (tensor<1x13x2880xbf16>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %415 = ttir.empty() : tensor<1x13x2880xf32>
        %416 = "ttir.pow"(%414, %19, %415) : (tensor<1x13x2880xf32>, tensor<1x1x1xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
        %417 = ttir.empty() : tensor<1x13xf32>
        %418 = "ttir.sum"(%416, %417) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %419 = ttir.empty() : tensor<1x13xf32>
        %420 = "ttir.multiply"(%418, %4, %419) : (tensor<1x13xf32>, tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
        %421 = ttir.empty() : tensor<13x1xf32>
        %422 = "ttir.reshape"(%420, %421) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %423 = ttir.empty() : tensor<13x1xf32>
        %424 = "ttir.add"(%422, %45, %423) : (tensor<13x1xf32>, tensor<1x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %425 = ttir.empty() : tensor<13x1xf32>
        %426 = "ttir.rsqrt"(%424, %425) : (tensor<13x1xf32>, tensor<13x1xf32>) -> tensor<13x1xf32>
        %427 = ttir.empty() : tensor<13x2880xf32>
        %428 = "ttir.reshape"(%414, %427) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %429 = ttir.empty() : tensor<13x2880xf32>
        %430 = "ttir.multiply"(%428, %426, %429) : (tensor<13x2880xf32>, tensor<13x1xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %431 = ttir.empty() : tensor<13x2880xf32>
        %432 = "ttir.multiply"(%255, %430, %431) : (tensor<1x2880xf32>, tensor<13x2880xf32>, tensor<13x2880xf32>) -> tensor<13x2880xf32>
        %433 = ttir.empty() : tensor<13x2880xbf16>
        %434 = "ttir.typecast"(%432, %433) <{conservative_folding = false}> : (tensor<13x2880xf32>, tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
        %435 = ttir.empty() : tensor<13x201088xbf16>
        %436 = "ttir.matmul"(%434, %arg10, %435) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16>, tensor<201088x2880xbf16>, tensor<13x201088xbf16>) -> tensor<13x201088xbf16>
        %437 = ttir.empty() : tensor<1x13x201088xbf16>
        %438 = "ttir.reshape"(%436, %437) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> tensor<1x13x201088xbf16>
        return %247, %249, %251, %177, %436, %438 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
      }
    }
  }
}


// -----// IR Dump After TTNNLayout (ttnn-layout) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<6284x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<90x128x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 2880 + d1, d2), <1x1>, memref<2880x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x180x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 2880 + d1, d2), <1x1>, memref<2880x180x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 416 + d1 * 32 + d2, d3), <1x1>, memref<13x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x6284x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x6284x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, u8>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, u8>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 * 64 + d2, d3), <1x1>, memref<2x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 64 + d2, d3), <1x1>, memref<26x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<64x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout44 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout45 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<64x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout46 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<64x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout47 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout48 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout49 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout50 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout51 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 256 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout52 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 2048 + d1 * 256 + d2 * 32 + d3, d4), <1x1>, memref<64x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout53 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 64 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout54 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout55 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout56 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout57 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout58 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<13x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout59 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x180x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout60 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout61 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<32x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout62 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<32x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout63 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<13x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout64 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16, #ttnn_layout1> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32, #ttnn_layout2> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32, #ttnn_layout3> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16, #ttnn_layout4> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16, #ttnn_layout5> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32, #ttnn_layout2> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32, #ttnn_layout6> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16, #ttnn_layout> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16, #ttnn_layout1> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16, #ttnn_layout4> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16, #ttnn_layout7> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16, #ttnn_layout8> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16, #ttnn_layout5> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16, #ttnn_layout9> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16, #ttnn_layout10> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16, #ttnn_layout11> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32, #ttnn_layout12> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32, #ttnn_layout2> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16, #ttnn_layout13> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16, #ttnn_layout14> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16, #ttnn_layout5> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16, #ttnn_layout8> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16, #ttnn_layout15> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32, #ttnn_layout2> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16, #ttnn_layout11> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16, #ttnn_layout11> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16, #ttnn_layout16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16, #ttnn_layout17> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16, #ttnn_layout11> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16, #ttnn_layout5> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16, #ttnn_layout18> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16, #ttnn_layout19> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16, #ttnn_layout20> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16, #ttnn_layout20> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16, #ttnn_layout21> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16, #ttnn_layout22> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32, #ttnn_layout23>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32, #ttnn_layout24>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8, #ttnn_layout25>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32, #ttnn_layout2>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32, #ttnn_layout26>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8, #ttnn_layout25>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn_layout11>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn_layout11>
        %8 = ttir.empty() : tensor<1x1xbf16, #ttnn_layout27>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1xbf16, #ttnn_layout27>) -> tensor<1x1xbf16, #ttnn_layout27>
        %10 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16, #ttnn_layout27>, tensor<13x32xbf16, #ttnn_layout27>) -> tensor<13x32xbf16, #ttnn_layout27>
        %12 = ttir.empty() : tensor<1x1x1xbf16, #ttnn_layout28>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1xbf16, #ttnn_layout28>) -> tensor<1x1x1xbf16, #ttnn_layout28>
        %14 = ttir.empty() : tensor<1x1x1x1xbf16, #ttnn_layout29>
        %15 = "ttir.reshape"(%7, %14) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1x1xbf16, #ttnn_layout29>) -> tensor<1x1x1x1xbf16, #ttnn_layout29>
        %16 = ttir.empty() : tensor<1x1x1x1xui8, #ttnn_layout30>
        %17 = "ttir.reshape"(%5, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8, #ttnn_layout25>, tensor<1x1x1x1xui8, #ttnn_layout30>) -> tensor<1x1x1x1xui8, #ttnn_layout30>
        %18 = ttir.empty() : tensor<1x1x1xf32, #ttnn_layout23>
        %19 = "ttir.reshape"(%3, %18) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1x1xf32, #ttnn_layout23>) -> tensor<1x1x1xf32, #ttnn_layout23>
        %20 = ttir.empty() : tensor<1x1x1x1xui8, #ttnn_layout30>
        %21 = "ttir.reshape"(%2, %20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8, #ttnn_layout25>, tensor<1x1x1x1xui8, #ttnn_layout30>) -> tensor<1x1x1x1xui8, #ttnn_layout30>
        %22 = ttir.empty() : tensor<2880xf32, #ttnn_layout31>
        %23 = "ttir.typecast"(%arg5, %22) <{conservative_folding = false}> : (tensor<2880xbf16, #ttnn_layout5>, tensor<2880xf32, #ttnn_layout31>) -> tensor<2880xf32, #ttnn_layout31>
        %24 = ttir.empty() : tensor<1x2880xf32, #ttnn_layout32>
        %25 = "ttir.reshape"(%23, %24) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32, #ttnn_layout31>, tensor<1x2880xf32, #ttnn_layout32>) -> tensor<1x2880xf32, #ttnn_layout32>
        %26 = ttir.empty() : tensor<1x13xui32, #ttnn_layout33>
        %27 = "ttir.typecast"(%arg3, %26) <{conservative_folding = false}> : (tensor<1x13xsi32, #ttnn_layout3>, tensor<1x13xui32, #ttnn_layout33>) -> tensor<1x13xui32, #ttnn_layout33>
        %28 = ttir.empty() : tensor<13xui32, #ttnn_layout34>
        %29 = "ttir.reshape"(%27, %28) <{shape = [13 : i32]}> : (tensor<1x13xui32, #ttnn_layout33>, tensor<13xui32, #ttnn_layout34>) -> tensor<13xui32, #ttnn_layout34>
        %30 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %31 = "ttir.embedding"(%29, %arg4, %30) : (tensor<13xui32, #ttnn_layout34>, tensor<201088x2880xbf16, #ttnn_layout4>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %32 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %34 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %35 = "ttir.typecast"(%33, %34) <{conservative_folding = false}> : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %36 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %37 = "ttir.pow"(%35, %19, %36) : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x1x1xf32, #ttnn_layout23>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %38 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %39 = "ttir.sum"(%37, %38) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %40 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %41 = "ttir.multiply"(%39, %4, %40) : (tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %42 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %43 = "ttir.reshape"(%41, %42) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %44 = ttir.empty() : tensor<1x1xf32, #ttnn_layout26>
        %45 = "ttir.reshape"(%arg2, %44) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1xf32, #ttnn_layout26>) -> tensor<1x1xf32, #ttnn_layout26>
        %46 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %47 = "ttir.add"(%43, %45, %46) : (tensor<13x1xf32, #ttnn_layout26>, tensor<1x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %48 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %49 = "ttir.rsqrt"(%47, %48) : (tensor<13x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %50 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %51 = "ttir.reshape"(%35, %50) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %52 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %53 = "ttir.multiply"(%51, %49, %52) : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x1xf32, #ttnn_layout26>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %54 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %55 = "ttir.multiply"(%25, %53, %54) : (tensor<1x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %56 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %57 = "ttir.typecast"(%55, %56) <{conservative_folding = false}> : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %58 = ttir.empty() : tensor<13x4096xbf16, #ttnn_layout37>
        %59 = "ttir.matmul"(%57, %arg20, %58) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<4096x2880xbf16, #ttnn_layout14>, tensor<13x4096xbf16, #ttnn_layout37>) -> tensor<13x4096xbf16, #ttnn_layout37>
        %60 = ttir.empty() : tensor<1x1x64x64xbf16, #ttnn_layout38>
        %61 = "ttir.reshape"(%arg19, %60) <{shape = [1 : i32, 1 : i32, 64 : i32, 64 : i32]}> : (tensor<4096xbf16, #ttnn_layout13>, tensor<1x1x64x64xbf16, #ttnn_layout38>) -> tensor<1x1x64x64xbf16, #ttnn_layout38>
        %62 = ttir.empty() : tensor<1x64x1x64xbf16, #ttnn_layout39>
        %63 = "ttir.permute"(%61, %62) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x64x64xbf16, #ttnn_layout38>, tensor<1x64x1x64xbf16, #ttnn_layout39>) -> tensor<1x64x1x64xbf16, #ttnn_layout39>
        %64 = ttir.empty() : tensor<1x13x64x64xbf16, #ttnn_layout40>
        %65 = "ttir.reshape"(%59, %64) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<13x4096xbf16, #ttnn_layout37>, tensor<1x13x64x64xbf16, #ttnn_layout40>) -> tensor<1x13x64x64xbf16, #ttnn_layout40>
        %66 = ttir.empty() : tensor<1x64x13x64xbf16, #ttnn_layout39>
        %67 = "ttir.permute"(%65, %66) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16, #ttnn_layout40>, tensor<1x64x13x64xbf16, #ttnn_layout39>) -> tensor<1x64x13x64xbf16, #ttnn_layout39>
        %68 = ttir.empty() : tensor<1x64x13x64xbf16, #ttnn_layout39>
        %69 = "ttir.add"(%67, %63, %68) : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x64x1x64xbf16, #ttnn_layout39>, tensor<1x64x13x64xbf16, #ttnn_layout39>) -> tensor<1x64x13x64xbf16, #ttnn_layout39>
        %70 = ttir.empty() : tensor<1x64x13x32xbf16, #ttnn_layout41>
        %71 = "ttir.slice_static"(%69, %70) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x64x13x32xbf16, #ttnn_layout41>) -> tensor<1x64x13x32xbf16, #ttnn_layout41>
        %72 = ttir.empty() : tensor<1x64x13x32xf32, #ttnn_layout42>
        %73 = "ttir.typecast"(%71, %72) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16, #ttnn_layout41>, tensor<1x64x13x32xf32, #ttnn_layout42>) -> tensor<1x64x13x32xf32, #ttnn_layout42>
        %74 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %75 = "ttir.reshape"(%73, %74) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32, #ttnn_layout42>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %76 = ttir.empty() : tensor<1x32x1xf32, #ttnn_layout23>
        %77 = "ttir.reshape"(%arg7, %76) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32, #ttnn_layout6>, tensor<1x32x1xf32, #ttnn_layout23>) -> tensor<1x32x1xf32, #ttnn_layout23>
        %78 = ttir.empty() : tensor<1x32x13xf32, #ttnn_layout23>
        %79 = "ttir.matmul"(%77, %0, %78) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32, #ttnn_layout23>, tensor<1x1x13xf32, #ttnn_layout23>, tensor<1x32x13xf32, #ttnn_layout23>) -> tensor<1x32x13xf32, #ttnn_layout23>
        %80 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %81 = "ttir.permute"(%79, %80) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32, #ttnn_layout23>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %82 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %83 = "ttir.cos"(%81, %82) : (tensor<1x13x32xf32, #ttnn_layout23>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %84 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %85 = "ttir.reshape"(%83, %84) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32, #ttnn_layout23>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %86 = ttir.empty() : tensor<1x1x1x1xf32, #ttnn_layout44>
        %87 = "ttir.reshape"(%arg6, %86) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1x1x1xf32, #ttnn_layout44>) -> tensor<1x1x1x1xf32, #ttnn_layout44>
        %88 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %89 = "ttir.multiply"(%85, %87, %88) : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x1x1x1xf32, #ttnn_layout44>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %90 = ttir.empty() : tensor<1x1x13x32xbf16, #ttnn_layout29>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x1x13x32xbf16, #ttnn_layout29>) -> tensor<1x1x13x32xbf16, #ttnn_layout29>
        %92 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %93 = "ttir.typecast"(%91, %92) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16, #ttnn_layout29>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %94 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %95 = "ttir.reshape"(%93, %94) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %96 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %97 = "ttir.multiply"(%75, %95, %96) : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<1x13x32xf32, #ttnn_layout23>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %98 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %99 = "ttir.typecast"(%97, %98) <{conservative_folding = false}> : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %100 = ttir.empty() : tensor<1x64x13x32xbf16, #ttnn_layout41>
        %101 = "ttir.slice_static"(%69, %100) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x64x13x32xbf16, #ttnn_layout41>) -> tensor<1x64x13x32xbf16, #ttnn_layout41>
        %102 = ttir.empty() : tensor<1x64x13x32xf32, #ttnn_layout42>
        %103 = "ttir.typecast"(%101, %102) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16, #ttnn_layout41>, tensor<1x64x13x32xf32, #ttnn_layout42>) -> tensor<1x64x13x32xf32, #ttnn_layout42>
        %104 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %105 = "ttir.reshape"(%103, %104) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32, #ttnn_layout42>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %106 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %107 = "ttir.sin"(%81, %106) : (tensor<1x13x32xf32, #ttnn_layout23>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %108 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32, #ttnn_layout23>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %110 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %111 = "ttir.multiply"(%109, %87, %110) : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x1x1x1xf32, #ttnn_layout44>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %112 = ttir.empty() : tensor<1x1x13x32xbf16, #ttnn_layout29>
        %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x1x13x32xbf16, #ttnn_layout29>) -> tensor<1x1x13x32xbf16, #ttnn_layout29>
        %114 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %115 = "ttir.typecast"(%113, %114) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16, #ttnn_layout29>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %116 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %117 = "ttir.reshape"(%115, %116) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %118 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %119 = "ttir.multiply"(%105, %117, %118) : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<1x13x32xf32, #ttnn_layout23>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %120 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %122 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %123 = "ttir.subtract"(%99, %121, %122) : (tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %124 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %125 = "ttir.multiply"(%105, %95, %124) : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<1x13x32xf32, #ttnn_layout23>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %126 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %128 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %129 = "ttir.multiply"(%75, %117, %128) : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<1x13x32xf32, #ttnn_layout23>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %130 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %132 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %133 = "ttir.add"(%127, %131, %132) : (tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %134 = ttir.empty() : tensor<64x13x64xbf16, #ttnn_layout46>
        %135 = "ttir.concat"(%123, %133, %134) <{dim = 2 : si32}> : (tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x64xbf16, #ttnn_layout46>) -> tensor<64x13x64xbf16, #ttnn_layout46>
        %136 = ttir.empty() : tensor<13x512xbf16, #ttnn_layout47>
        %137 = "ttir.matmul"(%57, %arg9, %136) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<512x2880xbf16, #ttnn_layout1>, tensor<13x512xbf16, #ttnn_layout47>) -> tensor<13x512xbf16, #ttnn_layout47>
        %138 = ttir.empty() : tensor<1x1x8x64xbf16, #ttnn_layout48>
        %139 = "ttir.reshape"(%arg8, %138) <{shape = [1 : i32, 1 : i32, 8 : i32, 64 : i32]}> : (tensor<512xbf16, #ttnn_layout>, tensor<1x1x8x64xbf16, #ttnn_layout48>) -> tensor<1x1x8x64xbf16, #ttnn_layout48>
        %140 = ttir.empty() : tensor<1x8x1x64xbf16, #ttnn_layout20>
        %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x8x64xbf16, #ttnn_layout48>, tensor<1x8x1x64xbf16, #ttnn_layout20>) -> tensor<1x8x1x64xbf16, #ttnn_layout20>
        %142 = ttir.empty() : tensor<1x13x8x64xbf16, #ttnn_layout19>
        %143 = "ttir.reshape"(%137, %142) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<13x512xbf16, #ttnn_layout47>, tensor<1x13x8x64xbf16, #ttnn_layout19>) -> tensor<1x13x8x64xbf16, #ttnn_layout19>
        %144 = ttir.empty() : tensor<1x8x13x64xbf16, #ttnn_layout20>
        %145 = "ttir.permute"(%143, %144) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16, #ttnn_layout19>, tensor<1x8x13x64xbf16, #ttnn_layout20>) -> tensor<1x8x13x64xbf16, #ttnn_layout20>
        %146 = ttir.empty() : tensor<1x8x13x64xbf16, #ttnn_layout20>
        %147 = "ttir.add"(%145, %141, %146) : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x1x64xbf16, #ttnn_layout20>, tensor<1x8x13x64xbf16, #ttnn_layout20>) -> tensor<1x8x13x64xbf16, #ttnn_layout20>
        %148 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %149 = "ttir.slice_static"(%147, %148) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %150 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %151 = "ttir.typecast"(%149, %150) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %152 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %153 = "ttir.multiply"(%151, %93, %152) : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %154 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %155 = "ttir.typecast"(%153, %154) <{conservative_folding = false}> : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %156 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %157 = "ttir.slice_static"(%147, %156) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %158 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %159 = "ttir.typecast"(%157, %158) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %160 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %161 = "ttir.multiply"(%159, %115, %160) : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %162 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %163 = "ttir.typecast"(%161, %162) <{conservative_folding = false}> : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %164 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %165 = "ttir.subtract"(%155, %163, %164) : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %166 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %167 = "ttir.multiply"(%159, %93, %166) : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %168 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %169 = "ttir.typecast"(%167, %168) <{conservative_folding = false}> : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %170 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %171 = "ttir.multiply"(%151, %115, %170) : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %172 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %173 = "ttir.typecast"(%171, %172) <{conservative_folding = false}> : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %174 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %175 = "ttir.add"(%169, %173, %174) : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %176 = ttir.empty() : tensor<1x8x13x64xbf16, #ttnn_layout20>
        %177 = "ttir.concat"(%165, %175, %176) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x64xbf16, #ttnn_layout20>) -> tensor<1x8x13x64xbf16, #ttnn_layout20>
        %178 = ttir.empty() : tensor<1x8x1x13x64xbf16, #ttnn_layout51>
        %179 = "ttir.reshape"(%177, %178) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x1x13x64xbf16, #ttnn_layout51>) -> tensor<1x8x1x13x64xbf16, #ttnn_layout51>
        %180 = ttir.empty() : tensor<1x8x8x13x64xbf16, #ttnn_layout52>
        %181 = "ttir.broadcast"(%179, %180) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16, #ttnn_layout51>, tensor<1x8x8x13x64xbf16, #ttnn_layout52>) -> tensor<1x8x8x13x64xbf16, #ttnn_layout52>
        %182 = ttir.empty() : tensor<1x64x13x64xbf16, #ttnn_layout39>
        %183 = "ttir.reshape"(%181, %182) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16, #ttnn_layout52>, tensor<1x64x13x64xbf16, #ttnn_layout39>) -> tensor<1x64x13x64xbf16, #ttnn_layout39>
        %184 = ttir.empty() : tensor<1x64x64x13xbf16, #ttnn_layout53>
        %185 = "ttir.permute"(%183, %184) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x64x64x13xbf16, #ttnn_layout53>) -> tensor<1x64x64x13xbf16, #ttnn_layout53>
        %186 = ttir.empty() : tensor<64x64x13xbf16, #ttnn_layout54>
        %187 = "ttir.reshape"(%185, %186) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16, #ttnn_layout53>, tensor<64x64x13xbf16, #ttnn_layout54>) -> tensor<64x64x13xbf16, #ttnn_layout54>
        %188 = ttir.empty() : tensor<64x13x13xbf16, #ttnn_layout45>
        %189 = "ttir.matmul"(%135, %187, %188) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16, #ttnn_layout46>, tensor<64x64x13xbf16, #ttnn_layout54>, tensor<64x13x13xbf16, #ttnn_layout45>) -> tensor<64x13x13xbf16, #ttnn_layout45>
        %190 = ttir.empty() : tensor<64x13x13xf32, #ttnn_layout43>
        %191 = "ttir.typecast"(%189, %190) <{conservative_folding = false}> : (tensor<64x13x13xbf16, #ttnn_layout45>, tensor<64x13x13xf32, #ttnn_layout43>) -> tensor<64x13x13xf32, #ttnn_layout43>
        %192 = ttir.empty() : tensor<1x64x13x13xf32, #ttnn_layout42>
        %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xf32, #ttnn_layout43>, tensor<1x64x13x13xf32, #ttnn_layout42>) -> tensor<1x64x13x13xf32, #ttnn_layout42>
        %194 = ttir.empty() : tensor<1x1x1x1xf32, #ttnn_layout44>
        %195 = "ttir.reshape"(%arg18, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1x1x1xf32, #ttnn_layout44>) -> tensor<1x1x1x1xf32, #ttnn_layout44>
        %196 = ttir.empty() : tensor<1x64x13x13xf32, #ttnn_layout42>
        %197 = "ttir.multiply"(%193, %195, %196) : (tensor<1x64x13x13xf32, #ttnn_layout42>, tensor<1x1x1x1xf32, #ttnn_layout44>, tensor<1x64x13x13xf32, #ttnn_layout42>) -> tensor<1x64x13x13xf32, #ttnn_layout42>
        %198 = ttir.empty() : tensor<1x64x13x13xbf16, #ttnn_layout41>
        %199 = "ttir.typecast"(%197, %198) <{conservative_folding = false}> : (tensor<1x64x13x13xf32, #ttnn_layout42>, tensor<1x64x13x13xbf16, #ttnn_layout41>) -> tensor<1x64x13x13xbf16, #ttnn_layout41>
        %200 = ttir.empty() : tensor<1x1x1x13xsi32, #ttnn_layout55>
        %201 = "ttir.reshape"(%1, %200) <{shape = [1 : i32, 1 : i32, 1 : i32, 13 : i32]}> : (tensor<13xsi32, #ttnn_layout24>, tensor<1x1x1x13xsi32, #ttnn_layout55>) -> tensor<1x1x1x13xsi32, #ttnn_layout55>
        %202 = ttir.empty() : tensor<1x1x1x1xsi32, #ttnn_layout55>
        %203 = "ttir.reshape"(%arg17, %202) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<si32, #ttnn_layout12>, tensor<1x1x1x1xsi32, #ttnn_layout55>) -> tensor<1x1x1x1xsi32, #ttnn_layout55>
        %204 = ttir.empty() : tensor<1x1x13x1xsi32, #ttnn_layout55>
        %205 = "ttir.reshape"(%1, %204) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32, #ttnn_layout24>, tensor<1x1x13x1xsi32, #ttnn_layout55>) -> tensor<1x1x13x1xsi32, #ttnn_layout55>
        %206 = ttir.empty() : tensor<1x1x13x1xsi32, #ttnn_layout55>
        %207 = "ttir.subtract"(%205, %203, %206) : (tensor<1x1x13x1xsi32, #ttnn_layout55>, tensor<1x1x1x1xsi32, #ttnn_layout55>, tensor<1x1x13x1xsi32, #ttnn_layout55>) -> tensor<1x1x13x1xsi32, #ttnn_layout55>
        %208 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %209 = "ttir.gt"(%201, %207, %208) : (tensor<1x1x1x13xsi32, #ttnn_layout55>, tensor<1x1x13x1xsi32, #ttnn_layout55>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %210 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %211 = "ttir.typecast"(%209, %210) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %212 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %213 = "ttir.bitwise_and"(%211, %17, %212) : (tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x1x1xui8, #ttnn_layout30>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %214 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %215 = "ttir.ne"(%213, %21, %214) : (tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x1x1xui8, #ttnn_layout30>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %216 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %218 = ttir.empty() : tensor<1x1x13x1xsi32, #ttnn_layout55>
        %219 = "ttir.reshape"(%1, %218) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32, #ttnn_layout24>, tensor<1x1x13x1xsi32, #ttnn_layout55>) -> tensor<1x1x13x1xsi32, #ttnn_layout55>
        %220 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %221 = "ttir.le"(%201, %219, %220) : (tensor<1x1x1x13xsi32, #ttnn_layout55>, tensor<1x1x13x1xsi32, #ttnn_layout55>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %222 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %223 = "ttir.typecast"(%221, %222) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %224 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %225 = "ttir.bitwise_and"(%217, %223, %224) : (tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %226 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %227 = "ttir.ne"(%225, %21, %226) : (tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x1x1xui8, #ttnn_layout30>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %228 = ttir.empty() : tensor<1x1x1x1xbf16, #ttnn_layout29>
        %229 = "ttir.reshape"(%arg16, %228) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1x1xbf16, #ttnn_layout29>) -> tensor<1x1x1x1xbf16, #ttnn_layout29>
        %230 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %231 = "ttir.where"(%227, %15, %229, %230) : (tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x1x1x1xbf16, #ttnn_layout29>, tensor<1x1x1x1xbf16, #ttnn_layout29>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %232 = ttir.empty() : tensor<1x64x13x13xbf16, #ttnn_layout41>
        %233 = "ttir.add"(%199, %231, %232) : (tensor<1x64x13x13xbf16, #ttnn_layout41>, tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x64x13x13xbf16, #ttnn_layout41>) -> tensor<1x64x13x13xbf16, #ttnn_layout41>
        %234 = ttir.empty() : tensor<1x64x1x1xbf16, #ttnn_layout41>
        %235 = "ttir.reshape"(%arg15, %234) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout10>, tensor<1x64x1x1xbf16, #ttnn_layout41>) -> tensor<1x64x1x1xbf16, #ttnn_layout41>
        %236 = ttir.empty() : tensor<1x64x13x1xbf16, #ttnn_layout41>
        %237 = "ttir.broadcast"(%235, %236) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout41>, tensor<1x64x13x1xbf16, #ttnn_layout41>) -> tensor<1x64x13x1xbf16, #ttnn_layout41>
        %238 = ttir.empty() : tensor<1x64x13x14xbf16, #ttnn_layout41>
        %239 = "ttir.concat"(%233, %237, %238) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16, #ttnn_layout41>, tensor<1x64x13x1xbf16, #ttnn_layout41>, tensor<1x64x13x14xbf16, #ttnn_layout41>) -> tensor<1x64x13x14xbf16, #ttnn_layout41>
        %240 = ttir.empty() : tensor<13x512xbf16, #ttnn_layout47>
        %241 = "ttir.matmul"(%57, %arg1, %240) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<512x2880xbf16, #ttnn_layout1>, tensor<13x512xbf16, #ttnn_layout47>) -> tensor<13x512xbf16, #ttnn_layout47>
        %242 = ttir.empty() : tensor<1x13x512xbf16, #ttnn_layout18>
        %243 = "ttir.reshape"(%241, %242) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16, #ttnn_layout47>, tensor<1x13x512xbf16, #ttnn_layout18>) -> tensor<1x13x512xbf16, #ttnn_layout18>
        %244 = ttir.empty() : tensor<1x1x512xbf16, #ttnn_layout18>
        %245 = "ttir.reshape"(%arg0, %244) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout>, tensor<1x1x512xbf16, #ttnn_layout18>) -> tensor<1x1x512xbf16, #ttnn_layout18>
        %246 = ttir.empty() : tensor<1x13x512xbf16, #ttnn_layout18>
        %247 = "ttir.add"(%243, %245, %246) : (tensor<1x13x512xbf16, #ttnn_layout18>, tensor<1x1x512xbf16, #ttnn_layout18>, tensor<1x13x512xbf16, #ttnn_layout18>) -> tensor<1x13x512xbf16, #ttnn_layout18>
        %248 = ttir.empty() : tensor<1x13x8x64xbf16, #ttnn_layout19>
        %249 = "ttir.reshape"(%247, %248) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16, #ttnn_layout18>, tensor<1x13x8x64xbf16, #ttnn_layout19>) -> tensor<1x13x8x64xbf16, #ttnn_layout19>
        %250 = ttir.empty() : tensor<1x8x13x64xbf16, #ttnn_layout20>
        %251 = "ttir.permute"(%249, %250) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16, #ttnn_layout19>, tensor<1x8x13x64xbf16, #ttnn_layout20>) -> tensor<1x8x13x64xbf16, #ttnn_layout20>
        %252 = ttir.empty() : tensor<2880xf32, #ttnn_layout31>
        %253 = "ttir.typecast"(%arg30, %252) <{conservative_folding = false}> : (tensor<2880xbf16, #ttnn_layout5>, tensor<2880xf32, #ttnn_layout31>) -> tensor<2880xf32, #ttnn_layout31>
        %254 = ttir.empty() : tensor<1x2880xf32, #ttnn_layout32>
        %255 = "ttir.reshape"(%253, %254) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32, #ttnn_layout31>, tensor<1x2880xf32, #ttnn_layout32>) -> tensor<1x2880xf32, #ttnn_layout32>
        %256 = ttir.empty() : tensor<1x64x13x14xbf16, #ttnn_layout41>
        %257 = "ttir.softmax"(%239, %256) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16, #ttnn_layout41>, tensor<1x64x13x14xbf16, #ttnn_layout41>) -> tensor<1x64x13x14xbf16, #ttnn_layout41>
        %258 = ttir.empty() : tensor<1x64x13x13xbf16, #ttnn_layout41>
        %259 = "ttir.slice_static"(%257, %258) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16, #ttnn_layout41>, tensor<1x64x13x13xbf16, #ttnn_layout41>) -> tensor<1x64x13x13xbf16, #ttnn_layout41>
        %260 = ttir.empty() : tensor<64x13x13xbf16, #ttnn_layout45>
        %261 = "ttir.reshape"(%259, %260) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16, #ttnn_layout41>, tensor<64x13x13xbf16, #ttnn_layout45>) -> tensor<64x13x13xbf16, #ttnn_layout45>
        %262 = ttir.empty() : tensor<1x8x1x13x64xbf16, #ttnn_layout51>
        %263 = "ttir.reshape"(%251, %262) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x1x13x64xbf16, #ttnn_layout51>) -> tensor<1x8x1x13x64xbf16, #ttnn_layout51>
        %264 = ttir.empty() : tensor<1x8x8x13x64xbf16, #ttnn_layout52>
        %265 = "ttir.broadcast"(%263, %264) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16, #ttnn_layout51>, tensor<1x8x8x13x64xbf16, #ttnn_layout52>) -> tensor<1x8x8x13x64xbf16, #ttnn_layout52>
        %266 = ttir.empty() : tensor<64x13x64xbf16, #ttnn_layout46>
        %267 = "ttir.reshape"(%265, %266) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16, #ttnn_layout52>, tensor<64x13x64xbf16, #ttnn_layout46>) -> tensor<64x13x64xbf16, #ttnn_layout46>
        %268 = ttir.empty() : tensor<64x13x64xbf16, #ttnn_layout46>
        %269 = "ttir.matmul"(%261, %267, %268) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16, #ttnn_layout45>, tensor<64x13x64xbf16, #ttnn_layout46>, tensor<64x13x64xbf16, #ttnn_layout46>) -> tensor<64x13x64xbf16, #ttnn_layout46>
        %270 = ttir.empty() : tensor<1x64x13x64xbf16, #ttnn_layout39>
        %271 = "ttir.reshape"(%269, %270) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16, #ttnn_layout46>, tensor<1x64x13x64xbf16, #ttnn_layout39>) -> tensor<1x64x13x64xbf16, #ttnn_layout39>
        %272 = ttir.empty() : tensor<13x4096xbf16, #ttnn_layout37>
        %273 = ttir.empty() : tensor<1x13x4096xbf16, #ttnn_layout56>
        %274 = "ttir.concatenate_heads"(%271, %273) : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x13x4096xbf16, #ttnn_layout56>) -> tensor<1x13x4096xbf16, #ttnn_layout56>
        %275 = "ttir.reshape"(%274, %272) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16, #ttnn_layout56>, tensor<13x4096xbf16, #ttnn_layout37>) -> tensor<13x4096xbf16, #ttnn_layout37>
        %276 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %277 = "ttir.matmul"(%275, %arg14, %276) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16, #ttnn_layout37>, tensor<2880x4096xbf16, #ttnn_layout9>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %278 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %280 = ttir.empty() : tensor<1x1x2880xbf16, #ttnn_layout35>
        %281 = "ttir.reshape"(%arg13, %280) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16, #ttnn_layout5>, tensor<1x1x2880xbf16, #ttnn_layout35>) -> tensor<1x1x2880xbf16, #ttnn_layout35>
        %282 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %283 = "ttir.add"(%279, %281, %282) : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x1x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %284 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %285 = "ttir.add"(%33, %283, %284) : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %286 = ttir.empty() : tensor<1x1x1xbf16, #ttnn_layout28>
        %287 = "ttir.reshape"(%arg29, %286) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1xbf16, #ttnn_layout28>) -> tensor<1x1x1xbf16, #ttnn_layout28>
        %288 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %289 = "ttir.broadcast"(%287, %288) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16, #ttnn_layout28>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %290 = ttir.empty() : tensor<2880xf32, #ttnn_layout31>
        %291 = "ttir.typecast"(%arg21, %290) <{conservative_folding = false}> : (tensor<2880xbf16, #ttnn_layout5>, tensor<2880xf32, #ttnn_layout31>) -> tensor<2880xf32, #ttnn_layout31>
        %292 = ttir.empty() : tensor<1x2880xf32, #ttnn_layout32>
        %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32, #ttnn_layout31>, tensor<1x2880xf32, #ttnn_layout32>) -> tensor<1x2880xf32, #ttnn_layout32>
        %294 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %295 = "ttir.typecast"(%285, %294) <{conservative_folding = false}> : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %296 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %297 = "ttir.pow"(%295, %19, %296) : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x1x1xf32, #ttnn_layout23>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %298 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %299 = "ttir.sum"(%297, %298) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %300 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %301 = "ttir.multiply"(%299, %4, %300) : (tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %302 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %303 = "ttir.reshape"(%301, %302) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %304 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %305 = "ttir.add"(%303, %45, %304) : (tensor<13x1xf32, #ttnn_layout26>, tensor<1x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %306 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %307 = "ttir.rsqrt"(%305, %306) : (tensor<13x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %308 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %309 = "ttir.reshape"(%295, %308) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %310 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %311 = "ttir.multiply"(%309, %307, %310) : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x1xf32, #ttnn_layout26>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %312 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %313 = "ttir.multiply"(%293, %311, %312) : (tensor<1x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %314 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %315 = "ttir.typecast"(%313, %314) <{conservative_folding = false}> : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %316 = ttir.empty() : tensor<416x2880xbf16, #ttnn_layout58>
        %317 = "ttir.concat"(%315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %316) <{dim = 0 : si32}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<416x2880xbf16, #ttnn_layout58>) -> tensor<416x2880xbf16, #ttnn_layout58>
        %318 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %319 = "ttir.reshape"(%317, %318) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16, #ttnn_layout58>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %320 = ttir.empty() : tensor<32x13x5760xbf16, #ttnn_layout59>
        %321 = "ttir.matmul"(%319, %arg28, %320) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x2880x5760xbf16, #ttnn_layout17>, tensor<32x13x5760xbf16, #ttnn_layout59>) -> tensor<32x13x5760xbf16, #ttnn_layout59>
        %322 = ttir.empty() : tensor<32x1x5760xbf16, #ttnn_layout59>
        %323 = "ttir.reshape"(%arg27, %322) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16, #ttnn_layout16>, tensor<32x1x5760xbf16, #ttnn_layout59>) -> tensor<32x1x5760xbf16, #ttnn_layout59>
        %324 = ttir.empty() : tensor<32x13x5760xbf16, #ttnn_layout59>
        %325 = "ttir.add"(%321, %323, %324) : (tensor<32x13x5760xbf16, #ttnn_layout59>, tensor<32x1x5760xbf16, #ttnn_layout59>, tensor<32x13x5760xbf16, #ttnn_layout59>) -> tensor<32x13x5760xbf16, #ttnn_layout59>
        %326 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %327 = "ttir.slice_static"(%325, %326) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16, #ttnn_layout59>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %328 = ttir.empty() : tensor<1x1x1xbf16, #ttnn_layout28>
        %329 = "ttir.reshape"(%arg25, %328) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1xbf16, #ttnn_layout28>) -> tensor<1x1x1xbf16, #ttnn_layout28>
        %330 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %331 = "ttir.broadcast"(%329, %330) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16, #ttnn_layout28>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %332 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %333 = "ttir.clamp_tensor"(%327, %289, %331, %332) : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %334 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %335 = "ttir.add"(%333, %13, %334) : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<1x1x1xbf16, #ttnn_layout28>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %336 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %337 = "ttir.typecast"(%335, %336) <{conservative_folding = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %338 = ttir.empty() : tensor<1x1x1xbf16, #ttnn_layout28>
        %339 = "ttir.reshape"(%arg26, %338) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1xbf16, #ttnn_layout28>) -> tensor<1x1x1xbf16, #ttnn_layout28>
        %340 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %341 = "ttir.broadcast"(%339, %340) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16, #ttnn_layout28>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %342 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %343 = "ttir.slice_static"(%325, %342) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16, #ttnn_layout59>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %344 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %345 = "ttir.clamp_tensor"(%343, %341, %331, %344) : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %346 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %347 = "ttir.typecast"(%345, %346) <{conservative_folding = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %348 = ttir.empty() : tensor<1x1x1xf32, #ttnn_layout23>
        %349 = "ttir.reshape"(%arg24, %348) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1x1xf32, #ttnn_layout23>) -> tensor<1x1x1xf32, #ttnn_layout23>
        %350 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %351 = "ttir.multiply"(%347, %349, %350) : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<1x1x1xf32, #ttnn_layout23>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %352 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %353 = "ttir.typecast"(%351, %352) <{conservative_folding = false}> : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %354 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %355 = "ttir.sigmoid"(%353, %354) : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %356 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %357 = "ttir.typecast"(%355, %356) <{conservative_folding = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %358 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %359 = "ttir.multiply"(%347, %357, %358) : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %360 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %361 = "ttir.multiply"(%337, %359, %360) : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %362 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %363 = "ttir.typecast"(%361, %362) <{conservative_folding = false}> : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %364 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %365 = "ttir.matmul"(%363, %arg23, %364) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x2880x2880xbf16, #ttnn_layout15>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %366 = ttir.empty() : tensor<32x1x1x2880xbf16, #ttnn_layout61>
        %367 = "ttir.reshape"(%arg22, %366) <{shape = [32 : i32, 1 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16, #ttnn_layout8>, tensor<32x1x1x2880xbf16, #ttnn_layout61>) -> tensor<32x1x1x2880xbf16, #ttnn_layout61>
        %368 = ttir.empty() : tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %369 = "ttir.reshape"(%365, %368) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x1x13x2880xbf16, #ttnn_layout61>) -> tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %370 = ttir.empty() : tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %371 = "ttir.add"(%369, %367, %370) : (tensor<32x1x13x2880xbf16, #ttnn_layout61>, tensor<32x1x1x2880xbf16, #ttnn_layout61>, tensor<32x1x13x2880xbf16, #ttnn_layout61>) -> tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %372 = ttir.empty() : tensor<32x1x13x2880xf32, #ttnn_layout62>
        %373 = "ttir.typecast"(%371, %372) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16, #ttnn_layout61>, tensor<32x1x13x2880xf32, #ttnn_layout62>) -> tensor<32x1x13x2880xf32, #ttnn_layout62>
        %374 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32, #ttnn_layout24>
        %375 = ttir.empty() : tensor<13x1x1xsi32, #ttnn_layout63>
        %376 = "ttir.reshape"(%374, %375) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32, #ttnn_layout24>, tensor<13x1x1xsi32, #ttnn_layout63>) -> tensor<13x1x1xsi32, #ttnn_layout63>
        %377 = ttir.empty() : tensor<13x4x1xsi32, #ttnn_layout63>
        %378 = "ttir.broadcast"(%376, %377) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32, #ttnn_layout63>, tensor<13x4x1xsi32, #ttnn_layout63>) -> tensor<13x4x1xsi32, #ttnn_layout63>
        %379 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %380 = "ttir.matmul"(%315, %arg12, %379) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<32x2880xbf16, #ttnn_layout8>, tensor<13x32xbf16, #ttnn_layout27>) -> tensor<13x32xbf16, #ttnn_layout27>
        %381 = ttir.empty() : tensor<1x32xbf16, #ttnn_layout27>
        %382 = "ttir.reshape"(%arg11, %381) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16, #ttnn_layout7>, tensor<1x32xbf16, #ttnn_layout27>) -> tensor<1x32xbf16, #ttnn_layout27>
        %383 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %384 = "ttir.add"(%380, %382, %383) : (tensor<13x32xbf16, #ttnn_layout27>, tensor<1x32xbf16, #ttnn_layout27>, tensor<13x32xbf16, #ttnn_layout27>) -> tensor<13x32xbf16, #ttnn_layout27>
        %385 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %386 = ttir.empty() : tensor<13x32xsi32, #ttnn_layout3>
        %values, %indices = "ttir.sort"(%384, %385, %386) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x32xbf16, #ttnn_layout27>, tensor<13x32xsi32, #ttnn_layout3>) -> (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x32xsi32, #ttnn_layout3>)
        %387 = ttir.empty() : tensor<13x4xsi32, #ttnn_layout3>
        %388 = "ttir.slice_static"(%indices, %387) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32, #ttnn_layout3>, tensor<13x4xsi32, #ttnn_layout3>) -> tensor<13x4xsi32, #ttnn_layout3>
        %389 = ttir.empty() : tensor<13x4x1xsi32, #ttnn_layout63>
        %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32, #ttnn_layout3>, tensor<13x4x1xsi32, #ttnn_layout63>) -> tensor<13x4x1xsi32, #ttnn_layout63>
        %391 = ttir.empty() : tensor<13x4x2xsi32, #ttnn_layout63>
        %392 = "ttir.concat"(%378, %390, %391) <{dim = 2 : si32}> : (tensor<13x4x1xsi32, #ttnn_layout63>, tensor<13x4x1xsi32, #ttnn_layout63>, tensor<13x4x2xsi32, #ttnn_layout63>) -> tensor<13x4x2xsi32, #ttnn_layout63>
        %393 = ttir.empty() : tensor<13x4xbf16, #ttnn_layout27>
        %394 = "ttir.slice_static"(%values, %393) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x4xbf16, #ttnn_layout27>) -> tensor<13x4xbf16, #ttnn_layout27>
        %395 = ttir.empty() : tensor<13x4xbf16, #ttnn_layout27>
        %396 = "ttir.softmax"(%394, %395) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16, #ttnn_layout27>, tensor<13x4xbf16, #ttnn_layout27>) -> tensor<13x4xbf16, #ttnn_layout27>
        %397 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %398 = "ttir.scatter"(%11, %392, %396, %397) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x4x2xsi32, #ttnn_layout63>, tensor<13x4xbf16, #ttnn_layout27>, tensor<13x32xbf16, #ttnn_layout27>) -> tensor<13x32xbf16, #ttnn_layout27>
        %399 = ttir.empty() : tensor<13x32xf32, #ttnn_layout26>
        %400 = "ttir.typecast"(%398, %399) <{conservative_folding = false}> : (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x32xf32, #ttnn_layout26>) -> tensor<13x32xf32, #ttnn_layout26>
        %401 = ttir.empty() : tensor<32x13xf32, #ttnn_layout26>
        %402 = "ttir.permute"(%400, %401) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xf32, #ttnn_layout26>, tensor<32x13xf32, #ttnn_layout26>) -> tensor<32x13xf32, #ttnn_layout26>
        %403 = ttir.empty() : tensor<32x1x13x1xf32, #ttnn_layout64>
        %404 = "ttir.reshape"(%402, %403) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xf32, #ttnn_layout26>, tensor<32x1x13x1xf32, #ttnn_layout64>) -> tensor<32x1x13x1xf32, #ttnn_layout64>
        %405 = ttir.empty() : tensor<32x1x13x2880xf32, #ttnn_layout62>
        %406 = "ttir.multiply"(%373, %404, %405) : (tensor<32x1x13x2880xf32, #ttnn_layout62>, tensor<32x1x13x1xf32, #ttnn_layout64>, tensor<32x1x13x2880xf32, #ttnn_layout62>) -> tensor<32x1x13x2880xf32, #ttnn_layout62>
        %407 = ttir.empty() : tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %408 = "ttir.typecast"(%406, %407) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32, #ttnn_layout62>, tensor<32x1x13x2880xbf16, #ttnn_layout61>) -> tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %409 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %410 = "ttir.sum"(%408, %409) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16, #ttnn_layout61>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %411 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %412 = "ttir.add"(%285, %410, %411) : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %413 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %414 = "ttir.typecast"(%412, %413) <{conservative_folding = false}> : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %415 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %416 = "ttir.pow"(%414, %19, %415) : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x1x1xf32, #ttnn_layout23>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %417 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %418 = "ttir.sum"(%416, %417) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %419 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %420 = "ttir.multiply"(%418, %4, %419) : (tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %421 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %422 = "ttir.reshape"(%420, %421) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %423 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %424 = "ttir.add"(%422, %45, %423) : (tensor<13x1xf32, #ttnn_layout26>, tensor<1x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %425 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %426 = "ttir.rsqrt"(%424, %425) : (tensor<13x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %427 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %428 = "ttir.reshape"(%414, %427) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %429 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %430 = "ttir.multiply"(%428, %426, %429) : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x1xf32, #ttnn_layout26>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %431 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %432 = "ttir.multiply"(%255, %430, %431) : (tensor<1x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %433 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %434 = "ttir.typecast"(%432, %433) <{conservative_folding = false}> : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %435 = ttir.empty() : tensor<13x201088xbf16, #ttnn_layout21>
        %436 = "ttir.matmul"(%434, %arg10, %435) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<201088x2880xbf16, #ttnn_layout4>, tensor<13x201088xbf16, #ttnn_layout21>) -> tensor<13x201088xbf16, #ttnn_layout21>
        %437 = ttir.empty() : tensor<1x13x201088xbf16, #ttnn_layout22>
        %438 = "ttir.reshape"(%436, %437) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16, #ttnn_layout21>, tensor<1x13x201088xbf16, #ttnn_layout22>) -> tensor<1x13x201088xbf16, #ttnn_layout22>
        return %247, %249, %251, %177, %436, %438 : tensor<1x13x512xbf16, #ttnn_layout18>, tensor<1x13x8x64xbf16, #ttnn_layout19>, tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<13x201088xbf16, #ttnn_layout21>, tensor<1x13x201088xbf16, #ttnn_layout22>
      }
    }
  }
}


// -----// IR Dump Before ConvertTTIRToTTNN (convert-ttir-to-ttnn) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<6284x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<90x128x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 2880 + d1, d2), <1x1>, memref<2880x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x180x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 2880 + d1, d2), <1x1>, memref<2880x180x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 416 + d1 * 32 + d2, d3), <1x1>, memref<13x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x6284x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x6284x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, u8>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, u8>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 * 64 + d2, d3), <1x1>, memref<2x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 64 + d2, d3), <1x1>, memref<26x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<64x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout44 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout45 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<64x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout46 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<64x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout47 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout48 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout49 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout50 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout51 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 256 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout52 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 2048 + d1 * 256 + d2 * 32 + d3, d4), <1x1>, memref<64x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout53 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 64 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout54 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout55 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout56 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout57 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout58 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<13x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout59 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x180x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout60 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout61 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<32x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout62 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<32x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout63 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<13x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout64 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16, #ttnn_layout1> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32, #ttnn_layout2> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32, #ttnn_layout3> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16, #ttnn_layout4> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16, #ttnn_layout5> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32, #ttnn_layout2> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32, #ttnn_layout6> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16, #ttnn_layout> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16, #ttnn_layout1> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16, #ttnn_layout4> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16, #ttnn_layout7> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16, #ttnn_layout8> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16, #ttnn_layout5> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16, #ttnn_layout9> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16, #ttnn_layout10> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16, #ttnn_layout11> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32, #ttnn_layout12> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32, #ttnn_layout2> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16, #ttnn_layout13> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16, #ttnn_layout14> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16, #ttnn_layout5> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16, #ttnn_layout8> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16, #ttnn_layout15> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32, #ttnn_layout2> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16, #ttnn_layout11> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16, #ttnn_layout11> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16, #ttnn_layout16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16, #ttnn_layout17> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16, #ttnn_layout11> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16, #ttnn_layout5> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16, #ttnn_layout18> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16, #ttnn_layout19> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16, #ttnn_layout20> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16, #ttnn_layout20> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16, #ttnn_layout21> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16, #ttnn_layout22> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32, #ttnn_layout23>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32, #ttnn_layout24>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8, #ttnn_layout25>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32, #ttnn_layout2>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32, #ttnn_layout26>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8, #ttnn_layout25>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn_layout11>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn_layout11>
        %8 = ttir.empty() : tensor<1x1xbf16, #ttnn_layout27>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1xbf16, #ttnn_layout27>) -> tensor<1x1xbf16, #ttnn_layout27>
        %10 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16, #ttnn_layout27>, tensor<13x32xbf16, #ttnn_layout27>) -> tensor<13x32xbf16, #ttnn_layout27>
        %12 = ttir.empty() : tensor<1x1x1xbf16, #ttnn_layout28>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1xbf16, #ttnn_layout28>) -> tensor<1x1x1xbf16, #ttnn_layout28>
        %14 = ttir.empty() : tensor<1x1x1x1xbf16, #ttnn_layout29>
        %15 = "ttir.reshape"(%7, %14) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1x1xbf16, #ttnn_layout29>) -> tensor<1x1x1x1xbf16, #ttnn_layout29>
        %16 = ttir.empty() : tensor<1x1x1x1xui8, #ttnn_layout30>
        %17 = "ttir.reshape"(%5, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8, #ttnn_layout25>, tensor<1x1x1x1xui8, #ttnn_layout30>) -> tensor<1x1x1x1xui8, #ttnn_layout30>
        %18 = ttir.empty() : tensor<1x1x1xf32, #ttnn_layout23>
        %19 = "ttir.reshape"(%3, %18) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1x1xf32, #ttnn_layout23>) -> tensor<1x1x1xf32, #ttnn_layout23>
        %20 = ttir.empty() : tensor<1x1x1x1xui8, #ttnn_layout30>
        %21 = "ttir.reshape"(%2, %20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8, #ttnn_layout25>, tensor<1x1x1x1xui8, #ttnn_layout30>) -> tensor<1x1x1x1xui8, #ttnn_layout30>
        %22 = ttir.empty() : tensor<2880xf32, #ttnn_layout31>
        %23 = "ttir.typecast"(%arg5, %22) <{conservative_folding = false}> : (tensor<2880xbf16, #ttnn_layout5>, tensor<2880xf32, #ttnn_layout31>) -> tensor<2880xf32, #ttnn_layout31>
        %24 = ttir.empty() : tensor<1x2880xf32, #ttnn_layout32>
        %25 = "ttir.reshape"(%23, %24) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32, #ttnn_layout31>, tensor<1x2880xf32, #ttnn_layout32>) -> tensor<1x2880xf32, #ttnn_layout32>
        %26 = ttir.empty() : tensor<1x13xui32, #ttnn_layout33>
        %27 = "ttir.typecast"(%arg3, %26) <{conservative_folding = false}> : (tensor<1x13xsi32, #ttnn_layout3>, tensor<1x13xui32, #ttnn_layout33>) -> tensor<1x13xui32, #ttnn_layout33>
        %28 = ttir.empty() : tensor<13xui32, #ttnn_layout34>
        %29 = "ttir.reshape"(%27, %28) <{shape = [13 : i32]}> : (tensor<1x13xui32, #ttnn_layout33>, tensor<13xui32, #ttnn_layout34>) -> tensor<13xui32, #ttnn_layout34>
        %30 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %31 = "ttir.embedding"(%29, %arg4, %30) : (tensor<13xui32, #ttnn_layout34>, tensor<201088x2880xbf16, #ttnn_layout4>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %32 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %34 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %35 = "ttir.typecast"(%33, %34) <{conservative_folding = false}> : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %36 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %37 = "ttir.pow"(%35, %19, %36) : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x1x1xf32, #ttnn_layout23>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %38 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %39 = "ttir.sum"(%37, %38) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %40 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %41 = "ttir.multiply"(%39, %4, %40) : (tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %42 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %43 = "ttir.reshape"(%41, %42) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %44 = ttir.empty() : tensor<1x1xf32, #ttnn_layout26>
        %45 = "ttir.reshape"(%arg2, %44) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1xf32, #ttnn_layout26>) -> tensor<1x1xf32, #ttnn_layout26>
        %46 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %47 = "ttir.add"(%43, %45, %46) : (tensor<13x1xf32, #ttnn_layout26>, tensor<1x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %48 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %49 = "ttir.rsqrt"(%47, %48) : (tensor<13x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %50 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %51 = "ttir.reshape"(%35, %50) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %52 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %53 = "ttir.multiply"(%51, %49, %52) : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x1xf32, #ttnn_layout26>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %54 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %55 = "ttir.multiply"(%25, %53, %54) : (tensor<1x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %56 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %57 = "ttir.typecast"(%55, %56) <{conservative_folding = false}> : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %58 = ttir.empty() : tensor<13x4096xbf16, #ttnn_layout37>
        %59 = "ttir.matmul"(%57, %arg20, %58) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<4096x2880xbf16, #ttnn_layout14>, tensor<13x4096xbf16, #ttnn_layout37>) -> tensor<13x4096xbf16, #ttnn_layout37>
        %60 = ttir.empty() : tensor<1x1x64x64xbf16, #ttnn_layout38>
        %61 = "ttir.reshape"(%arg19, %60) <{shape = [1 : i32, 1 : i32, 64 : i32, 64 : i32]}> : (tensor<4096xbf16, #ttnn_layout13>, tensor<1x1x64x64xbf16, #ttnn_layout38>) -> tensor<1x1x64x64xbf16, #ttnn_layout38>
        %62 = ttir.empty() : tensor<1x64x1x64xbf16, #ttnn_layout39>
        %63 = "ttir.permute"(%61, %62) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x64x64xbf16, #ttnn_layout38>, tensor<1x64x1x64xbf16, #ttnn_layout39>) -> tensor<1x64x1x64xbf16, #ttnn_layout39>
        %64 = ttir.empty() : tensor<1x13x64x64xbf16, #ttnn_layout40>
        %65 = "ttir.reshape"(%59, %64) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<13x4096xbf16, #ttnn_layout37>, tensor<1x13x64x64xbf16, #ttnn_layout40>) -> tensor<1x13x64x64xbf16, #ttnn_layout40>
        %66 = ttir.empty() : tensor<1x64x13x64xbf16, #ttnn_layout39>
        %67 = "ttir.permute"(%65, %66) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16, #ttnn_layout40>, tensor<1x64x13x64xbf16, #ttnn_layout39>) -> tensor<1x64x13x64xbf16, #ttnn_layout39>
        %68 = ttir.empty() : tensor<1x64x13x64xbf16, #ttnn_layout39>
        %69 = "ttir.add"(%67, %63, %68) : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x64x1x64xbf16, #ttnn_layout39>, tensor<1x64x13x64xbf16, #ttnn_layout39>) -> tensor<1x64x13x64xbf16, #ttnn_layout39>
        %70 = ttir.empty() : tensor<1x64x13x32xbf16, #ttnn_layout41>
        %71 = "ttir.slice_static"(%69, %70) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x64x13x32xbf16, #ttnn_layout41>) -> tensor<1x64x13x32xbf16, #ttnn_layout41>
        %72 = ttir.empty() : tensor<1x64x13x32xf32, #ttnn_layout42>
        %73 = "ttir.typecast"(%71, %72) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16, #ttnn_layout41>, tensor<1x64x13x32xf32, #ttnn_layout42>) -> tensor<1x64x13x32xf32, #ttnn_layout42>
        %74 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %75 = "ttir.reshape"(%73, %74) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32, #ttnn_layout42>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %76 = ttir.empty() : tensor<1x32x1xf32, #ttnn_layout23>
        %77 = "ttir.reshape"(%arg7, %76) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32, #ttnn_layout6>, tensor<1x32x1xf32, #ttnn_layout23>) -> tensor<1x32x1xf32, #ttnn_layout23>
        %78 = ttir.empty() : tensor<1x32x13xf32, #ttnn_layout23>
        %79 = "ttir.matmul"(%77, %0, %78) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32, #ttnn_layout23>, tensor<1x1x13xf32, #ttnn_layout23>, tensor<1x32x13xf32, #ttnn_layout23>) -> tensor<1x32x13xf32, #ttnn_layout23>
        %80 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %81 = "ttir.permute"(%79, %80) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32, #ttnn_layout23>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %82 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %83 = "ttir.cos"(%81, %82) : (tensor<1x13x32xf32, #ttnn_layout23>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %84 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %85 = "ttir.reshape"(%83, %84) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32, #ttnn_layout23>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %86 = ttir.empty() : tensor<1x1x1x1xf32, #ttnn_layout44>
        %87 = "ttir.reshape"(%arg6, %86) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1x1x1xf32, #ttnn_layout44>) -> tensor<1x1x1x1xf32, #ttnn_layout44>
        %88 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %89 = "ttir.multiply"(%85, %87, %88) : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x1x1x1xf32, #ttnn_layout44>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %90 = ttir.empty() : tensor<1x1x13x32xbf16, #ttnn_layout29>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x1x13x32xbf16, #ttnn_layout29>) -> tensor<1x1x13x32xbf16, #ttnn_layout29>
        %92 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %93 = "ttir.typecast"(%91, %92) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16, #ttnn_layout29>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %94 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %95 = "ttir.reshape"(%93, %94) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %96 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %97 = "ttir.multiply"(%75, %95, %96) : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<1x13x32xf32, #ttnn_layout23>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %98 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %99 = "ttir.typecast"(%97, %98) <{conservative_folding = false}> : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %100 = ttir.empty() : tensor<1x64x13x32xbf16, #ttnn_layout41>
        %101 = "ttir.slice_static"(%69, %100) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x64x13x32xbf16, #ttnn_layout41>) -> tensor<1x64x13x32xbf16, #ttnn_layout41>
        %102 = ttir.empty() : tensor<1x64x13x32xf32, #ttnn_layout42>
        %103 = "ttir.typecast"(%101, %102) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16, #ttnn_layout41>, tensor<1x64x13x32xf32, #ttnn_layout42>) -> tensor<1x64x13x32xf32, #ttnn_layout42>
        %104 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %105 = "ttir.reshape"(%103, %104) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32, #ttnn_layout42>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %106 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %107 = "ttir.sin"(%81, %106) : (tensor<1x13x32xf32, #ttnn_layout23>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %108 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32, #ttnn_layout23>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %110 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %111 = "ttir.multiply"(%109, %87, %110) : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x1x1x1xf32, #ttnn_layout44>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %112 = ttir.empty() : tensor<1x1x13x32xbf16, #ttnn_layout29>
        %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x1x13x32xbf16, #ttnn_layout29>) -> tensor<1x1x13x32xbf16, #ttnn_layout29>
        %114 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %115 = "ttir.typecast"(%113, %114) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16, #ttnn_layout29>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %116 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %117 = "ttir.reshape"(%115, %116) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %118 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %119 = "ttir.multiply"(%105, %117, %118) : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<1x13x32xf32, #ttnn_layout23>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %120 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %122 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %123 = "ttir.subtract"(%99, %121, %122) : (tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %124 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %125 = "ttir.multiply"(%105, %95, %124) : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<1x13x32xf32, #ttnn_layout23>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %126 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %128 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %129 = "ttir.multiply"(%75, %117, %128) : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<1x13x32xf32, #ttnn_layout23>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %130 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %132 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %133 = "ttir.add"(%127, %131, %132) : (tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %134 = ttir.empty() : tensor<64x13x64xbf16, #ttnn_layout46>
        %135 = "ttir.concat"(%123, %133, %134) <{dim = 2 : si32}> : (tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x64xbf16, #ttnn_layout46>) -> tensor<64x13x64xbf16, #ttnn_layout46>
        %136 = ttir.empty() : tensor<13x512xbf16, #ttnn_layout47>
        %137 = "ttir.matmul"(%57, %arg9, %136) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<512x2880xbf16, #ttnn_layout1>, tensor<13x512xbf16, #ttnn_layout47>) -> tensor<13x512xbf16, #ttnn_layout47>
        %138 = ttir.empty() : tensor<1x1x8x64xbf16, #ttnn_layout48>
        %139 = "ttir.reshape"(%arg8, %138) <{shape = [1 : i32, 1 : i32, 8 : i32, 64 : i32]}> : (tensor<512xbf16, #ttnn_layout>, tensor<1x1x8x64xbf16, #ttnn_layout48>) -> tensor<1x1x8x64xbf16, #ttnn_layout48>
        %140 = ttir.empty() : tensor<1x8x1x64xbf16, #ttnn_layout20>
        %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x8x64xbf16, #ttnn_layout48>, tensor<1x8x1x64xbf16, #ttnn_layout20>) -> tensor<1x8x1x64xbf16, #ttnn_layout20>
        %142 = ttir.empty() : tensor<1x13x8x64xbf16, #ttnn_layout19>
        %143 = "ttir.reshape"(%137, %142) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<13x512xbf16, #ttnn_layout47>, tensor<1x13x8x64xbf16, #ttnn_layout19>) -> tensor<1x13x8x64xbf16, #ttnn_layout19>
        %144 = ttir.empty() : tensor<1x8x13x64xbf16, #ttnn_layout20>
        %145 = "ttir.permute"(%143, %144) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16, #ttnn_layout19>, tensor<1x8x13x64xbf16, #ttnn_layout20>) -> tensor<1x8x13x64xbf16, #ttnn_layout20>
        %146 = ttir.empty() : tensor<1x8x13x64xbf16, #ttnn_layout20>
        %147 = "ttir.add"(%145, %141, %146) : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x1x64xbf16, #ttnn_layout20>, tensor<1x8x13x64xbf16, #ttnn_layout20>) -> tensor<1x8x13x64xbf16, #ttnn_layout20>
        %148 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %149 = "ttir.slice_static"(%147, %148) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %150 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %151 = "ttir.typecast"(%149, %150) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %152 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %153 = "ttir.multiply"(%151, %93, %152) : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %154 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %155 = "ttir.typecast"(%153, %154) <{conservative_folding = false}> : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %156 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %157 = "ttir.slice_static"(%147, %156) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %158 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %159 = "ttir.typecast"(%157, %158) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %160 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %161 = "ttir.multiply"(%159, %115, %160) : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %162 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %163 = "ttir.typecast"(%161, %162) <{conservative_folding = false}> : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %164 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %165 = "ttir.subtract"(%155, %163, %164) : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %166 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %167 = "ttir.multiply"(%159, %93, %166) : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %168 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %169 = "ttir.typecast"(%167, %168) <{conservative_folding = false}> : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %170 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %171 = "ttir.multiply"(%151, %115, %170) : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %172 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %173 = "ttir.typecast"(%171, %172) <{conservative_folding = false}> : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %174 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %175 = "ttir.add"(%169, %173, %174) : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %176 = ttir.empty() : tensor<1x8x13x64xbf16, #ttnn_layout20>
        %177 = "ttir.concat"(%165, %175, %176) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x64xbf16, #ttnn_layout20>) -> tensor<1x8x13x64xbf16, #ttnn_layout20>
        %178 = ttir.empty() : tensor<1x8x1x13x64xbf16, #ttnn_layout51>
        %179 = "ttir.reshape"(%177, %178) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x1x13x64xbf16, #ttnn_layout51>) -> tensor<1x8x1x13x64xbf16, #ttnn_layout51>
        %180 = ttir.empty() : tensor<1x8x8x13x64xbf16, #ttnn_layout52>
        %181 = "ttir.broadcast"(%179, %180) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16, #ttnn_layout51>, tensor<1x8x8x13x64xbf16, #ttnn_layout52>) -> tensor<1x8x8x13x64xbf16, #ttnn_layout52>
        %182 = ttir.empty() : tensor<1x64x13x64xbf16, #ttnn_layout39>
        %183 = "ttir.reshape"(%181, %182) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16, #ttnn_layout52>, tensor<1x64x13x64xbf16, #ttnn_layout39>) -> tensor<1x64x13x64xbf16, #ttnn_layout39>
        %184 = ttir.empty() : tensor<1x64x64x13xbf16, #ttnn_layout53>
        %185 = "ttir.permute"(%183, %184) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x64x64x13xbf16, #ttnn_layout53>) -> tensor<1x64x64x13xbf16, #ttnn_layout53>
        %186 = ttir.empty() : tensor<64x64x13xbf16, #ttnn_layout54>
        %187 = "ttir.reshape"(%185, %186) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16, #ttnn_layout53>, tensor<64x64x13xbf16, #ttnn_layout54>) -> tensor<64x64x13xbf16, #ttnn_layout54>
        %188 = ttir.empty() : tensor<64x13x13xbf16, #ttnn_layout45>
        %189 = "ttir.matmul"(%135, %187, %188) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16, #ttnn_layout46>, tensor<64x64x13xbf16, #ttnn_layout54>, tensor<64x13x13xbf16, #ttnn_layout45>) -> tensor<64x13x13xbf16, #ttnn_layout45>
        %190 = ttir.empty() : tensor<64x13x13xf32, #ttnn_layout43>
        %191 = "ttir.typecast"(%189, %190) <{conservative_folding = false}> : (tensor<64x13x13xbf16, #ttnn_layout45>, tensor<64x13x13xf32, #ttnn_layout43>) -> tensor<64x13x13xf32, #ttnn_layout43>
        %192 = ttir.empty() : tensor<1x64x13x13xf32, #ttnn_layout42>
        %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xf32, #ttnn_layout43>, tensor<1x64x13x13xf32, #ttnn_layout42>) -> tensor<1x64x13x13xf32, #ttnn_layout42>
        %194 = ttir.empty() : tensor<1x1x1x1xf32, #ttnn_layout44>
        %195 = "ttir.reshape"(%arg18, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1x1x1xf32, #ttnn_layout44>) -> tensor<1x1x1x1xf32, #ttnn_layout44>
        %196 = ttir.empty() : tensor<1x64x13x13xf32, #ttnn_layout42>
        %197 = "ttir.multiply"(%193, %195, %196) : (tensor<1x64x13x13xf32, #ttnn_layout42>, tensor<1x1x1x1xf32, #ttnn_layout44>, tensor<1x64x13x13xf32, #ttnn_layout42>) -> tensor<1x64x13x13xf32, #ttnn_layout42>
        %198 = ttir.empty() : tensor<1x64x13x13xbf16, #ttnn_layout41>
        %199 = "ttir.typecast"(%197, %198) <{conservative_folding = false}> : (tensor<1x64x13x13xf32, #ttnn_layout42>, tensor<1x64x13x13xbf16, #ttnn_layout41>) -> tensor<1x64x13x13xbf16, #ttnn_layout41>
        %200 = ttir.empty() : tensor<1x1x1x13xsi32, #ttnn_layout55>
        %201 = "ttir.reshape"(%1, %200) <{shape = [1 : i32, 1 : i32, 1 : i32, 13 : i32]}> : (tensor<13xsi32, #ttnn_layout24>, tensor<1x1x1x13xsi32, #ttnn_layout55>) -> tensor<1x1x1x13xsi32, #ttnn_layout55>
        %202 = ttir.empty() : tensor<1x1x1x1xsi32, #ttnn_layout55>
        %203 = "ttir.reshape"(%arg17, %202) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<si32, #ttnn_layout12>, tensor<1x1x1x1xsi32, #ttnn_layout55>) -> tensor<1x1x1x1xsi32, #ttnn_layout55>
        %204 = ttir.empty() : tensor<1x1x13x1xsi32, #ttnn_layout55>
        %205 = "ttir.reshape"(%1, %204) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32, #ttnn_layout24>, tensor<1x1x13x1xsi32, #ttnn_layout55>) -> tensor<1x1x13x1xsi32, #ttnn_layout55>
        %206 = ttir.empty() : tensor<1x1x13x1xsi32, #ttnn_layout55>
        %207 = "ttir.subtract"(%205, %203, %206) : (tensor<1x1x13x1xsi32, #ttnn_layout55>, tensor<1x1x1x1xsi32, #ttnn_layout55>, tensor<1x1x13x1xsi32, #ttnn_layout55>) -> tensor<1x1x13x1xsi32, #ttnn_layout55>
        %208 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %209 = "ttir.gt"(%201, %207, %208) : (tensor<1x1x1x13xsi32, #ttnn_layout55>, tensor<1x1x13x1xsi32, #ttnn_layout55>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %210 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %211 = "ttir.typecast"(%209, %210) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %212 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %213 = "ttir.bitwise_and"(%211, %17, %212) : (tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x1x1xui8, #ttnn_layout30>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %214 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %215 = "ttir.ne"(%213, %21, %214) : (tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x1x1xui8, #ttnn_layout30>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %216 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %218 = ttir.empty() : tensor<1x1x13x1xsi32, #ttnn_layout55>
        %219 = "ttir.reshape"(%1, %218) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32, #ttnn_layout24>, tensor<1x1x13x1xsi32, #ttnn_layout55>) -> tensor<1x1x13x1xsi32, #ttnn_layout55>
        %220 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %221 = "ttir.le"(%201, %219, %220) : (tensor<1x1x1x13xsi32, #ttnn_layout55>, tensor<1x1x13x1xsi32, #ttnn_layout55>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %222 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %223 = "ttir.typecast"(%221, %222) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %224 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %225 = "ttir.bitwise_and"(%217, %223, %224) : (tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %226 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %227 = "ttir.ne"(%225, %21, %226) : (tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x1x1xui8, #ttnn_layout30>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %228 = ttir.empty() : tensor<1x1x1x1xbf16, #ttnn_layout29>
        %229 = "ttir.reshape"(%arg16, %228) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1x1xbf16, #ttnn_layout29>) -> tensor<1x1x1x1xbf16, #ttnn_layout29>
        %230 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %231 = "ttir.where"(%227, %15, %229, %230) : (tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x1x1x1xbf16, #ttnn_layout29>, tensor<1x1x1x1xbf16, #ttnn_layout29>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %232 = ttir.empty() : tensor<1x64x13x13xbf16, #ttnn_layout41>
        %233 = "ttir.add"(%199, %231, %232) : (tensor<1x64x13x13xbf16, #ttnn_layout41>, tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x64x13x13xbf16, #ttnn_layout41>) -> tensor<1x64x13x13xbf16, #ttnn_layout41>
        %234 = ttir.empty() : tensor<1x64x1x1xbf16, #ttnn_layout41>
        %235 = "ttir.reshape"(%arg15, %234) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout10>, tensor<1x64x1x1xbf16, #ttnn_layout41>) -> tensor<1x64x1x1xbf16, #ttnn_layout41>
        %236 = ttir.empty() : tensor<1x64x13x1xbf16, #ttnn_layout41>
        %237 = "ttir.broadcast"(%235, %236) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout41>, tensor<1x64x13x1xbf16, #ttnn_layout41>) -> tensor<1x64x13x1xbf16, #ttnn_layout41>
        %238 = ttir.empty() : tensor<1x64x13x14xbf16, #ttnn_layout41>
        %239 = "ttir.concat"(%233, %237, %238) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16, #ttnn_layout41>, tensor<1x64x13x1xbf16, #ttnn_layout41>, tensor<1x64x13x14xbf16, #ttnn_layout41>) -> tensor<1x64x13x14xbf16, #ttnn_layout41>
        %240 = ttir.empty() : tensor<13x512xbf16, #ttnn_layout47>
        %241 = "ttir.matmul"(%57, %arg1, %240) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<512x2880xbf16, #ttnn_layout1>, tensor<13x512xbf16, #ttnn_layout47>) -> tensor<13x512xbf16, #ttnn_layout47>
        %242 = ttir.empty() : tensor<1x13x512xbf16, #ttnn_layout18>
        %243 = "ttir.reshape"(%241, %242) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16, #ttnn_layout47>, tensor<1x13x512xbf16, #ttnn_layout18>) -> tensor<1x13x512xbf16, #ttnn_layout18>
        %244 = ttir.empty() : tensor<1x1x512xbf16, #ttnn_layout18>
        %245 = "ttir.reshape"(%arg0, %244) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout>, tensor<1x1x512xbf16, #ttnn_layout18>) -> tensor<1x1x512xbf16, #ttnn_layout18>
        %246 = ttir.empty() : tensor<1x13x512xbf16, #ttnn_layout18>
        %247 = "ttir.add"(%243, %245, %246) : (tensor<1x13x512xbf16, #ttnn_layout18>, tensor<1x1x512xbf16, #ttnn_layout18>, tensor<1x13x512xbf16, #ttnn_layout18>) -> tensor<1x13x512xbf16, #ttnn_layout18>
        %248 = ttir.empty() : tensor<1x13x8x64xbf16, #ttnn_layout19>
        %249 = "ttir.reshape"(%247, %248) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16, #ttnn_layout18>, tensor<1x13x8x64xbf16, #ttnn_layout19>) -> tensor<1x13x8x64xbf16, #ttnn_layout19>
        %250 = ttir.empty() : tensor<1x8x13x64xbf16, #ttnn_layout20>
        %251 = "ttir.permute"(%249, %250) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16, #ttnn_layout19>, tensor<1x8x13x64xbf16, #ttnn_layout20>) -> tensor<1x8x13x64xbf16, #ttnn_layout20>
        %252 = ttir.empty() : tensor<2880xf32, #ttnn_layout31>
        %253 = "ttir.typecast"(%arg30, %252) <{conservative_folding = false}> : (tensor<2880xbf16, #ttnn_layout5>, tensor<2880xf32, #ttnn_layout31>) -> tensor<2880xf32, #ttnn_layout31>
        %254 = ttir.empty() : tensor<1x2880xf32, #ttnn_layout32>
        %255 = "ttir.reshape"(%253, %254) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32, #ttnn_layout31>, tensor<1x2880xf32, #ttnn_layout32>) -> tensor<1x2880xf32, #ttnn_layout32>
        %256 = ttir.empty() : tensor<1x64x13x14xbf16, #ttnn_layout41>
        %257 = "ttir.softmax"(%239, %256) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16, #ttnn_layout41>, tensor<1x64x13x14xbf16, #ttnn_layout41>) -> tensor<1x64x13x14xbf16, #ttnn_layout41>
        %258 = ttir.empty() : tensor<1x64x13x13xbf16, #ttnn_layout41>
        %259 = "ttir.slice_static"(%257, %258) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16, #ttnn_layout41>, tensor<1x64x13x13xbf16, #ttnn_layout41>) -> tensor<1x64x13x13xbf16, #ttnn_layout41>
        %260 = ttir.empty() : tensor<64x13x13xbf16, #ttnn_layout45>
        %261 = "ttir.reshape"(%259, %260) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16, #ttnn_layout41>, tensor<64x13x13xbf16, #ttnn_layout45>) -> tensor<64x13x13xbf16, #ttnn_layout45>
        %262 = ttir.empty() : tensor<1x8x1x13x64xbf16, #ttnn_layout51>
        %263 = "ttir.reshape"(%251, %262) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x1x13x64xbf16, #ttnn_layout51>) -> tensor<1x8x1x13x64xbf16, #ttnn_layout51>
        %264 = ttir.empty() : tensor<1x8x8x13x64xbf16, #ttnn_layout52>
        %265 = "ttir.broadcast"(%263, %264) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16, #ttnn_layout51>, tensor<1x8x8x13x64xbf16, #ttnn_layout52>) -> tensor<1x8x8x13x64xbf16, #ttnn_layout52>
        %266 = ttir.empty() : tensor<64x13x64xbf16, #ttnn_layout46>
        %267 = "ttir.reshape"(%265, %266) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16, #ttnn_layout52>, tensor<64x13x64xbf16, #ttnn_layout46>) -> tensor<64x13x64xbf16, #ttnn_layout46>
        %268 = ttir.empty() : tensor<64x13x64xbf16, #ttnn_layout46>
        %269 = "ttir.matmul"(%261, %267, %268) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16, #ttnn_layout45>, tensor<64x13x64xbf16, #ttnn_layout46>, tensor<64x13x64xbf16, #ttnn_layout46>) -> tensor<64x13x64xbf16, #ttnn_layout46>
        %270 = ttir.empty() : tensor<1x64x13x64xbf16, #ttnn_layout39>
        %271 = "ttir.reshape"(%269, %270) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16, #ttnn_layout46>, tensor<1x64x13x64xbf16, #ttnn_layout39>) -> tensor<1x64x13x64xbf16, #ttnn_layout39>
        %272 = ttir.empty() : tensor<13x4096xbf16, #ttnn_layout37>
        %273 = ttir.empty() : tensor<1x13x4096xbf16, #ttnn_layout56>
        %274 = "ttir.concatenate_heads"(%271, %273) : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x13x4096xbf16, #ttnn_layout56>) -> tensor<1x13x4096xbf16, #ttnn_layout56>
        %275 = "ttir.reshape"(%274, %272) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16, #ttnn_layout56>, tensor<13x4096xbf16, #ttnn_layout37>) -> tensor<13x4096xbf16, #ttnn_layout37>
        %276 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %277 = "ttir.matmul"(%275, %arg14, %276) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16, #ttnn_layout37>, tensor<2880x4096xbf16, #ttnn_layout9>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %278 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %280 = ttir.empty() : tensor<1x1x2880xbf16, #ttnn_layout35>
        %281 = "ttir.reshape"(%arg13, %280) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16, #ttnn_layout5>, tensor<1x1x2880xbf16, #ttnn_layout35>) -> tensor<1x1x2880xbf16, #ttnn_layout35>
        %282 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %283 = "ttir.add"(%279, %281, %282) : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x1x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %284 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %285 = "ttir.add"(%33, %283, %284) : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %286 = ttir.empty() : tensor<1x1x1xbf16, #ttnn_layout28>
        %287 = "ttir.reshape"(%arg29, %286) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1xbf16, #ttnn_layout28>) -> tensor<1x1x1xbf16, #ttnn_layout28>
        %288 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %289 = "ttir.broadcast"(%287, %288) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16, #ttnn_layout28>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %290 = ttir.empty() : tensor<2880xf32, #ttnn_layout31>
        %291 = "ttir.typecast"(%arg21, %290) <{conservative_folding = false}> : (tensor<2880xbf16, #ttnn_layout5>, tensor<2880xf32, #ttnn_layout31>) -> tensor<2880xf32, #ttnn_layout31>
        %292 = ttir.empty() : tensor<1x2880xf32, #ttnn_layout32>
        %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32, #ttnn_layout31>, tensor<1x2880xf32, #ttnn_layout32>) -> tensor<1x2880xf32, #ttnn_layout32>
        %294 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %295 = "ttir.typecast"(%285, %294) <{conservative_folding = false}> : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %296 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %297 = "ttir.pow"(%295, %19, %296) : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x1x1xf32, #ttnn_layout23>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %298 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %299 = "ttir.sum"(%297, %298) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %300 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %301 = "ttir.multiply"(%299, %4, %300) : (tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %302 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %303 = "ttir.reshape"(%301, %302) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %304 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %305 = "ttir.add"(%303, %45, %304) : (tensor<13x1xf32, #ttnn_layout26>, tensor<1x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %306 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %307 = "ttir.rsqrt"(%305, %306) : (tensor<13x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %308 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %309 = "ttir.reshape"(%295, %308) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %310 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %311 = "ttir.multiply"(%309, %307, %310) : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x1xf32, #ttnn_layout26>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %312 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %313 = "ttir.multiply"(%293, %311, %312) : (tensor<1x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %314 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %315 = "ttir.typecast"(%313, %314) <{conservative_folding = false}> : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %316 = ttir.empty() : tensor<416x2880xbf16, #ttnn_layout58>
        %317 = "ttir.concat"(%315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %316) <{dim = 0 : si32}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<416x2880xbf16, #ttnn_layout58>) -> tensor<416x2880xbf16, #ttnn_layout58>
        %318 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %319 = "ttir.reshape"(%317, %318) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16, #ttnn_layout58>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %320 = ttir.empty() : tensor<32x13x5760xbf16, #ttnn_layout59>
        %321 = "ttir.matmul"(%319, %arg28, %320) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x2880x5760xbf16, #ttnn_layout17>, tensor<32x13x5760xbf16, #ttnn_layout59>) -> tensor<32x13x5760xbf16, #ttnn_layout59>
        %322 = ttir.empty() : tensor<32x1x5760xbf16, #ttnn_layout59>
        %323 = "ttir.reshape"(%arg27, %322) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16, #ttnn_layout16>, tensor<32x1x5760xbf16, #ttnn_layout59>) -> tensor<32x1x5760xbf16, #ttnn_layout59>
        %324 = ttir.empty() : tensor<32x13x5760xbf16, #ttnn_layout59>
        %325 = "ttir.add"(%321, %323, %324) : (tensor<32x13x5760xbf16, #ttnn_layout59>, tensor<32x1x5760xbf16, #ttnn_layout59>, tensor<32x13x5760xbf16, #ttnn_layout59>) -> tensor<32x13x5760xbf16, #ttnn_layout59>
        %326 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %327 = "ttir.slice_static"(%325, %326) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16, #ttnn_layout59>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %328 = ttir.empty() : tensor<1x1x1xbf16, #ttnn_layout28>
        %329 = "ttir.reshape"(%arg25, %328) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1xbf16, #ttnn_layout28>) -> tensor<1x1x1xbf16, #ttnn_layout28>
        %330 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %331 = "ttir.broadcast"(%329, %330) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16, #ttnn_layout28>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %332 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %333 = "ttir.clamp_tensor"(%327, %289, %331, %332) : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %334 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %335 = "ttir.add"(%333, %13, %334) : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<1x1x1xbf16, #ttnn_layout28>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %336 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %337 = "ttir.typecast"(%335, %336) <{conservative_folding = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %338 = ttir.empty() : tensor<1x1x1xbf16, #ttnn_layout28>
        %339 = "ttir.reshape"(%arg26, %338) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1xbf16, #ttnn_layout28>) -> tensor<1x1x1xbf16, #ttnn_layout28>
        %340 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %341 = "ttir.broadcast"(%339, %340) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16, #ttnn_layout28>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %342 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %343 = "ttir.slice_static"(%325, %342) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16, #ttnn_layout59>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %344 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %345 = "ttir.clamp_tensor"(%343, %341, %331, %344) : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %346 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %347 = "ttir.typecast"(%345, %346) <{conservative_folding = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %348 = ttir.empty() : tensor<1x1x1xf32, #ttnn_layout23>
        %349 = "ttir.reshape"(%arg24, %348) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1x1xf32, #ttnn_layout23>) -> tensor<1x1x1xf32, #ttnn_layout23>
        %350 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %351 = "ttir.multiply"(%347, %349, %350) : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<1x1x1xf32, #ttnn_layout23>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %352 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %353 = "ttir.typecast"(%351, %352) <{conservative_folding = false}> : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %354 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %355 = "ttir.sigmoid"(%353, %354) : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %356 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %357 = "ttir.typecast"(%355, %356) <{conservative_folding = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %358 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %359 = "ttir.multiply"(%347, %357, %358) : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %360 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %361 = "ttir.multiply"(%337, %359, %360) : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %362 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %363 = "ttir.typecast"(%361, %362) <{conservative_folding = false}> : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %364 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %365 = "ttir.matmul"(%363, %arg23, %364) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x2880x2880xbf16, #ttnn_layout15>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %366 = ttir.empty() : tensor<32x1x1x2880xbf16, #ttnn_layout61>
        %367 = "ttir.reshape"(%arg22, %366) <{shape = [32 : i32, 1 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16, #ttnn_layout8>, tensor<32x1x1x2880xbf16, #ttnn_layout61>) -> tensor<32x1x1x2880xbf16, #ttnn_layout61>
        %368 = ttir.empty() : tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %369 = "ttir.reshape"(%365, %368) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x1x13x2880xbf16, #ttnn_layout61>) -> tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %370 = ttir.empty() : tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %371 = "ttir.add"(%369, %367, %370) : (tensor<32x1x13x2880xbf16, #ttnn_layout61>, tensor<32x1x1x2880xbf16, #ttnn_layout61>, tensor<32x1x13x2880xbf16, #ttnn_layout61>) -> tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %372 = ttir.empty() : tensor<32x1x13x2880xf32, #ttnn_layout62>
        %373 = "ttir.typecast"(%371, %372) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16, #ttnn_layout61>, tensor<32x1x13x2880xf32, #ttnn_layout62>) -> tensor<32x1x13x2880xf32, #ttnn_layout62>
        %374 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32, #ttnn_layout24>
        %375 = ttir.empty() : tensor<13x1x1xsi32, #ttnn_layout63>
        %376 = "ttir.reshape"(%374, %375) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32, #ttnn_layout24>, tensor<13x1x1xsi32, #ttnn_layout63>) -> tensor<13x1x1xsi32, #ttnn_layout63>
        %377 = ttir.empty() : tensor<13x4x1xsi32, #ttnn_layout63>
        %378 = "ttir.broadcast"(%376, %377) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32, #ttnn_layout63>, tensor<13x4x1xsi32, #ttnn_layout63>) -> tensor<13x4x1xsi32, #ttnn_layout63>
        %379 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %380 = "ttir.matmul"(%315, %arg12, %379) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<32x2880xbf16, #ttnn_layout8>, tensor<13x32xbf16, #ttnn_layout27>) -> tensor<13x32xbf16, #ttnn_layout27>
        %381 = ttir.empty() : tensor<1x32xbf16, #ttnn_layout27>
        %382 = "ttir.reshape"(%arg11, %381) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16, #ttnn_layout7>, tensor<1x32xbf16, #ttnn_layout27>) -> tensor<1x32xbf16, #ttnn_layout27>
        %383 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %384 = "ttir.add"(%380, %382, %383) : (tensor<13x32xbf16, #ttnn_layout27>, tensor<1x32xbf16, #ttnn_layout27>, tensor<13x32xbf16, #ttnn_layout27>) -> tensor<13x32xbf16, #ttnn_layout27>
        %385 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %386 = ttir.empty() : tensor<13x32xsi32, #ttnn_layout3>
        %values, %indices = "ttir.sort"(%384, %385, %386) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x32xbf16, #ttnn_layout27>, tensor<13x32xsi32, #ttnn_layout3>) -> (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x32xsi32, #ttnn_layout3>)
        %387 = ttir.empty() : tensor<13x4xsi32, #ttnn_layout3>
        %388 = "ttir.slice_static"(%indices, %387) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32, #ttnn_layout3>, tensor<13x4xsi32, #ttnn_layout3>) -> tensor<13x4xsi32, #ttnn_layout3>
        %389 = ttir.empty() : tensor<13x4x1xsi32, #ttnn_layout63>
        %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32, #ttnn_layout3>, tensor<13x4x1xsi32, #ttnn_layout63>) -> tensor<13x4x1xsi32, #ttnn_layout63>
        %391 = ttir.empty() : tensor<13x4x2xsi32, #ttnn_layout63>
        %392 = "ttir.concat"(%378, %390, %391) <{dim = 2 : si32}> : (tensor<13x4x1xsi32, #ttnn_layout63>, tensor<13x4x1xsi32, #ttnn_layout63>, tensor<13x4x2xsi32, #ttnn_layout63>) -> tensor<13x4x2xsi32, #ttnn_layout63>
        %393 = ttir.empty() : tensor<13x4xbf16, #ttnn_layout27>
        %394 = "ttir.slice_static"(%values, %393) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x4xbf16, #ttnn_layout27>) -> tensor<13x4xbf16, #ttnn_layout27>
        %395 = ttir.empty() : tensor<13x4xbf16, #ttnn_layout27>
        %396 = "ttir.softmax"(%394, %395) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16, #ttnn_layout27>, tensor<13x4xbf16, #ttnn_layout27>) -> tensor<13x4xbf16, #ttnn_layout27>
        %397 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %398 = "ttir.scatter"(%11, %392, %396, %397) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x4x2xsi32, #ttnn_layout63>, tensor<13x4xbf16, #ttnn_layout27>, tensor<13x32xbf16, #ttnn_layout27>) -> tensor<13x32xbf16, #ttnn_layout27>
        %399 = ttir.empty() : tensor<13x32xf32, #ttnn_layout26>
        %400 = "ttir.typecast"(%398, %399) <{conservative_folding = false}> : (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x32xf32, #ttnn_layout26>) -> tensor<13x32xf32, #ttnn_layout26>
        %401 = ttir.empty() : tensor<32x13xf32, #ttnn_layout26>
        %402 = "ttir.permute"(%400, %401) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xf32, #ttnn_layout26>, tensor<32x13xf32, #ttnn_layout26>) -> tensor<32x13xf32, #ttnn_layout26>
        %403 = ttir.empty() : tensor<32x1x13x1xf32, #ttnn_layout64>
        %404 = "ttir.reshape"(%402, %403) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xf32, #ttnn_layout26>, tensor<32x1x13x1xf32, #ttnn_layout64>) -> tensor<32x1x13x1xf32, #ttnn_layout64>
        %405 = ttir.empty() : tensor<32x1x13x2880xf32, #ttnn_layout62>
        %406 = "ttir.multiply"(%373, %404, %405) : (tensor<32x1x13x2880xf32, #ttnn_layout62>, tensor<32x1x13x1xf32, #ttnn_layout64>, tensor<32x1x13x2880xf32, #ttnn_layout62>) -> tensor<32x1x13x2880xf32, #ttnn_layout62>
        %407 = ttir.empty() : tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %408 = "ttir.typecast"(%406, %407) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32, #ttnn_layout62>, tensor<32x1x13x2880xbf16, #ttnn_layout61>) -> tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %409 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %410 = "ttir.sum"(%408, %409) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16, #ttnn_layout61>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %411 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %412 = "ttir.add"(%285, %410, %411) : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %413 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %414 = "ttir.typecast"(%412, %413) <{conservative_folding = false}> : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %415 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %416 = "ttir.pow"(%414, %19, %415) : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x1x1xf32, #ttnn_layout23>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %417 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %418 = "ttir.sum"(%416, %417) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %419 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %420 = "ttir.multiply"(%418, %4, %419) : (tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %421 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %422 = "ttir.reshape"(%420, %421) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %423 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %424 = "ttir.add"(%422, %45, %423) : (tensor<13x1xf32, #ttnn_layout26>, tensor<1x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %425 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %426 = "ttir.rsqrt"(%424, %425) : (tensor<13x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %427 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %428 = "ttir.reshape"(%414, %427) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %429 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %430 = "ttir.multiply"(%428, %426, %429) : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x1xf32, #ttnn_layout26>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %431 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %432 = "ttir.multiply"(%255, %430, %431) : (tensor<1x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %433 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %434 = "ttir.typecast"(%432, %433) <{conservative_folding = false}> : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %435 = ttir.empty() : tensor<13x201088xbf16, #ttnn_layout21>
        %436 = "ttir.matmul"(%434, %arg10, %435) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<201088x2880xbf16, #ttnn_layout4>, tensor<13x201088xbf16, #ttnn_layout21>) -> tensor<13x201088xbf16, #ttnn_layout21>
        %437 = ttir.empty() : tensor<1x13x201088xbf16, #ttnn_layout22>
        %438 = "ttir.reshape"(%436, %437) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16, #ttnn_layout21>, tensor<1x13x201088xbf16, #ttnn_layout22>) -> tensor<1x13x201088xbf16, #ttnn_layout22>
        return %247, %249, %251, %177, %436, %438 : tensor<1x13x512xbf16, #ttnn_layout18>, tensor<1x13x8x64xbf16, #ttnn_layout19>, tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<13x201088xbf16, #ttnn_layout21>, tensor<1x13x201088xbf16, #ttnn_layout22>
      }
    }
  }
}


loc("scatter.460"): error: failed to legalize operation 'ttir.scatter'
// -----// IR Dump After ConvertTTIRToTTNN Failed (convert-ttir-to-ttnn) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073160160, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99712, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073176768, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<6284x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<90x128x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 2880 + d1, d2), <1x1>, memref<2880x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x180x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 2880 + d1, d2), <1x1>, memref<2880x180x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 416 + d1 * 32 + d2, d3), <1x1>, memref<13x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x6284x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x6284x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, u8>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, u8>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 * 64 + d2, d3), <1x1>, memref<2x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 64 + d2, d3), <1x1>, memref<26x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<64x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout44 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout45 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<64x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout46 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<64x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout47 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout48 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout49 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout50 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout51 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 256 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout52 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 2048 + d1 * 256 + d2 * 32 + d3, d4), <1x1>, memref<64x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout53 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 64 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout54 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout55 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout56 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout57 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout58 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<13x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout59 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x180x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout60 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<32x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout61 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<32x90x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout62 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<32x90x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout63 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<13x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout64 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func @main(%arg0: tensor<512xbf16, #ttnn_layout> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16, #ttnn_layout1> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32, #ttnn_layout2> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xsi32, #ttnn_layout3> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16, #ttnn_layout4> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16, #ttnn_layout5> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32, #ttnn_layout2> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32, #ttnn_layout6> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16, #ttnn_layout> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16, #ttnn_layout1> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16, #ttnn_layout4> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16, #ttnn_layout7> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16, #ttnn_layout8> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16, #ttnn_layout5> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16, #ttnn_layout9> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16, #ttnn_layout10> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16, #ttnn_layout11> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<si32, #ttnn_layout12> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32, #ttnn_layout2> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16, #ttnn_layout13> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16, #ttnn_layout14> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16, #ttnn_layout5> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16, #ttnn_layout8> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16, #ttnn_layout15> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32, #ttnn_layout2> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16, #ttnn_layout11> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16, #ttnn_layout11> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16, #ttnn_layout16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16, #ttnn_layout17> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16, #ttnn_layout11> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16, #ttnn_layout5> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16, #ttnn_layout18> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16, #ttnn_layout19> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16, #ttnn_layout20> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16, #ttnn_layout20> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16, #ttnn_layout21> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16, #ttnn_layout22> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32, #ttnn_layout23>
        %1 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xsi32>}> : () -> tensor<13xsi32, #ttnn_layout24>
        %2 = "ttir.full"() <{fill_value = 0 : i32, shape = array<i32>}> : () -> tensor<ui8, #ttnn_layout25>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32, #ttnn_layout2>
        %4 = "ttir.full"() <{fill_value = 3.47222231E-4 : f32, shape = array<i32: 1, 13>}> : () -> tensor<1x13xf32, #ttnn_layout26>
        %5 = "ttir.full"() <{fill_value = 1 : i32, shape = array<i32>}> : () -> tensor<ui8, #ttnn_layout25>
        %6 = "ttir.full"() <{fill_value = 1.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn_layout11>
        %7 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn_layout11>
        %8 = ttir.empty() : tensor<1x1xbf16, #ttnn_layout27>
        %9 = "ttir.reshape"(%7, %8) <{shape = [1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1xbf16, #ttnn_layout27>) -> tensor<1x1xbf16, #ttnn_layout27>
        %10 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %11 = "ttir.broadcast"(%9, %10) <{broadcast_dimensions = array<i64: 13, 32>}> : (tensor<1x1xbf16, #ttnn_layout27>, tensor<13x32xbf16, #ttnn_layout27>) -> tensor<13x32xbf16, #ttnn_layout27>
        %12 = ttir.empty() : tensor<1x1x1xbf16, #ttnn_layout28>
        %13 = "ttir.reshape"(%6, %12) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1xbf16, #ttnn_layout28>) -> tensor<1x1x1xbf16, #ttnn_layout28>
        %14 = ttir.empty() : tensor<1x1x1x1xbf16, #ttnn_layout29>
        %15 = "ttir.reshape"(%7, %14) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1x1xbf16, #ttnn_layout29>) -> tensor<1x1x1x1xbf16, #ttnn_layout29>
        %16 = ttir.empty() : tensor<1x1x1x1xui8, #ttnn_layout30>
        %17 = "ttir.reshape"(%5, %16) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8, #ttnn_layout25>, tensor<1x1x1x1xui8, #ttnn_layout30>) -> tensor<1x1x1x1xui8, #ttnn_layout30>
        %18 = ttir.empty() : tensor<1x1x1xf32, #ttnn_layout23>
        %19 = "ttir.reshape"(%3, %18) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1x1xf32, #ttnn_layout23>) -> tensor<1x1x1xf32, #ttnn_layout23>
        %20 = ttir.empty() : tensor<1x1x1x1xui8, #ttnn_layout30>
        %21 = "ttir.reshape"(%2, %20) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<ui8, #ttnn_layout25>, tensor<1x1x1x1xui8, #ttnn_layout30>) -> tensor<1x1x1x1xui8, #ttnn_layout30>
        %22 = ttir.empty() : tensor<2880xf32, #ttnn_layout31>
        %23 = "ttir.typecast"(%arg5, %22) <{conservative_folding = false}> : (tensor<2880xbf16, #ttnn_layout5>, tensor<2880xf32, #ttnn_layout31>) -> tensor<2880xf32, #ttnn_layout31>
        %24 = ttir.empty() : tensor<1x2880xf32, #ttnn_layout32>
        %25 = "ttir.reshape"(%23, %24) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32, #ttnn_layout31>, tensor<1x2880xf32, #ttnn_layout32>) -> tensor<1x2880xf32, #ttnn_layout32>
        %26 = ttir.empty() : tensor<1x13xui32, #ttnn_layout33>
        %27 = "ttir.typecast"(%arg3, %26) <{conservative_folding = false}> : (tensor<1x13xsi32, #ttnn_layout3>, tensor<1x13xui32, #ttnn_layout33>) -> tensor<1x13xui32, #ttnn_layout33>
        %28 = ttir.empty() : tensor<13xui32, #ttnn_layout34>
        %29 = "ttir.reshape"(%27, %28) <{shape = [13 : i32]}> : (tensor<1x13xui32, #ttnn_layout33>, tensor<13xui32, #ttnn_layout34>) -> tensor<13xui32, #ttnn_layout34>
        %30 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %31 = "ttir.embedding"(%29, %arg4, %30) : (tensor<13xui32, #ttnn_layout34>, tensor<201088x2880xbf16, #ttnn_layout4>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %32 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %34 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %35 = "ttir.typecast"(%33, %34) <{conservative_folding = false}> : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %36 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %37 = "ttir.pow"(%35, %19, %36) : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x1x1xf32, #ttnn_layout23>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %38 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %39 = "ttir.sum"(%37, %38) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %40 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %41 = "ttir.multiply"(%39, %4, %40) : (tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %42 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %43 = "ttir.reshape"(%41, %42) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %44 = ttir.empty() : tensor<1x1xf32, #ttnn_layout26>
        %45 = "ttir.reshape"(%arg2, %44) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1xf32, #ttnn_layout26>) -> tensor<1x1xf32, #ttnn_layout26>
        %46 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %47 = "ttir.add"(%43, %45, %46) : (tensor<13x1xf32, #ttnn_layout26>, tensor<1x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %48 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %49 = "ttir.rsqrt"(%47, %48) : (tensor<13x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %50 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %51 = "ttir.reshape"(%35, %50) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %52 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %53 = "ttir.multiply"(%51, %49, %52) : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x1xf32, #ttnn_layout26>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %54 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %55 = "ttir.multiply"(%25, %53, %54) : (tensor<1x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %56 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %57 = "ttir.typecast"(%55, %56) <{conservative_folding = false}> : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %58 = ttir.empty() : tensor<13x4096xbf16, #ttnn_layout37>
        %59 = "ttir.matmul"(%57, %arg20, %58) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<4096x2880xbf16, #ttnn_layout14>, tensor<13x4096xbf16, #ttnn_layout37>) -> tensor<13x4096xbf16, #ttnn_layout37>
        %60 = ttir.empty() : tensor<1x1x64x64xbf16, #ttnn_layout38>
        %61 = "ttir.reshape"(%arg19, %60) <{shape = [1 : i32, 1 : i32, 64 : i32, 64 : i32]}> : (tensor<4096xbf16, #ttnn_layout13>, tensor<1x1x64x64xbf16, #ttnn_layout38>) -> tensor<1x1x64x64xbf16, #ttnn_layout38>
        %62 = ttir.empty() : tensor<1x64x1x64xbf16, #ttnn_layout39>
        %63 = "ttir.permute"(%61, %62) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x64x64xbf16, #ttnn_layout38>, tensor<1x64x1x64xbf16, #ttnn_layout39>) -> tensor<1x64x1x64xbf16, #ttnn_layout39>
        %64 = ttir.empty() : tensor<1x13x64x64xbf16, #ttnn_layout40>
        %65 = "ttir.reshape"(%59, %64) <{shape = [1 : i32, 13 : i32, 64 : i32, 64 : i32]}> : (tensor<13x4096xbf16, #ttnn_layout37>, tensor<1x13x64x64xbf16, #ttnn_layout40>) -> tensor<1x13x64x64xbf16, #ttnn_layout40>
        %66 = ttir.empty() : tensor<1x64x13x64xbf16, #ttnn_layout39>
        %67 = "ttir.permute"(%65, %66) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x64x64xbf16, #ttnn_layout40>, tensor<1x64x13x64xbf16, #ttnn_layout39>) -> tensor<1x64x13x64xbf16, #ttnn_layout39>
        %68 = ttir.empty() : tensor<1x64x13x64xbf16, #ttnn_layout39>
        %69 = "ttir.add"(%67, %63, %68) : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x64x1x64xbf16, #ttnn_layout39>, tensor<1x64x13x64xbf16, #ttnn_layout39>) -> tensor<1x64x13x64xbf16, #ttnn_layout39>
        %70 = ttir.empty() : tensor<1x64x13x32xbf16, #ttnn_layout41>
        %71 = "ttir.slice_static"(%69, %70) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x64x13x32xbf16, #ttnn_layout41>) -> tensor<1x64x13x32xbf16, #ttnn_layout41>
        %72 = ttir.empty() : tensor<1x64x13x32xf32, #ttnn_layout42>
        %73 = "ttir.typecast"(%71, %72) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16, #ttnn_layout41>, tensor<1x64x13x32xf32, #ttnn_layout42>) -> tensor<1x64x13x32xf32, #ttnn_layout42>
        %74 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %75 = "ttir.reshape"(%73, %74) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32, #ttnn_layout42>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %76 = ttir.empty() : tensor<1x32x1xf32, #ttnn_layout23>
        %77 = "ttir.reshape"(%arg7, %76) <{shape = [1 : i32, 32 : i32, 1 : i32]}> : (tensor<32xf32, #ttnn_layout6>, tensor<1x32x1xf32, #ttnn_layout23>) -> tensor<1x32x1xf32, #ttnn_layout23>
        %78 = ttir.empty() : tensor<1x32x13xf32, #ttnn_layout23>
        %79 = "ttir.matmul"(%77, %0, %78) <{transpose_a = false, transpose_b = false}> : (tensor<1x32x1xf32, #ttnn_layout23>, tensor<1x1x13xf32, #ttnn_layout23>, tensor<1x32x13xf32, #ttnn_layout23>) -> tensor<1x32x13xf32, #ttnn_layout23>
        %80 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %81 = "ttir.permute"(%79, %80) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x32x13xf32, #ttnn_layout23>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %82 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %83 = "ttir.cos"(%81, %82) : (tensor<1x13x32xf32, #ttnn_layout23>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %84 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %85 = "ttir.reshape"(%83, %84) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32, #ttnn_layout23>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %86 = ttir.empty() : tensor<1x1x1x1xf32, #ttnn_layout44>
        %87 = "ttir.reshape"(%arg6, %86) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1x1x1xf32, #ttnn_layout44>) -> tensor<1x1x1x1xf32, #ttnn_layout44>
        %88 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %89 = "ttir.multiply"(%85, %87, %88) : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x1x1x1xf32, #ttnn_layout44>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %90 = ttir.empty() : tensor<1x1x13x32xbf16, #ttnn_layout29>
        %91 = "ttir.typecast"(%89, %90) <{conservative_folding = false}> : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x1x13x32xbf16, #ttnn_layout29>) -> tensor<1x1x13x32xbf16, #ttnn_layout29>
        %92 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %93 = "ttir.typecast"(%91, %92) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16, #ttnn_layout29>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %94 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %95 = "ttir.reshape"(%93, %94) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %96 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %97 = "ttir.multiply"(%75, %95, %96) : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<1x13x32xf32, #ttnn_layout23>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %98 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %99 = "ttir.typecast"(%97, %98) <{conservative_folding = false}> : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %100 = ttir.empty() : tensor<1x64x13x32xbf16, #ttnn_layout41>
        %101 = "ttir.slice_static"(%69, %100) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x64x13x32xbf16, #ttnn_layout41>) -> tensor<1x64x13x32xbf16, #ttnn_layout41>
        %102 = ttir.empty() : tensor<1x64x13x32xf32, #ttnn_layout42>
        %103 = "ttir.typecast"(%101, %102) <{conservative_folding = false}> : (tensor<1x64x13x32xbf16, #ttnn_layout41>, tensor<1x64x13x32xf32, #ttnn_layout42>) -> tensor<1x64x13x32xf32, #ttnn_layout42>
        %104 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %105 = "ttir.reshape"(%103, %104) <{shape = [64 : i32, 13 : i32, 32 : i32]}> : (tensor<1x64x13x32xf32, #ttnn_layout42>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %106 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %107 = "ttir.sin"(%81, %106) : (tensor<1x13x32xf32, #ttnn_layout23>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %108 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %109 = "ttir.reshape"(%107, %108) <{shape = [1 : i32, 1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x13x32xf32, #ttnn_layout23>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %110 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %111 = "ttir.multiply"(%109, %87, %110) : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x1x1x1xf32, #ttnn_layout44>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %112 = ttir.empty() : tensor<1x1x13x32xbf16, #ttnn_layout29>
        %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x1x13x32xbf16, #ttnn_layout29>) -> tensor<1x1x13x32xbf16, #ttnn_layout29>
        %114 = ttir.empty() : tensor<1x1x13x32xf32, #ttnn_layout44>
        %115 = "ttir.typecast"(%113, %114) <{conservative_folding = false}> : (tensor<1x1x13x32xbf16, #ttnn_layout29>, tensor<1x1x13x32xf32, #ttnn_layout44>) -> tensor<1x1x13x32xf32, #ttnn_layout44>
        %116 = ttir.empty() : tensor<1x13x32xf32, #ttnn_layout23>
        %117 = "ttir.reshape"(%115, %116) <{shape = [1 : i32, 13 : i32, 32 : i32]}> : (tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x13x32xf32, #ttnn_layout23>) -> tensor<1x13x32xf32, #ttnn_layout23>
        %118 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %119 = "ttir.multiply"(%105, %117, %118) : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<1x13x32xf32, #ttnn_layout23>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %120 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %122 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %123 = "ttir.subtract"(%99, %121, %122) : (tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %124 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %125 = "ttir.multiply"(%105, %95, %124) : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<1x13x32xf32, #ttnn_layout23>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %126 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %128 = ttir.empty() : tensor<64x13x32xf32, #ttnn_layout43>
        %129 = "ttir.multiply"(%75, %117, %128) : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<1x13x32xf32, #ttnn_layout23>, tensor<64x13x32xf32, #ttnn_layout43>) -> tensor<64x13x32xf32, #ttnn_layout43>
        %130 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %131 = "ttir.typecast"(%129, %130) <{conservative_folding = false}> : (tensor<64x13x32xf32, #ttnn_layout43>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %132 = ttir.empty() : tensor<64x13x32xbf16, #ttnn_layout45>
        %133 = "ttir.add"(%127, %131, %132) : (tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>) -> tensor<64x13x32xbf16, #ttnn_layout45>
        %134 = ttir.empty() : tensor<64x13x64xbf16, #ttnn_layout46>
        %135 = "ttir.concat"(%123, %133, %134) <{dim = 2 : si32}> : (tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x32xbf16, #ttnn_layout45>, tensor<64x13x64xbf16, #ttnn_layout46>) -> tensor<64x13x64xbf16, #ttnn_layout46>
        %136 = ttir.empty() : tensor<13x512xbf16, #ttnn_layout47>
        %137 = "ttir.matmul"(%57, %arg9, %136) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<512x2880xbf16, #ttnn_layout1>, tensor<13x512xbf16, #ttnn_layout47>) -> tensor<13x512xbf16, #ttnn_layout47>
        %138 = ttir.empty() : tensor<1x1x8x64xbf16, #ttnn_layout48>
        %139 = "ttir.reshape"(%arg8, %138) <{shape = [1 : i32, 1 : i32, 8 : i32, 64 : i32]}> : (tensor<512xbf16, #ttnn_layout>, tensor<1x1x8x64xbf16, #ttnn_layout48>) -> tensor<1x1x8x64xbf16, #ttnn_layout48>
        %140 = ttir.empty() : tensor<1x8x1x64xbf16, #ttnn_layout20>
        %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x8x64xbf16, #ttnn_layout48>, tensor<1x8x1x64xbf16, #ttnn_layout20>) -> tensor<1x8x1x64xbf16, #ttnn_layout20>
        %142 = ttir.empty() : tensor<1x13x8x64xbf16, #ttnn_layout19>
        %143 = "ttir.reshape"(%137, %142) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<13x512xbf16, #ttnn_layout47>, tensor<1x13x8x64xbf16, #ttnn_layout19>) -> tensor<1x13x8x64xbf16, #ttnn_layout19>
        %144 = ttir.empty() : tensor<1x8x13x64xbf16, #ttnn_layout20>
        %145 = "ttir.permute"(%143, %144) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16, #ttnn_layout19>, tensor<1x8x13x64xbf16, #ttnn_layout20>) -> tensor<1x8x13x64xbf16, #ttnn_layout20>
        %146 = ttir.empty() : tensor<1x8x13x64xbf16, #ttnn_layout20>
        %147 = "ttir.add"(%145, %141, %146) : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x1x64xbf16, #ttnn_layout20>, tensor<1x8x13x64xbf16, #ttnn_layout20>) -> tensor<1x8x13x64xbf16, #ttnn_layout20>
        %148 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %149 = "ttir.slice_static"(%147, %148) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %150 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %151 = "ttir.typecast"(%149, %150) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %152 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %153 = "ttir.multiply"(%151, %93, %152) : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %154 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %155 = "ttir.typecast"(%153, %154) <{conservative_folding = false}> : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %156 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %157 = "ttir.slice_static"(%147, %156) <{begins = [0 : i32, 0 : i32, 0 : i32, 32 : i32], ends = [1 : i32, 8 : i32, 13 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %158 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %159 = "ttir.typecast"(%157, %158) <{conservative_folding = false}> : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %160 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %161 = "ttir.multiply"(%159, %115, %160) : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %162 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %163 = "ttir.typecast"(%161, %162) <{conservative_folding = false}> : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %164 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %165 = "ttir.subtract"(%155, %163, %164) : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %166 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %167 = "ttir.multiply"(%159, %93, %166) : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %168 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %169 = "ttir.typecast"(%167, %168) <{conservative_folding = false}> : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %170 = ttir.empty() : tensor<1x8x13x32xf32, #ttnn_layout50>
        %171 = "ttir.multiply"(%151, %115, %170) : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x1x13x32xf32, #ttnn_layout44>, tensor<1x8x13x32xf32, #ttnn_layout50>) -> tensor<1x8x13x32xf32, #ttnn_layout50>
        %172 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %173 = "ttir.typecast"(%171, %172) <{conservative_folding = false}> : (tensor<1x8x13x32xf32, #ttnn_layout50>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %174 = ttir.empty() : tensor<1x8x13x32xbf16, #ttnn_layout49>
        %175 = "ttir.add"(%169, %173, %174) : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>) -> tensor<1x8x13x32xbf16, #ttnn_layout49>
        %176 = ttir.empty() : tensor<1x8x13x64xbf16, #ttnn_layout20>
        %177 = "ttir.concat"(%165, %175, %176) <{dim = 3 : si32}> : (tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x32xbf16, #ttnn_layout49>, tensor<1x8x13x64xbf16, #ttnn_layout20>) -> tensor<1x8x13x64xbf16, #ttnn_layout20>
        %178 = ttir.empty() : tensor<1x8x1x13x64xbf16, #ttnn_layout51>
        %179 = "ttir.reshape"(%177, %178) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x1x13x64xbf16, #ttnn_layout51>) -> tensor<1x8x1x13x64xbf16, #ttnn_layout51>
        %180 = ttir.empty() : tensor<1x8x8x13x64xbf16, #ttnn_layout52>
        %181 = "ttir.broadcast"(%179, %180) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16, #ttnn_layout51>, tensor<1x8x8x13x64xbf16, #ttnn_layout52>) -> tensor<1x8x8x13x64xbf16, #ttnn_layout52>
        %182 = ttir.empty() : tensor<1x64x13x64xbf16, #ttnn_layout39>
        %183 = "ttir.reshape"(%181, %182) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16, #ttnn_layout52>, tensor<1x64x13x64xbf16, #ttnn_layout39>) -> tensor<1x64x13x64xbf16, #ttnn_layout39>
        %184 = ttir.empty() : tensor<1x64x64x13xbf16, #ttnn_layout53>
        %185 = "ttir.permute"(%183, %184) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x64x64x13xbf16, #ttnn_layout53>) -> tensor<1x64x64x13xbf16, #ttnn_layout53>
        %186 = ttir.empty() : tensor<64x64x13xbf16, #ttnn_layout54>
        %187 = "ttir.reshape"(%185, %186) <{shape = [64 : i32, 64 : i32, 13 : i32]}> : (tensor<1x64x64x13xbf16, #ttnn_layout53>, tensor<64x64x13xbf16, #ttnn_layout54>) -> tensor<64x64x13xbf16, #ttnn_layout54>
        %188 = ttir.empty() : tensor<64x13x13xbf16, #ttnn_layout45>
        %189 = "ttir.matmul"(%135, %187, %188) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x64xbf16, #ttnn_layout46>, tensor<64x64x13xbf16, #ttnn_layout54>, tensor<64x13x13xbf16, #ttnn_layout45>) -> tensor<64x13x13xbf16, #ttnn_layout45>
        %190 = ttir.empty() : tensor<64x13x13xf32, #ttnn_layout43>
        %191 = "ttir.typecast"(%189, %190) <{conservative_folding = false}> : (tensor<64x13x13xbf16, #ttnn_layout45>, tensor<64x13x13xf32, #ttnn_layout43>) -> tensor<64x13x13xf32, #ttnn_layout43>
        %192 = ttir.empty() : tensor<1x64x13x13xf32, #ttnn_layout42>
        %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 64 : i32, 13 : i32, 13 : i32]}> : (tensor<64x13x13xf32, #ttnn_layout43>, tensor<1x64x13x13xf32, #ttnn_layout42>) -> tensor<1x64x13x13xf32, #ttnn_layout42>
        %194 = ttir.empty() : tensor<1x1x1x1xf32, #ttnn_layout44>
        %195 = "ttir.reshape"(%arg18, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1x1x1xf32, #ttnn_layout44>) -> tensor<1x1x1x1xf32, #ttnn_layout44>
        %196 = ttir.empty() : tensor<1x64x13x13xf32, #ttnn_layout42>
        %197 = "ttir.multiply"(%193, %195, %196) : (tensor<1x64x13x13xf32, #ttnn_layout42>, tensor<1x1x1x1xf32, #ttnn_layout44>, tensor<1x64x13x13xf32, #ttnn_layout42>) -> tensor<1x64x13x13xf32, #ttnn_layout42>
        %198 = ttir.empty() : tensor<1x64x13x13xbf16, #ttnn_layout41>
        %199 = "ttir.typecast"(%197, %198) <{conservative_folding = false}> : (tensor<1x64x13x13xf32, #ttnn_layout42>, tensor<1x64x13x13xbf16, #ttnn_layout41>) -> tensor<1x64x13x13xbf16, #ttnn_layout41>
        %200 = ttir.empty() : tensor<1x1x1x13xsi32, #ttnn_layout55>
        %201 = "ttir.reshape"(%1, %200) <{shape = [1 : i32, 1 : i32, 1 : i32, 13 : i32]}> : (tensor<13xsi32, #ttnn_layout24>, tensor<1x1x1x13xsi32, #ttnn_layout55>) -> tensor<1x1x1x13xsi32, #ttnn_layout55>
        %202 = ttir.empty() : tensor<1x1x1x1xsi32, #ttnn_layout55>
        %203 = "ttir.reshape"(%arg17, %202) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<si32, #ttnn_layout12>, tensor<1x1x1x1xsi32, #ttnn_layout55>) -> tensor<1x1x1x1xsi32, #ttnn_layout55>
        %204 = ttir.empty() : tensor<1x1x13x1xsi32, #ttnn_layout55>
        %205 = "ttir.reshape"(%1, %204) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32, #ttnn_layout24>, tensor<1x1x13x1xsi32, #ttnn_layout55>) -> tensor<1x1x13x1xsi32, #ttnn_layout55>
        %206 = ttir.empty() : tensor<1x1x13x1xsi32, #ttnn_layout55>
        %207 = "ttir.subtract"(%205, %203, %206) : (tensor<1x1x13x1xsi32, #ttnn_layout55>, tensor<1x1x1x1xsi32, #ttnn_layout55>, tensor<1x1x13x1xsi32, #ttnn_layout55>) -> tensor<1x1x13x1xsi32, #ttnn_layout55>
        %208 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %209 = "ttir.gt"(%201, %207, %208) : (tensor<1x1x1x13xsi32, #ttnn_layout55>, tensor<1x1x13x1xsi32, #ttnn_layout55>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %210 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %211 = "ttir.typecast"(%209, %210) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %212 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %213 = "ttir.bitwise_and"(%211, %17, %212) : (tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x1x1xui8, #ttnn_layout30>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %214 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %215 = "ttir.ne"(%213, %21, %214) : (tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x1x1xui8, #ttnn_layout30>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %216 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %217 = "ttir.typecast"(%215, %216) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %218 = ttir.empty() : tensor<1x1x13x1xsi32, #ttnn_layout55>
        %219 = "ttir.reshape"(%1, %218) <{shape = [1 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<13xsi32, #ttnn_layout24>, tensor<1x1x13x1xsi32, #ttnn_layout55>) -> tensor<1x1x13x1xsi32, #ttnn_layout55>
        %220 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %221 = "ttir.le"(%201, %219, %220) : (tensor<1x1x1x13xsi32, #ttnn_layout55>, tensor<1x1x13x1xsi32, #ttnn_layout55>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %222 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %223 = "ttir.typecast"(%221, %222) <{conservative_folding = false}> : (tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %224 = ttir.empty() : tensor<1x1x13x13xui8, #ttnn_layout30>
        %225 = "ttir.bitwise_and"(%217, %223, %224) : (tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x13x13xui8, #ttnn_layout30>) -> tensor<1x1x13x13xui8, #ttnn_layout30>
        %226 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %227 = "ttir.ne"(%225, %21, %226) : (tensor<1x1x13x13xui8, #ttnn_layout30>, tensor<1x1x1x1xui8, #ttnn_layout30>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %228 = ttir.empty() : tensor<1x1x1x1xbf16, #ttnn_layout29>
        %229 = "ttir.reshape"(%arg16, %228) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1x1xbf16, #ttnn_layout29>) -> tensor<1x1x1x1xbf16, #ttnn_layout29>
        %230 = ttir.empty() : tensor<1x1x13x13xbf16, #ttnn_layout29>
        %231 = "ttir.where"(%227, %15, %229, %230) : (tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x1x1x1xbf16, #ttnn_layout29>, tensor<1x1x1x1xbf16, #ttnn_layout29>, tensor<1x1x13x13xbf16, #ttnn_layout29>) -> tensor<1x1x13x13xbf16, #ttnn_layout29>
        %232 = ttir.empty() : tensor<1x64x13x13xbf16, #ttnn_layout41>
        %233 = "ttir.add"(%199, %231, %232) : (tensor<1x64x13x13xbf16, #ttnn_layout41>, tensor<1x1x13x13xbf16, #ttnn_layout29>, tensor<1x64x13x13xbf16, #ttnn_layout41>) -> tensor<1x64x13x13xbf16, #ttnn_layout41>
        %234 = ttir.empty() : tensor<1x64x1x1xbf16, #ttnn_layout41>
        %235 = "ttir.reshape"(%arg15, %234) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout10>, tensor<1x64x1x1xbf16, #ttnn_layout41>) -> tensor<1x64x1x1xbf16, #ttnn_layout41>
        %236 = ttir.empty() : tensor<1x64x13x1xbf16, #ttnn_layout41>
        %237 = "ttir.broadcast"(%235, %236) <{broadcast_dimensions = array<i64: 1, 1, 13, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout41>, tensor<1x64x13x1xbf16, #ttnn_layout41>) -> tensor<1x64x13x1xbf16, #ttnn_layout41>
        %238 = ttir.empty() : tensor<1x64x13x14xbf16, #ttnn_layout41>
        %239 = "ttir.concat"(%233, %237, %238) <{dim = 3 : si32}> : (tensor<1x64x13x13xbf16, #ttnn_layout41>, tensor<1x64x13x1xbf16, #ttnn_layout41>, tensor<1x64x13x14xbf16, #ttnn_layout41>) -> tensor<1x64x13x14xbf16, #ttnn_layout41>
        %240 = ttir.empty() : tensor<13x512xbf16, #ttnn_layout47>
        %241 = "ttir.matmul"(%57, %arg1, %240) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<512x2880xbf16, #ttnn_layout1>, tensor<13x512xbf16, #ttnn_layout47>) -> tensor<13x512xbf16, #ttnn_layout47>
        %242 = ttir.empty() : tensor<1x13x512xbf16, #ttnn_layout18>
        %243 = "ttir.reshape"(%241, %242) <{shape = [1 : i32, 13 : i32, 512 : i32]}> : (tensor<13x512xbf16, #ttnn_layout47>, tensor<1x13x512xbf16, #ttnn_layout18>) -> tensor<1x13x512xbf16, #ttnn_layout18>
        %244 = ttir.empty() : tensor<1x1x512xbf16, #ttnn_layout18>
        %245 = "ttir.reshape"(%arg0, %244) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16, #ttnn_layout>, tensor<1x1x512xbf16, #ttnn_layout18>) -> tensor<1x1x512xbf16, #ttnn_layout18>
        %246 = ttir.empty() : tensor<1x13x512xbf16, #ttnn_layout18>
        %247 = "ttir.add"(%243, %245, %246) : (tensor<1x13x512xbf16, #ttnn_layout18>, tensor<1x1x512xbf16, #ttnn_layout18>, tensor<1x13x512xbf16, #ttnn_layout18>) -> tensor<1x13x512xbf16, #ttnn_layout18>
        %248 = ttir.empty() : tensor<1x13x8x64xbf16, #ttnn_layout19>
        %249 = "ttir.reshape"(%247, %248) <{shape = [1 : i32, 13 : i32, 8 : i32, 64 : i32]}> : (tensor<1x13x512xbf16, #ttnn_layout18>, tensor<1x13x8x64xbf16, #ttnn_layout19>) -> tensor<1x13x8x64xbf16, #ttnn_layout19>
        %250 = ttir.empty() : tensor<1x8x13x64xbf16, #ttnn_layout20>
        %251 = "ttir.permute"(%249, %250) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x13x8x64xbf16, #ttnn_layout19>, tensor<1x8x13x64xbf16, #ttnn_layout20>) -> tensor<1x8x13x64xbf16, #ttnn_layout20>
        %252 = ttir.empty() : tensor<2880xf32, #ttnn_layout31>
        %253 = "ttir.typecast"(%arg30, %252) <{conservative_folding = false}> : (tensor<2880xbf16, #ttnn_layout5>, tensor<2880xf32, #ttnn_layout31>) -> tensor<2880xf32, #ttnn_layout31>
        %254 = ttir.empty() : tensor<1x2880xf32, #ttnn_layout32>
        %255 = "ttir.reshape"(%253, %254) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32, #ttnn_layout31>, tensor<1x2880xf32, #ttnn_layout32>) -> tensor<1x2880xf32, #ttnn_layout32>
        %256 = ttir.empty() : tensor<1x64x13x14xbf16, #ttnn_layout41>
        %257 = "ttir.softmax"(%239, %256) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x64x13x14xbf16, #ttnn_layout41>, tensor<1x64x13x14xbf16, #ttnn_layout41>) -> tensor<1x64x13x14xbf16, #ttnn_layout41>
        %258 = ttir.empty() : tensor<1x64x13x13xbf16, #ttnn_layout41>
        %259 = "ttir.slice_static"(%257, %258) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 13 : i32, 13 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x64x13x14xbf16, #ttnn_layout41>, tensor<1x64x13x13xbf16, #ttnn_layout41>) -> tensor<1x64x13x13xbf16, #ttnn_layout41>
        %260 = ttir.empty() : tensor<64x13x13xbf16, #ttnn_layout45>
        %261 = "ttir.reshape"(%259, %260) <{shape = [64 : i32, 13 : i32, 13 : i32]}> : (tensor<1x64x13x13xbf16, #ttnn_layout41>, tensor<64x13x13xbf16, #ttnn_layout45>) -> tensor<64x13x13xbf16, #ttnn_layout45>
        %262 = ttir.empty() : tensor<1x8x1x13x64xbf16, #ttnn_layout51>
        %263 = "ttir.reshape"(%251, %262) <{shape = [1 : i32, 8 : i32, 1 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x1x13x64xbf16, #ttnn_layout51>) -> tensor<1x8x1x13x64xbf16, #ttnn_layout51>
        %264 = ttir.empty() : tensor<1x8x8x13x64xbf16, #ttnn_layout52>
        %265 = "ttir.broadcast"(%263, %264) <{broadcast_dimensions = array<i64: 1, 1, 8, 1, 1>}> : (tensor<1x8x1x13x64xbf16, #ttnn_layout51>, tensor<1x8x8x13x64xbf16, #ttnn_layout52>) -> tensor<1x8x8x13x64xbf16, #ttnn_layout52>
        %266 = ttir.empty() : tensor<64x13x64xbf16, #ttnn_layout46>
        %267 = "ttir.reshape"(%265, %266) <{shape = [64 : i32, 13 : i32, 64 : i32]}> : (tensor<1x8x8x13x64xbf16, #ttnn_layout52>, tensor<64x13x64xbf16, #ttnn_layout46>) -> tensor<64x13x64xbf16, #ttnn_layout46>
        %268 = ttir.empty() : tensor<64x13x64xbf16, #ttnn_layout46>
        %269 = "ttir.matmul"(%261, %267, %268) <{transpose_a = false, transpose_b = false}> : (tensor<64x13x13xbf16, #ttnn_layout45>, tensor<64x13x64xbf16, #ttnn_layout46>, tensor<64x13x64xbf16, #ttnn_layout46>) -> tensor<64x13x64xbf16, #ttnn_layout46>
        %270 = ttir.empty() : tensor<1x64x13x64xbf16, #ttnn_layout39>
        %271 = "ttir.reshape"(%269, %270) <{shape = [1 : i32, 64 : i32, 13 : i32, 64 : i32]}> : (tensor<64x13x64xbf16, #ttnn_layout46>, tensor<1x64x13x64xbf16, #ttnn_layout39>) -> tensor<1x64x13x64xbf16, #ttnn_layout39>
        %272 = ttir.empty() : tensor<13x4096xbf16, #ttnn_layout37>
        %273 = ttir.empty() : tensor<1x13x4096xbf16, #ttnn_layout56>
        %274 = "ttir.concatenate_heads"(%271, %273) : (tensor<1x64x13x64xbf16, #ttnn_layout39>, tensor<1x13x4096xbf16, #ttnn_layout56>) -> tensor<1x13x4096xbf16, #ttnn_layout56>
        %275 = "ttir.reshape"(%274, %272) <{shape = [13 : i32, 4096 : i32]}> : (tensor<1x13x4096xbf16, #ttnn_layout56>, tensor<13x4096xbf16, #ttnn_layout37>) -> tensor<13x4096xbf16, #ttnn_layout37>
        %276 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %277 = "ttir.matmul"(%275, %arg14, %276) <{transpose_a = false, transpose_b = true}> : (tensor<13x4096xbf16, #ttnn_layout37>, tensor<2880x4096xbf16, #ttnn_layout9>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %278 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 13 : i32, 2880 : i32]}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %280 = ttir.empty() : tensor<1x1x2880xbf16, #ttnn_layout35>
        %281 = "ttir.reshape"(%arg13, %280) <{shape = [1 : i32, 1 : i32, 2880 : i32]}> : (tensor<2880xbf16, #ttnn_layout5>, tensor<1x1x2880xbf16, #ttnn_layout35>) -> tensor<1x1x2880xbf16, #ttnn_layout35>
        %282 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %283 = "ttir.add"(%279, %281, %282) : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x1x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %284 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %285 = "ttir.add"(%33, %283, %284) : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %286 = ttir.empty() : tensor<1x1x1xbf16, #ttnn_layout28>
        %287 = "ttir.reshape"(%arg29, %286) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1xbf16, #ttnn_layout28>) -> tensor<1x1x1xbf16, #ttnn_layout28>
        %288 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %289 = "ttir.broadcast"(%287, %288) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16, #ttnn_layout28>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %290 = ttir.empty() : tensor<2880xf32, #ttnn_layout31>
        %291 = "ttir.typecast"(%arg21, %290) <{conservative_folding = false}> : (tensor<2880xbf16, #ttnn_layout5>, tensor<2880xf32, #ttnn_layout31>) -> tensor<2880xf32, #ttnn_layout31>
        %292 = ttir.empty() : tensor<1x2880xf32, #ttnn_layout32>
        %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 2880 : i32]}> : (tensor<2880xf32, #ttnn_layout31>, tensor<1x2880xf32, #ttnn_layout32>) -> tensor<1x2880xf32, #ttnn_layout32>
        %294 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %295 = "ttir.typecast"(%285, %294) <{conservative_folding = false}> : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %296 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %297 = "ttir.pow"(%295, %19, %296) : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x1x1xf32, #ttnn_layout23>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %298 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %299 = "ttir.sum"(%297, %298) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %300 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %301 = "ttir.multiply"(%299, %4, %300) : (tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %302 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %303 = "ttir.reshape"(%301, %302) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %304 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %305 = "ttir.add"(%303, %45, %304) : (tensor<13x1xf32, #ttnn_layout26>, tensor<1x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %306 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %307 = "ttir.rsqrt"(%305, %306) : (tensor<13x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %308 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %309 = "ttir.reshape"(%295, %308) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %310 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %311 = "ttir.multiply"(%309, %307, %310) : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x1xf32, #ttnn_layout26>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %312 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %313 = "ttir.multiply"(%293, %311, %312) : (tensor<1x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %314 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %315 = "ttir.typecast"(%313, %314) <{conservative_folding = false}> : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %316 = ttir.empty() : tensor<416x2880xbf16, #ttnn_layout58>
        %317 = "ttir.concat"(%315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %315, %316) <{dim = 0 : si32}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<13x2880xbf16, #ttnn_layout8>, tensor<416x2880xbf16, #ttnn_layout58>) -> tensor<416x2880xbf16, #ttnn_layout58>
        %318 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %319 = "ttir.reshape"(%317, %318) <{shape = [32 : i32, 13 : i32, 2880 : i32]}> : (tensor<416x2880xbf16, #ttnn_layout58>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %320 = ttir.empty() : tensor<32x13x5760xbf16, #ttnn_layout59>
        %321 = "ttir.matmul"(%319, %arg28, %320) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x2880x5760xbf16, #ttnn_layout17>, tensor<32x13x5760xbf16, #ttnn_layout59>) -> tensor<32x13x5760xbf16, #ttnn_layout59>
        %322 = ttir.empty() : tensor<32x1x5760xbf16, #ttnn_layout59>
        %323 = "ttir.reshape"(%arg27, %322) <{shape = [32 : i32, 1 : i32, 5760 : i32]}> : (tensor<32x5760xbf16, #ttnn_layout16>, tensor<32x1x5760xbf16, #ttnn_layout59>) -> tensor<32x1x5760xbf16, #ttnn_layout59>
        %324 = ttir.empty() : tensor<32x13x5760xbf16, #ttnn_layout59>
        %325 = "ttir.add"(%321, %323, %324) : (tensor<32x13x5760xbf16, #ttnn_layout59>, tensor<32x1x5760xbf16, #ttnn_layout59>, tensor<32x13x5760xbf16, #ttnn_layout59>) -> tensor<32x13x5760xbf16, #ttnn_layout59>
        %326 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %327 = "ttir.slice_static"(%325, %326) <{begins = [0 : i32, 0 : i32, 1 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16, #ttnn_layout59>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %328 = ttir.empty() : tensor<1x1x1xbf16, #ttnn_layout28>
        %329 = "ttir.reshape"(%arg25, %328) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1xbf16, #ttnn_layout28>) -> tensor<1x1x1xbf16, #ttnn_layout28>
        %330 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %331 = "ttir.broadcast"(%329, %330) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16, #ttnn_layout28>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %332 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %333 = "ttir.clamp_tensor"(%327, %289, %331, %332) : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %334 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %335 = "ttir.add"(%333, %13, %334) : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<1x1x1xbf16, #ttnn_layout28>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %336 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %337 = "ttir.typecast"(%335, %336) <{conservative_folding = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %338 = ttir.empty() : tensor<1x1x1xbf16, #ttnn_layout28>
        %339 = "ttir.reshape"(%arg26, %338) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn_layout11>, tensor<1x1x1xbf16, #ttnn_layout28>) -> tensor<1x1x1xbf16, #ttnn_layout28>
        %340 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %341 = "ttir.broadcast"(%339, %340) <{broadcast_dimensions = array<i64: 32, 13, 2880>}> : (tensor<1x1x1xbf16, #ttnn_layout28>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %342 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %343 = "ttir.slice_static"(%325, %342) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [32 : i32, 13 : i32, 5760 : i32], step = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<32x13x5760xbf16, #ttnn_layout59>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %344 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %345 = "ttir.clamp_tensor"(%343, %341, %331, %344) : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %346 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %347 = "ttir.typecast"(%345, %346) <{conservative_folding = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %348 = ttir.empty() : tensor<1x1x1xf32, #ttnn_layout23>
        %349 = "ttir.reshape"(%arg24, %348) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout2>, tensor<1x1x1xf32, #ttnn_layout23>) -> tensor<1x1x1xf32, #ttnn_layout23>
        %350 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %351 = "ttir.multiply"(%347, %349, %350) : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<1x1x1xf32, #ttnn_layout23>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %352 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %353 = "ttir.typecast"(%351, %352) <{conservative_folding = false}> : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %354 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %355 = "ttir.sigmoid"(%353, %354) : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %356 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %357 = "ttir.typecast"(%355, %356) <{conservative_folding = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %358 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %359 = "ttir.multiply"(%347, %357, %358) : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %360 = ttir.empty() : tensor<32x13x2880xf32, #ttnn_layout60>
        %361 = "ttir.multiply"(%337, %359, %360) : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xf32, #ttnn_layout60>) -> tensor<32x13x2880xf32, #ttnn_layout60>
        %362 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %363 = "ttir.typecast"(%361, %362) <{conservative_folding = false}> : (tensor<32x13x2880xf32, #ttnn_layout60>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %364 = ttir.empty() : tensor<32x13x2880xbf16, #ttnn_layout57>
        %365 = "ttir.matmul"(%363, %arg23, %364) <{transpose_a = false, transpose_b = false}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x2880x2880xbf16, #ttnn_layout15>, tensor<32x13x2880xbf16, #ttnn_layout57>) -> tensor<32x13x2880xbf16, #ttnn_layout57>
        %366 = ttir.empty() : tensor<32x1x1x2880xbf16, #ttnn_layout61>
        %367 = "ttir.reshape"(%arg22, %366) <{shape = [32 : i32, 1 : i32, 1 : i32, 2880 : i32]}> : (tensor<32x2880xbf16, #ttnn_layout8>, tensor<32x1x1x2880xbf16, #ttnn_layout61>) -> tensor<32x1x1x2880xbf16, #ttnn_layout61>
        %368 = ttir.empty() : tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %369 = "ttir.reshape"(%365, %368) <{shape = [32 : i32, 1 : i32, 13 : i32, 2880 : i32]}> : (tensor<32x13x2880xbf16, #ttnn_layout57>, tensor<32x1x13x2880xbf16, #ttnn_layout61>) -> tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %370 = ttir.empty() : tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %371 = "ttir.add"(%369, %367, %370) : (tensor<32x1x13x2880xbf16, #ttnn_layout61>, tensor<32x1x1x2880xbf16, #ttnn_layout61>, tensor<32x1x13x2880xbf16, #ttnn_layout61>) -> tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %372 = ttir.empty() : tensor<32x1x13x2880xf32, #ttnn_layout62>
        %373 = "ttir.typecast"(%371, %372) <{conservative_folding = false}> : (tensor<32x1x13x2880xbf16, #ttnn_layout61>, tensor<32x1x13x2880xf32, #ttnn_layout62>) -> tensor<32x1x13x2880xf32, #ttnn_layout62>
        %374 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 13 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<13xsi32, #ttnn_layout24>
        %375 = ttir.empty() : tensor<13x1x1xsi32, #ttnn_layout63>
        %376 = "ttir.reshape"(%374, %375) <{shape = [13 : i32, 1 : i32, 1 : i32]}> : (tensor<13xsi32, #ttnn_layout24>, tensor<13x1x1xsi32, #ttnn_layout63>) -> tensor<13x1x1xsi32, #ttnn_layout63>
        %377 = ttir.empty() : tensor<13x4x1xsi32, #ttnn_layout63>
        %378 = "ttir.broadcast"(%376, %377) <{broadcast_dimensions = array<i64: 1, 4, 1>}> : (tensor<13x1x1xsi32, #ttnn_layout63>, tensor<13x4x1xsi32, #ttnn_layout63>) -> tensor<13x4x1xsi32, #ttnn_layout63>
        %379 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %380 = "ttir.matmul"(%315, %arg12, %379) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<32x2880xbf16, #ttnn_layout8>, tensor<13x32xbf16, #ttnn_layout27>) -> tensor<13x32xbf16, #ttnn_layout27>
        %381 = ttir.empty() : tensor<1x32xbf16, #ttnn_layout27>
        %382 = "ttir.reshape"(%arg11, %381) <{shape = [1 : i32, 32 : i32]}> : (tensor<32xbf16, #ttnn_layout7>, tensor<1x32xbf16, #ttnn_layout27>) -> tensor<1x32xbf16, #ttnn_layout27>
        %383 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %384 = "ttir.add"(%380, %382, %383) : (tensor<13x32xbf16, #ttnn_layout27>, tensor<1x32xbf16, #ttnn_layout27>, tensor<13x32xbf16, #ttnn_layout27>) -> tensor<13x32xbf16, #ttnn_layout27>
        %385 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %386 = ttir.empty() : tensor<13x32xsi32, #ttnn_layout3>
        %values, %indices = "ttir.sort"(%384, %385, %386) <{descending = true, dim = 1 : si32, stable = false}> : (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x32xbf16, #ttnn_layout27>, tensor<13x32xsi32, #ttnn_layout3>) -> (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x32xsi32, #ttnn_layout3>)
        %387 = ttir.empty() : tensor<13x4xsi32, #ttnn_layout3>
        %388 = "ttir.slice_static"(%indices, %387) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xsi32, #ttnn_layout3>, tensor<13x4xsi32, #ttnn_layout3>) -> tensor<13x4xsi32, #ttnn_layout3>
        %389 = ttir.empty() : tensor<13x4x1xsi32, #ttnn_layout63>
        %390 = "ttir.reshape"(%388, %389) <{shape = [13 : i32, 4 : i32, 1 : i32]}> : (tensor<13x4xsi32, #ttnn_layout3>, tensor<13x4x1xsi32, #ttnn_layout63>) -> tensor<13x4x1xsi32, #ttnn_layout63>
        %391 = ttir.empty() : tensor<13x4x2xsi32, #ttnn_layout63>
        %392 = "ttir.concat"(%378, %390, %391) <{dim = 2 : si32}> : (tensor<13x4x1xsi32, #ttnn_layout63>, tensor<13x4x1xsi32, #ttnn_layout63>, tensor<13x4x2xsi32, #ttnn_layout63>) -> tensor<13x4x2xsi32, #ttnn_layout63>
        %393 = ttir.empty() : tensor<13x4xbf16, #ttnn_layout27>
        %394 = "ttir.slice_static"(%values, %393) <{begins = [0 : i32, 0 : i32], ends = [13 : i32, 4 : i32], step = [1 : i32, 1 : i32]}> : (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x4xbf16, #ttnn_layout27>) -> tensor<13x4xbf16, #ttnn_layout27>
        %395 = ttir.empty() : tensor<13x4xbf16, #ttnn_layout27>
        %396 = "ttir.softmax"(%394, %395) <{dimension = 1 : si32, numericStable = true}> : (tensor<13x4xbf16, #ttnn_layout27>, tensor<13x4xbf16, #ttnn_layout27>) -> tensor<13x4xbf16, #ttnn_layout27>
        %397 = ttir.empty() : tensor<13x32xbf16, #ttnn_layout27>
        %398 = "ttir.scatter"(%11, %392, %396, %397) <{index_vector_dim = 2 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 0, 1>, scatter_dims_to_operand_dims = array<i32: 0, 1>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32>}> : (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x4x2xsi32, #ttnn_layout63>, tensor<13x4xbf16, #ttnn_layout27>, tensor<13x32xbf16, #ttnn_layout27>) -> tensor<13x32xbf16, #ttnn_layout27>
        %399 = ttir.empty() : tensor<13x32xf32, #ttnn_layout26>
        %400 = "ttir.typecast"(%398, %399) <{conservative_folding = false}> : (tensor<13x32xbf16, #ttnn_layout27>, tensor<13x32xf32, #ttnn_layout26>) -> tensor<13x32xf32, #ttnn_layout26>
        %401 = ttir.empty() : tensor<32x13xf32, #ttnn_layout26>
        %402 = "ttir.permute"(%400, %401) <{permutation = array<i64: 1, 0>}> : (tensor<13x32xf32, #ttnn_layout26>, tensor<32x13xf32, #ttnn_layout26>) -> tensor<32x13xf32, #ttnn_layout26>
        %403 = ttir.empty() : tensor<32x1x13x1xf32, #ttnn_layout64>
        %404 = "ttir.reshape"(%402, %403) <{shape = [32 : i32, 1 : i32, 13 : i32, 1 : i32]}> : (tensor<32x13xf32, #ttnn_layout26>, tensor<32x1x13x1xf32, #ttnn_layout64>) -> tensor<32x1x13x1xf32, #ttnn_layout64>
        %405 = ttir.empty() : tensor<32x1x13x2880xf32, #ttnn_layout62>
        %406 = "ttir.multiply"(%373, %404, %405) : (tensor<32x1x13x2880xf32, #ttnn_layout62>, tensor<32x1x13x1xf32, #ttnn_layout64>, tensor<32x1x13x2880xf32, #ttnn_layout62>) -> tensor<32x1x13x2880xf32, #ttnn_layout62>
        %407 = ttir.empty() : tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %408 = "ttir.typecast"(%406, %407) <{conservative_folding = false}> : (tensor<32x1x13x2880xf32, #ttnn_layout62>, tensor<32x1x13x2880xbf16, #ttnn_layout61>) -> tensor<32x1x13x2880xbf16, #ttnn_layout61>
        %409 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %410 = "ttir.sum"(%408, %409) <{dim_arg = [0 : i32], keep_dim = false}> : (tensor<32x1x13x2880xbf16, #ttnn_layout61>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %411 = ttir.empty() : tensor<1x13x2880xbf16, #ttnn_layout35>
        %412 = "ttir.add"(%285, %410, %411) : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xbf16, #ttnn_layout35>) -> tensor<1x13x2880xbf16, #ttnn_layout35>
        %413 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %414 = "ttir.typecast"(%412, %413) <{conservative_folding = false}> : (tensor<1x13x2880xbf16, #ttnn_layout35>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %415 = ttir.empty() : tensor<1x13x2880xf32, #ttnn_layout36>
        %416 = "ttir.pow"(%414, %19, %415) : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x1x1xf32, #ttnn_layout23>, tensor<1x13x2880xf32, #ttnn_layout36>) -> tensor<1x13x2880xf32, #ttnn_layout36>
        %417 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %418 = "ttir.sum"(%416, %417) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %419 = ttir.empty() : tensor<1x13xf32, #ttnn_layout26>
        %420 = "ttir.multiply"(%418, %4, %419) : (tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>, tensor<1x13xf32, #ttnn_layout26>) -> tensor<1x13xf32, #ttnn_layout26>
        %421 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %422 = "ttir.reshape"(%420, %421) <{shape = [13 : i32, 1 : i32]}> : (tensor<1x13xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %423 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %424 = "ttir.add"(%422, %45, %423) : (tensor<13x1xf32, #ttnn_layout26>, tensor<1x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %425 = ttir.empty() : tensor<13x1xf32, #ttnn_layout26>
        %426 = "ttir.rsqrt"(%424, %425) : (tensor<13x1xf32, #ttnn_layout26>, tensor<13x1xf32, #ttnn_layout26>) -> tensor<13x1xf32, #ttnn_layout26>
        %427 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %428 = "ttir.reshape"(%414, %427) <{shape = [13 : i32, 2880 : i32]}> : (tensor<1x13x2880xf32, #ttnn_layout36>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %429 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %430 = "ttir.multiply"(%428, %426, %429) : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x1xf32, #ttnn_layout26>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %431 = ttir.empty() : tensor<13x2880xf32, #ttnn_layout32>
        %432 = "ttir.multiply"(%255, %430, %431) : (tensor<1x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xf32, #ttnn_layout32>) -> tensor<13x2880xf32, #ttnn_layout32>
        %433 = ttir.empty() : tensor<13x2880xbf16, #ttnn_layout8>
        %434 = "ttir.typecast"(%432, %433) <{conservative_folding = false}> : (tensor<13x2880xf32, #ttnn_layout32>, tensor<13x2880xbf16, #ttnn_layout8>) -> tensor<13x2880xbf16, #ttnn_layout8>
        %435 = ttir.empty() : tensor<13x201088xbf16, #ttnn_layout21>
        %436 = "ttir.matmul"(%434, %arg10, %435) <{transpose_a = false, transpose_b = true}> : (tensor<13x2880xbf16, #ttnn_layout8>, tensor<201088x2880xbf16, #ttnn_layout4>, tensor<13x201088xbf16, #ttnn_layout21>) -> tensor<13x201088xbf16, #ttnn_layout21>
        %437 = ttir.empty() : tensor<1x13x201088xbf16, #ttnn_layout22>
        %438 = "ttir.reshape"(%436, %437) <{shape = [1 : i32, 13 : i32, 201088 : i32]}> : (tensor<13x201088xbf16, #ttnn_layout21>, tensor<1x13x201088xbf16, #ttnn_layout22>) -> tensor<1x13x201088xbf16, #ttnn_layout22>
        return %247, %249, %251, %177, %436, %438 : tensor<1x13x512xbf16, #ttnn_layout18>, tensor<1x13x8x64xbf16, #ttnn_layout19>, tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<1x8x13x64xbf16, #ttnn_layout20>, tensor<13x201088xbf16, #ttnn_layout21>, tensor<1x13x201088xbf16, #ttnn_layout22>
      }
    }
  }
}


Running Tensor Parallel Inference
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
[HET PYTH] get_op_sharding_v2
Tensor parallel sharding applied successfully!
Preparing inputs for TP: batch_size=1, seq_length=13
[HET PYTH] get_op_sharding_v2
Error during execution: Error code: 13
