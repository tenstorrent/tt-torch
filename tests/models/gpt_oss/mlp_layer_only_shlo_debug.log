WARNING:root:Defaulting to PJRT_DEVICE=CPU
2025-09-16 12:48:54.716622: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<32x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg8: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %3 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %5 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>
    %6 = "vhlo.transpose_v1"(%arg2) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,32]{0,1}">} : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>
    %7 = "vhlo.dot_general_v2"(%5, %6) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %8 = "vhlo.broadcast_in_dim_v1"(%arg1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %9 = "vhlo.add_v1"(%7, %8) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %10 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.i32_v1>
    %12:2 = "vhlo.sort_v1"(%9, %11) <{dimension = #vhlo.integer_v1<1 : i64>, is_stable = #vhlo.bool_v1<false>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg14: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %67 = "vhlo.compare_v1"(%arg11, %arg12) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      "vhlo.return_v1"(%67) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>)
    %13 = "vhlo.slice_v1"(%12#1) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i32_v1>
    %14 = "vhlo.convert_v1"(%13) : (!vhlo.tensor_v1<128x4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i64_v1>
    %15 = "vhlo.slice_v1"(%12#0) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %16 = "vhlo.reduce_v1"(%15, %0) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %67 = "vhlo.maximum_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%67) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %17 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %18 = "vhlo.subtract_v1"(%15, %17) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %19 = "vhlo.exponential_v2"(%18) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %20 = "vhlo.reduce_v1"(%19, %1) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %67 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%67) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %22 = "vhlo.divide_v1"(%19, %21) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %23 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<128x!vhlo.i64_v1>
    %24 = "vhlo.broadcast_in_dim_v1"(%23) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %25 = "vhlo.reshape_v1"(%14) : (!vhlo.tensor_v1<128x4x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %26 = "vhlo.concatenate_v1"(%24, %25) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>
    %27 = "vhlo.scatter_v2"(%4, %26, %22) <{index_vector_dim = #vhlo.integer_v1<2 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %28 = "vhlo.concatenate_v1"(%5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5) <{dimension = #vhlo.integer_v1<0 : i64>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>
    %29 = "vhlo.reshape_v1"(%28) : (!vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %30 = "vhlo.dot_general_v2"(%29, %arg4) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %31 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %32 = "vhlo.add_v1"(%30, %31) : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %33 = "vhlo.slice_v1"(%32) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %34 = "vhlo.slice_v1"(%32) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 1]> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %35 = "vhlo.broadcast_in_dim_v1"(%arg6) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %36 = "vhlo.broadcast_in_dim_v1"(%arg5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %37 = "vhlo.clamp_v1"(%35, %33, %36) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %38 = "vhlo.convert_v1"(%37) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %40 = "vhlo.multiply_v1"(%38, %39) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %41 = "vhlo.convert_v1"(%40) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %42 = "vhlo.logistic_v2"(%41) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %43 = "vhlo.convert_v1"(%42) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %44 = "vhlo.multiply_v1"(%38, %43) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %46 = "vhlo.broadcast_in_dim_v1"(%arg8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %47 = "vhlo.clamp_v1"(%46, %34, %36) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %48 = "vhlo.add_v1"(%47, %3) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %50 = "vhlo.convert_v1"(%45) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %51 = "vhlo.multiply_v1"(%49, %50) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %52 = "vhlo.convert_v1"(%51) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %53 = "vhlo.dot_general_v2"(%52, %arg10) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %54 = "vhlo.broadcast_in_dim_v1"(%arg9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %55 = "vhlo.add_v1"(%53, %54) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %57 = "vhlo.transpose_v1"(%27) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[32,128]{0,1}">} : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>
    %58 = "vhlo.reshape_v1"(%57) : (!vhlo.tensor_v1<32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>
    %59 = "vhlo.reshape_v1"(%57) : (!vhlo.tensor_v1<32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>
    %60 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %61 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>
    %62 = "vhlo.reshape_v1"(%61) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>
    %63 = "vhlo.broadcast_in_dim_v1"(%62) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %64 = "vhlo.multiply_v1"(%60, %63) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %65 = "vhlo.convert_v1"(%64) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %66 = "vhlo.reduce_v1"(%65, %1) <{dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %67 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%67) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>
    "vhlo.return_v1"(%5, %14, %14, %22, %4, %27, %27, %28, %29, %32, %32, %33, %33, %34, %34, %37, %37, %42, %42, %45, %45, %48, %48, %52, %52, %55, %55, %56, %56, %57, %57, %58, %58, %59, %59, %66, %66) : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
// -----// IR Dump Before VhloToVersionPass (vhlo-to-version) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<32x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg8: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %3 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %5 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>
    %6 = "vhlo.transpose_v1"(%arg2) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,32]{0,1}">} : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>
    %7 = "vhlo.dot_general_v2"(%5, %6) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %8 = "vhlo.broadcast_in_dim_v1"(%arg1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %9 = "vhlo.add_v1"(%7, %8) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %10 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.i32_v1>
    %12:2 = "vhlo.sort_v1"(%9, %11) <{dimension = #vhlo.integer_v1<1 : i64>, is_stable = #vhlo.bool_v1<false>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg14: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %67 = "vhlo.compare_v1"(%arg11, %arg12) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      "vhlo.return_v1"(%67) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>)
    %13 = "vhlo.slice_v1"(%12#1) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i32_v1>
    %14 = "vhlo.convert_v1"(%13) : (!vhlo.tensor_v1<128x4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i64_v1>
    %15 = "vhlo.slice_v1"(%12#0) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %16 = "vhlo.reduce_v1"(%15, %0) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %67 = "vhlo.maximum_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%67) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %17 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %18 = "vhlo.subtract_v1"(%15, %17) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %19 = "vhlo.exponential_v2"(%18) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %20 = "vhlo.reduce_v1"(%19, %1) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %67 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%67) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %22 = "vhlo.divide_v1"(%19, %21) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %23 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<128x!vhlo.i64_v1>
    %24 = "vhlo.broadcast_in_dim_v1"(%23) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %25 = "vhlo.reshape_v1"(%14) : (!vhlo.tensor_v1<128x4x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %26 = "vhlo.concatenate_v1"(%24, %25) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>
    %27 = "vhlo.scatter_v2"(%4, %26, %22) <{index_vector_dim = #vhlo.integer_v1<2 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %28 = "vhlo.concatenate_v1"(%5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5) <{dimension = #vhlo.integer_v1<0 : i64>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>
    %29 = "vhlo.reshape_v1"(%28) : (!vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %30 = "vhlo.dot_general_v2"(%29, %arg4) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %31 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %32 = "vhlo.add_v1"(%30, %31) : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %33 = "vhlo.slice_v1"(%32) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %34 = "vhlo.slice_v1"(%32) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 1]> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %35 = "vhlo.broadcast_in_dim_v1"(%arg6) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %36 = "vhlo.broadcast_in_dim_v1"(%arg5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %37 = "vhlo.clamp_v1"(%35, %33, %36) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %38 = "vhlo.convert_v1"(%37) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %40 = "vhlo.multiply_v1"(%38, %39) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %41 = "vhlo.convert_v1"(%40) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %42 = "vhlo.logistic_v2"(%41) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %43 = "vhlo.convert_v1"(%42) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %44 = "vhlo.multiply_v1"(%38, %43) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %46 = "vhlo.broadcast_in_dim_v1"(%arg8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %47 = "vhlo.clamp_v1"(%46, %34, %36) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %48 = "vhlo.add_v1"(%47, %3) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %50 = "vhlo.convert_v1"(%45) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %51 = "vhlo.multiply_v1"(%49, %50) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %52 = "vhlo.convert_v1"(%51) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %53 = "vhlo.dot_general_v2"(%52, %arg10) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %54 = "vhlo.broadcast_in_dim_v1"(%arg9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %55 = "vhlo.add_v1"(%53, %54) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %57 = "vhlo.transpose_v1"(%27) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[32,128]{0,1}">} : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>
    %58 = "vhlo.reshape_v1"(%57) : (!vhlo.tensor_v1<32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>
    %59 = "vhlo.reshape_v1"(%57) : (!vhlo.tensor_v1<32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>
    %60 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %61 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>
    %62 = "vhlo.reshape_v1"(%61) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>
    %63 = "vhlo.broadcast_in_dim_v1"(%62) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %64 = "vhlo.multiply_v1"(%60, %63) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %65 = "vhlo.convert_v1"(%64) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %66 = "vhlo.reduce_v1"(%65, %1) <{dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %67 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%67) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>
    "vhlo.return_v1"(%5, %14, %14, %22, %4, %27, %27, %28, %29, %32, %32, %33, %33, %34, %34, %37, %37, %42, %42, %45, %45, %48, %48, %52, %52, %55, %55, %56, %56, %57, %57, %58, %58, %59, %59, %66, %66) : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump Before VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<32x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg8: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %3 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %4 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %5 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>
    %6 = "vhlo.transpose_v1"(%arg2) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,32]{0,1}">} : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>
    %7 = "vhlo.dot_general_v2"(%5, %6) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %8 = "vhlo.broadcast_in_dim_v1"(%arg1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %9 = "vhlo.add_v1"(%7, %8) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %10 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.i32_v1>
    %12:2 = "vhlo.sort_v1"(%9, %11) <{dimension = #vhlo.integer_v1<1 : i64>, is_stable = #vhlo.bool_v1<false>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg14: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %67 = "vhlo.compare_v1"(%arg11, %arg12) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      "vhlo.return_v1"(%67) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.i32_v1>)
    %13 = "vhlo.slice_v1"(%12#1) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i32_v1>
    %14 = "vhlo.convert_v1"(%13) : (!vhlo.tensor_v1<128x4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.i64_v1>
    %15 = "vhlo.slice_v1"(%12#0) <{limit_indices = #vhlo.tensor_v1<dense<[128, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %16 = "vhlo.reduce_v1"(%15, %0) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %67 = "vhlo.maximum_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%67) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %17 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %18 = "vhlo.subtract_v1"(%15, %17) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %19 = "vhlo.exponential_v2"(%18) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %20 = "vhlo.reduce_v1"(%19, %1) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %67 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%67) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %22 = "vhlo.divide_v1"(%19, %21) : (!vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>
    %23 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<128x!vhlo.i64_v1>
    %24 = "vhlo.broadcast_in_dim_v1"(%23) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %25 = "vhlo.reshape_v1"(%14) : (!vhlo.tensor_v1<128x4x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>
    %26 = "vhlo.concatenate_v1"(%24, %25) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>
    %27 = "vhlo.scatter_v2"(%4, %26, %22) <{index_vector_dim = #vhlo.integer_v1<2 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x2x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>
    %28 = "vhlo.concatenate_v1"(%5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5) <{dimension = #vhlo.integer_v1<0 : i64>}> : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>
    %29 = "vhlo.reshape_v1"(%28) : (!vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %30 = "vhlo.dot_general_v2"(%29, %arg4) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %31 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %32 = "vhlo.add_v1"(%30, %31) : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>
    %33 = "vhlo.slice_v1"(%32) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %34 = "vhlo.slice_v1"(%32) <{limit_indices = #vhlo.tensor_v1<dense<[32, 128, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 1]> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %35 = "vhlo.broadcast_in_dim_v1"(%arg6) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %36 = "vhlo.broadcast_in_dim_v1"(%arg5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %37 = "vhlo.clamp_v1"(%35, %33, %36) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %38 = "vhlo.convert_v1"(%37) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %40 = "vhlo.multiply_v1"(%38, %39) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %41 = "vhlo.convert_v1"(%40) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %42 = "vhlo.logistic_v2"(%41) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %43 = "vhlo.convert_v1"(%42) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %44 = "vhlo.multiply_v1"(%38, %43) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %46 = "vhlo.broadcast_in_dim_v1"(%arg8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %47 = "vhlo.clamp_v1"(%46, %34, %36) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %48 = "vhlo.add_v1"(%47, %3) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %50 = "vhlo.convert_v1"(%45) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %51 = "vhlo.multiply_v1"(%49, %50) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>
    %52 = "vhlo.convert_v1"(%51) : (!vhlo.tensor_v1<32x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %53 = "vhlo.dot_general_v2"(%52, %arg10) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %54 = "vhlo.broadcast_in_dim_v1"(%arg9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %55 = "vhlo.add_v1"(%53, %54) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %57 = "vhlo.transpose_v1"(%27) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[32,128]{0,1}">} : (!vhlo.tensor_v1<128x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>
    %58 = "vhlo.reshape_v1"(%57) : (!vhlo.tensor_v1<32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>
    %59 = "vhlo.reshape_v1"(%57) : (!vhlo.tensor_v1<32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>
    %60 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %61 = "vhlo.convert_v1"(%59) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>
    %62 = "vhlo.reshape_v1"(%61) : (!vhlo.tensor_v1<32x1x128x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>
    %63 = "vhlo.broadcast_in_dim_v1"(%62) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %64 = "vhlo.multiply_v1"(%60, %63) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>
    %65 = "vhlo.convert_v1"(%64) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>
    %66 = "vhlo.reduce_v1"(%65, %1) <{dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> ({
    ^bb0(%arg11: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %67 = "vhlo.add_v1"(%arg11, %arg12) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%67) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>
    "vhlo.return_v1"(%5, %14, %14, %22, %4, %27, %27, %28, %29, %32, %32, %33, %33, %34, %34, %37, %37, %42, %42, %45, %45, %48, %48, %52, %52, %55, %55, %56, %56, %57, %57, %58, %58, %59, %59, %66, %66) : (!vhlo.tensor_v1<128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x4x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.i64_v1>, !vhlo.tensor_v1<128x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x1x128x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128x2880x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump After VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg9: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}) -> (tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg9: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}) -> (tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg9: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}) -> (tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg9: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}) -> (tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg9: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}) -> (tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg9: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}) -> (tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg4: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg5: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg9: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}) -> (tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4096x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1x128x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4096x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4096x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4096x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4096x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %64 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %64 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %26 = stablehlo.reshape %25 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %27 = stablehlo.dot_general %26, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %28 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %29 = stablehlo.add %27, %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
    %30 = stablehlo.slice %29 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %31 = stablehlo.slice %29 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %32 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %33 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %34 = stablehlo.clamp %32, %30, %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %35 = stablehlo.convert %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %36 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %37 = stablehlo.multiply %35, %36 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %38 = stablehlo.convert %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %39 = stablehlo.logistic %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %40 = stablehlo.convert %39 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %41 = stablehlo.multiply %35, %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %42 = stablehlo.convert %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %44 = stablehlo.clamp %43, %31, %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %45 = stablehlo.add %44, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %46 = stablehlo.convert %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %47 = stablehlo.convert %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %48 = stablehlo.multiply %46, %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %50 = stablehlo.dot_general %49, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %51 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %52 = stablehlo.add %50, %51 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %53 = stablehlo.reshape %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %54 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %55 = stablehlo.reshape %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %56 = stablehlo.reshape %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %57 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %58 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %59 = stablehlo.reshape %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %60 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %61 = stablehlo.multiply %57, %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
    %62 = stablehlo.convert %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %63 = stablehlo.reduce(%62 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %25, %26, %29, %29, %30, %30, %31, %31, %34, %34, %39, %39, %42, %42, %45, %45, %49, %49, %52, %52, %53, %53, %54, %54, %55, %55, %56, %56, %63, %63 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %97 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %97 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %26 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %27 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %28 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %29 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %30 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %31 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %32 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %33 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %34 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %35 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %36 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %37 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %38 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %39 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %40 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %41 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %42 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %43 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %44 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %45 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %46 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %47 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %48 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %49 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %50 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %51 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %52 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %53 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %54 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %55 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %56 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %57 = stablehlo.concatenate %25, %26, %27, %28, %29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, %42, %43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, %56, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %58 = stablehlo.reshape %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %59 = stablehlo.dot_general %58, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %60 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %61 = stablehlo.add %59, %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
    %62 = stablehlo.slice %61 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %63 = stablehlo.slice %61 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %64 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %65 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %66 = stablehlo.clamp %64, %62, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %67 = stablehlo.convert %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %68 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %69 = stablehlo.multiply %67, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %71 = stablehlo.logistic %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %72 = stablehlo.convert %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %73 = stablehlo.multiply %67, %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %74 = stablehlo.convert %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %75 = stablehlo.broadcast_in_dim %arg8, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %76 = stablehlo.clamp %75, %63, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %77 = stablehlo.add %76, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %78 = stablehlo.convert %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %79 = stablehlo.convert %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %80 = stablehlo.multiply %78, %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %81 = stablehlo.convert %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %82 = stablehlo.dot_general %81, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %83 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %84 = stablehlo.add %82, %83 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %85 = stablehlo.reshape %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %86 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %88 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %89 = stablehlo.convert %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %90 = stablehlo.convert %88 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %91 = stablehlo.reshape %90 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %92 = stablehlo.broadcast_in_dim %91, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %93 = stablehlo.multiply %89, %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
    %94 = stablehlo.convert %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %95 = stablehlo.reduce(%94 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    %96 = sdy.all_reduce {"_axis_0"} %95 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %57, %58, %61, %61, %62, %62, %63, %63, %66, %66, %71, %71, %74, %74, %77, %77, %81, %81, %84, %84, %85, %85, %86, %86, %87, %87, %88, %88, %96, %96 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
    %2 = stablehlo.reshape %arg0 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
    %3 = stablehlo.transpose %arg2, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
    %6 = stablehlo.add %4, %5 : tensor<128x32xbf16>
    %7 = stablehlo.iota dim = 0 : tensor<32xi32>
    %8 = stablehlo.broadcast_in_dim %7, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
    %9:2 = "stablehlo.sort"(%6, %8) <{dimension = 1 : i64}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>, %arg13: tensor<i32>, %arg14: tensor<i32>):
      %97 = stablehlo.compare  GT, %arg11, %arg12,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %97 : tensor<i1>
    }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
    %10 = stablehlo.slice %9#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
    %11 = stablehlo.convert %10 : (tensor<128x4xi32>) -> tensor<128x4xi64>
    %12 = stablehlo.slice %9#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
    %13 = stablehlo.reduce(%12 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %15 = stablehlo.subtract %12, %14 : tensor<128x4xbf16>
    %16 = stablehlo.exponential %15 : tensor<128x4xbf16>
    %17 = stablehlo.reduce(%16 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
    %19 = stablehlo.divide %16, %18 : tensor<128x4xbf16>
    %20 = stablehlo.iota dim = 0 : tensor<128xi64>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
    %22 = stablehlo.reshape %11 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
    %23 = stablehlo.concatenate %21, %22, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
    %24 = "stablehlo.scatter"(%1, %23, %19) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg11: tensor<bf16>, %arg12: tensor<bf16>):
      stablehlo.return %arg12 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
    %25 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %26 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %27 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %28 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %29 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %30 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %31 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %32 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %33 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %34 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %35 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %36 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %37 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %38 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %39 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %40 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %41 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %42 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %43 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %44 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %45 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %46 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %47 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %48 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %49 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %50 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %51 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %52 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %53 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %54 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %55 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %56 = sdy.reshard %2 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
    %57 = stablehlo.concatenate %25, %26, %27, %28, %29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, %42, %43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, %56, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
    %58 = stablehlo.reshape %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
    %59 = stablehlo.dot_general %58, %arg4, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
    %60 = stablehlo.broadcast_in_dim %arg3, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
    %61 = stablehlo.add %59, %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
    %62 = stablehlo.slice %61 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %63 = stablehlo.slice %61 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
    %64 = stablehlo.broadcast_in_dim %arg6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %65 = stablehlo.broadcast_in_dim %arg5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %66 = stablehlo.clamp %64, %62, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %67 = stablehlo.convert %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %68 = stablehlo.broadcast_in_dim %arg7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
    %69 = stablehlo.multiply %67, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %70 = stablehlo.convert %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %71 = stablehlo.logistic %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %72 = stablehlo.convert %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %73 = stablehlo.multiply %67, %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %74 = stablehlo.convert %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %75 = stablehlo.broadcast_in_dim %arg8, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
    %76 = stablehlo.clamp %75, %63, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %77 = stablehlo.add %76, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %78 = stablehlo.convert %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %79 = stablehlo.convert %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
    %80 = stablehlo.multiply %78, %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
    %81 = stablehlo.convert %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
    %82 = stablehlo.dot_general %81, %arg10, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
    %83 = stablehlo.broadcast_in_dim %arg9, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
    %84 = stablehlo.add %82, %83 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
    %85 = stablehlo.reshape %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
    %86 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
    %87 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
    %88 = stablehlo.reshape %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
    %89 = stablehlo.convert %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
    %90 = stablehlo.convert %88 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
    %91 = stablehlo.reshape %90 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
    %92 = stablehlo.broadcast_in_dim %91, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
    %93 = stablehlo.multiply %89, %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
    %94 = stablehlo.convert %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
    %95 = stablehlo.reduce(%94 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
    %96 = sdy.all_reduce {"_axis_0"} %95 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
    return %2, %11, %11, %19, %1, %24, %24, %57, %58, %61, %61, %62, %62, %63, %63, %66, %66, %71, %71, %74, %74, %77, %77, %81, %81, %84, %84, %85, %85, %86, %86, %87, %87, %88, %88, %96, %96 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:37 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>] out_shardings=[<@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg11: tensor<1x128x2880xbf16>, %arg12: tensor<32xbf16>, %arg13: tensor<32x2880xbf16>, %arg14: tensor<32x5760xbf16>, %arg15: tensor<32x2880x5760xbf16>, %arg16: tensor<bf16>, %arg17: tensor<bf16>, %arg18: tensor<f32>, %arg19: tensor<bf16>, %arg20: tensor<32x2880xbf16>, %arg21: tensor<32x2880x2880xbf16>) {
      %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
      %3 = stablehlo.reshape %arg11 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %4 = stablehlo.transpose %arg13, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %5 = stablehlo.dot_general %3, %4, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %6 = stablehlo.broadcast_in_dim %arg12, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %7 = stablehlo.add %5, %6 : tensor<128x32xbf16>
      %8 = stablehlo.iota dim = 0 : tensor<32xi32>
      %9 = stablehlo.broadcast_in_dim %8, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %10:2 = "stablehlo.sort"(%7, %9) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %98 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %98 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %11 = stablehlo.slice %10#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %12 = stablehlo.convert %11 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %13 = stablehlo.slice %10#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %15 = stablehlo.broadcast_in_dim %14, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %16 = stablehlo.subtract %13, %15 : tensor<128x4xbf16>
      %17 = stablehlo.exponential %16 : tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.divide %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.iota dim = 0 : tensor<128xi64>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %23 = stablehlo.reshape %12 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %24 = stablehlo.concatenate %22, %23, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %25 = "stablehlo.scatter"(%2, %24, %20) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
      %26 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %27 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %28 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %29 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %30 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %31 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %32 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %33 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %34 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %35 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %36 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %37 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %38 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %39 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %40 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %41 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %42 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %43 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %44 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %45 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %46 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %47 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %48 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %49 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %50 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %51 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %52 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %53 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %54 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %55 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %56 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %57 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %58 = stablehlo.concatenate %26, %27, %28, %29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, %42, %43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, %56, %57, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
      %59 = stablehlo.reshape %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
      %60 = stablehlo.dot_general %59, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
      %61 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
      %62 = stablehlo.add %60, %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
      %63 = stablehlo.slice %62 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %64 = stablehlo.slice %62 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %65 = stablehlo.broadcast_in_dim %arg17, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %66 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %67 = stablehlo.clamp %65, %63, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %68 = stablehlo.convert %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %69 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
      %70 = stablehlo.multiply %68, %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %71 = stablehlo.convert %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %72 = stablehlo.logistic %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %73 = stablehlo.convert %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %74 = stablehlo.multiply %68, %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %75 = stablehlo.convert %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %76 = stablehlo.broadcast_in_dim %arg19, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %77 = stablehlo.clamp %76, %64, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %78 = stablehlo.add %77, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %80 = stablehlo.convert %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %81 = stablehlo.multiply %79, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %83 = stablehlo.dot_general %82, %arg21, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
      %84 = stablehlo.broadcast_in_dim %arg20, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
      %85 = stablehlo.add %83, %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %86 = stablehlo.reshape %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
      %87 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
      %88 = stablehlo.reshape %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
      %89 = stablehlo.reshape %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
      %90 = stablehlo.convert %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
      %91 = stablehlo.convert %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
      %92 = stablehlo.reshape %91 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
      %93 = stablehlo.broadcast_in_dim %92, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
      %94 = stablehlo.multiply %90, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
      %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
      %96 = stablehlo.reduce(%95 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %97 = sdy.all_reduce {"_axis_0"} %96 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
      sdy.return %3, %12, %12, %20, %2, %25, %25, %58, %59, %62, %62, %63, %63, %64, %64, %67, %67, %72, %72, %75, %75, %78, %78, %82, %82, %85, %85, %86, %86, %87, %87, %88, %88, %89, %89, %97, %97 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<1x128x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>, tensor<bf16>, tensor<f32>, tensor<bf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>) -> (tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4, %0#5, %0#6, %0#7, %0#8, %0#9, %0#10, %0#11, %0#12, %0#13, %0#14, %0#15, %0#16, %0#17, %0#18, %0#19, %0#20, %0#21, %0#22, %0#23, %0#24, %0#25, %0#26, %0#27, %0#28, %0#29, %0#30, %0#31, %0#32, %0#33, %0#34, %0#35, %0#36 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:37 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>] out_shardings=[<@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg11: tensor<1x128x2880xbf16>, %arg12: tensor<32xbf16>, %arg13: tensor<32x2880xbf16>, %arg14: tensor<32x5760xbf16>, %arg15: tensor<32x2880x5760xbf16>, %arg16: tensor<bf16>, %arg17: tensor<bf16>, %arg18: tensor<f32>, %arg19: tensor<bf16>, %arg20: tensor<32x2880xbf16>, %arg21: tensor<32x2880x2880xbf16>) {
      %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
      %3 = stablehlo.reshape %arg11 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %4 = stablehlo.transpose %arg13, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %5 = stablehlo.dot_general %3, %4, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %6 = stablehlo.broadcast_in_dim %arg12, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %7 = stablehlo.add %5, %6 : tensor<128x32xbf16>
      %8 = stablehlo.iota dim = 0 : tensor<32xi32>
      %9 = stablehlo.broadcast_in_dim %8, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %10:2 = "stablehlo.sort"(%7, %9) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %98 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %98 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %11 = stablehlo.slice %10#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %12 = stablehlo.convert %11 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %13 = stablehlo.slice %10#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %15 = stablehlo.broadcast_in_dim %14, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %16 = stablehlo.subtract %13, %15 : tensor<128x4xbf16>
      %17 = stablehlo.exponential %16 : tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.divide %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.iota dim = 0 : tensor<128xi64>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %23 = stablehlo.reshape %12 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %24 = stablehlo.concatenate %22, %23, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %25 = "stablehlo.scatter"(%2, %24, %20) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
      %26 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %27 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %28 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %29 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %30 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %31 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %32 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %33 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %34 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %35 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %36 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %37 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %38 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %39 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %40 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %41 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %42 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %43 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %44 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %45 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %46 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %47 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %48 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %49 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %50 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %51 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %52 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %53 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %54 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %55 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %56 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %57 = sdy.reshard %3 <@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %58 = stablehlo.concatenate %26, %27, %28, %29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, %42, %43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, %56, %57, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
      %59 = stablehlo.reshape %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
      %60 = stablehlo.dot_general %59, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
      %61 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
      %62 = stablehlo.add %60, %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
      %63 = stablehlo.slice %62 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %64 = stablehlo.slice %62 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %65 = stablehlo.broadcast_in_dim %arg17, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %66 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %67 = stablehlo.clamp %65, %63, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %68 = stablehlo.convert %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %69 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
      %70 = stablehlo.multiply %68, %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %71 = stablehlo.convert %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %72 = stablehlo.logistic %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %73 = stablehlo.convert %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %74 = stablehlo.multiply %68, %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %75 = stablehlo.convert %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %76 = stablehlo.broadcast_in_dim %arg19, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %77 = stablehlo.clamp %76, %64, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %78 = stablehlo.add %77, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %80 = stablehlo.convert %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %81 = stablehlo.multiply %79, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %83 = stablehlo.dot_general %82, %arg21, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
      %84 = stablehlo.broadcast_in_dim %arg20, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
      %85 = stablehlo.add %83, %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %86 = stablehlo.reshape %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
      %87 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
      %88 = stablehlo.reshape %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
      %89 = stablehlo.reshape %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
      %90 = stablehlo.convert %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
      %91 = stablehlo.convert %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
      %92 = stablehlo.reshape %91 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
      %93 = stablehlo.broadcast_in_dim %92, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
      %94 = stablehlo.multiply %90, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
      %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
      %96 = stablehlo.reduce(%95 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %97 = sdy.all_reduce {"_axis_0"} %96 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
      sdy.return %3, %12, %12, %20, %2, %25, %25, %58, %59, %62, %62, %63, %63, %64, %64, %67, %67, %72, %72, %75, %75, %78, %78, %82, %82, %85, %85, %86, %86, %87, %87, %88, %88, %89, %89, %97, %97 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<1x128x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>, tensor<bf16>, tensor<f32>, tensor<bf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>) -> (tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4, %0#5, %0#6, %0#7, %0#8, %0#9, %0#10, %0#11, %0#12, %0#13, %0#14, %0#15, %0#16, %0#17, %0#18, %0#19, %0#20, %0#21, %0#22, %0#23, %0#24, %0#25, %0#26, %0#27, %0#28, %0#29, %0#30, %0#31, %0#32, %0#33, %0#34, %0#35, %0#36 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump After ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x4xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<128x32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x1x128x1xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128x2880xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:37 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10) in_shardings=[<@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>] out_shardings=[<@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg11: tensor<1x128x2880xbf16>, %arg12: tensor<32xbf16>, %arg13: tensor<32x2880xbf16>, %arg14: tensor<32x5760xbf16>, %arg15: tensor<32x2880x5760xbf16>, %arg16: tensor<bf16>, %arg17: tensor<bf16>, %arg18: tensor<f32>, %arg19: tensor<bf16>, %arg20: tensor<32x2880xbf16>, %arg21: tensor<32x2880x2880xbf16>) {
      %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
      %3 = stablehlo.reshape %arg11 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %4 = stablehlo.transpose %arg13, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %5 = stablehlo.dot_general %3, %4, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %6 = stablehlo.broadcast_in_dim %arg12, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %7 = stablehlo.add %5, %6 : tensor<128x32xbf16>
      %8 = stablehlo.iota dim = 0 : tensor<32xi32>
      %9 = stablehlo.broadcast_in_dim %8, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %10:2 = "stablehlo.sort"(%7, %9) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %98 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %98 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %11 = stablehlo.slice %10#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %12 = stablehlo.convert %11 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %13 = stablehlo.slice %10#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %15 = stablehlo.broadcast_in_dim %14, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %16 = stablehlo.subtract %13, %15 : tensor<128x4xbf16>
      %17 = stablehlo.exponential %16 : tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.divide %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.iota dim = 0 : tensor<128xi64>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %23 = stablehlo.reshape %12 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %24 = stablehlo.concatenate %22, %23, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %25 = "stablehlo.scatter"(%2, %24, %20) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
      %26 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %27 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %28 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %29 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %30 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %31 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %32 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %33 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %34 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %35 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %36 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %37 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %38 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %39 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %40 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %41 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %42 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %43 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %44 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %45 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %46 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %47 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %48 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %49 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %50 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %51 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %52 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %53 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %54 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %55 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %56 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %57 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %58 = stablehlo.concatenate %26, %27, %28, %29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, %42, %43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, %56, %57, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
      %59 = stablehlo.reshape %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
      %60 = stablehlo.dot_general %59, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
      %61 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
      %62 = stablehlo.add %60, %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
      %63 = stablehlo.slice %62 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %64 = stablehlo.slice %62 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %65 = stablehlo.broadcast_in_dim %arg17, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %66 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %67 = stablehlo.clamp %65, %63, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %68 = stablehlo.convert %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %69 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
      %70 = stablehlo.multiply %68, %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %71 = stablehlo.convert %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %72 = stablehlo.logistic %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %73 = stablehlo.convert %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %74 = stablehlo.multiply %68, %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %75 = stablehlo.convert %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %76 = stablehlo.broadcast_in_dim %arg19, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %77 = stablehlo.clamp %76, %64, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %78 = stablehlo.add %77, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %80 = stablehlo.convert %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %81 = stablehlo.multiply %79, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %83 = stablehlo.dot_general %82, %arg21, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
      %84 = stablehlo.broadcast_in_dim %arg20, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
      %85 = stablehlo.add %83, %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %86 = stablehlo.reshape %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
      %87 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
      %88 = stablehlo.reshape %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
      %89 = stablehlo.reshape %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
      %90 = stablehlo.convert %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
      %91 = stablehlo.convert %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
      %92 = stablehlo.reshape %91 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
      %93 = stablehlo.broadcast_in_dim %92, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
      %94 = stablehlo.multiply %90, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
      %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
      %96 = stablehlo.reduce(%95 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %97 = sdy.all_reduce {"_axis_0"} %96 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
      sdy.return %3, %12, %12, %20, %2, %25, %25, %58, %59, %62, %62, %63, %63, %64, %64, %67, %67, %72, %72, %75, %75, %78, %78, %82, %82, %85, %85, %86, %86, %87, %87, %88, %88, %89, %89, %97, %97 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<1x128x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>, tensor<bf16>, tensor<f32>, tensor<bf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>) -> (tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4, %0#5, %0#6, %0#7, %0#8, %0#9, %0#10, %0#11, %0#12, %0#13, %0#14, %0#15, %0#16, %0#17, %0#18, %0#19, %0#20, %0#21, %0#22, %0#23, %0#24, %0#25, %0#26, %0#27, %0#28, %0#29, %0#30, %0#31, %0#32, %0#33, %0#34, %0#35, %0#36 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(
    %arg0: tensor<1x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<presharded>
    },
    %arg1: tensor<32xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{?}]>,
      ttcore.shard_status = #ttcore.shard_status<presharded>
    },
    %arg2: tensor<32x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<presharded>
    },
    %arg3: tensor<32x5760xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<presharded>
    },
    %arg4: tensor<32x2880x5760xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<presharded>
    },
    %arg5: tensor<bf16> {
      sdy.sharding = #sdy.sharding<@mesh, []>,
      ttcore.shard_status = #ttcore.shard_status<presharded>
    },
    %arg6: tensor<bf16> {
      sdy.sharding = #sdy.sharding<@mesh, []>,
      ttcore.shard_status = #ttcore.shard_status<presharded>
    },
    %arg7: tensor<f32> {
      sdy.sharding = #sdy.sharding<@mesh, []>,
      ttcore.shard_status = #ttcore.shard_status<presharded>
    },
    %arg8: tensor<bf16> {
      sdy.sharding = #sdy.sharding<@mesh, []>,
      ttcore.shard_status = #ttcore.shard_status<presharded>
    },
    %arg9: tensor<32x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<presharded>
    },
    %arg10: tensor<32x2880x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<presharded>
    }
  ) -> (
    tensor<128x2880xbf16> {
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<128x4xi64> {
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<128x4xi64> {
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<128x4xbf16> {
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<128x32xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<128x32xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<128x32xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<4096x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x5760xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x5760xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x1x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x1x128x2880xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x128xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x1x128xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x1x128xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x1x128x1xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<32x1x128x1xbf16> {
      sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>,
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<1x128x2880xbf16> {
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    },
    tensor<1x128x2880xbf16> {
      ttcore.shard_status = #ttcore.shard_status<unsharded>
    }
  ) {
    %0:37 = sdy.manual_computation(
        %arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10
    )
    in_shardings=[
        <@mesh, [{?}, {?}, {?}]>,
        <@mesh, [{?}]>,
        <@mesh, [{?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, []>,
        <@mesh, []>,
        <@mesh, []>,
        <@mesh, []>,
        <@mesh, [{"_axis_0", ?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>
    ]
    out_shardings=[
        <@mesh, [{?}, {?}]>,
        <@mesh, [{?}, {?}]>,
        <@mesh, [{?}, {?}]>,
        <@mesh, [{?}, {?}]>,
        <@mesh, [{?}, {"_axis_0", ?}]>,
        <@mesh, [{?}, {"_axis_0", ?}]>,
        <@mesh, [{?}, {"_axis_0", ?}]>,
        <@mesh, [{"_axis_0", ?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>,
        <@mesh, [{?}, {?}, {?}]>,
        <@mesh, [{?}, {?}, {?}]>
    ]
    manual_axes={}
    (
        %arg11: tensor<1x128x2880xbf16>,
        %arg12: tensor<32xbf16>,
        %arg13: tensor<32x2880xbf16>,
        %arg14: tensor<32x5760xbf16>,
        %arg15: tensor<32x2880x5760xbf16>,
        %arg16: tensor<bf16>,
        %arg17: tensor<bf16>,
        %arg18: tensor<f32>,
        %arg19: tensor<bf16>,
        %arg20: tensor<32x2880xbf16>,
        %arg21: tensor<32x2880x2880xbf16>
    ) {
      %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<128x32xbf16>
      %3 = stablehlo.reshape %arg11 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %4 = stablehlo.transpose %arg13, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %5 = stablehlo.dot_general %3, %4, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %6 = stablehlo.broadcast_in_dim %arg12, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %7 = stablehlo.add %5, %6 : tensor<128x32xbf16>
      %8 = stablehlo.iota dim = 0 : tensor<32xi32>
      %9 = stablehlo.broadcast_in_dim %8, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %10:2 = "stablehlo.sort"(%7, %9) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %98 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %98 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %11 = stablehlo.slice %10#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %12 = stablehlo.convert %11 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %13 = stablehlo.slice %10#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %15 = stablehlo.broadcast_in_dim %14, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %16 = stablehlo.subtract %13, %15 : tensor<128x4xbf16>
      %17 = stablehlo.exponential %16 : tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.divide %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.iota dim = 0 : tensor<128xi64>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %23 = stablehlo.reshape %12 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %24 = stablehlo.concatenate %22, %23, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %25 = "stablehlo.scatter"(%2, %24, %20) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128x32xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x32xbf16>
      %26 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %27 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %28 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %29 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %30 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %31 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %32 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %33 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %34 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %35 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %36 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %37 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %38 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %39 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %40 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %41 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %42 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %43 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %44 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %45 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %46 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %47 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %48 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %49 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %50 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %51 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %52 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %53 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %54 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %55 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %56 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %57 = sdy.all_slice [{"_axis_0"}, {}] %3 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<128x2880xbf16>
      %58 = stablehlo.concatenate %26, %27, %28, %29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, %42, %43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, %56, %57, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880xbf16>) -> tensor<4096x2880xbf16>
      %59 = stablehlo.reshape %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<4096x2880xbf16>) -> tensor<32x128x2880xbf16>
      %60 = stablehlo.dot_general %59, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x128x5760xbf16>
      %61 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16>
      %62 = stablehlo.add %60, %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x5760xbf16>
      %63 = stablehlo.slice %62 [0:32, 0:128, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %64 = stablehlo.slice %62 [0:32, 0:128, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16>
      %65 = stablehlo.broadcast_in_dim %arg17, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %66 = stablehlo.broadcast_in_dim %arg16, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %67 = stablehlo.clamp %65, %63, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %68 = stablehlo.convert %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %69 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x128x2880xf32>
      %70 = stablehlo.multiply %68, %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %71 = stablehlo.convert %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %72 = stablehlo.logistic %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %73 = stablehlo.convert %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %74 = stablehlo.multiply %68, %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %75 = stablehlo.convert %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %76 = stablehlo.broadcast_in_dim %arg19, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x128x2880xbf16>
      %77 = stablehlo.clamp %76, %64, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %78 = stablehlo.add %77, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %80 = stablehlo.convert %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x128x2880xf32>
      %81 = stablehlo.multiply %79, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xf32>
      %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xf32>) -> tensor<32x128x2880xbf16>
      %83 = stablehlo.dot_general %82, %arg21, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x128x2880xbf16>
      %84 = stablehlo.broadcast_in_dim %arg20, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x128x2880xbf16>
      %85 = stablehlo.add %83, %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x128x2880xbf16>
      %86 = stablehlo.reshape %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128x2880xbf16>) -> tensor<32x1x128x2880xbf16>
      %87 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16>
      %88 = stablehlo.reshape %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16>
      %89 = stablehlo.reshape %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x128xbf16>) -> tensor<32x1x128x1xbf16>
      %90 = stablehlo.convert %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xbf16>) -> tensor<32x1x128x2880xf32>
      %91 = stablehlo.convert %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x1xbf16>) -> tensor<32x1x128x1xf32>
      %92 = stablehlo.reshape %91 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x1x128x1xf32>) -> tensor<32x1x128xf32>
      %93 = stablehlo.broadcast_in_dim %92, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128xf32>) -> tensor<32x1x128x2880xf32>
      %94 = stablehlo.multiply %90, %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x128x2880xf32>
      %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x128x2880xf32>) -> tensor<32x1x128x2880xbf16>
      %96 = stablehlo.reduce(%95 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %97 = sdy.all_reduce {"_axis_0"} %96 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x128x2880xbf16>
      sdy.return %3, %12, %12, %20, %2, %25, %25, %58, %59, %62, %62, %63, %63, %64, %64, %67, %67, %72, %72, %75, %75, %78, %78, %82, %82, %85, %85, %86, %86, %87, %87, %88, %88, %89, %89, %97, %97 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<1x128x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>, tensor<bf16>, tensor<f32>, tensor<bf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>) -> (tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4, %0#5, %0#6, %0#7, %0#8, %0#9, %0#10, %0#11, %0#12, %0#13, %0#14, %0#15, %0#16, %0#17, %0#18, %0#19, %0#20, %0#21, %0#22, %0#23, %0#24, %0#25, %0#26, %0#27, %0#28, %0#29, %0#30, %0#31, %0#32, %0#33, %0#34, %0#35, %0#36 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}


#sdy<dimension_sharding{?}>
#sdy<dimension_sharding{"_axis_0", ?}>
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered ScatterOp
[HET DEBUG][SCATTER] Operand dim shardings start:


[HET DEBUG][SCATTER] Operand dim shardings end
[HET DEBUG][SCATTER] attrDict elements start:
  scatter_dimension_numbers: #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>
  sdy.sharding: #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>
[HET DEBUG][SCATTER] attrDict elements end
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered SliceOp
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered SliceOp
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] entered createNewOperationState
[HET DEBUG] populateShardyCCLToStableHLOCCLPatterns called
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] AllSliceOp rewrite success
[HET DEBUG] AllSliceOp rewrite called
[HET DEBUG] A// -----// IR Dump After UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.137) //----- //
module @SyncTensorsGraph.137 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(
    %arg0:  tensor<1x128x2880xbf16>         {ttcore.shard_status = #ttcore.shard_status<presharded>},
    %arg1:  tensor<32xbf16>                 {ttcore.shard_status = #ttcore.shard_status<presharded>},
    %arg2:  tensor<32x2880xbf16>            {ttcore.shard_status = #ttcore.shard_status<presharded>},
    %arg3:  tensor<32x5760xbf16>            {ttcore.shard_status = #ttcore.shard_status<presharded>},
    %arg4:  tensor<32x2880x5760xbf16>       {ttcore.shard_status = #ttcore.shard_status<presharded>},
    %arg5:  tensor<bf16>                    {ttcore.shard_status = #ttcore.shard_status<presharded>},
    %arg6:  tensor<bf16>                    {ttcore.shard_status = #ttcore.shard_status<presharded>},
    %arg7:  tensor<f32>                     {ttcore.shard_status = #ttcore.shard_status<presharded>},
    %arg8:  tensor<bf16>                    {ttcore.shard_status = #ttcore.shard_status<presharded>},
    %arg9:  tensor<32x2880xbf16>            {ttcore.shard_status = #ttcore.shard_status<presharded>},
    %arg10: tensor<32x2880x2880xbf16>       {ttcore.shard_status = #ttcore.shard_status<presharded>}
  ) -> (
    tensor<128x2880xbf16>         {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<128x4xi64>             {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<128x4xi64>             {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<128x4xbf16>            {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<128x32xbf16>           {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<128x32xbf16>           {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<128x32xbf16>           {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<4096x2880xbf16>        {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x5760xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x5760xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128x2880xbf16>      {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x1x128x2880xbf16>    {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x1x128x2880xbf16>    {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128xbf16>           {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x128xbf16>           {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x1x128xbf16>         {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x1x128xbf16>         {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x1x128x1xbf16>       {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<32x1x128x1xbf16>       {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<1x128x2880xbf16>       {ttcore.shard_status = #ttcore.shard_status<unsharded>},
    tensor<1x128x2880xbf16>       {ttcore.shard_status = #ttcore.shard_status<unsharded>}
  ) {
    %0:37 = sdy.manual_computation(
        %arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10
    )
    in_shardings=[
        <@mesh, [{?}, {?}, {?}]>,
        <@mesh, [{?}]>,
        <@mesh, [{?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, []>,
        <@mesh, []>,
        <@mesh, []>,
        <@mesh, []>,
        <@mesh, [{"_axis_0", ?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>
    ]
    out_shardings=[
        <@mesh, [{?}, {?}]>,
        <@mesh, [{?}, {?}]>,
        <@mesh, [{?}, {?}]>,
        <@mesh, [{?}, {?}]>,
        <@mesh, [{?}, {"_axis_0", ?}]>,
        <@mesh, [{?}, {"_axis_0", ?}]>,
        <@mesh, [{?}, {"_axis_0", ?}]>,
        <@mesh, [{"_axis_0", ?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>,
        <@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>,
        <@mesh, [{?}, {?}, {?}]>,
        <@mesh, [{?}, {?}, {?}]>
    ]
    manual_axes={"_axis_0_updated", "_axis_0"}
    (
        %arg11: tensor<1x128x2880xbf16>,
        %arg12: tensor<32xbf16>,
        %arg13: tensor<32x2880xbf16>,
        %arg14: tensor<4x5760xbf16>,
        %arg15: tensor<4x2880x5760xbf16>,
        %arg16: tensor<bf16>,
        %arg17: tensor<bf16>,
        %arg18: tensor<f32>,
        %arg19: tensor<bf16>,
        %arg20: tensor<4x2880xbf16>,
        %arg21: tensor<4x2880x2880xbf16>
    ) {
      %cst = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128x4xbf16>
      %3 = stablehlo.reshape %arg11 : (tensor<1x128x2880xbf16>) -> tensor<128x2880xbf16>
      %4 = stablehlo.transpose %arg13, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %5 = stablehlo.dot_general %3, %4, contracting_dims = [1] x [0] : (tensor<128x2880xbf16>, tensor<2880x32xbf16>) -> tensor<128x32xbf16>
      %6 = stablehlo.broadcast_in_dim %arg12, dims = [1] : (tensor<32xbf16>) -> tensor<128x32xbf16>
      %7 = stablehlo.add %5, %6 : tensor<128x32xbf16>
      %8 = stablehlo.iota dim = 0 : tensor<32xi32>
      %9 = stablehlo.broadcast_in_dim %8, dims = [1] : (tensor<32xi32>) -> tensor<128x32xi32>
      %10:2 = "stablehlo.sort"(%7, %9) <{dimension = 1 : i64}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>, %arg24: tensor<i32>, %arg25: tensor<i32>):
        %194 = stablehlo.compare  GT, %arg22, %arg23,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %194 : tensor<i1>
      }) : (tensor<128x32xbf16>, tensor<128x32xi32>) -> (tensor<128x32xbf16>, tensor<128x32xi32>)
      %11 = stablehlo.slice %10#1 [0:128, 0:4] : (tensor<128x32xi32>) -> tensor<128x4xi32>
      %12 = stablehlo.convert %11 : (tensor<128x4xi32>) -> tensor<128x4xi64>
      %13 = stablehlo.slice %10#0 [0:128, 0:4] : (tensor<128x32xbf16>) -> tensor<128x4xbf16>
      %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %15 = stablehlo.broadcast_in_dim %14, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %16 = stablehlo.subtract %13, %15 : tensor<128x4xbf16>
      %17 = stablehlo.exponential %16 : tensor<128x4xbf16>
      %18 = stablehlo.reduce(%17 init: %cst_0) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16>
      %19 = stablehlo.broadcast_in_dim %18, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16>
      %20 = stablehlo.divide %17, %19 : tensor<128x4xbf16>
      %21 = stablehlo.iota dim = 0 : tensor<128xi64>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64>
      %23 = stablehlo.reshape %12 : (tensor<128x4xi64>) -> tensor<128x4x1xi64>
      %24 = stablehlo.concatenate %22, %23, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64>
      %25 = "stablehlo.scatter"(%2, %24, %20) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        stablehlo.return %arg23 : tensor<bf16>
      }) : (tensor<128x4xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x4xbf16>
      %26 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %27 = "stablehlo.all_to_all"(%26) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %28 = stablehlo.slice %27 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %29 = stablehlo.reshape %28 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %30 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %31 = "stablehlo.all_to_all"(%30) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %32 = stablehlo.slice %31 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %33 = stablehlo.reshape %32 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %34 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %35 = "stablehlo.all_to_all"(%34) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %36 = stablehlo.slice %35 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %37 = stablehlo.reshape %36 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %38 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %39 = "stablehlo.all_to_all"(%38) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %40 = stablehlo.slice %39 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %41 = stablehlo.reshape %40 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %42 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %43 = "stablehlo.all_to_all"(%42) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %44 = stablehlo.slice %43 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %45 = stablehlo.reshape %44 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %46 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %47 = "stablehlo.all_to_all"(%46) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %48 = stablehlo.slice %47 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %49 = stablehlo.reshape %48 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %50 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %51 = "stablehlo.all_to_all"(%50) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %52 = stablehlo.slice %51 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %53 = stablehlo.reshape %52 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %54 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %55 = "stablehlo.all_to_all"(%54) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %56 = stablehlo.slice %55 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %57 = stablehlo.reshape %56 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %58 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %59 = "stablehlo.all_to_all"(%58) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %60 = stablehlo.slice %59 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %61 = stablehlo.reshape %60 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %62 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %63 = "stablehlo.all_to_all"(%62) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %64 = stablehlo.slice %63 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %65 = stablehlo.reshape %64 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %66 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %67 = "stablehlo.all_to_all"(%66) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %68 = stablehlo.slice %67 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %69 = stablehlo.reshape %68 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %70 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %71 = "stablehlo.all_to_all"(%70) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %72 = stablehlo.slice %71 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %73 = stablehlo.reshape %72 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %74 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %75 = "stablehlo.all_to_all"(%74) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %76 = stablehlo.slice %75 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %77 = stablehlo.reshape %76 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %78 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %79 = "stablehlo.all_to_all"(%78) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %80 = stablehlo.slice %79 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %81 = stablehlo.reshape %80 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %82 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %83 = "stablehlo.all_to_all"(%82) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %84 = stablehlo.slice %83 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %85 = stablehlo.reshape %84 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %86 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %87 = "stablehlo.all_to_all"(%86) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %88 = stablehlo.slice %87 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %89 = stablehlo.reshape %88 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %90 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %91 = "stablehlo.all_to_all"(%90) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %92 = stablehlo.slice %91 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %93 = stablehlo.reshape %92 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %94 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %95 = "stablehlo.all_to_all"(%94) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %96 = stablehlo.slice %95 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %97 = stablehlo.reshape %96 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %98 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %99 = "stablehlo.all_to_all"(%98) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %100 = stablehlo.slice %99 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %101 = stablehlo.reshape %100 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %102 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %103 = "stablehlo.all_to_all"(%102) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %104 = stablehlo.slice %103 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %105 = stablehlo.reshape %104 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %106 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %107 = "stablehlo.all_to_all"(%106) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %108 = stablehlo.slice %107 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %109 = stablehlo.reshape %108 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %110 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %111 = "stablehlo.all_to_all"(%110) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %112 = stablehlo.slice %111 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %113 = stablehlo.reshape %112 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %114 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %115 = "stablehlo.all_to_all"(%114) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %116 = stablehlo.slice %115 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %117 = stablehlo.reshape %116 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %118 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %119 = "stablehlo.all_to_all"(%118) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %120 = stablehlo.slice %119 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %121 = stablehlo.reshape %120 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %122 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %123 = "stablehlo.all_to_all"(%122) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %124 = stablehlo.slice %123 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %125 = stablehlo.reshape %124 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %126 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %127 = "stablehlo.all_to_all"(%126) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %128 = stablehlo.slice %127 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %129 = stablehlo.reshape %128 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %130 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %131 = "stablehlo.all_to_all"(%130) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %132 = stablehlo.slice %131 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %133 = stablehlo.reshape %132 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %134 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %135 = "stablehlo.all_to_all"(%134) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %136 = stablehlo.slice %135 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %137 = stablehlo.reshape %136 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %138 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %139 = "stablehlo.all_to_all"(%138) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %140 = stablehlo.slice %139 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %141 = stablehlo.reshape %140 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %142 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %143 = "stablehlo.all_to_all"(%142) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %144 = stablehlo.slice %143 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %145 = stablehlo.reshape %144 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %146 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %147 = "stablehlo.all_to_all"(%146) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %148 = stablehlo.slice %147 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %149 = stablehlo.reshape %148 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %150 = stablehlo.reshape %3 : (tensor<128x2880xbf16>) -> tensor<8x16x2880xbf16>
      %151 = "stablehlo.all_to_all"(%150) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x16x2880xbf16>) -> tensor<8x16x2880xbf16>
      %152 = stablehlo.slice %151 [0:1, 0:16, 0:2880] : (tensor<8x16x2880xbf16>) -> tensor<1x16x2880xbf16>
      %153 = stablehlo.reshape %152 : (tensor<1x16x2880xbf16>) -> tensor<16x2880xbf16>
      %154 = stablehlo.concatenate %29, %33, %37, %41, %45, %49, %53, %57, %61, %65, %69, %73, %77, %81, %85, %89, %93, %97, %101, %105, %109, %113, %117, %121, %125, %129, %133, %137, %141, %145, %149, %153, dim = 0 : (tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>, tensor<16x2880xbf16>) -> tensor<512x2880xbf16>
      %155 = stablehlo.reshape %154 : (tensor<512x2880xbf16>) -> tensor<4x128x2880xbf16>
      %156 = stablehlo.dot_general %155, %arg15, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x128x5760xbf16>
      %157 = stablehlo.broadcast_in_dim %arg14, dims = [0, 2] : (tensor<4x5760xbf16>) -> tensor<4x128x5760xbf16>
      %158 = stablehlo.add %156, %157 : tensor<4x128x5760xbf16>
      %159 = stablehlo.slice %158 [0:4, 0:128, 0:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %160 = stablehlo.slice %158 [0:4, 0:128, 1:5760:2] : (tensor<4x128x5760xbf16>) -> tensor<4x128x2880xbf16>
      %161 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %162 = stablehlo.broadcast_in_dim %arg16, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %163 = stablehlo.clamp %161, %159, %162 : tensor<4x128x2880xbf16>
      %164 = stablehlo.convert %163 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %165 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<4x128x2880xf32>
      %166 = stablehlo.multiply %164, %165 : tensor<4x128x2880xf32>
      %167 = stablehlo.convert %166 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %168 = stablehlo.logistic %167 : tensor<4x128x2880xbf16>
      %169 = stablehlo.convert %168 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %170 = stablehlo.multiply %164, %169 : tensor<4x128x2880xf32>
      %171 = stablehlo.convert %170 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %172 = stablehlo.broadcast_in_dim %arg19, dims = [] : (tensor<bf16>) -> tensor<4x128x2880xbf16>
      %173 = stablehlo.clamp %172, %160, %162 : tensor<4x128x2880xbf16>
      %174 = stablehlo.add %173, %1 : tensor<4x128x2880xbf16>
      %175 = stablehlo.convert %174 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %176 = stablehlo.convert %171 : (tensor<4x128x2880xbf16>) -> tensor<4x128x2880xf32>
      %177 = stablehlo.multiply %175, %176 : tensor<4x128x2880xf32>
      %178 = stablehlo.convert %177 : (tensor<4x128x2880xf32>) -> tensor<4x128x2880xbf16>
      %179 = stablehlo.dot_general %178, %arg21, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<4x128x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x128x2880xbf16>
      %180 = stablehlo.broadcast_in_dim %arg20, dims = [0, 2] : (tensor<4x2880xbf16>) -> tensor<4x128x2880xbf16>
      %181 = stablehlo.add %179, %180 : tensor<4x128x2880xbf16>
      %182 = stablehlo.reshape %181 : (tensor<4x128x2880xbf16>) -> tensor<4x1x128x2880xbf16>
      %183 = stablehlo.transpose %25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,128]{0,1}"} : (tensor<128x4xbf16>) -> tensor<4x128xbf16>
      %184 = stablehlo.reshape %183 : (tensor<4x128xbf16>) -> tensor<4x1x128xbf16>
      %185 = stablehlo.reshape %183 : (tensor<4x128xbf16>) -> tensor<4x1x128x1xbf16>
      %186 = stablehlo.convert %182 : (tensor<4x1x128x2880xbf16>) -> tensor<4x1x128x2880xf32>
      %187 = stablehlo.convert %185 : (tensor<4x1x128x1xbf16>) -> tensor<4x1x128x1xf32>
      %188 = stablehlo.reshape %187 : (tensor<4x1x128x1xf32>) -> tensor<4x1x128xf32>
      %189 = stablehlo.broadcast_in_dim %188, dims = [0, 1, 2] : (tensor<4x1x128xf32>) -> tensor<4x1x128x2880xf32>
      %190 = stablehlo.multiply %186, %189 : tensor<4x1x128x2880xf32>
      %191 = stablehlo.convert %190 : (tensor<4x1x128x2880xf32>) -> tensor<4x1x128x2880xbf16>
      %192 = stablehlo.reduce(%191 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<4x1x128x2880xbf16>, tensor<bf16>) -> tensor<1x128x2880xbf16>
      %193 = "stablehlo.all_reduce"(%192) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
        %194 = stablehlo.add %arg22, %arg23 : tensor<bf16>
        stablehlo.return %194 : tensor<bf16>
      }) : (tensor<1x128x2880xbf16>) -> tensor<1x128x2880xbf16>
      sdy.return %3, %12, %12, %20, %2, %25, %25, %154, %155, %158, %158, %159, %159, %160, %160, %163, %163, %168, %168, %171, %171, %174, %174, %178, %178, %181, %181, %182, %182, %183, %183, %184, %184, %185, %185, %193, %193 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<128x4xbf16>, tensor<512x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x5760xbf16>, tensor<4x128x5760xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x128x2880xbf16>, tensor<4x1x128x2880xbf16>, tensor<4x1x128x2880xbf16>, tensor<4x128xbf16>, tensor<4x128xbf16>, tensor<4x1x128xbf16>, tensor<4x1x128xbf16>, tensor<4x1x128x1xbf16>, tensor<4x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
    } : (tensor<1x128x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>, tensor<bf16>, tensor<f32>, tensor<bf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>) -> (tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4, %0#5, %0#6, %0#7, %0#8, %0#9, %0#10, %0#11, %0#12, %0#13, %0#14, %0#15, %0#16, %0#17, %0#18, %0#19, %0#20, %0#21, %0#22, %0#23, %0#24, %0#25, %0#26, %0#27, %0#28, %0#29, %0#30, %0#31, %0#32, %0#33, %0#34, %0#35, %0#36 : tensor<128x2880xbf16>, tensor<128x4xi64>, tensor<128x4xi64>, tensor<128x4xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<128x32xbf16>, tensor<4096x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x5760xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x1x128x2880xbf16>, tensor<32x128xbf16>, tensor<32x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128xbf16>, tensor<32x1x128x1xbf16>, tensor<32x1x128x1xbf16>, tensor<1x128x2880xbf16>, tensor<1x128x2880xbf16>
  }
}