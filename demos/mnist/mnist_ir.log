[32m2025-04-30 17:02:15.897[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Opened PCI device 0; KMD version: 1.32.0, IOMMU: disabled
New chip! We now have 1 chips
Chip initialization complete (found )
Chip initializing complete...
 ARC

 [4/4] DRAM

 [16/16] ETH

 CPU

New chip! We now have 2 chips
Chip initialization complete (found )
Chip initializing complete...
 ARC

 [4/4] DRAM

 [16/16] ETH

 CPU

New chip! We now have 3 chips
Chip initialization complete (found )
Chip initializing complete...
 ARC

 [4/4] DRAM

 [16/16] ETH

 CPU

Ok I was wrong we actually have 2 chips
Chip detection complete (found )

[32m2025-04-30 17:02:15.967[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Opened PCI device 0; KMD version: 1.32.0, IOMMU: disabled
[32m2025-04-30 17:02:15.970[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Harvesting mask for chip 0 is 0x220 (physical layout: 0x101, logical: 0x220, simulated harvesting mask: 0x0).
[32m2025-04-30 17:02:15.984[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Opened PCI device 0; KMD version: 1.32.0, IOMMU: disabled
[32m2025-04-30 17:02:16.037[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Harvesting mask for chip 1 is 0x300 (physical layout: 0x5, logical: 0x300, simulated harvesting mask: 0x0).
[32m2025-04-30 17:02:16.051[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Detected PCI devices: [0]
[32m2025-04-30 17:02:16.051[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Using local chip ids: {0} and remote chip ids {1}
[32m2025-04-30 17:02:16.054[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Software version 6.0.0, Ethernet FW version 6.10.0 (Device 0)
[32m2025-04-30 17:02:16.054[0m | [1m[38;2;100;149;237mINFO    [0m | [36mSiliconDriver  [0m - Software version 6.0.0, Ethernet FW version 6.10.0 (Device 1)
[TTMLIR] ASYNC ENABLED: 1
Read unexpected run_mailbox value: 0x40 (expected 0x80 or 0x0)
Read unexpected run_mailbox value: 0x40 (expected 0x80 or 0x0)
  0%|          | 0.00/9.91M [00:00<?, ?B/s]  2%|▏         | 197k/9.91M [00:00<00:05, 1.80MB/s]  8%|▊         | 819k/9.91M [00:00<00:02, 4.03MB/s] 23%|██▎       | 2.26M/9.91M [00:00<00:00, 8.26MB/s] 60%|█████▉    | 5.90M/9.91M [00:00<00:00, 17.8MB/s]100%|██████████| 9.91M/9.91M [00:00<00:00, 20.5MB/s]
  0%|          | 0.00/28.9k [00:00<?, ?B/s]100%|██████████| 28.9k/28.9k [00:00<00:00, 770kB/s]
  0%|          | 0.00/1.65M [00:00<?, ?B/s] 12%|█▏        | 197k/1.65M [00:00<00:00, 1.76MB/s] 50%|████▉     | 819k/1.65M [00:00<00:00, 4.15MB/s]100%|██████████| 1.65M/1.65M [00:00<00:00, 6.26MB/s]
  0%|          | 0.00/4.54k [00:00<?, ?B/s]100%|██████████| 4.54k/4.54k [00:00<00:00, 8.39MB/s]
/localdev/hshah/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/experimental/const_fold.py:264: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer
  new_node = root_const_gm.graph.get_attr(in_node.target)
Torch FX module module
module {
  func.func @main(%arg0: !torch.vtensor<[32,1,3,3],bf16>, %arg1: !torch.vtensor<[32],bf16>, %arg2: !torch.vtensor<[64,32,3,3],bf16>, %arg3: !torch.vtensor<[64],bf16>, %arg4: !torch.vtensor<[9216,128],f32>, %arg5: !torch.vtensor<[128],f32>, %arg6: !torch.vtensor<[128,10],f32>, %arg7: !torch.vtensor<[10],f32>, %arg8: !torch.vtensor<[128,9216],bf16>, %arg9: !torch.vtensor<[128],bf16>, %arg10: !torch.vtensor<[10,128],bf16>, %arg11: !torch.vtensor<[10],bf16>, %arg12: !torch.vtensor<[1,1,28,28],bf16>) -> !torch.vtensor<[1,10],bf16> attributes {torch.assume_strict_symbolic_shapes} {
    %int1 = torch.constant.int 1
    %int1_0 = torch.constant.int 1
    %0 = torch.prim.ListConstruct %int1, %int1_0 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0 = torch.constant.int 0
    %int0_1 = torch.constant.int 0
    %1 = torch.prim.ListConstruct %int0, %int0_1 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2 = torch.constant.int 1
    %int1_3 = torch.constant.int 1
    %2 = torch.prim.ListConstruct %int1_2, %int1_3 : (!torch.int, !torch.int) -> !torch.list<int>
    %false = torch.constant.bool false
    %int0_4 = torch.constant.int 0
    %int0_5 = torch.constant.int 0
    %3 = torch.prim.ListConstruct %int0_4, %int0_5 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6 = torch.constant.int 1
    %4 = torch.aten.convolution %arg12, %arg0, %arg1, %0, %1, %2, %false, %3, %int1_6 : !torch.vtensor<[1,1,28,28],bf16>, !torch.vtensor<[32,1,3,3],bf16>, !torch.vtensor<[32],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,32,26,26],bf16>
    %5 = torch.aten.relu %4 : !torch.vtensor<[1,32,26,26],bf16> -> !torch.vtensor<[1,32,26,26],bf16>
    %int1_7 = torch.constant.int 1
    %int1_8 = torch.constant.int 1
    %6 = torch.prim.ListConstruct %int1_7, %int1_8 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_9 = torch.constant.int 0
    %int0_10 = torch.constant.int 0
    %7 = torch.prim.ListConstruct %int0_9, %int0_10 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_11 = torch.constant.int 1
    %int1_12 = torch.constant.int 1
    %8 = torch.prim.ListConstruct %int1_11, %int1_12 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_13 = torch.constant.bool false
    %int0_14 = torch.constant.int 0
    %int0_15 = torch.constant.int 0
    %9 = torch.prim.ListConstruct %int0_14, %int0_15 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_16 = torch.constant.int 1
    %10 = torch.aten.convolution %5, %arg2, %arg3, %6, %7, %8, %false_13, %9, %int1_16 : !torch.vtensor<[1,32,26,26],bf16>, !torch.vtensor<[64,32,3,3],bf16>, !torch.vtensor<[64],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,24,24],bf16>
    %11 = torch.aten.relu %10 : !torch.vtensor<[1,64,24,24],bf16> -> !torch.vtensor<[1,64,24,24],bf16>
    %int2 = torch.constant.int 2
    %int2_17 = torch.constant.int 2
    %12 = torch.prim.ListConstruct %int2, %int2_17 : (!torch.int, !torch.int) -> !torch.list<int>
    %13 = torch.prim.ListConstruct  : () -> !torch.list<int>
    %int0_18 = torch.constant.int 0
    %int0_19 = torch.constant.int 0
    %14 = torch.prim.ListConstruct %int0_18, %int0_19 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_20 = torch.constant.int 1
    %int1_21 = torch.constant.int 1
    %15 = torch.prim.ListConstruct %int1_20, %int1_21 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_22 = torch.constant.bool false
    %result0, %result1 = torch.aten.max_pool2d_with_indices %11, %12, %13, %14, %15, %false_22 : !torch.vtensor<[1,64,24,24],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,12,12],bf16>, !torch.vtensor<[1,64,12,12],si64>
    %none = torch.constant.none
    %16 = torch.aten.clone %result0, %none : !torch.vtensor<[1,64,12,12],bf16>, !torch.none -> !torch.vtensor<[1,64,12,12],bf16>
    %int1_23 = torch.constant.int 1
    %int9216 = torch.constant.int 9216
    %17 = torch.prim.ListConstruct %int1_23, %int9216 : (!torch.int, !torch.int) -> !torch.list<int>
    %18 = torch.aten.view %16, %17 : !torch.vtensor<[1,64,12,12],bf16>, !torch.list<int> -> !torch.vtensor<[1,9216],bf16>
    %int6 = torch.constant.int 6
    %19 = torch.prims.convert_element_type %18, %int6 : !torch.vtensor<[1,9216],bf16>, !torch.int -> !torch.vtensor<[1,9216],f32>
    %20 = torch.aten.mm %19, %arg4 : !torch.vtensor<[1,9216],f32>, !torch.vtensor<[9216,128],f32> -> !torch.vtensor<[1,128],f32>
    %int1_24 = torch.constant.int 1
    %21 = torch.aten.mul.Scalar %20, %int1_24 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %int1_25 = torch.constant.int 1
    %22 = torch.aten.add.Tensor %21, %arg5, %int1_25 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %int15 = torch.constant.int 15
    %23 = torch.prims.convert_element_type %22, %int15 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],bf16>
    %24 = torch.aten.relu %23 : !torch.vtensor<[1,128],bf16> -> !torch.vtensor<[1,128],bf16>
    %none_26 = torch.constant.none
    %25 = torch.aten.clone %24, %none_26 : !torch.vtensor<[1,128],bf16>, !torch.none -> !torch.vtensor<[1,128],bf16>
    %int6_27 = torch.constant.int 6
    %26 = torch.prims.convert_element_type %25, %int6_27 : !torch.vtensor<[1,128],bf16>, !torch.int -> !torch.vtensor<[1,128],f32>
    %27 = torch.aten.mm %26, %arg6 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128,10],f32> -> !torch.vtensor<[1,10],f32>
    %int1_28 = torch.constant.int 1
    %28 = torch.aten.mul.Scalar %27, %int1_28 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %int1_29 = torch.constant.int 1
    %29 = torch.aten.add.Tensor %28, %arg7, %int1_29 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %int15_30 = torch.constant.int 15
    %30 = torch.prims.convert_element_type %29, %int15_30 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],bf16>
    %int6_31 = torch.constant.int 6
    %31 = torch.prims.convert_element_type %30, %int6_31 : !torch.vtensor<[1,10],bf16>, !torch.int -> !torch.vtensor<[1,10],f32>
    %int1_32 = torch.constant.int 1
    %32 = torch.prim.ListConstruct %int1_32 : (!torch.int) -> !torch.list<int>
    %true = torch.constant.bool true
    %33 = torch.aten.amax %31, %32, %true : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,1],f32>
    %int1_33 = torch.constant.int 1
    %34 = torch.aten.sub.Tensor %31, %33, %int1_33 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %35 = torch.aten.exp %34 : !torch.vtensor<[1,10],f32> -> !torch.vtensor<[1,10],f32>
    %int1_34 = torch.constant.int 1
    %36 = torch.prim.ListConstruct %int1_34 : (!torch.int) -> !torch.list<int>
    %true_35 = torch.constant.bool true
    %none_36 = torch.constant.none
    %37 = torch.aten.sum.dim_IntList %35, %36, %true_35, %none_36 : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[1,1],f32>
    %38 = torch.aten.log %37 : !torch.vtensor<[1,1],f32> -> !torch.vtensor<[1,1],f32>
    %int1_37 = torch.constant.int 1
    %39 = torch.aten.sub.Tensor %34, %38, %int1_37 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %int15_38 = torch.constant.int 15
    %40 = torch.prims.convert_element_type %39, %int15_38 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],bf16>
    return %40 : !torch.vtensor<[1,10],bf16>
  }
}

// -----// IR Dump After ReduceOpVariants (torch-reduce-op-variants) //----- //
func.func @main(%arg0: !torch.vtensor<[32,1,3,3],bf16>, %arg1: !torch.vtensor<[32],bf16>, %arg2: !torch.vtensor<[64,32,3,3],bf16>, %arg3: !torch.vtensor<[64],bf16>, %arg4: !torch.vtensor<[9216,128],f32>, %arg5: !torch.vtensor<[128],f32>, %arg6: !torch.vtensor<[128,10],f32>, %arg7: !torch.vtensor<[10],f32>, %arg8: !torch.vtensor<[128,9216],bf16>, %arg9: !torch.vtensor<[128],bf16>, %arg10: !torch.vtensor<[10,128],bf16>, %arg11: !torch.vtensor<[10],bf16>, %arg12: !torch.vtensor<[1,1,28,28],bf16>) -> !torch.vtensor<[1,10],bf16> attributes {torch.assume_strict_symbolic_shapes} {
  %int1 = torch.constant.int 1
  %int1_0 = torch.constant.int 1
  %0 = torch.prim.ListConstruct %int1, %int1_0 : (!torch.int, !torch.int) -> !torch.list<int>
  %int0 = torch.constant.int 0
  %int0_1 = torch.constant.int 0
  %1 = torch.prim.ListConstruct %int0, %int0_1 : (!torch.int, !torch.int) -> !torch.list<int>
  %int1_2 = torch.constant.int 1
  %int1_3 = torch.constant.int 1
  %2 = torch.prim.ListConstruct %int1_2, %int1_3 : (!torch.int, !torch.int) -> !torch.list<int>
  %false = torch.constant.bool false
  %int0_4 = torch.constant.int 0
  %int0_5 = torch.constant.int 0
  %3 = torch.prim.ListConstruct %int0_4, %int0_5 : (!torch.int, !torch.int) -> !torch.list<int>
  %int1_6 = torch.constant.int 1
  %4 = torch.aten.convolution %arg12, %arg0, %arg1, %0, %1, %2, %false, %3, %int1_6 : !torch.vtensor<[1,1,28,28],bf16>, !torch.vtensor<[32,1,3,3],bf16>, !torch.vtensor<[32],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,32,26,26],bf16>
  %5 = torch.aten.relu %4 : !torch.vtensor<[1,32,26,26],bf16> -> !torch.vtensor<[1,32,26,26],bf16>
  %int1_7 = torch.constant.int 1
  %int1_8 = torch.constant.int 1
  %6 = torch.prim.ListConstruct %int1_7, %int1_8 : (!torch.int, !torch.int) -> !torch.list<int>
  %int0_9 = torch.constant.int 0
  %int0_10 = torch.constant.int 0
  %7 = torch.prim.ListConstruct %int0_9, %int0_10 : (!torch.int, !torch.int) -> !torch.list<int>
  %int1_11 = torch.constant.int 1
  %int1_12 = torch.constant.int 1
  %8 = torch.prim.ListConstruct %int1_11, %int1_12 : (!torch.int, !torch.int) -> !torch.list<int>
  %false_13 = torch.constant.bool false
  %int0_14 = torch.constant.int 0
  %int0_15 = torch.constant.int 0
  %9 = torch.prim.ListConstruct %int0_14, %int0_15 : (!torch.int, !torch.int) -> !torch.list<int>
  %int1_16 = torch.constant.int 1
  %10 = torch.aten.convolution %5, %arg2, %arg3, %6, %7, %8, %false_13, %9, %int1_16 : !torch.vtensor<[1,32,26,26],bf16>, !torch.vtensor<[64,32,3,3],bf16>, !torch.vtensor<[64],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,24,24],bf16>
  %11 = torch.aten.relu %10 : !torch.vtensor<[1,64,24,24],bf16> -> !torch.vtensor<[1,64,24,24],bf16>
  %int2 = torch.constant.int 2
  %int2_17 = torch.constant.int 2
  %12 = torch.prim.ListConstruct %int2, %int2_17 : (!torch.int, !torch.int) -> !torch.list<int>
  %13 = torch.prim.ListConstruct  : () -> !torch.list<int>
  %int0_18 = torch.constant.int 0
  %int0_19 = torch.constant.int 0
  %14 = torch.prim.ListConstruct %int0_18, %int0_19 : (!torch.int, !torch.int) -> !torch.list<int>
  %int1_20 = torch.constant.int 1
  %int1_21 = torch.constant.int 1
  %15 = torch.prim.ListConstruct %int1_20, %int1_21 : (!torch.int, !torch.int) -> !torch.list<int>
  %false_22 = torch.constant.bool false
  %result0, %result1 = torch.aten.max_pool2d_with_indices %11, %12, %13, %14, %15, %false_22 : !torch.vtensor<[1,64,24,24],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,12,12],bf16>, !torch.vtensor<[1,64,12,12],si64>
  %none = torch.constant.none
  %16 = torch.aten.clone %result0, %none : !torch.vtensor<[1,64,12,12],bf16>, !torch.none -> !torch.vtensor<[1,64,12,12],bf16>
  %int1_23 = torch.constant.int 1
  %int9216 = torch.constant.int 9216
  %17 = torch.prim.ListConstruct %int1_23, %int9216 : (!torch.int, !torch.int) -> !torch.list<int>
  %18 = torch.aten.view %16, %17 : !torch.vtensor<[1,64,12,12],bf16>, !torch.list<int> -> !torch.vtensor<[1,9216],bf16>
  %int6 = torch.constant.int 6
  %19 = torch.prims.convert_element_type %18, %int6 : !torch.vtensor<[1,9216],bf16>, !torch.int -> !torch.vtensor<[1,9216],f32>
  %20 = torch.aten.mm %19, %arg4 : !torch.vtensor<[1,9216],f32>, !torch.vtensor<[9216,128],f32> -> !torch.vtensor<[1,128],f32>
  %int1_24 = torch.constant.int 1
  %21 = torch.aten.mul.Scalar %20, %int1_24 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %int1_25 = torch.constant.int 1
  %22 = torch.aten.add.Tensor %21, %arg5, %int1_25 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %int15 = torch.constant.int 15
  %23 = torch.prims.convert_element_type %22, %int15 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],bf16>
  %24 = torch.aten.relu %23 : !torch.vtensor<[1,128],bf16> -> !torch.vtensor<[1,128],bf16>
  %none_26 = torch.constant.none
  %25 = torch.aten.clone %24, %none_26 : !torch.vtensor<[1,128],bf16>, !torch.none -> !torch.vtensor<[1,128],bf16>
  %int6_27 = torch.constant.int 6
  %26 = torch.prims.convert_element_type %25, %int6_27 : !torch.vtensor<[1,128],bf16>, !torch.int -> !torch.vtensor<[1,128],f32>
  %27 = torch.aten.mm %26, %arg6 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128,10],f32> -> !torch.vtensor<[1,10],f32>
  %int1_28 = torch.constant.int 1
  %28 = torch.aten.mul.Scalar %27, %int1_28 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %int1_29 = torch.constant.int 1
  %29 = torch.aten.add.Tensor %28, %arg7, %int1_29 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %int15_30 = torch.constant.int 15
  %30 = torch.prims.convert_element_type %29, %int15_30 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],bf16>
  %int6_31 = torch.constant.int 6
  %31 = torch.prims.convert_element_type %30, %int6_31 : !torch.vtensor<[1,10],bf16>, !torch.int -> !torch.vtensor<[1,10],f32>
  %int1_32 = torch.constant.int 1
  %32 = torch.prim.ListConstruct %int1_32 : (!torch.int) -> !torch.list<int>
  %true = torch.constant.bool true
  %33 = torch.aten.amax %31, %32, %true : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,1],f32>
  %int1_33 = torch.constant.int 1
  %34 = torch.aten.sub.Tensor %31, %33, %int1_33 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %35 = torch.aten.exp %34 : !torch.vtensor<[1,10],f32> -> !torch.vtensor<[1,10],f32>
  %int1_34 = torch.constant.int 1
  %36 = torch.prim.ListConstruct %int1_34 : (!torch.int) -> !torch.list<int>
  %true_35 = torch.constant.bool true
  %none_36 = torch.constant.none
  %37 = torch.aten.sum.dim_IntList %35, %36, %true_35, %none_36 : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[1,1],f32>
  %38 = torch.aten.log %37 : !torch.vtensor<[1,1],f32> -> !torch.vtensor<[1,1],f32>
  %int1_37 = torch.constant.int 1
  %39 = torch.aten.sub.Tensor %34, %38, %int1_37 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %int15_38 = torch.constant.int 15
  %40 = torch.prims.convert_element_type %39, %int15_38 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],bf16>
  return %40 : !torch.vtensor<[1,10],bf16>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !torch.vtensor<[32,1,3,3],bf16>, %arg1: !torch.vtensor<[32],bf16>, %arg2: !torch.vtensor<[64,32,3,3],bf16>, %arg3: !torch.vtensor<[64],bf16>, %arg4: !torch.vtensor<[9216,128],f32>, %arg5: !torch.vtensor<[128],f32>, %arg6: !torch.vtensor<[128,10],f32>, %arg7: !torch.vtensor<[10],f32>, %arg8: !torch.vtensor<[128,9216],bf16>, %arg9: !torch.vtensor<[128],bf16>, %arg10: !torch.vtensor<[10,128],bf16>, %arg11: !torch.vtensor<[10],bf16>, %arg12: !torch.vtensor<[1,1,28,28],bf16>) -> !torch.vtensor<[1,10],bf16> attributes {torch.assume_strict_symbolic_shapes} {
  %true = torch.constant.bool true
  %int15 = torch.constant.int 15
  %int6 = torch.constant.int 6
  %int9216 = torch.constant.int 9216
  %none = torch.constant.none
  %int2 = torch.constant.int 2
  %false = torch.constant.bool false
  %int0 = torch.constant.int 0
  %int1 = torch.constant.int 1
  %0 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %1 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %2 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %3 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %4 = torch.aten.convolution %arg12, %arg0, %arg1, %0, %1, %2, %false, %3, %int1 : !torch.vtensor<[1,1,28,28],bf16>, !torch.vtensor<[32,1,3,3],bf16>, !torch.vtensor<[32],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,32,26,26],bf16>
  %5 = torch.aten.relu %4 : !torch.vtensor<[1,32,26,26],bf16> -> !torch.vtensor<[1,32,26,26],bf16>
  %6 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %7 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %8 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %9 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %10 = torch.aten.convolution %5, %arg2, %arg3, %6, %7, %8, %false, %9, %int1 : !torch.vtensor<[1,32,26,26],bf16>, !torch.vtensor<[64,32,3,3],bf16>, !torch.vtensor<[64],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,24,24],bf16>
  %11 = torch.aten.relu %10 : !torch.vtensor<[1,64,24,24],bf16> -> !torch.vtensor<[1,64,24,24],bf16>
  %12 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
  %13 = torch.prim.ListConstruct  : () -> !torch.list<int>
  %14 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %15 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %16 = torch.aten.max_pool2d %11, %12, %13, %14, %15, %false : !torch.vtensor<[1,64,24,24],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,12,12],bf16>
  %17 = torch.prim.ListConstruct %int1, %int9216 : (!torch.int, !torch.int) -> !torch.list<int>
  %18 = torch.aten.view %16, %17 : !torch.vtensor<[1,64,12,12],bf16>, !torch.list<int> -> !torch.vtensor<[1,9216],bf16>
  %19 = torch.prims.convert_element_type %18, %int6 : !torch.vtensor<[1,9216],bf16>, !torch.int -> !torch.vtensor<[1,9216],f32>
  %20 = torch.aten.mm %19, %arg4 : !torch.vtensor<[1,9216],f32>, !torch.vtensor<[9216,128],f32> -> !torch.vtensor<[1,128],f32>
  %21 = torch.aten.mul.Scalar %20, %int1 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %22 = torch.aten.add.Tensor %21, %arg5, %int1 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %23 = torch.prims.convert_element_type %22, %int15 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],bf16>
  %24 = torch.aten.relu %23 : !torch.vtensor<[1,128],bf16> -> !torch.vtensor<[1,128],bf16>
  %25 = torch.prims.convert_element_type %24, %int6 : !torch.vtensor<[1,128],bf16>, !torch.int -> !torch.vtensor<[1,128],f32>
  %26 = torch.aten.mm %25, %arg6 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128,10],f32> -> !torch.vtensor<[1,10],f32>
  %27 = torch.aten.mul.Scalar %26, %int1 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %28 = torch.aten.add.Tensor %27, %arg7, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %29 = torch.prims.convert_element_type %28, %int15 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],bf16>
  %30 = torch.prims.convert_element_type %29, %int6 : !torch.vtensor<[1,10],bf16>, !torch.int -> !torch.vtensor<[1,10],f32>
  %31 = torch.prim.ListConstruct %int1 : (!torch.int) -> !torch.list<int>
  %32 = torch.aten.amax %30, %31, %true : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,1],f32>
  %33 = torch.aten.sub.Tensor %30, %32, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %34 = torch.aten.exp %33 : !torch.vtensor<[1,10],f32> -> !torch.vtensor<[1,10],f32>
  %35 = torch.prim.ListConstruct %int1 : (!torch.int) -> !torch.list<int>
  %36 = torch.aten.sum.dim_IntList %34, %35, %true, %none : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[1,1],f32>
  %37 = torch.aten.log %36 : !torch.vtensor<[1,1],f32> -> !torch.vtensor<[1,1],f32>
  %38 = torch.aten.sub.Tensor %33, %37, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %39 = torch.prims.convert_element_type %38, %int15 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],bf16>
  return %39 : !torch.vtensor<[1,10],bf16>
}

// -----// IR Dump After DecomposeComplexOps (torch-decompose-complex-ops) //----- //
func.func @main(%arg0: !torch.vtensor<[32,1,3,3],bf16>, %arg1: !torch.vtensor<[32],bf16>, %arg2: !torch.vtensor<[64,32,3,3],bf16>, %arg3: !torch.vtensor<[64],bf16>, %arg4: !torch.vtensor<[9216,128],f32>, %arg5: !torch.vtensor<[128],f32>, %arg6: !torch.vtensor<[128,10],f32>, %arg7: !torch.vtensor<[10],f32>, %arg8: !torch.vtensor<[128,9216],bf16>, %arg9: !torch.vtensor<[128],bf16>, %arg10: !torch.vtensor<[10,128],bf16>, %arg11: !torch.vtensor<[10],bf16>, %arg12: !torch.vtensor<[1,1,28,28],bf16>) -> !torch.vtensor<[1,10],bf16> attributes {torch.assume_strict_symbolic_shapes} {
  %true = torch.constant.bool true
  %int15 = torch.constant.int 15
  %int6 = torch.constant.int 6
  %int9216 = torch.constant.int 9216
  %none = torch.constant.none
  %int2 = torch.constant.int 2
  %false = torch.constant.bool false
  %int0 = torch.constant.int 0
  %int1 = torch.constant.int 1
  %0 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %1 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %2 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %3 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %4 = torch.aten.convolution %arg12, %arg0, %arg1, %0, %1, %2, %false, %3, %int1 : !torch.vtensor<[1,1,28,28],bf16>, !torch.vtensor<[32,1,3,3],bf16>, !torch.vtensor<[32],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,32,26,26],bf16>
  %5 = torch.aten.relu %4 : !torch.vtensor<[1,32,26,26],bf16> -> !torch.vtensor<[1,32,26,26],bf16>
  %6 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %7 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %8 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %9 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %10 = torch.aten.convolution %5, %arg2, %arg3, %6, %7, %8, %false, %9, %int1 : !torch.vtensor<[1,32,26,26],bf16>, !torch.vtensor<[64,32,3,3],bf16>, !torch.vtensor<[64],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,24,24],bf16>
  %11 = torch.aten.relu %10 : !torch.vtensor<[1,64,24,24],bf16> -> !torch.vtensor<[1,64,24,24],bf16>
  %12 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
  %13 = torch.prim.ListConstruct  : () -> !torch.list<int>
  %14 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %15 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %16 = torch.aten.max_pool2d %11, %12, %13, %14, %15, %false : !torch.vtensor<[1,64,24,24],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,12,12],bf16>
  %17 = torch.prim.ListConstruct %int1, %int9216 : (!torch.int, !torch.int) -> !torch.list<int>
  %18 = torch.aten.view %16, %17 : !torch.vtensor<[1,64,12,12],bf16>, !torch.list<int> -> !torch.vtensor<[1,9216],bf16>
  %19 = torch.aten.to.dtype %18, %int6, %false, %false, %none : !torch.vtensor<[1,9216],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,9216],f32>
  %20 = torch.aten.mm %19, %arg4 : !torch.vtensor<[1,9216],f32>, !torch.vtensor<[9216,128],f32> -> !torch.vtensor<[1,128],f32>
  %21 = torch.aten.mul.Scalar %20, %int1 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %22 = torch.aten.add.Tensor %21, %arg5, %int1 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %23 = torch.aten.to.dtype %22, %int15, %false, %false, %none : !torch.vtensor<[1,128],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,128],bf16>
  %24 = torch.aten.relu %23 : !torch.vtensor<[1,128],bf16> -> !torch.vtensor<[1,128],bf16>
  %25 = torch.aten.to.dtype %24, %int6, %false, %false, %none : !torch.vtensor<[1,128],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,128],f32>
  %26 = torch.aten.mm %25, %arg6 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128,10],f32> -> !torch.vtensor<[1,10],f32>
  %27 = torch.aten.mul.Scalar %26, %int1 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %28 = torch.aten.add.Tensor %27, %arg7, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %29 = torch.aten.to.dtype %28, %int15, %false, %false, %none : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],bf16>
  %30 = torch.aten.to.dtype %29, %int6, %false, %false, %none : !torch.vtensor<[1,10],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],f32>
  %values, %indices = torch.aten.max.dim %30, %int1, %true : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool -> !torch.vtensor<[1,1],f32>, !torch.vtensor<[1,1],si64>
  %31 = torch.aten.sub.Tensor %30, %values, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %32 = torch.aten.exp %31 : !torch.vtensor<[1,10],f32> -> !torch.vtensor<[1,10],f32>
  %33 = torch.prim.ListConstruct %int1 : (!torch.int) -> !torch.list<int>
  %34 = torch.aten.sum.dim_IntList %32, %33, %true, %none : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[1,1],f32>
  %35 = torch.aten.log %34 : !torch.vtensor<[1,1],f32> -> !torch.vtensor<[1,1],f32>
  %36 = torch.aten.sub.Tensor %31, %35, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %37 = torch.aten.to.dtype %36, %int15, %false, %false, %none : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],bf16>
  return %37 : !torch.vtensor<[1,10],bf16>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !torch.vtensor<[32,1,3,3],bf16>, %arg1: !torch.vtensor<[32],bf16>, %arg2: !torch.vtensor<[64,32,3,3],bf16>, %arg3: !torch.vtensor<[64],bf16>, %arg4: !torch.vtensor<[9216,128],f32>, %arg5: !torch.vtensor<[128],f32>, %arg6: !torch.vtensor<[128,10],f32>, %arg7: !torch.vtensor<[10],f32>, %arg8: !torch.vtensor<[128,9216],bf16>, %arg9: !torch.vtensor<[128],bf16>, %arg10: !torch.vtensor<[10,128],bf16>, %arg11: !torch.vtensor<[10],bf16>, %arg12: !torch.vtensor<[1,1,28,28],bf16>) -> !torch.vtensor<[1,10],bf16> attributes {torch.assume_strict_symbolic_shapes} {
  %true = torch.constant.bool true
  %int15 = torch.constant.int 15
  %int6 = torch.constant.int 6
  %int9216 = torch.constant.int 9216
  %none = torch.constant.none
  %int2 = torch.constant.int 2
  %false = torch.constant.bool false
  %int0 = torch.constant.int 0
  %int1 = torch.constant.int 1
  %0 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %1 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %2 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %3 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %4 = torch.aten.convolution %arg12, %arg0, %arg1, %0, %1, %2, %false, %3, %int1 : !torch.vtensor<[1,1,28,28],bf16>, !torch.vtensor<[32,1,3,3],bf16>, !torch.vtensor<[32],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,32,26,26],bf16>
  %5 = torch.aten.relu %4 : !torch.vtensor<[1,32,26,26],bf16> -> !torch.vtensor<[1,32,26,26],bf16>
  %6 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %7 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %8 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %9 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %10 = torch.aten.convolution %5, %arg2, %arg3, %6, %7, %8, %false, %9, %int1 : !torch.vtensor<[1,32,26,26],bf16>, !torch.vtensor<[64,32,3,3],bf16>, !torch.vtensor<[64],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,24,24],bf16>
  %11 = torch.aten.relu %10 : !torch.vtensor<[1,64,24,24],bf16> -> !torch.vtensor<[1,64,24,24],bf16>
  %12 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
  %13 = torch.prim.ListConstruct  : () -> !torch.list<int>
  %14 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %15 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %16 = torch.aten.max_pool2d %11, %12, %13, %14, %15, %false : !torch.vtensor<[1,64,24,24],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,12,12],bf16>
  %17 = torch.prim.ListConstruct %int1, %int9216 : (!torch.int, !torch.int) -> !torch.list<int>
  %18 = torch.aten.view %16, %17 : !torch.vtensor<[1,64,12,12],bf16>, !torch.list<int> -> !torch.vtensor<[1,9216],bf16>
  %19 = torch.aten.to.dtype %18, %int6, %false, %false, %none : !torch.vtensor<[1,9216],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,9216],f32>
  %20 = torch.aten.mm %19, %arg4 : !torch.vtensor<[1,9216],f32>, !torch.vtensor<[9216,128],f32> -> !torch.vtensor<[1,128],f32>
  %21 = torch.aten.mul.Scalar %20, %int1 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %22 = torch.aten.add.Tensor %21, %arg5, %int1 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %23 = torch.aten.to.dtype %22, %int15, %false, %false, %none : !torch.vtensor<[1,128],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,128],bf16>
  %24 = torch.aten.relu %23 : !torch.vtensor<[1,128],bf16> -> !torch.vtensor<[1,128],bf16>
  %25 = torch.aten.to.dtype %24, %int6, %false, %false, %none : !torch.vtensor<[1,128],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,128],f32>
  %26 = torch.aten.mm %25, %arg6 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128,10],f32> -> !torch.vtensor<[1,10],f32>
  %27 = torch.aten.mul.Scalar %26, %int1 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %28 = torch.aten.add.Tensor %27, %arg7, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %29 = torch.aten.to.dtype %28, %int15, %false, %false, %none : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],bf16>
  %30 = torch.aten.to.dtype %29, %int6, %false, %false, %none : !torch.vtensor<[1,10],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],f32>
  %values, %indices = torch.aten.max.dim %30, %int1, %true : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool -> !torch.vtensor<[1,1],f32>, !torch.vtensor<[1,1],si64>
  %31 = torch.aten.sub.Tensor %30, %values, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %32 = torch.aten.exp %31 : !torch.vtensor<[1,10],f32> -> !torch.vtensor<[1,10],f32>
  %33 = torch.prim.ListConstruct %int1 : (!torch.int) -> !torch.list<int>
  %34 = torch.aten.sum.dim_IntList %32, %33, %true, %none : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[1,1],f32>
  %35 = torch.aten.log %34 : !torch.vtensor<[1,1],f32> -> !torch.vtensor<[1,1],f32>
  %36 = torch.aten.sub.Tensor %31, %35, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %37 = torch.aten.to.dtype %36, %int15, %false, %false, %none : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],bf16>
  return %37 : !torch.vtensor<[1,10],bf16>
}

Torch Backend module module
module {
  func.func @main(%arg0: !torch.vtensor<[32,1,3,3],bf16>, %arg1: !torch.vtensor<[32],bf16>, %arg2: !torch.vtensor<[64,32,3,3],bf16>, %arg3: !torch.vtensor<[64],bf16>, %arg4: !torch.vtensor<[9216,128],f32>, %arg5: !torch.vtensor<[128],f32>, %arg6: !torch.vtensor<[128,10],f32>, %arg7: !torch.vtensor<[10],f32>, %arg8: !torch.vtensor<[128,9216],bf16>, %arg9: !torch.vtensor<[128],bf16>, %arg10: !torch.vtensor<[10,128],bf16>, %arg11: !torch.vtensor<[10],bf16>, %arg12: !torch.vtensor<[1,1,28,28],bf16>) -> !torch.vtensor<[1,10],bf16> attributes {torch.assume_strict_symbolic_shapes} {
    %true = torch.constant.bool true
    %int15 = torch.constant.int 15
    %int6 = torch.constant.int 6
    %int9216 = torch.constant.int 9216
    %none = torch.constant.none
    %int2 = torch.constant.int 2
    %false = torch.constant.bool false
    %int0 = torch.constant.int 0
    %int1 = torch.constant.int 1
    %0 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %2 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %4 = torch.aten.convolution %arg12, %arg0, %arg1, %0, %1, %2, %false, %3, %int1 : !torch.vtensor<[1,1,28,28],bf16>, !torch.vtensor<[32,1,3,3],bf16>, !torch.vtensor<[32],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,32,26,26],bf16>
    %5 = torch.aten.relu %4 : !torch.vtensor<[1,32,26,26],bf16> -> !torch.vtensor<[1,32,26,26],bf16>
    %6 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %7 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %9 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %10 = torch.aten.convolution %5, %arg2, %arg3, %6, %7, %8, %false, %9, %int1 : !torch.vtensor<[1,32,26,26],bf16>, !torch.vtensor<[64,32,3,3],bf16>, !torch.vtensor<[64],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,24,24],bf16>
    %11 = torch.aten.relu %10 : !torch.vtensor<[1,64,24,24],bf16> -> !torch.vtensor<[1,64,24,24],bf16>
    %12 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
    %13 = torch.prim.ListConstruct  : () -> !torch.list<int>
    %14 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16 = torch.aten.max_pool2d %11, %12, %13, %14, %15, %false : !torch.vtensor<[1,64,24,24],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,12,12],bf16>
    %17 = torch.prim.ListConstruct %int1, %int9216 : (!torch.int, !torch.int) -> !torch.list<int>
    %18 = torch.aten.view %16, %17 : !torch.vtensor<[1,64,12,12],bf16>, !torch.list<int> -> !torch.vtensor<[1,9216],bf16>
    %19 = torch.aten.to.dtype %18, %int6, %false, %false, %none : !torch.vtensor<[1,9216],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,9216],f32>
    %20 = torch.aten.mm %19, %arg4 : !torch.vtensor<[1,9216],f32>, !torch.vtensor<[9216,128],f32> -> !torch.vtensor<[1,128],f32>
    %21 = torch.aten.mul.Scalar %20, %int1 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %22 = torch.aten.add.Tensor %21, %arg5, %int1 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %23 = torch.aten.to.dtype %22, %int15, %false, %false, %none : !torch.vtensor<[1,128],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,128],bf16>
    %24 = torch.aten.relu %23 : !torch.vtensor<[1,128],bf16> -> !torch.vtensor<[1,128],bf16>
    %25 = torch.aten.to.dtype %24, %int6, %false, %false, %none : !torch.vtensor<[1,128],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,128],f32>
    %26 = torch.aten.mm %25, %arg6 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128,10],f32> -> !torch.vtensor<[1,10],f32>
    %27 = torch.aten.mul.Scalar %26, %int1 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %28 = torch.aten.add.Tensor %27, %arg7, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %29 = torch.aten.to.dtype %28, %int15, %false, %false, %none : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],bf16>
    %30 = torch.aten.to.dtype %29, %int6, %false, %false, %none : !torch.vtensor<[1,10],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],f32>
    %values, %indices = torch.aten.max.dim %30, %int1, %true : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool -> !torch.vtensor<[1,1],f32>, !torch.vtensor<[1,1],si64>
    %31 = torch.aten.sub.Tensor %30, %values, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %32 = torch.aten.exp %31 : !torch.vtensor<[1,10],f32> -> !torch.vtensor<[1,10],f32>
    %33 = torch.prim.ListConstruct %int1 : (!torch.int) -> !torch.list<int>
    %34 = torch.aten.sum.dim_IntList %32, %33, %true, %none : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[1,1],f32>
    %35 = torch.aten.log %34 : !torch.vtensor<[1,1],f32> -> !torch.vtensor<[1,1],f32>
    %36 = torch.aten.sub.Tensor %31, %35, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %37 = torch.aten.to.dtype %36, %int15, %false, %false, %none : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],bf16>
    return %37 : !torch.vtensor<[1,10],bf16>
  }
}

StableHLO module module
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_5 = arith.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %cst_5 : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}

// -----// IR Dump Before ConvertArithToStableHLO (convert-arith-to-stablehlo) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_5 = arith.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %cst_5 : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}


// -----// IR Dump After ConvertArithToStableHLO (convert-arith-to-stablehlo) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %c : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before StablehloLegalizeCompositeToCallPass (stablehlo-legalize-composite-to-call) ('func.func' operation: @main) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %c : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %c : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %c : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %c : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}


// -----// IR Dump After ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0xFF80> : tensor<1xbf16>}> : () -> tensor<1xbf16>
    %3 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %4 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<1xf32>}> : () -> tensor<1xf32>
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1xf32>}> : () -> tensor<1xf32>
    %6 = "ttir.constant"() <{value = dense<1> : tensor<1xi64>}> : () -> tensor<1xi64>
    %7 = ttir.empty() : tensor<1x32x26x26xbf16>
    %8 = "ttir.convolution"(%arg12, %arg0, %7) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %9 = ttir.empty() : tensor<32x1x1xbf16>
    %10 = "ttir.reshape"(%arg1, %9) <{shape = [32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<32x1x1xbf16>) -> tensor<32x1x1xbf16>
    %11 = ttir.empty() : tensor<1x32x26x26xbf16>
    %12 = "ttir.broadcast"(%8, %11) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %13 = ttir.empty() : tensor<1x32x1x1xbf16>
    %14 = "ttir.reshape"(%10, %13) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x1x1xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %15 = ttir.empty() : tensor<1x32x26x26xbf16>
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %17 = ttir.empty() : tensor<1x32x26x26xbf16>
    %18 = "ttir.add"(%12, %16, %17) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %19 = ttir.empty() : tensor<1x32x26x26xbf16>
    %20 = "ttir.maximum"(%18, %0, %19) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %21 = ttir.empty() : tensor<1x64x24x24xbf16>
    %22 = "ttir.convolution"(%20, %arg2, %21) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %23 = ttir.empty() : tensor<64x1x1xbf16>
    %24 = "ttir.reshape"(%arg3, %23) <{shape = [64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<64x1x1xbf16>) -> tensor<64x1x1xbf16>
    %25 = ttir.empty() : tensor<1x64x24x24xbf16>
    %26 = "ttir.broadcast"(%22, %25) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %27 = ttir.empty() : tensor<1x64x1x1xbf16>
    %28 = "ttir.reshape"(%24, %27) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64x1x1xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %29 = ttir.empty() : tensor<1x64x24x24xbf16>
    %30 = "ttir.broadcast"(%28, %29) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %31 = ttir.empty() : tensor<1x64x24x24xbf16>
    %32 = "ttir.add"(%26, %30, %31) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %33 = ttir.empty() : tensor<1x64x24x24xbf16>
    %34 = "ttir.maximum"(%32, %1, %33) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %35 = ttir.empty() : tensor<1x64x12x12xbf16>
    %36 = "ttir.pooling"(%34, %35) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %37 = ttir.empty() : tensor<1x9216xbf16>
    %38 = "ttir.reshape"(%36, %37) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %39 = ttir.empty() : tensor<1x9216xf32>
    %40 = "ttir.typecast"(%38, %39) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %41 = "ttir.dot_general"(%40, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %42 = ttir.empty() : tensor<1xf32>
    %43 = "ttir.typecast"(%6, %42) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32>
    %44 = ttir.empty() : tensor<1xf32>
    %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32]}> : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
    %46 = ttir.empty() : tensor<1x128xf32>
    %47 = "ttir.broadcast"(%41, %46) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %48 = ttir.empty() : tensor<1x1xf32>
    %49 = "ttir.reshape"(%45, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %50 = ttir.empty() : tensor<1x128xf32>
    %51 = "ttir.broadcast"(%49, %50) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %52 = ttir.empty() : tensor<1x128xf32>
    %53 = "ttir.multiply"(%47, %51, %52) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %54 = ttir.empty() : tensor<1x128xf32>
    %55 = "ttir.broadcast"(%53, %54) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %56 = ttir.empty() : tensor<1x128xf32>
    %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %58 = ttir.empty() : tensor<1x128xf32>
    %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %60 = ttir.empty() : tensor<1x128xf32>
    %61 = "ttir.add"(%55, %59, %60) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %62 = ttir.empty() : tensor<1x128xbf16>
    %63 = "ttir.typecast"(%61, %62) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %64 = ttir.empty() : tensor<1x128xbf16>
    %65 = "ttir.maximum"(%63, %3, %64) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %66 = ttir.empty() : tensor<1x128xf32>
    %67 = "ttir.typecast"(%65, %66) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %68 = "ttir.dot_general"(%67, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %69 = ttir.empty() : tensor<1x10xf32>
    %70 = "ttir.broadcast"(%68, %69) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %71 = ttir.empty() : tensor<1x1xf32>
    %72 = "ttir.reshape"(%45, %71) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %73 = ttir.empty() : tensor<1x10xf32>
    %74 = "ttir.broadcast"(%72, %73) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %75 = ttir.empty() : tensor<1x10xf32>
    %76 = "ttir.multiply"(%70, %74, %75) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %77 = ttir.empty() : tensor<1x10xf32>
    %78 = "ttir.broadcast"(%76, %77) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %79 = ttir.empty() : tensor<1x10xf32>
    %80 = "ttir.reshape"(%arg7, %79) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %81 = ttir.empty() : tensor<1x10xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %83 = ttir.empty() : tensor<1x10xf32>
    %84 = "ttir.add"(%78, %82, %83) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %85 = ttir.empty() : tensor<1x10xbf16>
    %86 = "ttir.typecast"(%84, %85) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    %87 = ttir.empty() : tensor<1x10xf32>
    %88 = "ttir.typecast"(%86, %87) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xbf16>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %89 = ttir.empty() : tensor<1xf32>
    %90 = "ttir.max"(%88, %89) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %91 = ttir.empty() : tensor<1x1xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %93 = ttir.empty() : tensor<1x10xf32>
    %94 = "ttir.broadcast"(%88, %93) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %95 = ttir.empty() : tensor<1x10xf32>
    %96 = "ttir.broadcast"(%92, %95) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %97 = ttir.empty() : tensor<1x10xf32>
    %98 = "ttir.subtract"(%94, %96, %97) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %99 = ttir.empty() : tensor<1x10xf32>
    %100 = "ttir.exp"(%98, %99) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %101 = ttir.empty() : tensor<1xf32>
    %102 = "ttir.sum"(%100, %101) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %103 = ttir.empty() : tensor<1x1xf32>
    %104 = "ttir.reshape"(%102, %103) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %105 = ttir.empty() : tensor<1x1xf32>
    %106 = "ttir.log"(%104, %105) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %107 = ttir.empty() : tensor<1x10xf32>
    %108 = "ttir.broadcast"(%98, %107) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %109 = ttir.empty() : tensor<1x10xf32>
    %110 = "ttir.broadcast"(%106, %109) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %111 = ttir.empty() : tensor<1x10xf32>
    %112 = "ttir.subtract"(%108, %110, %111) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %113 = ttir.empty() : tensor<1x10xbf16>
    %114 = "ttir.typecast"(%112, %113) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %114 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before mlir::tt::TTIRTensorAnnotationCleanupPass (shardy-tensor-annotation-cleanup) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0xFF80> : tensor<1xbf16>}> : () -> tensor<1xbf16>
    %3 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %4 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<1xf32>}> : () -> tensor<1xf32>
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1xf32>}> : () -> tensor<1xf32>
    %6 = "ttir.constant"() <{value = dense<1> : tensor<1xi64>}> : () -> tensor<1xi64>
    %7 = ttir.empty() : tensor<1x32x26x26xbf16>
    %8 = "ttir.convolution"(%arg12, %arg0, %7) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %9 = ttir.empty() : tensor<32x1x1xbf16>
    %10 = "ttir.reshape"(%arg1, %9) <{shape = [32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<32x1x1xbf16>) -> tensor<32x1x1xbf16>
    %11 = ttir.empty() : tensor<1x32x26x26xbf16>
    %12 = "ttir.broadcast"(%8, %11) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %13 = ttir.empty() : tensor<1x32x1x1xbf16>
    %14 = "ttir.reshape"(%10, %13) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x1x1xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %15 = ttir.empty() : tensor<1x32x26x26xbf16>
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %17 = ttir.empty() : tensor<1x32x26x26xbf16>
    %18 = "ttir.add"(%12, %16, %17) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %19 = ttir.empty() : tensor<1x32x26x26xbf16>
    %20 = "ttir.maximum"(%18, %0, %19) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %21 = ttir.empty() : tensor<1x64x24x24xbf16>
    %22 = "ttir.convolution"(%20, %arg2, %21) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %23 = ttir.empty() : tensor<64x1x1xbf16>
    %24 = "ttir.reshape"(%arg3, %23) <{shape = [64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<64x1x1xbf16>) -> tensor<64x1x1xbf16>
    %25 = ttir.empty() : tensor<1x64x24x24xbf16>
    %26 = "ttir.broadcast"(%22, %25) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %27 = ttir.empty() : tensor<1x64x1x1xbf16>
    %28 = "ttir.reshape"(%24, %27) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64x1x1xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %29 = ttir.empty() : tensor<1x64x24x24xbf16>
    %30 = "ttir.broadcast"(%28, %29) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %31 = ttir.empty() : tensor<1x64x24x24xbf16>
    %32 = "ttir.add"(%26, %30, %31) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %33 = ttir.empty() : tensor<1x64x24x24xbf16>
    %34 = "ttir.maximum"(%32, %1, %33) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %35 = ttir.empty() : tensor<1x64x12x12xbf16>
    %36 = "ttir.pooling"(%34, %35) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %37 = ttir.empty() : tensor<1x9216xbf16>
    %38 = "ttir.reshape"(%36, %37) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %39 = ttir.empty() : tensor<1x9216xf32>
    %40 = "ttir.typecast"(%38, %39) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %41 = "ttir.dot_general"(%40, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %42 = ttir.empty() : tensor<1xf32>
    %43 = "ttir.typecast"(%6, %42) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32>
    %44 = ttir.empty() : tensor<1xf32>
    %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32]}> : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
    %46 = ttir.empty() : tensor<1x128xf32>
    %47 = "ttir.broadcast"(%41, %46) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %48 = ttir.empty() : tensor<1x1xf32>
    %49 = "ttir.reshape"(%45, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %50 = ttir.empty() : tensor<1x128xf32>
    %51 = "ttir.broadcast"(%49, %50) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %52 = ttir.empty() : tensor<1x128xf32>
    %53 = "ttir.multiply"(%47, %51, %52) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %54 = ttir.empty() : tensor<1x128xf32>
    %55 = "ttir.broadcast"(%53, %54) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %56 = ttir.empty() : tensor<1x128xf32>
    %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %58 = ttir.empty() : tensor<1x128xf32>
    %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %60 = ttir.empty() : tensor<1x128xf32>
    %61 = "ttir.add"(%55, %59, %60) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %62 = ttir.empty() : tensor<1x128xbf16>
    %63 = "ttir.typecast"(%61, %62) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %64 = ttir.empty() : tensor<1x128xbf16>
    %65 = "ttir.maximum"(%63, %3, %64) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %66 = ttir.empty() : tensor<1x128xf32>
    %67 = "ttir.typecast"(%65, %66) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %68 = "ttir.dot_general"(%67, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %69 = ttir.empty() : tensor<1x10xf32>
    %70 = "ttir.broadcast"(%68, %69) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %71 = ttir.empty() : tensor<1x1xf32>
    %72 = "ttir.reshape"(%45, %71) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %73 = ttir.empty() : tensor<1x10xf32>
    %74 = "ttir.broadcast"(%72, %73) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %75 = ttir.empty() : tensor<1x10xf32>
    %76 = "ttir.multiply"(%70, %74, %75) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %77 = ttir.empty() : tensor<1x10xf32>
    %78 = "ttir.broadcast"(%76, %77) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %79 = ttir.empty() : tensor<1x10xf32>
    %80 = "ttir.reshape"(%arg7, %79) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %81 = ttir.empty() : tensor<1x10xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %83 = ttir.empty() : tensor<1x10xf32>
    %84 = "ttir.add"(%78, %82, %83) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %85 = ttir.empty() : tensor<1x10xbf16>
    %86 = "ttir.typecast"(%84, %85) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    %87 = ttir.empty() : tensor<1x10xf32>
    %88 = "ttir.typecast"(%86, %87) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xbf16>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %89 = ttir.empty() : tensor<1xf32>
    %90 = "ttir.max"(%88, %89) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %91 = ttir.empty() : tensor<1x1xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %93 = ttir.empty() : tensor<1x10xf32>
    %94 = "ttir.broadcast"(%88, %93) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %95 = ttir.empty() : tensor<1x10xf32>
    %96 = "ttir.broadcast"(%92, %95) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %97 = ttir.empty() : tensor<1x10xf32>
    %98 = "ttir.subtract"(%94, %96, %97) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %99 = ttir.empty() : tensor<1x10xf32>
    %100 = "ttir.exp"(%98, %99) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %101 = ttir.empty() : tensor<1xf32>
    %102 = "ttir.sum"(%100, %101) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %103 = ttir.empty() : tensor<1x1xf32>
    %104 = "ttir.reshape"(%102, %103) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %105 = ttir.empty() : tensor<1x1xf32>
    %106 = "ttir.log"(%104, %105) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %107 = ttir.empty() : tensor<1x10xf32>
    %108 = "ttir.broadcast"(%98, %107) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %109 = ttir.empty() : tensor<1x10xf32>
    %110 = "ttir.broadcast"(%106, %109) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %111 = ttir.empty() : tensor<1x10xf32>
    %112 = "ttir.subtract"(%108, %110, %111) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %113 = ttir.empty() : tensor<1x10xbf16>
    %114 = "ttir.typecast"(%112, %113) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %114 : tensor<1x10xbf16>
  }
}


TTIR module module
#loc1 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":31:0)
#loc2 = loc("convolution")
#loc36 = loc(fused[#loc1, #loc2])
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16> loc(fused[#loc1, #loc2]), %arg1: tensor<32xbf16> loc(fused[#loc1, #loc2]), %arg2: tensor<64x32x3x3xbf16> loc(fused[#loc1, #loc2]), %arg3: tensor<64xbf16> loc(fused[#loc1, #loc2]), %arg4: tensor<9216x128xf32> loc(fused[#loc1, #loc2]), %arg5: tensor<128xf32> loc(fused[#loc1, #loc2]), %arg6: tensor<128x10xf32> loc(fused[#loc1, #loc2]), %arg7: tensor<10xf32> loc(fused[#loc1, #loc2]), %arg8: tensor<128x9216xbf16> loc(fused[#loc1, #loc2]), %arg9: tensor<128xbf16> loc(fused[#loc1, #loc2]), %arg10: tensor<10x128xbf16> loc(fused[#loc1, #loc2]), %arg11: tensor<10xbf16> loc(fused[#loc1, #loc2]), %arg12: tensor<1x1x28x28xbf16> loc(fused[#loc1, #loc2])) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16> loc(#loc)
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16> loc(#loc)
    %2 = "ttir.constant"() <{value = dense<0xFF80> : tensor<1xbf16>}> : () -> tensor<1xbf16> loc(#loc)
    %3 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16> loc(#loc)
    %4 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<1xf32>}> : () -> tensor<1xf32> loc(#loc)
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1xf32>}> : () -> tensor<1xf32> loc(#loc)
    %6 = "ttir.constant"() <{value = dense<1> : tensor<1xi64>}> : () -> tensor<1xi64> loc(#loc)
    %7 = ttir.empty() : tensor<1x32x26x26xbf16> loc(#loc36)
    %8 = "ttir.convolution"(%arg12, %arg0, %7) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16> loc(#loc36)
    %9 = ttir.empty() : tensor<32x1x1xbf16> loc(#loc36)
    %10 = "ttir.reshape"(%arg1, %9) <{shape = [32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<32x1x1xbf16>) -> tensor<32x1x1xbf16> loc(#loc36)
    %11 = ttir.empty() : tensor<1x32x26x26xbf16> loc(#loc36)
    %12 = "ttir.broadcast"(%8, %11) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16> loc(#loc36)
    %13 = ttir.empty() : tensor<1x32x1x1xbf16> loc(#loc36)
    %14 = "ttir.reshape"(%10, %13) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x1x1xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16> loc(#loc36)
    %15 = ttir.empty() : tensor<1x32x26x26xbf16> loc(#loc36)
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16> loc(#loc36)
    %17 = ttir.empty() : tensor<1x32x26x26xbf16> loc(#loc36)
    %18 = "ttir.add"(%12, %16, %17) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16> loc(#loc36)
    %19 = ttir.empty() : tensor<1x32x26x26xbf16> loc(#loc37)
    %20 = "ttir.maximum"(%18, %0, %19) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16> loc(#loc37)
    %21 = ttir.empty() : tensor<1x64x24x24xbf16> loc(#loc38)
    %22 = "ttir.convolution"(%20, %arg2, %21) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16> loc(#loc38)
    %23 = ttir.empty() : tensor<64x1x1xbf16> loc(#loc38)
    %24 = "ttir.reshape"(%arg3, %23) <{shape = [64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<64x1x1xbf16>) -> tensor<64x1x1xbf16> loc(#loc38)
    %25 = ttir.empty() : tensor<1x64x24x24xbf16> loc(#loc38)
    %26 = "ttir.broadcast"(%22, %25) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16> loc(#loc38)
    %27 = ttir.empty() : tensor<1x64x1x1xbf16> loc(#loc38)
    %28 = "ttir.reshape"(%24, %27) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64x1x1xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16> loc(#loc38)
    %29 = ttir.empty() : tensor<1x64x24x24xbf16> loc(#loc38)
    %30 = "ttir.broadcast"(%28, %29) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16> loc(#loc38)
    %31 = ttir.empty() : tensor<1x64x24x24xbf16> loc(#loc38)
    %32 = "ttir.add"(%26, %30, %31) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16> loc(#loc38)
    %33 = ttir.empty() : tensor<1x64x24x24xbf16> loc(#loc39)
    %34 = "ttir.maximum"(%32, %1, %33) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16> loc(#loc39)
    %35 = ttir.empty() : tensor<1x64x12x12xbf16> loc(#loc40)
    %36 = "ttir.pooling"(%34, %35) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16> loc(#loc40)
    %37 = ttir.empty() : tensor<1x9216xbf16> loc(#loc41)
    %38 = "ttir.reshape"(%36, %37) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16> loc(#loc41)
    %39 = ttir.empty() : tensor<1x9216xf32> loc(#loc42)
    %40 = "ttir.typecast"(%38, %39) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32> loc(#loc42)
    %41 = "ttir.dot_general"(%40, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32> loc(#loc43)
    %42 = ttir.empty() : tensor<1xf32> loc(#loc44)
    %43 = "ttir.typecast"(%6, %42) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32> loc(#loc44)
    %44 = ttir.empty() : tensor<1xf32> loc(#loc44)
    %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32]}> : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32> loc(#loc44)
    %46 = ttir.empty() : tensor<1x128xf32> loc(#loc44)
    %47 = "ttir.broadcast"(%41, %46) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc44)
    %48 = ttir.empty() : tensor<1x1xf32> loc(#loc44)
    %49 = "ttir.reshape"(%45, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc44)
    %50 = ttir.empty() : tensor<1x128xf32> loc(#loc44)
    %51 = "ttir.broadcast"(%49, %50) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc44)
    %52 = ttir.empty() : tensor<1x128xf32> loc(#loc44)
    %53 = "ttir.multiply"(%47, %51, %52) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc44)
    %54 = ttir.empty() : tensor<1x128xf32> loc(#loc45)
    %55 = "ttir.broadcast"(%53, %54) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc45)
    %56 = ttir.empty() : tensor<1x128xf32> loc(#loc45)
    %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc45)
    %58 = ttir.empty() : tensor<1x128xf32> loc(#loc45)
    %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc45)
    %60 = ttir.empty() : tensor<1x128xf32> loc(#loc45)
    %61 = "ttir.add"(%55, %59, %60) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc45)
    %62 = ttir.empty() : tensor<1x128xbf16> loc(#loc46)
    %63 = "ttir.typecast"(%61, %62) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc46)
    %64 = ttir.empty() : tensor<1x128xbf16> loc(#loc47)
    %65 = "ttir.maximum"(%63, %3, %64) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc47)
    %66 = ttir.empty() : tensor<1x128xf32> loc(#loc48)
    %67 = "ttir.typecast"(%65, %66) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc48)
    %68 = "ttir.dot_general"(%67, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32> loc(#loc49)
    %69 = ttir.empty() : tensor<1x10xf32> loc(#loc50)
    %70 = "ttir.broadcast"(%68, %69) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc50)
    %71 = ttir.empty() : tensor<1x1xf32> loc(#loc50)
    %72 = "ttir.reshape"(%45, %71) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc50)
    %73 = ttir.empty() : tensor<1x10xf32> loc(#loc50)
    %74 = "ttir.broadcast"(%72, %73) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc50)
    %75 = ttir.empty() : tensor<1x10xf32> loc(#loc50)
    %76 = "ttir.multiply"(%70, %74, %75) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc50)
    %77 = ttir.empty() : tensor<1x10xf32> loc(#loc51)
    %78 = "ttir.broadcast"(%76, %77) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc51)
    %79 = ttir.empty() : tensor<1x10xf32> loc(#loc51)
    %80 = "ttir.reshape"(%arg7, %79) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc51)
    %81 = ttir.empty() : tensor<1x10xf32> loc(#loc51)
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc51)
    %83 = ttir.empty() : tensor<1x10xf32> loc(#loc51)
    %84 = "ttir.add"(%78, %82, %83) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc51)
    %85 = ttir.empty() : tensor<1x10xbf16> loc(#loc52)
    %86 = "ttir.typecast"(%84, %85) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16> loc(#loc52)
    %87 = ttir.empty() : tensor<1x10xf32> loc(#loc53)
    %88 = "ttir.typecast"(%86, %87) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xbf16>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc53)
    %89 = ttir.empty() : tensor<1xf32> loc(#loc54)
    %90 = "ttir.max"(%88, %89) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32> loc(#loc54)
    %91 = ttir.empty() : tensor<1x1xf32> loc(#loc54)
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc54)
    %93 = ttir.empty() : tensor<1x10xf32> loc(#loc55)
    %94 = "ttir.broadcast"(%88, %93) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc55)
    %95 = ttir.empty() : tensor<1x10xf32> loc(#loc55)
    %96 = "ttir.broadcast"(%92, %95) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc55)
    %97 = ttir.empty() : tensor<1x10xf32> loc(#loc55)
    %98 = "ttir.subtract"(%94, %96, %97) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc55)
    %99 = ttir.empty() : tensor<1x10xf32> loc(#loc56)
    %100 = "ttir.exp"(%98, %99) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc56)
    %101 = ttir.empty() : tensor<1xf32> loc(#loc57)
    %102 = "ttir.sum"(%100, %101) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32> loc(#loc57)
    %103 = ttir.empty() : tensor<1x1xf32> loc(#loc57)
    %104 = "ttir.reshape"(%102, %103) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc57)
    %105 = ttir.empty() : tensor<1x1xf32> loc(#loc58)
    %106 = "ttir.log"(%104, %105) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc58)
    %107 = ttir.empty() : tensor<1x10xf32> loc(#loc59)
    %108 = "ttir.broadcast"(%98, %107) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc59)
    %109 = ttir.empty() : tensor<1x10xf32> loc(#loc59)
    %110 = "ttir.broadcast"(%106, %109) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc59)
    %111 = ttir.empty() : tensor<1x10xf32> loc(#loc59)
    %112 = "ttir.subtract"(%108, %110, %111) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc59)
    %113 = ttir.empty() : tensor<1x10xbf16> loc(#loc60)
    %114 = "ttir.typecast"(%112, %113) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16> loc(#loc60)
    return %114 : tensor<1x10xbf16> loc(#loc36)
  } loc(#loc36)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":32:0)
#loc4 = loc("relu")
#loc5 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":33:0)
#loc6 = loc("convolution_1")
#loc7 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":34:0)
#loc8 = loc("relu_1")
#loc9 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":35:0)
#loc10 = loc("max_pool2d_with_indices")
#loc11 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":37:0)
#loc12 = loc("view")
#loc13 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":38:0)
#loc14 = loc("convert_element_type")
#loc15 = loc("mm")
#loc16 = loc("mul")
#loc17 = loc("add")
#loc18 = loc("convert_element_type_1")
#loc19 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":39:0)
#loc20 = loc("relu_2")
#loc21 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":41:0)
#loc22 = loc("convert_element_type_2")
#loc23 = loc("mm_1")
#loc24 = loc("mul_1")
#loc25 = loc("add_1")
#loc26 = loc("convert_element_type_3")
#loc27 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":42:0)
#loc28 = loc("convert_element_type_4")
#loc29 = loc("amax")
#loc30 = loc("sub")
#loc31 = loc("exp")
#loc32 = loc("sum_1")
#loc33 = loc("log")
#loc34 = loc("sub_1")
#loc35 = loc("convert_element_type_5")
#loc37 = loc(fused[#loc3, #loc4])
#loc38 = loc(fused[#loc5, #loc6])
#loc39 = loc(fused[#loc7, #loc8])
#loc40 = loc(fused[#loc9, #loc10])
#loc41 = loc(fused[#loc11, #loc12])
#loc42 = loc(fused[#loc13, #loc14])
#loc43 = loc(fused[#loc13, #loc15])
#loc44 = loc(fused[#loc13, #loc16])
#loc45 = loc(fused[#loc13, #loc17])
#loc46 = loc(fused[#loc13, #loc18])
#loc47 = loc(fused[#loc19, #loc20])
#loc48 = loc(fused[#loc21, #loc22])
#loc49 = loc(fused[#loc21, #loc23])
#loc50 = loc(fused[#loc21, #loc24])
#loc51 = loc(fused[#loc21, #loc25])
#loc52 = loc(fused[#loc21, #loc26])
#loc53 = loc(fused[#loc27, #loc28])
#loc54 = loc(fused[#loc27, #loc29])
#loc55 = loc(fused[#loc27, #loc30])
#loc56 = loc(fused[#loc27, #loc31])
#loc57 = loc(fused[#loc27, #loc32])
#loc58 = loc(fused[#loc27, #loc33])
#loc59 = loc(fused[#loc27, #loc34])
#loc60 = loc(fused[#loc27, #loc35])

// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0xFF80> : tensor<1xbf16>}> : () -> tensor<1xbf16>
    %3 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %4 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<1xf32>}> : () -> tensor<1xf32>
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1xf32>}> : () -> tensor<1xf32>
    %6 = "ttir.constant"() <{value = dense<1> : tensor<1xi64>}> : () -> tensor<1xi64>
    %7 = ttir.empty() : tensor<1x32x26x26xbf16>
    %8 = "ttir.convolution"(%arg12, %arg0, %7) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %9 = ttir.empty() : tensor<32x1x1xbf16>
    %10 = "ttir.reshape"(%arg1, %9) <{shape = [32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<32x1x1xbf16>) -> tensor<32x1x1xbf16>
    %11 = ttir.empty() : tensor<1x32x26x26xbf16>
    %12 = "ttir.broadcast"(%8, %11) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %13 = ttir.empty() : tensor<1x32x1x1xbf16>
    %14 = "ttir.reshape"(%10, %13) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x1x1xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %15 = ttir.empty() : tensor<1x32x26x26xbf16>
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %17 = ttir.empty() : tensor<1x32x26x26xbf16>
    %18 = "ttir.add"(%12, %16, %17) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %19 = ttir.empty() : tensor<1x32x26x26xbf16>
    %20 = "ttir.maximum"(%18, %0, %19) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %21 = ttir.empty() : tensor<1x64x24x24xbf16>
    %22 = "ttir.convolution"(%20, %arg2, %21) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %23 = ttir.empty() : tensor<64x1x1xbf16>
    %24 = "ttir.reshape"(%arg3, %23) <{shape = [64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<64x1x1xbf16>) -> tensor<64x1x1xbf16>
    %25 = ttir.empty() : tensor<1x64x24x24xbf16>
    %26 = "ttir.broadcast"(%22, %25) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %27 = ttir.empty() : tensor<1x64x1x1xbf16>
    %28 = "ttir.reshape"(%24, %27) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64x1x1xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %29 = ttir.empty() : tensor<1x64x24x24xbf16>
    %30 = "ttir.broadcast"(%28, %29) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %31 = ttir.empty() : tensor<1x64x24x24xbf16>
    %32 = "ttir.add"(%26, %30, %31) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %33 = ttir.empty() : tensor<1x64x24x24xbf16>
    %34 = "ttir.maximum"(%32, %1, %33) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %35 = ttir.empty() : tensor<1x64x12x12xbf16>
    %36 = "ttir.pooling"(%34, %35) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %37 = ttir.empty() : tensor<1x9216xbf16>
    %38 = "ttir.reshape"(%36, %37) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %39 = ttir.empty() : tensor<1x9216xf32>
    %40 = "ttir.typecast"(%38, %39) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %41 = "ttir.dot_general"(%40, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %42 = ttir.empty() : tensor<1xf32>
    %43 = "ttir.typecast"(%6, %42) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32>
    %44 = ttir.empty() : tensor<1xf32>
    %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32]}> : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
    %46 = ttir.empty() : tensor<1x128xf32>
    %47 = "ttir.broadcast"(%41, %46) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %48 = ttir.empty() : tensor<1x1xf32>
    %49 = "ttir.reshape"(%45, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %50 = ttir.empty() : tensor<1x128xf32>
    %51 = "ttir.broadcast"(%49, %50) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %52 = ttir.empty() : tensor<1x128xf32>
    %53 = "ttir.multiply"(%47, %51, %52) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %54 = ttir.empty() : tensor<1x128xf32>
    %55 = "ttir.broadcast"(%53, %54) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %56 = ttir.empty() : tensor<1x128xf32>
    %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %58 = ttir.empty() : tensor<1x128xf32>
    %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %60 = ttir.empty() : tensor<1x128xf32>
    %61 = "ttir.add"(%55, %59, %60) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %62 = ttir.empty() : tensor<1x128xbf16>
    %63 = "ttir.typecast"(%61, %62) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %64 = ttir.empty() : tensor<1x128xbf16>
    %65 = "ttir.maximum"(%63, %3, %64) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %66 = ttir.empty() : tensor<1x128xf32>
    %67 = "ttir.typecast"(%65, %66) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %68 = "ttir.dot_general"(%67, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %69 = ttir.empty() : tensor<1x10xf32>
    %70 = "ttir.broadcast"(%68, %69) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %71 = ttir.empty() : tensor<1x1xf32>
    %72 = "ttir.reshape"(%45, %71) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %73 = ttir.empty() : tensor<1x10xf32>
    %74 = "ttir.broadcast"(%72, %73) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %75 = ttir.empty() : tensor<1x10xf32>
    %76 = "ttir.multiply"(%70, %74, %75) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %77 = ttir.empty() : tensor<1x10xf32>
    %78 = "ttir.broadcast"(%76, %77) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %79 = ttir.empty() : tensor<1x10xf32>
    %80 = "ttir.reshape"(%arg7, %79) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %81 = ttir.empty() : tensor<1x10xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %83 = ttir.empty() : tensor<1x10xf32>
    %84 = "ttir.add"(%78, %82, %83) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %85 = ttir.empty() : tensor<1x10xbf16>
    %86 = "ttir.typecast"(%84, %85) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    %87 = ttir.empty() : tensor<1x10xf32>
    %88 = "ttir.typecast"(%86, %87) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xbf16>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %89 = ttir.empty() : tensor<1xf32>
    %90 = "ttir.max"(%88, %89) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %91 = ttir.empty() : tensor<1x1xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %93 = ttir.empty() : tensor<1x10xf32>
    %94 = "ttir.broadcast"(%88, %93) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %95 = ttir.empty() : tensor<1x10xf32>
    %96 = "ttir.broadcast"(%92, %95) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %97 = ttir.empty() : tensor<1x10xf32>
    %98 = "ttir.subtract"(%94, %96, %97) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %99 = ttir.empty() : tensor<1x10xf32>
    %100 = "ttir.exp"(%98, %99) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %101 = ttir.empty() : tensor<1xf32>
    %102 = "ttir.sum"(%100, %101) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %103 = ttir.empty() : tensor<1x1xf32>
    %104 = "ttir.reshape"(%102, %103) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %105 = ttir.empty() : tensor<1x1xf32>
    %106 = "ttir.log"(%104, %105) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %107 = ttir.empty() : tensor<1x10xf32>
    %108 = "ttir.broadcast"(%98, %107) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %109 = ttir.empty() : tensor<1x10xf32>
    %110 = "ttir.broadcast"(%106, %109) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %111 = ttir.empty() : tensor<1x10xf32>
    %112 = "ttir.subtract"(%108, %110, %111) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %113 = ttir.empty() : tensor<1x10xbf16>
    %114 = "ttir.typecast"(%112, %113) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %114 : tensor<1x10xbf16>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %3 = "ttir.constant"() <{value = dense<1> : tensor<1xi64>}> : () -> tensor<1xi64>
    %4 = ttir.empty() : tensor<1x32x26x26xbf16>
    %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %6 = ttir.empty() : tensor<1x32x1x1xbf16>
    %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %8 = ttir.empty() : tensor<1x32x26x26xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %10 = ttir.empty() : tensor<1x32x26x26xbf16>
    %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %12 = ttir.empty() : tensor<1x32x26x26xbf16>
    %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %14 = ttir.empty() : tensor<1x64x24x24xbf16>
    %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %16 = ttir.empty() : tensor<1x64x1x1xbf16>
    %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %18 = ttir.empty() : tensor<1x64x24x24xbf16>
    %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %20 = ttir.empty() : tensor<1x64x24x24xbf16>
    %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %22 = ttir.empty() : tensor<1x64x24x24xbf16>
    %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %24 = ttir.empty() : tensor<1x64x12x12xbf16>
    %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %26 = ttir.empty() : tensor<1x9216xbf16>
    %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %28 = ttir.empty() : tensor<1x9216xf32>
    %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %31 = ttir.empty() : tensor<1xf32>
    %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32>
    %33 = ttir.empty() : tensor<1x1xf32>
    %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %35 = ttir.empty() : tensor<1x128xf32>
    %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %37 = ttir.empty() : tensor<1x128xf32>
    %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %39 = ttir.empty() : tensor<1x128xf32>
    %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %41 = ttir.empty() : tensor<1x128xf32>
    %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %43 = ttir.empty() : tensor<1x128xbf16>
    %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %45 = ttir.empty() : tensor<1x128xbf16>
    %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %47 = ttir.empty() : tensor<1x128xf32>
    %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %50 = ttir.empty() : tensor<1x1xf32>
    %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %52 = ttir.empty() : tensor<1x10xf32>
    %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %54 = ttir.empty() : tensor<1x10xf32>
    %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %56 = ttir.empty() : tensor<1x10xf32>
    %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %58 = ttir.empty() : tensor<1x10xf32>
    %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %60 = ttir.empty() : tensor<1xf32>
    %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %62 = ttir.empty() : tensor<1x1xf32>
    %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %64 = ttir.empty() : tensor<1x10xf32>
    %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %66 = ttir.empty() : tensor<1x10xf32>
    %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %68 = ttir.empty() : tensor<1x10xf32>
    %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %70 = ttir.empty() : tensor<1xf32>
    %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %72 = ttir.empty() : tensor<1x1xf32>
    %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %74 = ttir.empty() : tensor<1x1xf32>
    %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %76 = ttir.empty() : tensor<1x10xf32>
    %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %78 = ttir.empty() : tensor<1x10xf32>
    %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %80 = ttir.empty() : tensor<1x10xbf16>
    %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %81 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before ElementTypeNormalization (ttir-element-type-normalization) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %3 = "ttir.constant"() <{value = dense<1> : tensor<1xi64>}> : () -> tensor<1xi64>
    %4 = ttir.empty() : tensor<1x32x26x26xbf16>
    %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %6 = ttir.empty() : tensor<1x32x1x1xbf16>
    %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %8 = ttir.empty() : tensor<1x32x26x26xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %10 = ttir.empty() : tensor<1x32x26x26xbf16>
    %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %12 = ttir.empty() : tensor<1x32x26x26xbf16>
    %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %14 = ttir.empty() : tensor<1x64x24x24xbf16>
    %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %16 = ttir.empty() : tensor<1x64x1x1xbf16>
    %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %18 = ttir.empty() : tensor<1x64x24x24xbf16>
    %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %20 = ttir.empty() : tensor<1x64x24x24xbf16>
    %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %22 = ttir.empty() : tensor<1x64x24x24xbf16>
    %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %24 = ttir.empty() : tensor<1x64x12x12xbf16>
    %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %26 = ttir.empty() : tensor<1x9216xbf16>
    %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %28 = ttir.empty() : tensor<1x9216xf32>
    %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %31 = ttir.empty() : tensor<1xf32>
    %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32>
    %33 = ttir.empty() : tensor<1x1xf32>
    %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %35 = ttir.empty() : tensor<1x128xf32>
    %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %37 = ttir.empty() : tensor<1x128xf32>
    %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %39 = ttir.empty() : tensor<1x128xf32>
    %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %41 = ttir.empty() : tensor<1x128xf32>
    %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %43 = ttir.empty() : tensor<1x128xbf16>
    %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %45 = ttir.empty() : tensor<1x128xbf16>
    %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %47 = ttir.empty() : tensor<1x128xf32>
    %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %50 = ttir.empty() : tensor<1x1xf32>
    %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %52 = ttir.empty() : tensor<1x10xf32>
    %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %54 = ttir.empty() : tensor<1x10xf32>
    %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %56 = ttir.empty() : tensor<1x10xf32>
    %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %58 = ttir.empty() : tensor<1x10xf32>
    %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %60 = ttir.empty() : tensor<1xf32>
    %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %62 = ttir.empty() : tensor<1x1xf32>
    %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %64 = ttir.empty() : tensor<1x10xf32>
    %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %66 = ttir.empty() : tensor<1x10xf32>
    %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %68 = ttir.empty() : tensor<1x10xf32>
    %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %70 = ttir.empty() : tensor<1xf32>
    %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %72 = ttir.empty() : tensor<1x1xf32>
    %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %74 = ttir.empty() : tensor<1x1xf32>
    %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %76 = ttir.empty() : tensor<1x10xf32>
    %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %78 = ttir.empty() : tensor<1x10xf32>
    %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %80 = ttir.empty() : tensor<1x10xbf16>
    %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %81 : tensor<1x10xbf16>
  }
}


// -----// IR Dump After ElementTypeNormalization (ttir-element-type-normalization) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
    %4 = ttir.empty() : tensor<1x32x26x26xbf16>
    %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %6 = ttir.empty() : tensor<1x32x1x1xbf16>
    %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %8 = ttir.empty() : tensor<1x32x26x26xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %10 = ttir.empty() : tensor<1x32x26x26xbf16>
    %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %12 = ttir.empty() : tensor<1x32x26x26xbf16>
    %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %14 = ttir.empty() : tensor<1x64x24x24xbf16>
    %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %16 = ttir.empty() : tensor<1x64x1x1xbf16>
    %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %18 = ttir.empty() : tensor<1x64x24x24xbf16>
    %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %20 = ttir.empty() : tensor<1x64x24x24xbf16>
    %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %22 = ttir.empty() : tensor<1x64x24x24xbf16>
    %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %24 = ttir.empty() : tensor<1x64x12x12xbf16>
    %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %26 = ttir.empty() : tensor<1x9216xbf16>
    %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %28 = ttir.empty() : tensor<1x9216xf32>
    %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %31 = ttir.empty() : tensor<1xf32>
    %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
    %33 = ttir.empty() : tensor<1x1xf32>
    %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %35 = ttir.empty() : tensor<1x128xf32>
    %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %37 = ttir.empty() : tensor<1x128xf32>
    %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %39 = ttir.empty() : tensor<1x128xf32>
    %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %41 = ttir.empty() : tensor<1x128xf32>
    %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %43 = ttir.empty() : tensor<1x128xbf16>
    %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %45 = ttir.empty() : tensor<1x128xbf16>
    %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %47 = ttir.empty() : tensor<1x128xf32>
    %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %50 = ttir.empty() : tensor<1x1xf32>
    %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %52 = ttir.empty() : tensor<1x10xf32>
    %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %54 = ttir.empty() : tensor<1x10xf32>
    %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %56 = ttir.empty() : tensor<1x10xf32>
    %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %58 = ttir.empty() : tensor<1x10xf32>
    %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %60 = ttir.empty() : tensor<1xf32>
    %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %62 = ttir.empty() : tensor<1x1xf32>
    %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %64 = ttir.empty() : tensor<1x10xf32>
    %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %66 = ttir.empty() : tensor<1x10xf32>
    %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %68 = ttir.empty() : tensor<1x10xf32>
    %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %70 = ttir.empty() : tensor<1xf32>
    %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %72 = ttir.empty() : tensor<1x1xf32>
    %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %74 = ttir.empty() : tensor<1x1xf32>
    %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %76 = ttir.empty() : tensor<1x10xf32>
    %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %78 = ttir.empty() : tensor<1x10xf32>
    %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %80 = ttir.empty() : tensor<1x10xbf16>
    %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %81 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before TTWrapDeviceModulePass (tt-wrap-device-module) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
    %4 = ttir.empty() : tensor<1x32x26x26xbf16>
    %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %6 = ttir.empty() : tensor<1x32x1x1xbf16>
    %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %8 = ttir.empty() : tensor<1x32x26x26xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %10 = ttir.empty() : tensor<1x32x26x26xbf16>
    %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %12 = ttir.empty() : tensor<1x32x26x26xbf16>
    %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %14 = ttir.empty() : tensor<1x64x24x24xbf16>
    %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %16 = ttir.empty() : tensor<1x64x1x1xbf16>
    %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %18 = ttir.empty() : tensor<1x64x24x24xbf16>
    %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %20 = ttir.empty() : tensor<1x64x24x24xbf16>
    %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %22 = ttir.empty() : tensor<1x64x24x24xbf16>
    %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %24 = ttir.empty() : tensor<1x64x12x12xbf16>
    %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %26 = ttir.empty() : tensor<1x9216xbf16>
    %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %28 = ttir.empty() : tensor<1x9216xf32>
    %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %31 = ttir.empty() : tensor<1xf32>
    %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
    %33 = ttir.empty() : tensor<1x1xf32>
    %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %35 = ttir.empty() : tensor<1x128xf32>
    %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %37 = ttir.empty() : tensor<1x128xf32>
    %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %39 = ttir.empty() : tensor<1x128xf32>
    %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %41 = ttir.empty() : tensor<1x128xf32>
    %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %43 = ttir.empty() : tensor<1x128xbf16>
    %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %45 = ttir.empty() : tensor<1x128xbf16>
    %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %47 = ttir.empty() : tensor<1x128xf32>
    %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %50 = ttir.empty() : tensor<1x1xf32>
    %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %52 = ttir.empty() : tensor<1x10xf32>
    %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %54 = ttir.empty() : tensor<1x10xf32>
    %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %56 = ttir.empty() : tensor<1x10xf32>
    %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %58 = ttir.empty() : tensor<1x10xf32>
    %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %60 = ttir.empty() : tensor<1xf32>
    %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %62 = ttir.empty() : tensor<1x1xf32>
    %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %64 = ttir.empty() : tensor<1x10xf32>
    %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %66 = ttir.empty() : tensor<1x10xf32>
    %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %68 = ttir.empty() : tensor<1x10xf32>
    %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %70 = ttir.empty() : tensor<1xf32>
    %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %72 = ttir.empty() : tensor<1x1xf32>
    %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %74 = ttir.empty() : tensor<1x1xf32>
    %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %76 = ttir.empty() : tensor<1x10xf32>
    %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %78 = ttir.empty() : tensor<1x10xf32>
    %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %80 = ttir.empty() : tensor<1x10xbf16>
    %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %81 : tensor<1x10xbf16>
  }
}


// -----// IR Dump After TTWrapDeviceModulePass (tt-wrap-device-module) ('builtin.module' operation) //----- //
module {
  tt.device_module {
    builtin.module {
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRHoistTransform (ttir-cpu-hoist-transform) ('builtin.module' operation) //----- //
module {
  tt.device_module {
    builtin.module {
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTRegisterDevicePass (tt-register-device) ('builtin.module' operation) //----- //
module {
  tt.device_module {
    builtin.module {
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After TTRegisterDevicePass (tt-register-device) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<32x1x3x3xbf16>
        %8 = "ttir.permute"(%arg0, %7) <{permutation = array<i64: 0, 1, 2, 3>}> : (tensor<32x1x3x3xbf16>, tensor<32x1x3x3xbf16>) -> tensor<32x1x3x3xbf16>
        %9 = ttir.empty() : tensor<1x26x26x32xbf16>
        %10 = "ttir.conv2d"(%6, %8, %9) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x28x28x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %11 = "ttir.permute"(%10, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x1x1xbf16>
        %13 = "ttir.reshape"(%arg1, %12) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %14 = ttir.empty() : tensor<1x32x26x26xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %16 = ttir.empty() : tensor<1x32x26x26xbf16>
        %17 = "ttir.add"(%11, %15, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %18 = ttir.empty() : tensor<1x32x26x26xbf16>
        %19 = "ttir.maximum"(%17, %0, %18) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = ttir.empty() : tensor<1x26x26x32xbf16>
        %22 = "ttir.permute"(%19, %21) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %23 = ttir.empty() : tensor<64x32x3x3xbf16>
        %24 = "ttir.permute"(%arg2, %23) <{permutation = array<i64: 0, 1, 2, 3>}> : (tensor<64x32x3x3xbf16>, tensor<64x32x3x3xbf16>) -> tensor<64x32x3x3xbf16>
        %25 = ttir.empty() : tensor<1x24x24x64xbf16>
        %26 = "ttir.conv2d"(%22, %24, %25) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x26x26x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %27 = "ttir.permute"(%26, %20) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %28 = ttir.empty() : tensor<1x64x1x1xbf16>
        %29 = "ttir.reshape"(%arg3, %28) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %30 = ttir.empty() : tensor<1x64x24x24xbf16>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %32 = ttir.empty() : tensor<1x64x24x24xbf16>
        %33 = "ttir.add"(%27, %31, %32) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %34 = ttir.empty() : tensor<1x64x24x24xbf16>
        %35 = "ttir.maximum"(%33, %1, %34) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %36 = ttir.empty() : tensor<1x64x12x12xbf16>
        %37 = ttir.empty() : tensor<1x24x24x64xbf16>
        %38 = "ttir.permute"(%35, %37) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %39 = ttir.empty() : tensor<1x12x12x64xbf16>
        %40 = "ttir.max_pool2d"(%38, %39) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x24x24x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %41 = ttir.empty() : tensor<1x64x12x12xbf16>
        %42 = "ttir.permute"(%40, %41) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %43 = ttir.empty() : tensor<1x9216xbf16>
        %44 = "ttir.reshape"(%42, %43) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %45 = ttir.empty() : tensor<1x9216xf32>
        %46 = "ttir.typecast"(%44, %45) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %47 = ttir.empty() : tensor<1x9216xf32>
        %48 = "ttir.permute"(%46, %47) <{permutation = array<i64: 0, 1>}> : (tensor<1x9216xf32>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %49 = ttir.empty() : tensor<9216x128xf32>
        %50 = "ttir.permute"(%arg4, %49) <{permutation = array<i64: 0, 1>}> : (tensor<9216x128xf32>, tensor<9216x128xf32>) -> tensor<9216x128xf32>
        %51 = ttir.empty() : tensor<1x9216xf32>
        %52 = "ttir.reshape"(%48, %51) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x9216xf32>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %53 = ttir.empty() : tensor<9216x128xf32>
        %54 = "ttir.reshape"(%50, %53) <{shape = [9216 : i32, 128 : i32]}> : (tensor<9216x128xf32>, tensor<9216x128xf32>) -> tensor<9216x128xf32>
        %55 = ttir.empty() : tensor<1x128xf32>
        %56 = "ttir.matmul"(%52, %54, %55) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %57 = ttir.empty() : tensor<1x128xf32>
        %58 = "ttir.reshape"(%56, %57) <{shape = [1 : i32, 128 : i32]}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %59 = ttir.empty() : tensor<1xf32>
        %60 = "ttir.typecast"(%3, %59) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %61 = ttir.empty() : tensor<1x1xf32>
        %62 = "ttir.reshape"(%60, %61) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %63 = ttir.empty() : tensor<1x128xf32>
        %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %65 = ttir.empty() : tensor<1x128xf32>
        %66 = "ttir.multiply"(%58, %64, %65) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %67 = ttir.empty() : tensor<1x128xf32>
        %68 = "ttir.reshape"(%arg5, %67) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %69 = ttir.empty() : tensor<1x128xf32>
        %70 = "ttir.add"(%66, %68, %69) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %71 = ttir.empty() : tensor<1x128xbf16>
        %72 = "ttir.typecast"(%70, %71) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %73 = ttir.empty() : tensor<1x128xbf16>
        %74 = "ttir.maximum"(%72, %2, %73) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %75 = ttir.empty() : tensor<1x128xf32>
        %76 = "ttir.typecast"(%74, %75) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %77 = ttir.empty() : tensor<1x128xf32>
        %78 = "ttir.permute"(%76, %77) <{permutation = array<i64: 0, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %79 = ttir.empty() : tensor<128x10xf32>
        %80 = "ttir.permute"(%arg6, %79) <{permutation = array<i64: 0, 1>}> : (tensor<128x10xf32>, tensor<128x10xf32>) -> tensor<128x10xf32>
        %81 = ttir.empty() : tensor<1x128xf32>
        %82 = "ttir.reshape"(%78, %81) <{shape = [1 : i32, 128 : i32]}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %83 = ttir.empty() : tensor<128x10xf32>
        %84 = "ttir.reshape"(%80, %83) <{shape = [128 : i32, 10 : i32]}> : (tensor<128x10xf32>, tensor<128x10xf32>) -> tensor<128x10xf32>
        %85 = ttir.empty() : tensor<1x10xf32>
        %86 = "ttir.matmul"(%82, %84, %85) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %87 = ttir.empty() : tensor<1x10xf32>
        %88 = "ttir.reshape"(%86, %87) <{shape = [1 : i32, 10 : i32]}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %89 = ttir.empty() : tensor<1x1xf32>
        %90 = "ttir.reshape"(%60, %89) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %91 = ttir.empty() : tensor<1x10xf32>
        %92 = "ttir.broadcast"(%90, %91) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %93 = ttir.empty() : tensor<1x10xf32>
        %94 = "ttir.multiply"(%88, %92, %93) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %95 = ttir.empty() : tensor<1x10xf32>
        %96 = "ttir.reshape"(%arg7, %95) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %97 = ttir.empty() : tensor<1x10xf32>
        %98 = "ttir.add"(%94, %96, %97) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %99 = ttir.empty() : tensor<1xf32>
        %100 = "ttir.max"(%98, %99) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %101 = ttir.empty() : tensor<1x1xf32>
        %102 = "ttir.reshape"(%100, %101) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %103 = ttir.empty() : tensor<1x10xf32>
        %104 = "ttir.broadcast"(%102, %103) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %105 = ttir.empty() : tensor<1x10xf32>
        %106 = "ttir.subtract"(%98, %104, %105) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %107 = ttir.empty() : tensor<1x10xf32>
        %108 = "ttir.exp"(%106, %107) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %109 = ttir.empty() : tensor<1xf32>
        %110 = "ttir.sum"(%108, %109) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %111 = ttir.empty() : tensor<1x1xf32>
        %112 = "ttir.reshape"(%110, %111) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %113 = ttir.empty() : tensor<1x1xf32>
        %114 = "ttir.log"(%112, %113) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %115 = ttir.empty() : tensor<1x10xf32>
        %116 = "ttir.broadcast"(%114, %115) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %117 = ttir.empty() : tensor<1x10xf32>
        %118 = "ttir.subtract"(%106, %116, %117) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %119 = ttir.empty() : tensor<1x10xbf16>
        %120 = "ttir.typecast"(%118, %119) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %120 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<32x1x3x3xbf16>
        %8 = "ttir.permute"(%arg0, %7) <{permutation = array<i64: 0, 1, 2, 3>}> : (tensor<32x1x3x3xbf16>, tensor<32x1x3x3xbf16>) -> tensor<32x1x3x3xbf16>
        %9 = ttir.empty() : tensor<1x26x26x32xbf16>
        %10 = "ttir.conv2d"(%6, %8, %9) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x28x28x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %11 = "ttir.permute"(%10, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x1x1xbf16>
        %13 = "ttir.reshape"(%arg1, %12) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %14 = ttir.empty() : tensor<1x32x26x26xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %16 = ttir.empty() : tensor<1x32x26x26xbf16>
        %17 = "ttir.add"(%11, %15, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %18 = ttir.empty() : tensor<1x32x26x26xbf16>
        %19 = "ttir.maximum"(%17, %0, %18) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = ttir.empty() : tensor<1x26x26x32xbf16>
        %22 = "ttir.permute"(%19, %21) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %23 = ttir.empty() : tensor<64x32x3x3xbf16>
        %24 = "ttir.permute"(%arg2, %23) <{permutation = array<i64: 0, 1, 2, 3>}> : (tensor<64x32x3x3xbf16>, tensor<64x32x3x3xbf16>) -> tensor<64x32x3x3xbf16>
        %25 = ttir.empty() : tensor<1x24x24x64xbf16>
        %26 = "ttir.conv2d"(%22, %24, %25) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x26x26x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %27 = "ttir.permute"(%26, %20) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %28 = ttir.empty() : tensor<1x64x1x1xbf16>
        %29 = "ttir.reshape"(%arg3, %28) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %30 = ttir.empty() : tensor<1x64x24x24xbf16>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %32 = ttir.empty() : tensor<1x64x24x24xbf16>
        %33 = "ttir.add"(%27, %31, %32) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %34 = ttir.empty() : tensor<1x64x24x24xbf16>
        %35 = "ttir.maximum"(%33, %1, %34) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %36 = ttir.empty() : tensor<1x64x12x12xbf16>
        %37 = ttir.empty() : tensor<1x24x24x64xbf16>
        %38 = "ttir.permute"(%35, %37) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %39 = ttir.empty() : tensor<1x12x12x64xbf16>
        %40 = "ttir.max_pool2d"(%38, %39) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x24x24x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %41 = ttir.empty() : tensor<1x64x12x12xbf16>
        %42 = "ttir.permute"(%40, %41) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %43 = ttir.empty() : tensor<1x9216xbf16>
        %44 = "ttir.reshape"(%42, %43) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %45 = ttir.empty() : tensor<1x9216xf32>
        %46 = "ttir.typecast"(%44, %45) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %47 = ttir.empty() : tensor<1x9216xf32>
        %48 = "ttir.permute"(%46, %47) <{permutation = array<i64: 0, 1>}> : (tensor<1x9216xf32>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %49 = ttir.empty() : tensor<9216x128xf32>
        %50 = "ttir.permute"(%arg4, %49) <{permutation = array<i64: 0, 1>}> : (tensor<9216x128xf32>, tensor<9216x128xf32>) -> tensor<9216x128xf32>
        %51 = ttir.empty() : tensor<1x9216xf32>
        %52 = "ttir.reshape"(%48, %51) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x9216xf32>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %53 = ttir.empty() : tensor<9216x128xf32>
        %54 = "ttir.reshape"(%50, %53) <{shape = [9216 : i32, 128 : i32]}> : (tensor<9216x128xf32>, tensor<9216x128xf32>) -> tensor<9216x128xf32>
        %55 = ttir.empty() : tensor<1x128xf32>
        %56 = "ttir.matmul"(%52, %54, %55) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %57 = ttir.empty() : tensor<1x128xf32>
        %58 = "ttir.reshape"(%56, %57) <{shape = [1 : i32, 128 : i32]}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %59 = ttir.empty() : tensor<1xf32>
        %60 = "ttir.typecast"(%3, %59) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %61 = ttir.empty() : tensor<1x1xf32>
        %62 = "ttir.reshape"(%60, %61) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %63 = ttir.empty() : tensor<1x128xf32>
        %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %65 = ttir.empty() : tensor<1x128xf32>
        %66 = "ttir.multiply"(%58, %64, %65) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %67 = ttir.empty() : tensor<1x128xf32>
        %68 = "ttir.reshape"(%arg5, %67) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %69 = ttir.empty() : tensor<1x128xf32>
        %70 = "ttir.add"(%66, %68, %69) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %71 = ttir.empty() : tensor<1x128xbf16>
        %72 = "ttir.typecast"(%70, %71) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %73 = ttir.empty() : tensor<1x128xbf16>
        %74 = "ttir.maximum"(%72, %2, %73) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %75 = ttir.empty() : tensor<1x128xf32>
        %76 = "ttir.typecast"(%74, %75) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %77 = ttir.empty() : tensor<1x128xf32>
        %78 = "ttir.permute"(%76, %77) <{permutation = array<i64: 0, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %79 = ttir.empty() : tensor<128x10xf32>
        %80 = "ttir.permute"(%arg6, %79) <{permutation = array<i64: 0, 1>}> : (tensor<128x10xf32>, tensor<128x10xf32>) -> tensor<128x10xf32>
        %81 = ttir.empty() : tensor<1x128xf32>
        %82 = "ttir.reshape"(%78, %81) <{shape = [1 : i32, 128 : i32]}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %83 = ttir.empty() : tensor<128x10xf32>
        %84 = "ttir.reshape"(%80, %83) <{shape = [128 : i32, 10 : i32]}> : (tensor<128x10xf32>, tensor<128x10xf32>) -> tensor<128x10xf32>
        %85 = ttir.empty() : tensor<1x10xf32>
        %86 = "ttir.matmul"(%82, %84, %85) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %87 = ttir.empty() : tensor<1x10xf32>
        %88 = "ttir.reshape"(%86, %87) <{shape = [1 : i32, 10 : i32]}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %89 = ttir.empty() : tensor<1x1xf32>
        %90 = "ttir.reshape"(%60, %89) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %91 = ttir.empty() : tensor<1x10xf32>
        %92 = "ttir.broadcast"(%90, %91) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %93 = ttir.empty() : tensor<1x10xf32>
        %94 = "ttir.multiply"(%88, %92, %93) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %95 = ttir.empty() : tensor<1x10xf32>
        %96 = "ttir.reshape"(%arg7, %95) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %97 = ttir.empty() : tensor<1x10xf32>
        %98 = "ttir.add"(%94, %96, %97) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %99 = ttir.empty() : tensor<1xf32>
        %100 = "ttir.max"(%98, %99) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %101 = ttir.empty() : tensor<1x1xf32>
        %102 = "ttir.reshape"(%100, %101) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %103 = ttir.empty() : tensor<1x10xf32>
        %104 = "ttir.broadcast"(%102, %103) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %105 = ttir.empty() : tensor<1x10xf32>
        %106 = "ttir.subtract"(%98, %104, %105) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %107 = ttir.empty() : tensor<1x10xf32>
        %108 = "ttir.exp"(%106, %107) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %109 = ttir.empty() : tensor<1xf32>
        %110 = "ttir.sum"(%108, %109) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %111 = ttir.empty() : tensor<1x1xf32>
        %112 = "ttir.reshape"(%110, %111) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %113 = ttir.empty() : tensor<1x1xf32>
        %114 = "ttir.log"(%112, %113) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %115 = ttir.empty() : tensor<1x10xf32>
        %116 = "ttir.broadcast"(%114, %115) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %117 = ttir.empty() : tensor<1x10xf32>
        %118 = "ttir.subtract"(%106, %116, %117) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %119 = ttir.empty() : tensor<1x10xbf16>
        %120 = "ttir.typecast"(%118, %119) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %120 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<1x26x26x32xbf16>
        %8 = "ttir.conv2d"(%6, %arg0, %7) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x28x28x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %9 = "ttir.permute"(%8, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x32x26x26xbf16>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %16 = ttir.empty() : tensor<1x32x26x26xbf16>
        %17 = "ttir.maximum"(%15, %0, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = ttir.empty() : tensor<1x26x26x32xbf16>
        %20 = "ttir.permute"(%17, %19) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %21 = ttir.empty() : tensor<1x24x24x64xbf16>
        %22 = "ttir.conv2d"(%20, %arg2, %21) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x26x26x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %23 = "ttir.permute"(%22, %18) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %26 = ttir.empty() : tensor<1x64x24x24xbf16>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %28 = ttir.empty() : tensor<1x64x24x24xbf16>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %30 = ttir.empty() : tensor<1x64x24x24xbf16>
        %31 = "ttir.maximum"(%29, %1, %30) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %32 = ttir.empty() : tensor<1x24x24x64xbf16>
        %33 = "ttir.permute"(%31, %32) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %34 = ttir.empty() : tensor<1x12x12x64xbf16>
        %35 = "ttir.max_pool2d"(%33, %34) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x24x24x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %36 = ttir.empty() : tensor<1x64x12x12xbf16>
        %37 = "ttir.permute"(%35, %36) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %38 = ttir.empty() : tensor<1x9216xbf16>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %40 = ttir.empty() : tensor<1x9216xf32>
        %41 = "ttir.typecast"(%39, %40) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %42 = ttir.empty() : tensor<1x128xf32>
        %43 = "ttir.matmul"(%41, %arg4, %42) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %44 = ttir.empty() : tensor<1xf32>
        %45 = "ttir.typecast"(%3, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %46 = ttir.empty() : tensor<1x1xf32>
        %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %48 = ttir.empty() : tensor<1x128xf32>
        %49 = "ttir.broadcast"(%47, %48) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %50 = ttir.empty() : tensor<1x128xf32>
        %51 = "ttir.multiply"(%43, %49, %50) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %52 = ttir.empty() : tensor<1x128xf32>
        %53 = "ttir.reshape"(%arg5, %52) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %54 = ttir.empty() : tensor<1x128xf32>
        %55 = "ttir.add"(%51, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %56 = ttir.empty() : tensor<1x128xbf16>
        %57 = "ttir.typecast"(%55, %56) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %58 = ttir.empty() : tensor<1x128xbf16>
        %59 = "ttir.maximum"(%57, %2, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %60 = ttir.empty() : tensor<1x128xf32>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %62 = ttir.empty() : tensor<1x10xf32>
        %63 = "ttir.matmul"(%61, %arg6, %62) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %64 = ttir.empty() : tensor<1x1xf32>
        %65 = "ttir.reshape"(%45, %64) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.broadcast"(%65, %66) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.multiply"(%63, %67, %68) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.reshape"(%arg7, %70) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.add"(%69, %71, %72) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1xf32>
        %75 = "ttir.max"(%73, %74) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %76 = ttir.empty() : tensor<1x1xf32>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.broadcast"(%77, %78) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xf32>
        %81 = "ttir.subtract"(%73, %79, %80) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %82 = ttir.empty() : tensor<1x10xf32>
        %83 = "ttir.exp"(%81, %82) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %84 = ttir.empty() : tensor<1xf32>
        %85 = "ttir.sum"(%83, %84) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %86 = ttir.empty() : tensor<1x1xf32>
        %87 = "ttir.reshape"(%85, %86) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %88 = ttir.empty() : tensor<1x1xf32>
        %89 = "ttir.log"(%87, %88) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %90 = ttir.empty() : tensor<1x10xf32>
        %91 = "ttir.broadcast"(%89, %90) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %92 = ttir.empty() : tensor<1x10xf32>
        %93 = "ttir.subtract"(%81, %91, %92) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %94 = ttir.empty() : tensor<1x10xbf16>
        %95 = "ttir.typecast"(%93, %94) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %95 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<1x26x26x32xbf16>
        %8 = "ttir.conv2d"(%6, %arg0, %7) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x28x28x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %9 = "ttir.permute"(%8, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x32x26x26xbf16>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %16 = ttir.empty() : tensor<1x32x26x26xbf16>
        %17 = "ttir.maximum"(%15, %0, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = ttir.empty() : tensor<1x26x26x32xbf16>
        %20 = "ttir.permute"(%17, %19) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %21 = ttir.empty() : tensor<1x24x24x64xbf16>
        %22 = "ttir.conv2d"(%20, %arg2, %21) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x26x26x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %23 = "ttir.permute"(%22, %18) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %26 = ttir.empty() : tensor<1x64x24x24xbf16>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %28 = ttir.empty() : tensor<1x64x24x24xbf16>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %30 = ttir.empty() : tensor<1x64x24x24xbf16>
        %31 = "ttir.maximum"(%29, %1, %30) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %32 = ttir.empty() : tensor<1x24x24x64xbf16>
        %33 = "ttir.permute"(%31, %32) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %34 = ttir.empty() : tensor<1x12x12x64xbf16>
        %35 = "ttir.max_pool2d"(%33, %34) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x24x24x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %36 = ttir.empty() : tensor<1x64x12x12xbf16>
        %37 = "ttir.permute"(%35, %36) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %38 = ttir.empty() : tensor<1x9216xbf16>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %40 = ttir.empty() : tensor<1x9216xf32>
        %41 = "ttir.typecast"(%39, %40) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %42 = ttir.empty() : tensor<1x128xf32>
        %43 = "ttir.matmul"(%41, %arg4, %42) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %44 = ttir.empty() : tensor<1xf32>
        %45 = "ttir.typecast"(%3, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %46 = ttir.empty() : tensor<1x1xf32>
        %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %48 = ttir.empty() : tensor<1x128xf32>
        %49 = "ttir.broadcast"(%47, %48) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %50 = ttir.empty() : tensor<1x128xf32>
        %51 = "ttir.multiply"(%43, %49, %50) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %52 = ttir.empty() : tensor<1x128xf32>
        %53 = "ttir.reshape"(%arg5, %52) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %54 = ttir.empty() : tensor<1x128xf32>
        %55 = "ttir.add"(%51, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %56 = ttir.empty() : tensor<1x128xbf16>
        %57 = "ttir.typecast"(%55, %56) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %58 = ttir.empty() : tensor<1x128xbf16>
        %59 = "ttir.maximum"(%57, %2, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %60 = ttir.empty() : tensor<1x128xf32>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %62 = ttir.empty() : tensor<1x10xf32>
        %63 = "ttir.matmul"(%61, %arg6, %62) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %64 = ttir.empty() : tensor<1x1xf32>
        %65 = "ttir.reshape"(%45, %64) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.broadcast"(%65, %66) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.multiply"(%63, %67, %68) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.reshape"(%arg7, %70) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.add"(%69, %71, %72) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1xf32>
        %75 = "ttir.max"(%73, %74) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %76 = ttir.empty() : tensor<1x1xf32>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.broadcast"(%77, %78) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xf32>
        %81 = "ttir.subtract"(%73, %79, %80) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %82 = ttir.empty() : tensor<1x10xf32>
        %83 = "ttir.exp"(%81, %82) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %84 = ttir.empty() : tensor<1xf32>
        %85 = "ttir.sum"(%83, %84) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %86 = ttir.empty() : tensor<1x1xf32>
        %87 = "ttir.reshape"(%85, %86) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %88 = ttir.empty() : tensor<1x1xf32>
        %89 = "ttir.log"(%87, %88) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %90 = ttir.empty() : tensor<1x10xf32>
        %91 = "ttir.broadcast"(%89, %90) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %92 = ttir.empty() : tensor<1x10xf32>
        %93 = "ttir.subtract"(%81, %91, %92) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %94 = ttir.empty() : tensor<1x10xbf16>
        %95 = "ttir.typecast"(%93, %94) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %95 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<1x26x26x32xbf16>
        %8 = "ttir.conv2d"(%6, %arg0, %7) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x28x28x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %9 = "ttir.permute"(%8, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x32x26x26xbf16>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %16 = ttir.empty() : tensor<1x32x26x26xbf16>
        %17 = "ttir.maximum"(%15, %0, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = ttir.empty() : tensor<1x26x26x32xbf16>
        %20 = "ttir.permute"(%17, %19) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %21 = ttir.empty() : tensor<1x24x24x64xbf16>
        %22 = "ttir.conv2d"(%20, %arg2, %21) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x26x26x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %23 = "ttir.permute"(%22, %18) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %26 = ttir.empty() : tensor<1x64x24x24xbf16>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %28 = ttir.empty() : tensor<1x64x24x24xbf16>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %30 = ttir.empty() : tensor<1x64x24x24xbf16>
        %31 = "ttir.maximum"(%29, %1, %30) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %32 = ttir.empty() : tensor<1x24x24x64xbf16>
        %33 = "ttir.permute"(%31, %32) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %34 = ttir.empty() : tensor<1x12x12x64xbf16>
        %35 = "ttir.max_pool2d"(%33, %34) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x24x24x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %36 = ttir.empty() : tensor<1x64x12x12xbf16>
        %37 = "ttir.permute"(%35, %36) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %38 = ttir.empty() : tensor<1x9216xbf16>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %40 = ttir.empty() : tensor<1x9216xf32>
        %41 = "ttir.typecast"(%39, %40) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %42 = ttir.empty() : tensor<1x128xf32>
        %43 = "ttir.matmul"(%41, %arg4, %42) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %44 = ttir.empty() : tensor<1xf32>
        %45 = "ttir.typecast"(%3, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %46 = ttir.empty() : tensor<1x1xf32>
        %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %48 = ttir.empty() : tensor<1x128xf32>
        %49 = "ttir.broadcast"(%47, %48) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %50 = ttir.empty() : tensor<1x128xf32>
        %51 = "ttir.multiply"(%43, %49, %50) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %52 = ttir.empty() : tensor<1x128xf32>
        %53 = "ttir.reshape"(%arg5, %52) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %54 = ttir.empty() : tensor<1x128xf32>
        %55 = "ttir.add"(%51, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %56 = ttir.empty() : tensor<1x128xbf16>
        %57 = "ttir.typecast"(%55, %56) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %58 = ttir.empty() : tensor<1x128xbf16>
        %59 = "ttir.maximum"(%57, %2, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %60 = ttir.empty() : tensor<1x128xf32>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %62 = ttir.empty() : tensor<1x10xf32>
        %63 = "ttir.matmul"(%61, %arg6, %62) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %64 = ttir.empty() : tensor<1x1xf32>
        %65 = "ttir.reshape"(%45, %64) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.broadcast"(%65, %66) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.multiply"(%63, %67, %68) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.reshape"(%arg7, %70) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.add"(%69, %71, %72) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1xf32>
        %75 = "ttir.max"(%73, %74) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %76 = ttir.empty() : tensor<1x1xf32>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.broadcast"(%77, %78) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xf32>
        %81 = "ttir.subtract"(%73, %79, %80) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %82 = ttir.empty() : tensor<1x10xf32>
        %83 = "ttir.exp"(%81, %82) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %84 = ttir.empty() : tensor<1xf32>
        %85 = "ttir.sum"(%83, %84) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %86 = ttir.empty() : tensor<1x1xf32>
        %87 = "ttir.reshape"(%85, %86) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %88 = ttir.empty() : tensor<1x1xf32>
        %89 = "ttir.log"(%87, %88) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %90 = ttir.empty() : tensor<1x10xf32>
        %91 = "ttir.broadcast"(%89, %90) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %92 = ttir.empty() : tensor<1x10xf32>
        %93 = "ttir.subtract"(%81, %91, %92) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %94 = ttir.empty() : tensor<1x10xbf16>
        %95 = "ttir.typecast"(%93, %94) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %95 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFlattenSlidingWindow (ttir-flatten-sliding-window) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<1x26x26x32xbf16>
        %8 = "ttir.conv2d"(%6, %arg0, %7) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x28x28x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %9 = "ttir.permute"(%8, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x32x26x26xbf16>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %16 = ttir.empty() : tensor<1x32x26x26xbf16>
        %17 = "ttir.maximum"(%15, %0, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = ttir.empty() : tensor<1x26x26x32xbf16>
        %20 = "ttir.permute"(%17, %19) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %21 = ttir.empty() : tensor<1x24x24x64xbf16>
        %22 = "ttir.conv2d"(%20, %arg2, %21) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x26x26x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %23 = "ttir.permute"(%22, %18) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %26 = ttir.empty() : tensor<1x64x24x24xbf16>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %28 = ttir.empty() : tensor<1x64x24x24xbf16>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %30 = ttir.empty() : tensor<1x64x24x24xbf16>
        %31 = "ttir.maximum"(%29, %1, %30) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %32 = ttir.empty() : tensor<1x24x24x64xbf16>
        %33 = "ttir.permute"(%31, %32) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %34 = ttir.empty() : tensor<1x12x12x64xbf16>
        %35 = "ttir.max_pool2d"(%33, %34) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x24x24x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %36 = ttir.empty() : tensor<1x64x12x12xbf16>
        %37 = "ttir.permute"(%35, %36) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %38 = ttir.empty() : tensor<1x9216xbf16>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %40 = ttir.empty() : tensor<1x9216xf32>
        %41 = "ttir.typecast"(%39, %40) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %42 = ttir.empty() : tensor<1x128xf32>
        %43 = "ttir.matmul"(%41, %arg4, %42) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %44 = ttir.empty() : tensor<1xf32>
        %45 = "ttir.typecast"(%3, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %46 = ttir.empty() : tensor<1x1xf32>
        %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %48 = ttir.empty() : tensor<1x128xf32>
        %49 = "ttir.broadcast"(%47, %48) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %50 = ttir.empty() : tensor<1x128xf32>
        %51 = "ttir.multiply"(%43, %49, %50) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %52 = ttir.empty() : tensor<1x128xf32>
        %53 = "ttir.reshape"(%arg5, %52) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %54 = ttir.empty() : tensor<1x128xf32>
        %55 = "ttir.add"(%51, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %56 = ttir.empty() : tensor<1x128xbf16>
        %57 = "ttir.typecast"(%55, %56) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %58 = ttir.empty() : tensor<1x128xbf16>
        %59 = "ttir.maximum"(%57, %2, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %60 = ttir.empty() : tensor<1x128xf32>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %62 = ttir.empty() : tensor<1x10xf32>
        %63 = "ttir.matmul"(%61, %arg6, %62) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %64 = ttir.empty() : tensor<1x1xf32>
        %65 = "ttir.reshape"(%45, %64) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.broadcast"(%65, %66) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.multiply"(%63, %67, %68) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.reshape"(%arg7, %70) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.add"(%69, %71, %72) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1xf32>
        %75 = "ttir.max"(%73, %74) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %76 = ttir.empty() : tensor<1x1xf32>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.broadcast"(%77, %78) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xf32>
        %81 = "ttir.subtract"(%73, %79, %80) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %82 = ttir.empty() : tensor<1x10xf32>
        %83 = "ttir.exp"(%81, %82) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %84 = ttir.empty() : tensor<1xf32>
        %85 = "ttir.sum"(%83, %84) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %86 = ttir.empty() : tensor<1x1xf32>
        %87 = "ttir.reshape"(%85, %86) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %88 = ttir.empty() : tensor<1x1xf32>
        %89 = "ttir.log"(%87, %88) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %90 = ttir.empty() : tensor<1x10xf32>
        %91 = "ttir.broadcast"(%89, %90) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %92 = ttir.empty() : tensor<1x10xf32>
        %93 = "ttir.subtract"(%81, %91, %92) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %94 = ttir.empty() : tensor<1x10xbf16>
        %95 = "ttir.typecast"(%93, %94) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %95 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRFlattenSlidingWindow (ttir-flatten-sliding-window) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<1x26x26x32xbf16>
        %8 = ttir.empty() : tensor<1x1x784x1xbf16>
        %9 = "ttir.reshape"(%6, %8) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16>, tensor<1x1x784x1xbf16>) -> tensor<1x1x784x1xbf16>
        %10 = ttir.empty() : tensor<1x1x676x32xbf16>
        %11 = "ttir.conv2d"(%9, %arg0, %10) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %12 = ttir.empty() : tensor<1x26x26x32xbf16>
        %13 = "ttir.reshape"(%11, %12) <{shape = [1 : i32, 26 : i32, 26 : i32, 32 : i32]}> : (tensor<1x1x676x32xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %14 = "ttir.permute"(%13, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %15 = ttir.empty() : tensor<1x32x1x1xbf16>
        %16 = "ttir.reshape"(%arg1, %15) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %17 = ttir.empty() : tensor<1x32x26x26xbf16>
        %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %19 = ttir.empty() : tensor<1x32x26x26xbf16>
        %20 = "ttir.add"(%14, %18, %19) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %21 = ttir.empty() : tensor<1x32x26x26xbf16>
        %22 = "ttir.maximum"(%20, %0, %21) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %23 = ttir.empty() : tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x26x26x32xbf16>
        %25 = "ttir.permute"(%22, %24) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %26 = ttir.empty() : tensor<1x24x24x64xbf16>
        %27 = ttir.empty() : tensor<1x1x676x32xbf16>
        %28 = "ttir.reshape"(%25, %27) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %29 = ttir.empty() : tensor<1x1x576x64xbf16>
        %30 = "ttir.conv2d"(%28, %arg2, %29) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %31 = ttir.empty() : tensor<1x24x24x64xbf16>
        %32 = "ttir.reshape"(%30, %31) <{shape = [1 : i32, 24 : i32, 24 : i32, 64 : i32]}> : (tensor<1x1x576x64xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %33 = "ttir.permute"(%32, %23) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %34 = ttir.empty() : tensor<1x64x1x1xbf16>
        %35 = "ttir.reshape"(%arg3, %34) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %36 = ttir.empty() : tensor<1x64x24x24xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %38 = ttir.empty() : tensor<1x64x24x24xbf16>
        %39 = "ttir.add"(%33, %37, %38) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %40 = ttir.empty() : tensor<1x64x24x24xbf16>
        %41 = "ttir.maximum"(%39, %1, %40) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %42 = ttir.empty() : tensor<1x24x24x64xbf16>
        %43 = "ttir.permute"(%41, %42) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %44 = ttir.empty() : tensor<1x12x12x64xbf16>
        %45 = ttir.empty() : tensor<1x1x576x64xbf16>
        %46 = "ttir.reshape"(%43, %45) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %47 = ttir.empty() : tensor<1x1x144x64xbf16>
        %48 = "ttir.max_pool2d"(%46, %47) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16>, tensor<1x1x144x64xbf16>) -> tensor<1x1x144x64xbf16>
        %49 = ttir.empty() : tensor<1x12x12x64xbf16>
        %50 = "ttir.reshape"(%48, %49) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %51 = ttir.empty() : tensor<1x64x12x12xbf16>
        %52 = "ttir.permute"(%50, %51) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %53 = ttir.empty() : tensor<1x9216xbf16>
        %54 = "ttir.reshape"(%52, %53) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %55 = ttir.empty() : tensor<1x9216xf32>
        %56 = "ttir.typecast"(%54, %55) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %57 = ttir.empty() : tensor<1x128xf32>
        %58 = "ttir.matmul"(%56, %arg4, %57) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %59 = ttir.empty() : tensor<1xf32>
        %60 = "ttir.typecast"(%3, %59) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %61 = ttir.empty() : tensor<1x1xf32>
        %62 = "ttir.reshape"(%60, %61) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %63 = ttir.empty() : tensor<1x128xf32>
        %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %65 = ttir.empty() : tensor<1x128xf32>
        %66 = "ttir.multiply"(%58, %64, %65) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %67 = ttir.empty() : tensor<1x128xf32>
        %68 = "ttir.reshape"(%arg5, %67) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %69 = ttir.empty() : tensor<1x128xf32>
        %70 = "ttir.add"(%66, %68, %69) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %71 = ttir.empty() : tensor<1x128xbf16>
        %72 = "ttir.typecast"(%70, %71) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %73 = ttir.empty() : tensor<1x128xbf16>
        %74 = "ttir.maximum"(%72, %2, %73) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %75 = ttir.empty() : tensor<1x128xf32>
        %76 = "ttir.typecast"(%74, %75) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %77 = ttir.empty() : tensor<1x10xf32>
        %78 = "ttir.matmul"(%76, %arg6, %77) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %79 = ttir.empty() : tensor<1x1xf32>
        %80 = "ttir.reshape"(%60, %79) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %81 = ttir.empty() : tensor<1x10xf32>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %83 = ttir.empty() : tensor<1x10xf32>
        %84 = "ttir.multiply"(%78, %82, %83) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %85 = ttir.empty() : tensor<1x10xf32>
        %86 = "ttir.reshape"(%arg7, %85) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %87 = ttir.empty() : tensor<1x10xf32>
        %88 = "ttir.add"(%84, %86, %87) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %89 = ttir.empty() : tensor<1xf32>
        %90 = "ttir.max"(%88, %89) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %91 = ttir.empty() : tensor<1x1xf32>
        %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %93 = ttir.empty() : tensor<1x10xf32>
        %94 = "ttir.broadcast"(%92, %93) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %95 = ttir.empty() : tensor<1x10xf32>
        %96 = "ttir.subtract"(%88, %94, %95) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %97 = ttir.empty() : tensor<1x10xf32>
        %98 = "ttir.exp"(%96, %97) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %99 = ttir.empty() : tensor<1xf32>
        %100 = "ttir.sum"(%98, %99) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %101 = ttir.empty() : tensor<1x1xf32>
        %102 = "ttir.reshape"(%100, %101) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %103 = ttir.empty() : tensor<1x1xf32>
        %104 = "ttir.log"(%102, %103) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %105 = ttir.empty() : tensor<1x10xf32>
        %106 = "ttir.broadcast"(%104, %105) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %107 = ttir.empty() : tensor<1x10xf32>
        %108 = "ttir.subtract"(%96, %106, %107) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %109 = ttir.empty() : tensor<1x10xbf16>
        %110 = "ttir.typecast"(%108, %109) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %110 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIREraseInverseOps (ttir-erase-inverse-ops) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<1x26x26x32xbf16>
        %8 = ttir.empty() : tensor<1x1x784x1xbf16>
        %9 = "ttir.reshape"(%6, %8) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16>, tensor<1x1x784x1xbf16>) -> tensor<1x1x784x1xbf16>
        %10 = ttir.empty() : tensor<1x1x676x32xbf16>
        %11 = "ttir.conv2d"(%9, %arg0, %10) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %12 = ttir.empty() : tensor<1x26x26x32xbf16>
        %13 = "ttir.reshape"(%11, %12) <{shape = [1 : i32, 26 : i32, 26 : i32, 32 : i32]}> : (tensor<1x1x676x32xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %14 = "ttir.permute"(%13, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %15 = ttir.empty() : tensor<1x32x1x1xbf16>
        %16 = "ttir.reshape"(%arg1, %15) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %17 = ttir.empty() : tensor<1x32x26x26xbf16>
        %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %19 = ttir.empty() : tensor<1x32x26x26xbf16>
        %20 = "ttir.add"(%14, %18, %19) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %21 = ttir.empty() : tensor<1x32x26x26xbf16>
        %22 = "ttir.maximum"(%20, %0, %21) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %23 = ttir.empty() : tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x26x26x32xbf16>
        %25 = "ttir.permute"(%22, %24) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %26 = ttir.empty() : tensor<1x24x24x64xbf16>
        %27 = ttir.empty() : tensor<1x1x676x32xbf16>
        %28 = "ttir.reshape"(%25, %27) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %29 = ttir.empty() : tensor<1x1x576x64xbf16>
        %30 = "ttir.conv2d"(%28, %arg2, %29) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %31 = ttir.empty() : tensor<1x24x24x64xbf16>
        %32 = "ttir.reshape"(%30, %31) <{shape = [1 : i32, 24 : i32, 24 : i32, 64 : i32]}> : (tensor<1x1x576x64xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %33 = "ttir.permute"(%32, %23) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %34 = ttir.empty() : tensor<1x64x1x1xbf16>
        %35 = "ttir.reshape"(%arg3, %34) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %36 = ttir.empty() : tensor<1x64x24x24xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %38 = ttir.empty() : tensor<1x64x24x24xbf16>
        %39 = "ttir.add"(%33, %37, %38) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %40 = ttir.empty() : tensor<1x64x24x24xbf16>
        %41 = "ttir.maximum"(%39, %1, %40) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %42 = ttir.empty() : tensor<1x24x24x64xbf16>
        %43 = "ttir.permute"(%41, %42) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %44 = ttir.empty() : tensor<1x12x12x64xbf16>
        %45 = ttir.empty() : tensor<1x1x576x64xbf16>
        %46 = "ttir.reshape"(%43, %45) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %47 = ttir.empty() : tensor<1x1x144x64xbf16>
        %48 = "ttir.max_pool2d"(%46, %47) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16>, tensor<1x1x144x64xbf16>) -> tensor<1x1x144x64xbf16>
        %49 = ttir.empty() : tensor<1x12x12x64xbf16>
        %50 = "ttir.reshape"(%48, %49) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %51 = ttir.empty() : tensor<1x64x12x12xbf16>
        %52 = "ttir.permute"(%50, %51) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %53 = ttir.empty() : tensor<1x9216xbf16>
        %54 = "ttir.reshape"(%52, %53) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %55 = ttir.empty() : tensor<1x9216xf32>
        %56 = "ttir.typecast"(%54, %55) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %57 = ttir.empty() : tensor<1x128xf32>
        %58 = "ttir.matmul"(%56, %arg4, %57) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %59 = ttir.empty() : tensor<1xf32>
        %60 = "ttir.typecast"(%3, %59) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %61 = ttir.empty() : tensor<1x1xf32>
        %62 = "ttir.reshape"(%60, %61) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %63 = ttir.empty() : tensor<1x128xf32>
        %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %65 = ttir.empty() : tensor<1x128xf32>
        %66 = "ttir.multiply"(%58, %64, %65) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %67 = ttir.empty() : tensor<1x128xf32>
        %68 = "ttir.reshape"(%arg5, %67) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %69 = ttir.empty() : tensor<1x128xf32>
        %70 = "ttir.add"(%66, %68, %69) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %71 = ttir.empty() : tensor<1x128xbf16>
        %72 = "ttir.typecast"(%70, %71) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %73 = ttir.empty() : tensor<1x128xbf16>
        %74 = "ttir.maximum"(%72, %2, %73) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %75 = ttir.empty() : tensor<1x128xf32>
        %76 = "ttir.typecast"(%74, %75) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %77 = ttir.empty() : tensor<1x10xf32>
        %78 = "ttir.matmul"(%76, %arg6, %77) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %79 = ttir.empty() : tensor<1x1xf32>
        %80 = "ttir.reshape"(%60, %79) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %81 = ttir.empty() : tensor<1x10xf32>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %83 = ttir.empty() : tensor<1x10xf32>
        %84 = "ttir.multiply"(%78, %82, %83) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %85 = ttir.empty() : tensor<1x10xf32>
        %86 = "ttir.reshape"(%arg7, %85) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %87 = ttir.empty() : tensor<1x10xf32>
        %88 = "ttir.add"(%84, %86, %87) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %89 = ttir.empty() : tensor<1xf32>
        %90 = "ttir.max"(%88, %89) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %91 = ttir.empty() : tensor<1x1xf32>
        %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %93 = ttir.empty() : tensor<1x10xf32>
        %94 = "ttir.broadcast"(%92, %93) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %95 = ttir.empty() : tensor<1x10xf32>
        %96 = "ttir.subtract"(%88, %94, %95) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %97 = ttir.empty() : tensor<1x10xf32>
        %98 = "ttir.exp"(%96, %97) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %99 = ttir.empty() : tensor<1xf32>
        %100 = "ttir.sum"(%98, %99) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %101 = ttir.empty() : tensor<1x1xf32>
        %102 = "ttir.reshape"(%100, %101) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %103 = ttir.empty() : tensor<1x1xf32>
        %104 = "ttir.log"(%102, %103) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %105 = ttir.empty() : tensor<1x10xf32>
        %106 = "ttir.broadcast"(%104, %105) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %107 = ttir.empty() : tensor<1x10xf32>
        %108 = "ttir.subtract"(%96, %106, %107) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %109 = ttir.empty() : tensor<1x10xbf16>
        %110 = "ttir.typecast"(%108, %109) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %110 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIREraseInverseOps (ttir-erase-inverse-ops) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x28x28x1xbf16>
        %5 = "ttir.permute"(%arg12, %4) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %6 = ttir.empty() : tensor<1x1x784x1xbf16>
        %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16>, tensor<1x1x784x1xbf16>) -> tensor<1x1x784x1xbf16>
        %8 = ttir.empty() : tensor<1x1x676x32xbf16>
        %9 = "ttir.conv2d"(%7, %arg0, %8) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x1x1x32xbf16>
        %13 = "ttir.permute"(%11, %12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16>, tensor<1x1x1x32xbf16>) -> tensor<1x1x1x32xbf16>
        %14 = ttir.empty() : tensor<1x1x676x32xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 1, 1, 676, 1>}> : (tensor<1x1x1x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %16 = ttir.empty() : tensor<1x1x676x32xbf16>
        %17 = "ttir.add"(%9, %15, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %18 = ttir.empty() : tensor<1x26x26x32xbf16>
        %19 = "ttir.permute"(%0, %18) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %20 = ttir.empty() : tensor<1x1x676x32xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %22 = ttir.empty() : tensor<1x1x676x32xbf16>
        %23 = "ttir.maximum"(%17, %21, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %24 = ttir.empty() : tensor<1x1x576x64xbf16>
        %25 = "ttir.conv2d"(%23, %arg2, %24) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %26 = ttir.empty() : tensor<1x64x1x1xbf16>
        %27 = "ttir.reshape"(%arg3, %26) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %28 = ttir.empty() : tensor<1x1x1x64xbf16>
        %29 = "ttir.permute"(%27, %28) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x1x1x64xbf16>) -> tensor<1x1x1x64xbf16>
        %30 = ttir.empty() : tensor<1x1x576x64xbf16>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 1, 1, 576, 1>}> : (tensor<1x1x1x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %32 = ttir.empty() : tensor<1x1x576x64xbf16>
        %33 = "ttir.add"(%25, %31, %32) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %34 = ttir.empty() : tensor<1x24x24x64xbf16>
        %35 = "ttir.permute"(%1, %34) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %36 = ttir.empty() : tensor<1x1x576x64xbf16>
        %37 = "ttir.reshape"(%35, %36) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %38 = ttir.empty() : tensor<1x1x576x64xbf16>
        %39 = "ttir.maximum"(%33, %37, %38) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %40 = ttir.empty() : tensor<1x1x144x64xbf16>
        %41 = "ttir.max_pool2d"(%39, %40) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16>, tensor<1x1x144x64xbf16>) -> tensor<1x1x144x64xbf16>
        %42 = ttir.empty() : tensor<1x12x12x64xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %44 = ttir.empty() : tensor<1x64x12x12xbf16>
        %45 = "ttir.permute"(%43, %44) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %46 = ttir.empty() : tensor<1x9216xbf16>
        %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %48 = ttir.empty() : tensor<1x9216xf32>
        %49 = "ttir.typecast"(%47, %48) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %50 = ttir.empty() : tensor<1x128xf32>
        %51 = "ttir.matmul"(%49, %arg4, %50) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %52 = ttir.empty() : tensor<1x1xsi32>
        %53 = "ttir.reshape"(%3, %52) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32>, tensor<1x1xsi32>) -> tensor<1x1xsi32>
        %54 = ttir.empty() : tensor<1x1xf32>
        %55 = "ttir.typecast"(%53, %54) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xsi32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %56 = ttir.empty() : tensor<1x128xf32>
        %57 = "ttir.broadcast"(%55, %56) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %58 = ttir.empty() : tensor<1x128xf32>
        %59 = "ttir.multiply"(%51, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %60 = ttir.empty() : tensor<1x128xf32>
        %61 = "ttir.reshape"(%arg5, %60) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %62 = ttir.empty() : tensor<1x128xf32>
        %63 = "ttir.add"(%59, %61, %62) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %64 = ttir.empty() : tensor<1x128xbf16>
        %65 = "ttir.typecast"(%63, %64) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %66 = ttir.empty() : tensor<1x128xbf16>
        %67 = "ttir.maximum"(%65, %2, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %68 = ttir.empty() : tensor<1x128xf32>
        %69 = "ttir.typecast"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.matmul"(%69, %arg6, %70) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.broadcast"(%55, %72) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1x10xf32>
        %75 = "ttir.multiply"(%71, %73, %74) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.reshape"(%arg7, %76) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.add"(%75, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1xf32>
        %81 = "ttir.max"(%79, %80) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %82 = ttir.empty() : tensor<1x1xf32>
        %83 = "ttir.reshape"(%81, %82) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %84 = ttir.empty() : tensor<1x10xf32>
        %85 = "ttir.broadcast"(%83, %84) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %86 = ttir.empty() : tensor<1x10xf32>
        %87 = "ttir.subtract"(%79, %85, %86) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %88 = ttir.empty() : tensor<1x10xf32>
        %89 = "ttir.exp"(%87, %88) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %90 = ttir.empty() : tensor<1xf32>
        %91 = "ttir.sum"(%89, %90) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %92 = ttir.empty() : tensor<1x1xf32>
        %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %94 = ttir.empty() : tensor<1x1xf32>
        %95 = "ttir.log"(%93, %94) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %96 = ttir.empty() : tensor<1x10xf32>
        %97 = "ttir.broadcast"(%95, %96) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %98 = ttir.empty() : tensor<1x10xf32>
        %99 = "ttir.subtract"(%87, %97, %98) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %100 = ttir.empty() : tensor<1x10xbf16>
        %101 = "ttir.typecast"(%99, %100) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %101 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x28x28x1xbf16>
        %5 = "ttir.permute"(%arg12, %4) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %6 = ttir.empty() : tensor<1x1x784x1xbf16>
        %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16>, tensor<1x1x784x1xbf16>) -> tensor<1x1x784x1xbf16>
        %8 = ttir.empty() : tensor<1x1x676x32xbf16>
        %9 = "ttir.conv2d"(%7, %arg0, %8) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x1x1x32xbf16>
        %13 = "ttir.permute"(%11, %12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16>, tensor<1x1x1x32xbf16>) -> tensor<1x1x1x32xbf16>
        %14 = ttir.empty() : tensor<1x1x676x32xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 1, 1, 676, 1>}> : (tensor<1x1x1x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %16 = ttir.empty() : tensor<1x1x676x32xbf16>
        %17 = "ttir.add"(%9, %15, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %18 = ttir.empty() : tensor<1x26x26x32xbf16>
        %19 = "ttir.permute"(%0, %18) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %20 = ttir.empty() : tensor<1x1x676x32xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %22 = ttir.empty() : tensor<1x1x676x32xbf16>
        %23 = "ttir.maximum"(%17, %21, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %24 = ttir.empty() : tensor<1x1x576x64xbf16>
        %25 = "ttir.conv2d"(%23, %arg2, %24) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %26 = ttir.empty() : tensor<1x64x1x1xbf16>
        %27 = "ttir.reshape"(%arg3, %26) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %28 = ttir.empty() : tensor<1x1x1x64xbf16>
        %29 = "ttir.permute"(%27, %28) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x1x1x64xbf16>) -> tensor<1x1x1x64xbf16>
        %30 = ttir.empty() : tensor<1x1x576x64xbf16>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 1, 1, 576, 1>}> : (tensor<1x1x1x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %32 = ttir.empty() : tensor<1x1x576x64xbf16>
        %33 = "ttir.add"(%25, %31, %32) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %34 = ttir.empty() : tensor<1x24x24x64xbf16>
        %35 = "ttir.permute"(%1, %34) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %36 = ttir.empty() : tensor<1x1x576x64xbf16>
        %37 = "ttir.reshape"(%35, %36) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %38 = ttir.empty() : tensor<1x1x576x64xbf16>
        %39 = "ttir.maximum"(%33, %37, %38) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %40 = ttir.empty() : tensor<1x1x144x64xbf16>
        %41 = "ttir.max_pool2d"(%39, %40) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16>, tensor<1x1x144x64xbf16>) -> tensor<1x1x144x64xbf16>
        %42 = ttir.empty() : tensor<1x12x12x64xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %44 = ttir.empty() : tensor<1x64x12x12xbf16>
        %45 = "ttir.permute"(%43, %44) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %46 = ttir.empty() : tensor<1x9216xbf16>
        %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %48 = ttir.empty() : tensor<1x9216xf32>
        %49 = "ttir.typecast"(%47, %48) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %50 = ttir.empty() : tensor<1x128xf32>
        %51 = "ttir.matmul"(%49, %arg4, %50) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %52 = ttir.empty() : tensor<1x1xsi32>
        %53 = "ttir.reshape"(%3, %52) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32>, tensor<1x1xsi32>) -> tensor<1x1xsi32>
        %54 = ttir.empty() : tensor<1x1xf32>
        %55 = "ttir.typecast"(%53, %54) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xsi32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %56 = ttir.empty() : tensor<1x128xf32>
        %57 = "ttir.broadcast"(%55, %56) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %58 = ttir.empty() : tensor<1x128xf32>
        %59 = "ttir.multiply"(%51, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %60 = ttir.empty() : tensor<1x128xf32>
        %61 = "ttir.reshape"(%arg5, %60) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %62 = ttir.empty() : tensor<1x128xf32>
        %63 = "ttir.add"(%59, %61, %62) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %64 = ttir.empty() : tensor<1x128xbf16>
        %65 = "ttir.typecast"(%63, %64) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %66 = ttir.empty() : tensor<1x128xbf16>
        %67 = "ttir.maximum"(%65, %2, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %68 = ttir.empty() : tensor<1x128xf32>
        %69 = "ttir.typecast"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.matmul"(%69, %arg6, %70) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.broadcast"(%55, %72) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1x10xf32>
        %75 = "ttir.multiply"(%71, %73, %74) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.reshape"(%arg7, %76) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.add"(%75, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1xf32>
        %81 = "ttir.max"(%79, %80) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %82 = ttir.empty() : tensor<1x1xf32>
        %83 = "ttir.reshape"(%81, %82) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %84 = ttir.empty() : tensor<1x10xf32>
        %85 = "ttir.broadcast"(%83, %84) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %86 = ttir.empty() : tensor<1x10xf32>
        %87 = "ttir.subtract"(%79, %85, %86) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %88 = ttir.empty() : tensor<1x10xf32>
        %89 = "ttir.exp"(%87, %88) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %90 = ttir.empty() : tensor<1xf32>
        %91 = "ttir.sum"(%89, %90) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %92 = ttir.empty() : tensor<1x1xf32>
        %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %94 = ttir.empty() : tensor<1x1xf32>
        %95 = "ttir.log"(%93, %94) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %96 = ttir.empty() : tensor<1x10xf32>
        %97 = "ttir.broadcast"(%95, %96) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %98 = ttir.empty() : tensor<1x10xf32>
        %99 = "ttir.subtract"(%87, %97, %98) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %100 = ttir.empty() : tensor<1x10xbf16>
        %101 = "ttir.typecast"(%99, %100) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %101 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x28x28x1xbf16>
        %5 = "ttir.permute"(%arg12, %4) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %6 = ttir.empty() : tensor<1x1x784x1xbf16>
        %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16>, tensor<1x1x784x1xbf16>) -> tensor<1x1x784x1xbf16>
        %8 = ttir.empty() : tensor<1x1x676x32xbf16>
        %9 = "ttir.conv2d"(%7, %arg0, %8) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x1x1x32xbf16>
        %13 = "ttir.permute"(%11, %12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16>, tensor<1x1x1x32xbf16>) -> tensor<1x1x1x32xbf16>
        %14 = ttir.empty() : tensor<1x1x676x32xbf16>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x1x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %16 = ttir.empty() : tensor<1x26x26x32xbf16>
        %17 = "ttir.permute"(%0, %16) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %18 = ttir.empty() : tensor<1x1x676x32xbf16>
        %19 = "ttir.reshape"(%17, %18) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %20 = ttir.empty() : tensor<1x1x676x32xbf16>
        %21 = "ttir.maximum"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %22 = ttir.empty() : tensor<1x1x576x64xbf16>
        %23 = "ttir.conv2d"(%21, %arg2, %22) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %26 = ttir.empty() : tensor<1x1x1x64xbf16>
        %27 = "ttir.permute"(%25, %26) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x1x1x64xbf16>) -> tensor<1x1x1x64xbf16>
        %28 = ttir.empty() : tensor<1x1x576x64xbf16>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x1x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %30 = ttir.empty() : tensor<1x24x24x64xbf16>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %32 = ttir.empty() : tensor<1x1x576x64xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %34 = ttir.empty() : tensor<1x1x576x64xbf16>
        %35 = "ttir.maximum"(%29, %33, %34) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %36 = ttir.empty() : tensor<1x1x144x64xbf16>
        %37 = "ttir.max_pool2d"(%35, %36) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16>, tensor<1x1x144x64xbf16>) -> tensor<1x1x144x64xbf16>
        %38 = ttir.empty() : tensor<1x12x12x64xbf16>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %40 = ttir.empty() : tensor<1x64x12x12xbf16>
        %41 = "ttir.permute"(%39, %40) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %42 = ttir.empty() : tensor<1x9216xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %44 = ttir.empty() : tensor<1x9216xf32>
        %45 = "ttir.typecast"(%43, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %46 = ttir.empty() : tensor<1x128xf32>
        %47 = "ttir.matmul"(%45, %arg4, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %48 = ttir.empty() : tensor<1x1xsi32>
        %49 = "ttir.reshape"(%3, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32>, tensor<1x1xsi32>) -> tensor<1x1xsi32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.typecast"(%49, %50) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xsi32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x128xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %54 = ttir.empty() : tensor<1x128xf32>
        %55 = "ttir.multiply"(%47, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %56 = ttir.empty() : tensor<1x128xf32>
        %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %58 = ttir.empty() : tensor<1x128xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %60 = ttir.empty() : tensor<1x128xbf16>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %62 = ttir.empty() : tensor<1x128xbf16>
        %63 = "ttir.maximum"(%61, %2, %62) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %64 = ttir.empty() : tensor<1x128xf32>
        %65 = "ttir.typecast"(%63, %64) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.matmul"(%65, %arg6, %66) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.broadcast"(%51, %68) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.multiply"(%67, %69, %70) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.reshape"(%arg7, %72) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1x10xf32>
        %75 = "ttir.add"(%71, %73, %74) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %76 = ttir.empty() : tensor<1xf32>
        %77 = "ttir.max"(%75, %76) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %78 = ttir.empty() : tensor<1x1xf32>
        %79 = "ttir.reshape"(%77, %78) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %80 = ttir.empty() : tensor<1x10xf32>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %82 = ttir.empty() : tensor<1x10xf32>
        %83 = "ttir.subtract"(%75, %81, %82) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %84 = ttir.empty() : tensor<1x10xf32>
        %85 = "ttir.exp"(%83, %84) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %86 = ttir.empty() : tensor<1xf32>
        %87 = "ttir.sum"(%85, %86) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %88 = ttir.empty() : tensor<1x1xf32>
        %89 = "ttir.reshape"(%87, %88) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %90 = ttir.empty() : tensor<1x1xf32>
        %91 = "ttir.log"(%89, %90) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %92 = ttir.empty() : tensor<1x10xf32>
        %93 = "ttir.broadcast"(%91, %92) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %94 = ttir.empty() : tensor<1x10xf32>
        %95 = "ttir.subtract"(%83, %93, %94) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %96 = ttir.empty() : tensor<1x10xbf16>
        %97 = "ttir.typecast"(%95, %96) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %97 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTNNLayout (ttnn-layout) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x28x28x1xbf16>
        %5 = "ttir.permute"(%arg12, %4) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %6 = ttir.empty() : tensor<1x1x784x1xbf16>
        %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16>, tensor<1x1x784x1xbf16>) -> tensor<1x1x784x1xbf16>
        %8 = ttir.empty() : tensor<1x1x676x32xbf16>
        %9 = "ttir.conv2d"(%7, %arg0, %8) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x1x1x32xbf16>
        %13 = "ttir.permute"(%11, %12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16>, tensor<1x1x1x32xbf16>) -> tensor<1x1x1x32xbf16>
        %14 = ttir.empty() : tensor<1x1x676x32xbf16>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x1x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %16 = ttir.empty() : tensor<1x26x26x32xbf16>
        %17 = "ttir.permute"(%0, %16) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %18 = ttir.empty() : tensor<1x1x676x32xbf16>
        %19 = "ttir.reshape"(%17, %18) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %20 = ttir.empty() : tensor<1x1x676x32xbf16>
        %21 = "ttir.maximum"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %22 = ttir.empty() : tensor<1x1x576x64xbf16>
        %23 = "ttir.conv2d"(%21, %arg2, %22) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %26 = ttir.empty() : tensor<1x1x1x64xbf16>
        %27 = "ttir.permute"(%25, %26) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x1x1x64xbf16>) -> tensor<1x1x1x64xbf16>
        %28 = ttir.empty() : tensor<1x1x576x64xbf16>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x1x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %30 = ttir.empty() : tensor<1x24x24x64xbf16>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %32 = ttir.empty() : tensor<1x1x576x64xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %34 = ttir.empty() : tensor<1x1x576x64xbf16>
        %35 = "ttir.maximum"(%29, %33, %34) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %36 = ttir.empty() : tensor<1x1x144x64xbf16>
        %37 = "ttir.max_pool2d"(%35, %36) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16>, tensor<1x1x144x64xbf16>) -> tensor<1x1x144x64xbf16>
        %38 = ttir.empty() : tensor<1x12x12x64xbf16>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %40 = ttir.empty() : tensor<1x64x12x12xbf16>
        %41 = "ttir.permute"(%39, %40) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %42 = ttir.empty() : tensor<1x9216xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %44 = ttir.empty() : tensor<1x9216xf32>
        %45 = "ttir.typecast"(%43, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %46 = ttir.empty() : tensor<1x128xf32>
        %47 = "ttir.matmul"(%45, %arg4, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %48 = ttir.empty() : tensor<1x1xsi32>
        %49 = "ttir.reshape"(%3, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32>, tensor<1x1xsi32>) -> tensor<1x1xsi32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.typecast"(%49, %50) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xsi32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x128xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %54 = ttir.empty() : tensor<1x128xf32>
        %55 = "ttir.multiply"(%47, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %56 = ttir.empty() : tensor<1x128xf32>
        %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %58 = ttir.empty() : tensor<1x128xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %60 = ttir.empty() : tensor<1x128xbf16>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %62 = ttir.empty() : tensor<1x128xbf16>
        %63 = "ttir.maximum"(%61, %2, %62) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %64 = ttir.empty() : tensor<1x128xf32>
        %65 = "ttir.typecast"(%63, %64) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.matmul"(%65, %arg6, %66) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.broadcast"(%51, %68) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.multiply"(%67, %69, %70) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.reshape"(%arg7, %72) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1x10xf32>
        %75 = "ttir.add"(%71, %73, %74) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %76 = ttir.empty() : tensor<1xf32>
        %77 = "ttir.max"(%75, %76) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %78 = ttir.empty() : tensor<1x1xf32>
        %79 = "ttir.reshape"(%77, %78) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %80 = ttir.empty() : tensor<1x10xf32>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %82 = ttir.empty() : tensor<1x10xf32>
        %83 = "ttir.subtract"(%75, %81, %82) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %84 = ttir.empty() : tensor<1x10xf32>
        %85 = "ttir.exp"(%83, %84) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %86 = ttir.empty() : tensor<1xf32>
        %87 = "ttir.sum"(%85, %86) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %88 = ttir.empty() : tensor<1x1xf32>
        %89 = "ttir.reshape"(%87, %88) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %90 = ttir.empty() : tensor<1x1xf32>
        %91 = "ttir.log"(%89, %90) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %92 = ttir.empty() : tensor<1x10xf32>
        %93 = "ttir.broadcast"(%91, %92) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %94 = ttir.empty() : tensor<1x10xf32>
        %95 = "ttir.subtract"(%83, %93, %94) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %96 = ttir.empty() : tensor<1x10xbf16>
        %97 = "ttir.typecast"(%95, %96) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %97 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After TTNNLayout (ttnn-layout) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16, #ttnn_layout10>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32, #ttnn_layout15>
        %4 = ttir.empty() : tensor<1x28x28x1xbf16, #ttnn_layout16>
        %5 = "ttir.permute"(%arg12, %4) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>, tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = ttir.empty() : tensor<1x1x784x1xbf16, #ttnn_layout17>
        %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>, tensor<1x1x784x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %8 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %9 = "ttir.conv2d"(%7, %arg0, %8) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>, tensor<32x1x3x3xbf16, #ttnn_layout>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16, #ttnn_layout13>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>, tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %12 = ttir.empty() : tensor<1x1x1x32xbf16, #ttnn_layout11>
        %13 = "ttir.permute"(%11, %12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %14 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x1x32xbf16, #ttnn_layout11>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %16 = ttir.empty() : tensor<1x26x26x32xbf16, #ttnn_layout19>
        %17 = "ttir.permute"(%0, %16) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>, tensor<1x26x26x32xbf16, #ttnn_layout19>) -> tensor<1x26x26x32xbf16, #ttnn_layout19>
        %18 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %19 = "ttir.reshape"(%17, %18) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout19>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %20 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %21 = "ttir.maximum"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %22 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %23 = "ttir.conv2d"(%21, %arg2, %22) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<64x32x3x3xbf16, #ttnn_layout2>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16, #ttnn_layout14>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>, tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %26 = ttir.empty() : tensor<1x1x1x64xbf16, #ttnn_layout21>
        %27 = "ttir.permute"(%25, %26) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>, tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x1x64xbf16, #ttnn_layout21>
        %28 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x1x64xbf16, #ttnn_layout21>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %30 = ttir.empty() : tensor<1x24x24x64xbf16, #ttnn_layout22>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>, tensor<1x24x24x64xbf16, #ttnn_layout22>) -> tensor<1x24x24x64xbf16, #ttnn_layout22>
        %32 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout22>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %34 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %35 = "ttir.maximum"(%29, %33, %34) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %36 = ttir.empty() : tensor<1x1x144x64xbf16, #ttnn_layout23>
        %37 = "ttir.max_pool2d"(%35, %36) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x144x64xbf16, #ttnn_layout23>) -> tensor<1x1x144x64xbf16, #ttnn_layout23>
        %38 = ttir.empty() : tensor<1x12x12x64xbf16, #ttnn_layout24>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout23>, tensor<1x12x12x64xbf16, #ttnn_layout24>) -> tensor<1x12x12x64xbf16, #ttnn_layout24>
        %40 = ttir.empty() : tensor<1x64x12x12xbf16, #ttnn_layout14>
        %41 = "ttir.permute"(%39, %40) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout24>, tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %42 = ttir.empty() : tensor<1x9216xbf16, #ttnn_layout25>
        %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>, tensor<1x9216xbf16, #ttnn_layout25>) -> tensor<1x9216xbf16, #ttnn_layout25>
        %44 = ttir.empty() : tensor<1x9216xf32, #ttnn_layout26>
        %45 = "ttir.typecast"(%43, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16, #ttnn_layout25>, tensor<1x9216xf32, #ttnn_layout26>) -> tensor<1x9216xf32, #ttnn_layout26>
        %46 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %47 = "ttir.matmul"(%45, %arg4, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout26>, tensor<9216x128xf32, #ttnn_layout4>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %48 = ttir.empty() : tensor<1x1xsi32, #ttnn_layout28>
        %49 = "ttir.reshape"(%3, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn_layout15>, tensor<1x1xsi32, #ttnn_layout28>) -> tensor<1x1xsi32, #ttnn_layout28>
        %50 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %51 = "ttir.typecast"(%49, %50) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xsi32, #ttnn_layout28>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %52 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %54 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %55 = "ttir.multiply"(%47, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %56 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %58 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %60 = ttir.empty() : tensor<1x128xbf16, #ttnn_layout10>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %62 = ttir.empty() : tensor<1x128xbf16, #ttnn_layout10>
        %63 = "ttir.maximum"(%61, %2, %62) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %64 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %65 = "ttir.typecast"(%63, %64) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %66 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %67 = "ttir.matmul"(%65, %arg6, %66) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<128x10xf32, #ttnn_layout6>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %68 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %69 = "ttir.broadcast"(%51, %68) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %70 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %71 = "ttir.multiply"(%67, %69, %70) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %72 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %73 = "ttir.reshape"(%arg7, %72) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %74 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %75 = "ttir.add"(%71, %73, %74) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %76 = ttir.empty() : tensor<1xf32, #ttnn_layout7>
        %77 = "ttir.max"(%75, %76) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1xf32, #ttnn_layout7>) -> tensor<1xf32, #ttnn_layout7>
        %78 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %79 = "ttir.reshape"(%77, %78) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %80 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %82 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %83 = "ttir.subtract"(%75, %81, %82) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %84 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %85 = "ttir.exp"(%83, %84) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %86 = ttir.empty() : tensor<1xf32, #ttnn_layout7>
        %87 = "ttir.sum"(%85, %86) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1xf32, #ttnn_layout7>) -> tensor<1xf32, #ttnn_layout7>
        %88 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %89 = "ttir.reshape"(%87, %88) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %90 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %91 = "ttir.log"(%89, %90) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %92 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %93 = "ttir.broadcast"(%91, %92) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %94 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %95 = "ttir.subtract"(%83, %93, %94) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %96 = ttir.empty() : tensor<1x10xbf16, #ttnn_layout12>
        %97 = "ttir.typecast"(%95, %96) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xbf16, #ttnn_layout12>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %97 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump Before ConvertTTIRToTTNN (convert-ttir-to-ttnn) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16, #ttnn_layout10>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32, #ttnn_layout15>
        %4 = ttir.empty() : tensor<1x28x28x1xbf16, #ttnn_layout16>
        %5 = "ttir.permute"(%arg12, %4) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>, tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = ttir.empty() : tensor<1x1x784x1xbf16, #ttnn_layout17>
        %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>, tensor<1x1x784x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %8 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %9 = "ttir.conv2d"(%7, %arg0, %8) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>, tensor<32x1x3x3xbf16, #ttnn_layout>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16, #ttnn_layout13>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>, tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %12 = ttir.empty() : tensor<1x1x1x32xbf16, #ttnn_layout11>
        %13 = "ttir.permute"(%11, %12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %14 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x1x32xbf16, #ttnn_layout11>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %16 = ttir.empty() : tensor<1x26x26x32xbf16, #ttnn_layout19>
        %17 = "ttir.permute"(%0, %16) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>, tensor<1x26x26x32xbf16, #ttnn_layout19>) -> tensor<1x26x26x32xbf16, #ttnn_layout19>
        %18 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %19 = "ttir.reshape"(%17, %18) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout19>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %20 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %21 = "ttir.maximum"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %22 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %23 = "ttir.conv2d"(%21, %arg2, %22) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<64x32x3x3xbf16, #ttnn_layout2>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16, #ttnn_layout14>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>, tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %26 = ttir.empty() : tensor<1x1x1x64xbf16, #ttnn_layout21>
        %27 = "ttir.permute"(%25, %26) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>, tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x1x64xbf16, #ttnn_layout21>
        %28 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x1x64xbf16, #ttnn_layout21>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %30 = ttir.empty() : tensor<1x24x24x64xbf16, #ttnn_layout22>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>, tensor<1x24x24x64xbf16, #ttnn_layout22>) -> tensor<1x24x24x64xbf16, #ttnn_layout22>
        %32 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout22>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %34 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %35 = "ttir.maximum"(%29, %33, %34) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %36 = ttir.empty() : tensor<1x1x144x64xbf16, #ttnn_layout23>
        %37 = "ttir.max_pool2d"(%35, %36) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x144x64xbf16, #ttnn_layout23>) -> tensor<1x1x144x64xbf16, #ttnn_layout23>
        %38 = ttir.empty() : tensor<1x12x12x64xbf16, #ttnn_layout24>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout23>, tensor<1x12x12x64xbf16, #ttnn_layout24>) -> tensor<1x12x12x64xbf16, #ttnn_layout24>
        %40 = ttir.empty() : tensor<1x64x12x12xbf16, #ttnn_layout14>
        %41 = "ttir.permute"(%39, %40) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout24>, tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %42 = ttir.empty() : tensor<1x9216xbf16, #ttnn_layout25>
        %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>, tensor<1x9216xbf16, #ttnn_layout25>) -> tensor<1x9216xbf16, #ttnn_layout25>
        %44 = ttir.empty() : tensor<1x9216xf32, #ttnn_layout26>
        %45 = "ttir.typecast"(%43, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16, #ttnn_layout25>, tensor<1x9216xf32, #ttnn_layout26>) -> tensor<1x9216xf32, #ttnn_layout26>
        %46 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %47 = "ttir.matmul"(%45, %arg4, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout26>, tensor<9216x128xf32, #ttnn_layout4>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %48 = ttir.empty() : tensor<1x1xsi32, #ttnn_layout28>
        %49 = "ttir.reshape"(%3, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn_layout15>, tensor<1x1xsi32, #ttnn_layout28>) -> tensor<1x1xsi32, #ttnn_layout28>
        %50 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %51 = "ttir.typecast"(%49, %50) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xsi32, #ttnn_layout28>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %52 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %54 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %55 = "ttir.multiply"(%47, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %56 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %58 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %60 = ttir.empty() : tensor<1x128xbf16, #ttnn_layout10>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %62 = ttir.empty() : tensor<1x128xbf16, #ttnn_layout10>
        %63 = "ttir.maximum"(%61, %2, %62) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %64 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %65 = "ttir.typecast"(%63, %64) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %66 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %67 = "ttir.matmul"(%65, %arg6, %66) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<128x10xf32, #ttnn_layout6>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %68 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %69 = "ttir.broadcast"(%51, %68) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %70 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %71 = "ttir.multiply"(%67, %69, %70) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %72 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %73 = "ttir.reshape"(%arg7, %72) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %74 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %75 = "ttir.add"(%71, %73, %74) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %76 = ttir.empty() : tensor<1xf32, #ttnn_layout7>
        %77 = "ttir.max"(%75, %76) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1xf32, #ttnn_layout7>) -> tensor<1xf32, #ttnn_layout7>
        %78 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %79 = "ttir.reshape"(%77, %78) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %80 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %82 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %83 = "ttir.subtract"(%75, %81, %82) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %84 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %85 = "ttir.exp"(%83, %84) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %86 = ttir.empty() : tensor<1xf32, #ttnn_layout7>
        %87 = "ttir.sum"(%85, %86) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1xf32, #ttnn_layout7>) -> tensor<1xf32, #ttnn_layout7>
        %88 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %89 = "ttir.reshape"(%87, %88) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %90 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %91 = "ttir.log"(%89, %90) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %92 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %93 = "ttir.broadcast"(%91, %92) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %94 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %95 = "ttir.subtract"(%83, %93, %94) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %96 = ttir.empty() : tensor<1x10xbf16, #ttnn_layout12>
        %97 = "ttir.typecast"(%95, %96) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xbf16, #ttnn_layout12>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %97 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump After ConvertTTIRToTTNN (convert-ttir-to-ttnn) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xsi32, #ttnn_layout15>
        %5 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<28x1>>, <interleaved>>, shape = #ttnn.shape<1x28x28x1>}> : (!ttnn.device) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %7 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<25x1>>, <interleaved>>, shape = #ttnn.shape<1x1x784x1>}> : (!ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %8 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %9 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %10 = "ttnn.conv2d"(%8, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %11 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<32x1>>, <interleaved>>, shape = #ttnn.shape<1x32x1x1>}> : (!ttnn.device) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %13 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1x1x32>}> : (!ttnn.device) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %14 = "ttnn.permute"(%12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %15 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %16 = "ttnn.add"(%10, %14) : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %17 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<26x1>>, <interleaved>>, shape = #ttnn.shape<1x26x26x32>}> : (!ttnn.device) -> tensor<1x26x26x32xbf16, #ttnn_layout19>
        %18 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout19>
        %19 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %20 = "ttnn.reshape"(%18) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout19>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %21 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %22 = "ttnn.maximum"(%16, %20) : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %23 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %24 = "ttnn.conv2d"(%22, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %25 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<64x1>>, <interleaved>>, shape = #ttnn.shape<1x64x1x1>}> : (!ttnn.device) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %26 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %27 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x2>>, <interleaved>>, shape = #ttnn.shape<1x1x1x64>}> : (!ttnn.device) -> tensor<1x1x1x64xbf16, #ttnn_layout21>
        %28 = "ttnn.permute"(%26) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout21>
        %29 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %30 = "ttnn.add"(%24, %28) : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %31 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<24x2>>, <interleaved>>, shape = #ttnn.shape<1x24x24x64>}> : (!ttnn.device) -> tensor<1x24x24x64xbf16, #ttnn_layout22>
        %32 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout22>
        %33 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %34 = "ttnn.reshape"(%32) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout22>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %35 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %36 = "ttnn.maximum"(%30, %34) : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %37 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<5x2>>, <interleaved>>, shape = #ttnn.shape<1x1x144x64>}> : (!ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout23>
        %38 = "ttnn.max_pool2d"(%36) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x144x64xbf16, #ttnn_layout23>
        %39 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<12x2>>, <interleaved>>, shape = #ttnn.shape<1x12x12x64>}> : (!ttnn.device) -> tensor<1x12x12x64xbf16, #ttnn_layout24>
        %40 = "ttnn.reshape"(%38) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout23>) -> tensor<1x12x12x64xbf16, #ttnn_layout24>
        %41 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<64x1>>, <interleaved>>, shape = #ttnn.shape<1x64x12x12>}> : (!ttnn.device) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %42 = "ttnn.permute"(%40) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout24>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %43 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x288>>, <interleaved>>, shape = #ttnn.shape<1x9216>}> : (!ttnn.device) -> tensor<1x9216xbf16, #ttnn_layout25>
        %44 = "ttnn.reshape"(%42) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout25>
        %45 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x288>>, <interleaved>>, shape = #ttnn.shape<1x9216>}> : (!ttnn.device) -> tensor<1x9216xf32, #ttnn_layout26>
        %46 = "ttnn.typecast"(%44) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout25>) -> tensor<1x9216xf32, #ttnn_layout26>
        %47 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %48 = "ttnn.matmul"(%46, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout26>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout27>
        %49 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xsi32, #ttnn_layout28>
        %50 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn_layout15>) -> tensor<1x1xsi32, #ttnn_layout28>
        %51 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %52 = "ttnn.typecast"(%50) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout28>) -> tensor<1x1xf32, #ttnn_layout29>
        %53 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %54 = "ttnn.repeat"(%52) <{repeat_dims = #ttnn.shape<1x128>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x128xf32, #ttnn_layout27>
        %55 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %56 = "ttnn.multiply"(%48, %54) : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %57 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %58 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout27>
        %59 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %60 = "ttnn.add"(%56, %58) : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %61 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %62 = "ttnn.typecast"(%60) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xbf16, #ttnn_layout10>
        %63 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %64 = "ttnn.maximum"(%62, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %65 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %66 = "ttnn.typecast"(%64) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout27>
        %67 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %68 = "ttnn.matmul"(%66, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout29>
        %69 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %70 = "ttnn.repeat"(%52) <{repeat_dims = #ttnn.shape<1x10>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %71 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %72 = "ttnn.multiply"(%68, %70) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %73 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %74 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout29>
        %75 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %76 = "ttnn.add"(%72, %74) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %77 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xf32, #ttnn_layout7>
        %78 = "ttnn.max"(%76) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1xf32, #ttnn_layout7>
        %79 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %80 = "ttnn.reshape"(%78) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout29>
        %81 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %82 = "ttnn.repeat"(%80) <{repeat_dims = #ttnn.shape<1x10>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %83 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %84 = "ttnn.subtract"(%76, %82) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %85 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %86 = "ttnn.exp"(%84) : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %87 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xf32, #ttnn_layout7>
        %88 = "ttnn.sum"(%86) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1xf32, #ttnn_layout7>
        %89 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %90 = "ttnn.reshape"(%88) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout29>
        %91 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %92 = "ttnn.log"(%90) : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %93 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %94 = "ttnn.repeat"(%92) <{repeat_dims = #ttnn.shape<1x10>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %95 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %96 = "ttnn.subtract"(%84, %94) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %97 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xbf16, #ttnn_layout12>
        %98 = "ttnn.typecast"(%96) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %98 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump Before TTNNWorkarounds (ttnn-workaround) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xsi32, #ttnn_layout15>
        %5 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<28x1>>, <interleaved>>, shape = #ttnn.shape<1x28x28x1>}> : (!ttnn.device) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %7 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<25x1>>, <interleaved>>, shape = #ttnn.shape<1x1x784x1>}> : (!ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %8 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %9 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %10 = "ttnn.conv2d"(%8, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %11 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<32x1>>, <interleaved>>, shape = #ttnn.shape<1x32x1x1>}> : (!ttnn.device) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %13 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1x1x32>}> : (!ttnn.device) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %14 = "ttnn.permute"(%12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %15 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %16 = "ttnn.add"(%10, %14) : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %17 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<26x1>>, <interleaved>>, shape = #ttnn.shape<1x26x26x32>}> : (!ttnn.device) -> tensor<1x26x26x32xbf16, #ttnn_layout19>
        %18 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout19>
        %19 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %20 = "ttnn.reshape"(%18) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout19>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %21 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %22 = "ttnn.maximum"(%16, %20) : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %23 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %24 = "ttnn.conv2d"(%22, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %25 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<64x1>>, <interleaved>>, shape = #ttnn.shape<1x64x1x1>}> : (!ttnn.device) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %26 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %27 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x2>>, <interleaved>>, shape = #ttnn.shape<1x1x1x64>}> : (!ttnn.device) -> tensor<1x1x1x64xbf16, #ttnn_layout21>
        %28 = "ttnn.permute"(%26) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout21>
        %29 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %30 = "ttnn.add"(%24, %28) : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %31 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<24x2>>, <interleaved>>, shape = #ttnn.shape<1x24x24x64>}> : (!ttnn.device) -> tensor<1x24x24x64xbf16, #ttnn_layout22>
        %32 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout22>
        %33 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %34 = "ttnn.reshape"(%32) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout22>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %35 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %36 = "ttnn.maximum"(%30, %34) : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %37 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<5x2>>, <interleaved>>, shape = #ttnn.shape<1x1x144x64>}> : (!ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout23>
        %38 = "ttnn.max_pool2d"(%36) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x144x64xbf16, #ttnn_layout23>
        %39 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<12x2>>, <interleaved>>, shape = #ttnn.shape<1x12x12x64>}> : (!ttnn.device) -> tensor<1x12x12x64xbf16, #ttnn_layout24>
        %40 = "ttnn.reshape"(%38) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout23>) -> tensor<1x12x12x64xbf16, #ttnn_layout24>
        %41 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<64x1>>, <interleaved>>, shape = #ttnn.shape<1x64x12x12>}> : (!ttnn.device) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %42 = "ttnn.permute"(%40) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout24>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %43 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x288>>, <interleaved>>, shape = #ttnn.shape<1x9216>}> : (!ttnn.device) -> tensor<1x9216xbf16, #ttnn_layout25>
        %44 = "ttnn.reshape"(%42) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout25>
        %45 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x288>>, <interleaved>>, shape = #ttnn.shape<1x9216>}> : (!ttnn.device) -> tensor<1x9216xf32, #ttnn_layout26>
        %46 = "ttnn.typecast"(%44) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout25>) -> tensor<1x9216xf32, #ttnn_layout26>
        %47 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %48 = "ttnn.matmul"(%46, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout26>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout27>
        %49 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xsi32, #ttnn_layout28>
        %50 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn_layout15>) -> tensor<1x1xsi32, #ttnn_layout28>
        %51 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %52 = "ttnn.typecast"(%50) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout28>) -> tensor<1x1xf32, #ttnn_layout29>
        %53 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %54 = "ttnn.repeat"(%52) <{repeat_dims = #ttnn.shape<1x128>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x128xf32, #ttnn_layout27>
        %55 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %56 = "ttnn.multiply"(%48, %54) : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %57 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %58 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout27>
        %59 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %60 = "ttnn.add"(%56, %58) : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %61 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %62 = "ttnn.typecast"(%60) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xbf16, #ttnn_layout10>
        %63 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %64 = "ttnn.maximum"(%62, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %65 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %66 = "ttnn.typecast"(%64) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout27>
        %67 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %68 = "ttnn.matmul"(%66, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout29>
        %69 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %70 = "ttnn.repeat"(%52) <{repeat_dims = #ttnn.shape<1x10>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %71 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %72 = "ttnn.multiply"(%68, %70) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %73 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %74 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout29>
        %75 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %76 = "ttnn.add"(%72, %74) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %77 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xf32, #ttnn_layout7>
        %78 = "ttnn.max"(%76) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1xf32, #ttnn_layout7>
        %79 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %80 = "ttnn.reshape"(%78) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout29>
        %81 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %82 = "ttnn.repeat"(%80) <{repeat_dims = #ttnn.shape<1x10>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %83 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %84 = "ttnn.subtract"(%76, %82) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %85 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %86 = "ttnn.exp"(%84) : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %87 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xf32, #ttnn_layout7>
        %88 = "ttnn.sum"(%86) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1xf32, #ttnn_layout7>
        %89 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %90 = "ttnn.reshape"(%88) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout29>
        %91 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %92 = "ttnn.log"(%90) : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %93 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %94 = "ttnn.repeat"(%92) <{repeat_dims = #ttnn.shape<1x10>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %95 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %96 = "ttnn.subtract"(%84, %94) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %97 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xbf16, #ttnn_layout12>
        %98 = "ttnn.typecast"(%96) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %98 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump After TTNNWorkarounds (ttnn-workaround) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.to_layout"(%4, %0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout15>, !ttnn.device) -> tensor<1xsi32, #ttnn_layout16>
        %6 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout17>
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        %8 = "ttnn.to_layout"(%7, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout19>
        %9 = "ttnn.conv2d"(%8, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %10 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %11 = "ttnn.permute"(%10) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %12 = "ttnn.add"(%9, %11) : (tensor<1x1x676x32xbf16, #ttnn_layout20>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %13 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout21>
        %14 = "ttnn.reshape"(%13) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %15 = "ttnn.maximum"(%12, %14) : (tensor<1x1x676x32xbf16, #ttnn_layout20>, tensor<1x1x676x32xbf16, #ttnn_layout20>) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %16 = "ttnn.to_layout"(%15, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout20>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout22>
        %17 = "ttnn.conv2d"(%16, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout22>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %18 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %19 = "ttnn.permute"(%18) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout24>
        %20 = "ttnn.add"(%17, %19) : (tensor<1x1x576x64xbf16, #ttnn_layout23>, tensor<1x1x1x64xbf16, #ttnn_layout24>) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %21 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout25>
        %22 = "ttnn.reshape"(%21) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout25>) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %23 = "ttnn.maximum"(%20, %22) : (tensor<1x1x576x64xbf16, #ttnn_layout23>, tensor<1x1x576x64xbf16, #ttnn_layout23>) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %24 = "ttnn.to_layout"(%23, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %25 = "ttnn.max_pool2d"(%24) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x144x64xbf16, #ttnn_layout27>
        %26 = "ttnn.to_layout"(%25, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<5x2>>, <interleaved>>}> : (tensor<1x1x144x64xbf16, #ttnn_layout27>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout28>
        %27 = "ttnn.reshape"(%26) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout28>) -> tensor<1x12x12x64xbf16, #ttnn_layout29>
        %28 = "ttnn.permute"(%27) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout29>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %29 = "ttnn.reshape"(%28) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout30>
        %30 = "ttnn.typecast"(%29) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout30>) -> tensor<1x9216xf32, #ttnn_layout31>
        %31 = "ttnn.matmul"(%30, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout31>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout32>
        %32 = "ttnn.to_layout"(%5, %0) <{dtype = #tt.supportedDataTypes<u32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xsi32, #ttnn_layout16>, !ttnn.device) -> tensor<1xui32, #ttnn_layout33>
        %33 = "ttnn.reshape"(%32) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout33>) -> tensor<1x1xui32, #ttnn_layout34>
        %34 = "ttnn.to_layout"(%33, %0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1x1xui32, #ttnn_layout34>, !ttnn.device) -> tensor<1x1xsi32, #ttnn_layout35>
        %35 = "ttnn.typecast"(%34) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout35>) -> tensor<1x1xf32, #ttnn_layout36>
        %36 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout32>
        %37 = "ttnn.add"(%35, %36) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xf32, #ttnn_layout32>
        %38 = "ttnn.multiply"(%31, %37) : (tensor<1x128xf32, #ttnn_layout32>, tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xf32, #ttnn_layout32>
        %39 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout32>
        %40 = "ttnn.add"(%38, %39) : (tensor<1x128xf32, #ttnn_layout32>, tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xf32, #ttnn_layout32>
        %41 = "ttnn.typecast"(%40) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xbf16, #ttnn_layout10>
        %42 = "ttnn.maximum"(%41, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %43 = "ttnn.typecast"(%42) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout32>
        %44 = "ttnn.matmul"(%43, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout32>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout36>
        %45 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout36>
        %46 = "ttnn.add"(%35, %45) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %47 = "ttnn.multiply"(%44, %46) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %48 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout36>
        %49 = "ttnn.add"(%47, %48) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %50 = "ttnn.max"(%49) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1xf32, #ttnn_layout7>
        %51 = "ttnn.reshape"(%50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout36>
        %52 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout36>
        %53 = "ttnn.add"(%51, %52) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %54 = "ttnn.subtract"(%49, %53) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %55 = "ttnn.exp"(%54) : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %56 = "ttnn.sum"(%55) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1xf32, #ttnn_layout7>
        %57 = "ttnn.reshape"(%56) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout36>
        %58 = "ttnn.log"(%57) : (tensor<1x1xf32, #ttnn_layout36>) -> tensor<1x1xf32, #ttnn_layout36>
        %59 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout36>
        %60 = "ttnn.add"(%58, %59) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %61 = "ttnn.subtract"(%54, %60) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %62 = "ttnn.typecast"(%61) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %62 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.to_layout"(%4, %0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout15>, !ttnn.device) -> tensor<1xsi32, #ttnn_layout16>
        %6 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout17>
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        %8 = "ttnn.to_layout"(%7, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout19>
        %9 = "ttnn.conv2d"(%8, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %10 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %11 = "ttnn.permute"(%10) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %12 = "ttnn.add"(%9, %11) : (tensor<1x1x676x32xbf16, #ttnn_layout20>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %13 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout21>
        %14 = "ttnn.reshape"(%13) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %15 = "ttnn.maximum"(%12, %14) : (tensor<1x1x676x32xbf16, #ttnn_layout20>, tensor<1x1x676x32xbf16, #ttnn_layout20>) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %16 = "ttnn.to_layout"(%15, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout20>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout22>
        %17 = "ttnn.conv2d"(%16, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout22>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %18 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %19 = "ttnn.permute"(%18) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout24>
        %20 = "ttnn.add"(%17, %19) : (tensor<1x1x576x64xbf16, #ttnn_layout23>, tensor<1x1x1x64xbf16, #ttnn_layout24>) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %21 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout25>
        %22 = "ttnn.reshape"(%21) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout25>) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %23 = "ttnn.maximum"(%20, %22) : (tensor<1x1x576x64xbf16, #ttnn_layout23>, tensor<1x1x576x64xbf16, #ttnn_layout23>) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %24 = "ttnn.to_layout"(%23, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %25 = "ttnn.max_pool2d"(%24) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x144x64xbf16, #ttnn_layout27>
        %26 = "ttnn.to_layout"(%25, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<5x2>>, <interleaved>>}> : (tensor<1x1x144x64xbf16, #ttnn_layout27>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout28>
        %27 = "ttnn.reshape"(%26) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout28>) -> tensor<1x12x12x64xbf16, #ttnn_layout29>
        %28 = "ttnn.permute"(%27) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout29>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %29 = "ttnn.reshape"(%28) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout30>
        %30 = "ttnn.typecast"(%29) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout30>) -> tensor<1x9216xf32, #ttnn_layout31>
        %31 = "ttnn.matmul"(%30, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout31>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout32>
        %32 = "ttnn.to_layout"(%5, %0) <{dtype = #tt.supportedDataTypes<u32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xsi32, #ttnn_layout16>, !ttnn.device) -> tensor<1xui32, #ttnn_layout33>
        %33 = "ttnn.reshape"(%32) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout33>) -> tensor<1x1xui32, #ttnn_layout34>
        %34 = "ttnn.to_layout"(%33, %0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1x1xui32, #ttnn_layout34>, !ttnn.device) -> tensor<1x1xsi32, #ttnn_layout35>
        %35 = "ttnn.typecast"(%34) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout35>) -> tensor<1x1xf32, #ttnn_layout36>
        %36 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout32>
        %37 = "ttnn.add"(%35, %36) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xf32, #ttnn_layout32>
        %38 = "ttnn.multiply"(%31, %37) : (tensor<1x128xf32, #ttnn_layout32>, tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xf32, #ttnn_layout32>
        %39 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout32>
        %40 = "ttnn.add"(%38, %39) : (tensor<1x128xf32, #ttnn_layout32>, tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xf32, #ttnn_layout32>
        %41 = "ttnn.typecast"(%40) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xbf16, #ttnn_layout10>
        %42 = "ttnn.maximum"(%41, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %43 = "ttnn.typecast"(%42) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout32>
        %44 = "ttnn.matmul"(%43, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout32>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout36>
        %45 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout36>
        %46 = "ttnn.add"(%35, %45) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %47 = "ttnn.multiply"(%44, %46) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %48 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout36>
        %49 = "ttnn.add"(%47, %48) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %50 = "ttnn.max"(%49) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1xf32, #ttnn_layout7>
        %51 = "ttnn.reshape"(%50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout36>
        %52 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout36>
        %53 = "ttnn.add"(%51, %52) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %54 = "ttnn.subtract"(%49, %53) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %55 = "ttnn.exp"(%54) : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %56 = "ttnn.sum"(%55) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1xf32, #ttnn_layout7>
        %57 = "ttnn.reshape"(%56) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout36>
        %58 = "ttnn.log"(%57) : (tensor<1x1xf32, #ttnn_layout36>) -> tensor<1x1xf32, #ttnn_layout36>
        %59 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout36>
        %60 = "ttnn.add"(%58, %59) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %61 = "ttnn.subtract"(%54, %60) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %62 = "ttnn.typecast"(%61) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %62 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %7 = "ttnn.to_layout"(%6, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        %8 = "ttnn.conv2d"(%7, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %9 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %10 = "ttnn.permute"(%9) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %11 = "ttnn.add"(%8, %10) : (tensor<1x1x676x32xbf16, #ttnn_layout19>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %12 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout20>
        %13 = "ttnn.reshape"(%12) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout20>) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %14 = "ttnn.maximum"(%11, %13) : (tensor<1x1x676x32xbf16, #ttnn_layout19>, tensor<1x1x676x32xbf16, #ttnn_layout19>) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %15 = "ttnn.to_layout"(%14, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %16 = "ttnn.conv2d"(%15, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %17 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %18 = "ttnn.permute"(%17) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout23>
        %19 = "ttnn.add"(%16, %18) : (tensor<1x1x576x64xbf16, #ttnn_layout22>, tensor<1x1x1x64xbf16, #ttnn_layout23>) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %20 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout24>
        %21 = "ttnn.reshape"(%20) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout24>) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %22 = "ttnn.maximum"(%19, %21) : (tensor<1x1x576x64xbf16, #ttnn_layout22>, tensor<1x1x576x64xbf16, #ttnn_layout22>) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %23 = "ttnn.to_layout"(%22, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout25>
        %24 = "ttnn.max_pool2d"(%23) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout25>) -> tensor<1x1x144x64xbf16, #ttnn_layout26>
        %25 = "ttnn.to_layout"(%24, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<5x2>>, <interleaved>>}> : (tensor<1x1x144x64xbf16, #ttnn_layout26>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout27>
        %26 = "ttnn.reshape"(%25) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout27>) -> tensor<1x12x12x64xbf16, #ttnn_layout28>
        %27 = "ttnn.permute"(%26) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout28>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %28 = "ttnn.reshape"(%27) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout29>
        %29 = "ttnn.typecast"(%28) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout29>) -> tensor<1x9216xf32, #ttnn_layout30>
        %30 = "ttnn.matmul"(%29, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout30>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout31>
        %31 = "ttnn.to_layout"(%4, %0) <{dtype = #tt.supportedDataTypes<u32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout15>, !ttnn.device) -> tensor<1xui32, #ttnn_layout32>
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout32>) -> tensor<1x1xui32, #ttnn_layout33>
        %33 = "ttnn.to_layout"(%32, %0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1x1xui32, #ttnn_layout33>, !ttnn.device) -> tensor<1x1xsi32, #ttnn_layout34>
        %34 = "ttnn.typecast"(%33) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout34>) -> tensor<1x1xf32, #ttnn_layout35>
        %35 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout31>
        %36 = "ttnn.add"(%34, %35) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xf32, #ttnn_layout31>
        %37 = "ttnn.multiply"(%30, %36) : (tensor<1x128xf32, #ttnn_layout31>, tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xf32, #ttnn_layout31>
        %38 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout31>
        %39 = "ttnn.add"(%37, %38) : (tensor<1x128xf32, #ttnn_layout31>, tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xf32, #ttnn_layout31>
        %40 = "ttnn.typecast"(%39) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xbf16, #ttnn_layout10>
        %41 = "ttnn.maximum"(%40, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %42 = "ttnn.typecast"(%41) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout31>
        %43 = "ttnn.matmul"(%42, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout31>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout35>
        %44 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout35>
        %45 = "ttnn.add"(%34, %44) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %46 = "ttnn.multiply"(%43, %45) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %47 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout35>
        %48 = "ttnn.add"(%46, %47) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %49 = "ttnn.max"(%48) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1xf32, #ttnn_layout7>
        %50 = "ttnn.reshape"(%49) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout35>
        %51 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout35>
        %52 = "ttnn.add"(%50, %51) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %53 = "ttnn.subtract"(%48, %52) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %54 = "ttnn.exp"(%53) : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %55 = "ttnn.sum"(%54) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1xf32, #ttnn_layout7>
        %56 = "ttnn.reshape"(%55) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout35>
        %57 = "ttnn.log"(%56) : (tensor<1x1xf32, #ttnn_layout35>) -> tensor<1x1xf32, #ttnn_layout35>
        %58 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout35>
        %59 = "ttnn.add"(%57, %58) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %60 = "ttnn.subtract"(%53, %59) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %61 = "ttnn.typecast"(%60) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %61 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump Before TTNNDecomposeLayouts (ttnn-decompose-layouts) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %7 = "ttnn.to_layout"(%6, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        %8 = "ttnn.conv2d"(%7, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %9 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %10 = "ttnn.permute"(%9) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %11 = "ttnn.add"(%8, %10) : (tensor<1x1x676x32xbf16, #ttnn_layout19>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %12 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout20>
        %13 = "ttnn.reshape"(%12) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout20>) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %14 = "ttnn.maximum"(%11, %13) : (tensor<1x1x676x32xbf16, #ttnn_layout19>, tensor<1x1x676x32xbf16, #ttnn_layout19>) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %15 = "ttnn.to_layout"(%14, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %16 = "ttnn.conv2d"(%15, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %17 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %18 = "ttnn.permute"(%17) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout23>
        %19 = "ttnn.add"(%16, %18) : (tensor<1x1x576x64xbf16, #ttnn_layout22>, tensor<1x1x1x64xbf16, #ttnn_layout23>) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %20 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout24>
        %21 = "ttnn.reshape"(%20) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout24>) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %22 = "ttnn.maximum"(%19, %21) : (tensor<1x1x576x64xbf16, #ttnn_layout22>, tensor<1x1x576x64xbf16, #ttnn_layout22>) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %23 = "ttnn.to_layout"(%22, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout25>
        %24 = "ttnn.max_pool2d"(%23) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout25>) -> tensor<1x1x144x64xbf16, #ttnn_layout26>
        %25 = "ttnn.to_layout"(%24, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<5x2>>, <interleaved>>}> : (tensor<1x1x144x64xbf16, #ttnn_layout26>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout27>
        %26 = "ttnn.reshape"(%25) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout27>) -> tensor<1x12x12x64xbf16, #ttnn_layout28>
        %27 = "ttnn.permute"(%26) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout28>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %28 = "ttnn.reshape"(%27) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout29>
        %29 = "ttnn.typecast"(%28) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout29>) -> tensor<1x9216xf32, #ttnn_layout30>
        %30 = "ttnn.matmul"(%29, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout30>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout31>
        %31 = "ttnn.to_layout"(%4, %0) <{dtype = #tt.supportedDataTypes<u32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout15>, !ttnn.device) -> tensor<1xui32, #ttnn_layout32>
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout32>) -> tensor<1x1xui32, #ttnn_layout33>
        %33 = "ttnn.to_layout"(%32, %0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1x1xui32, #ttnn_layout33>, !ttnn.device) -> tensor<1x1xsi32, #ttnn_layout34>
        %34 = "ttnn.typecast"(%33) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout34>) -> tensor<1x1xf32, #ttnn_layout35>
        %35 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout31>
        %36 = "ttnn.add"(%34, %35) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xf32, #ttnn_layout31>
        %37 = "ttnn.multiply"(%30, %36) : (tensor<1x128xf32, #ttnn_layout31>, tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xf32, #ttnn_layout31>
        %38 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout31>
        %39 = "ttnn.add"(%37, %38) : (tensor<1x128xf32, #ttnn_layout31>, tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xf32, #ttnn_layout31>
        %40 = "ttnn.typecast"(%39) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xbf16, #ttnn_layout10>
        %41 = "ttnn.maximum"(%40, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %42 = "ttnn.typecast"(%41) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout31>
        %43 = "ttnn.matmul"(%42, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout31>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout35>
        %44 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout35>
        %45 = "ttnn.add"(%34, %44) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %46 = "ttnn.multiply"(%43, %45) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %47 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout35>
        %48 = "ttnn.add"(%46, %47) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %49 = "ttnn.max"(%48) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1xf32, #ttnn_layout7>
        %50 = "ttnn.reshape"(%49) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout35>
        %51 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout35>
        %52 = "ttnn.add"(%50, %51) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %53 = "ttnn.subtract"(%48, %52) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %54 = "ttnn.exp"(%53) : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %55 = "ttnn.sum"(%54) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1xf32, #ttnn_layout7>
        %56 = "ttnn.reshape"(%55) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout35>
        %57 = "ttnn.log"(%56) : (tensor<1x1xf32, #ttnn_layout35>) -> tensor<1x1xf32, #ttnn_layout35>
        %58 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout35>
        %59 = "ttnn.add"(%57, %58) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %60 = "ttnn.subtract"(%53, %59) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %61 = "ttnn.typecast"(%60) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %61 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump After TTNNDecomposeLayouts (ttnn-decompose-layouts) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #system_memory>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #system_memory>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #system_memory>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #system_memory>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #system_memory>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %7 = "ttnn.from_device"(%6) : (tensor<1x1x784x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        %8 = "ttnn.to_layout"(%7) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>) -> tensor<1x1x784x1xbf16, #ttnn_layout19>
        %9 = "ttnn.to_device"(%8, %0) <{memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout20>
        %10 = "ttnn.conv2d"(%9, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout20>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %11 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %12 = "ttnn.permute"(%11) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %13 = "ttnn.add"(%10, %12) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %14 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout22>
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout22>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %16 = "ttnn.maximum"(%13, %15) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %17 = "ttnn.from_device"(%16) : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout23>
        %18 = "ttnn.to_layout"(%17) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x676x32xbf16, #ttnn_layout23>) -> tensor<1x1x676x32xbf16, #ttnn_layout24>
        %19 = "ttnn.to_device"(%18, %0) <{memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout24>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout25>
        %20 = "ttnn.conv2d"(%19, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout25>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %21 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %22 = "ttnn.permute"(%21) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout27>
        %23 = "ttnn.add"(%20, %22) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x1x64xbf16, #ttnn_layout27>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %24 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout28>
        %25 = "ttnn.reshape"(%24) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout28>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %26 = "ttnn.maximum"(%23, %25) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %27 = "ttnn.from_device"(%26) : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout29>
        %28 = "ttnn.to_layout"(%27) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x576x64xbf16, #ttnn_layout29>) -> tensor<1x1x576x64xbf16, #ttnn_layout30>
        %29 = "ttnn.to_device"(%28, %0) <{memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout31>
        %30 = "ttnn.max_pool2d"(%29) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout31>) -> tensor<1x1x144x64xbf16, #ttnn_layout32>
        %31 = "ttnn.to_layout"(%30, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1x1x144x64xbf16, #ttnn_layout32>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout33>
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout33>) -> tensor<1x12x12x64xbf16, #ttnn_layout34>
        %33 = "ttnn.permute"(%32) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout34>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %34 = "ttnn.reshape"(%33) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout35>
        %35 = "ttnn.typecast"(%34) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout35>) -> tensor<1x9216xf32, #ttnn_layout36>
        %36 = "ttnn.matmul"(%35, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout36>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout37>
        %37 = "ttnn.from_device"(%4) : (tensor<1xui32, #ttnn_layout15>) -> tensor<1xui32, #ttnn_layout38>
        %38 = "ttnn.to_layout"(%37) <{layout = #ttnn.layout<tile>}> : (tensor<1xui32, #ttnn_layout38>) -> tensor<1xui32, #ttnn_layout39>
        %39 = "ttnn.to_device"(%38, %0) <{memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout39>, !ttnn.device) -> tensor<1xui32, #ttnn_layout40>
        %40 = "ttnn.reshape"(%39) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout40>) -> tensor<1x1xui32, #ttnn_layout41>
        %41 = "ttnn.typecast"(%40) <{dtype = #tt.supportedDataTypes<si32>}> : (tensor<1x1xui32, #ttnn_layout41>) -> tensor<1x1xsi32, #ttnn_layout42>
        %42 = "ttnn.typecast"(%41) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout42>) -> tensor<1x1xf32, #ttnn_layout43>
        %43 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout37>
        %44 = "ttnn.add"(%42, %43) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        %45 = "ttnn.multiply"(%36, %44) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        %46 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout37>
        %47 = "ttnn.add"(%45, %46) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        %48 = "ttnn.typecast"(%47) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xbf16, #ttnn_layout10>
        %49 = "ttnn.maximum"(%48, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %50 = "ttnn.typecast"(%49) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout37>
        %51 = "ttnn.matmul"(%50, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout37>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout43>
        %52 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %53 = "ttnn.add"(%42, %52) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %54 = "ttnn.multiply"(%51, %53) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %55 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout43>
        %56 = "ttnn.add"(%54, %55) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %57 = "ttnn.max"(%56) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7>
        %58 = "ttnn.reshape"(%57) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43>
        %59 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %60 = "ttnn.add"(%58, %59) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %61 = "ttnn.subtract"(%56, %60) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %62 = "ttnn.exp"(%61) : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %63 = "ttnn.sum"(%62) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7>
        %64 = "ttnn.reshape"(%63) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43>
        %65 = "ttnn.log"(%64) : (tensor<1x1xf32, #ttnn_layout43>) -> tensor<1x1xf32, #ttnn_layout43>
        %66 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %67 = "ttnn.add"(%65, %66) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %68 = "ttnn.subtract"(%61, %67) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %69 = "ttnn.typecast"(%68) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %69 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump Before TTNNDeallocate (ttnn-deallocate) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #system_memory>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #system_memory>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #system_memory>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #system_memory>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #system_memory>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %7 = "ttnn.from_device"(%6) : (tensor<1x1x784x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        %8 = "ttnn.to_layout"(%7) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>) -> tensor<1x1x784x1xbf16, #ttnn_layout19>
        %9 = "ttnn.to_device"(%8, %0) <{memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout20>
        %10 = "ttnn.conv2d"(%9, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout20>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %11 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %12 = "ttnn.permute"(%11) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %13 = "ttnn.add"(%10, %12) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %14 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout22>
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout22>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %16 = "ttnn.maximum"(%13, %15) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %17 = "ttnn.from_device"(%16) : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout23>
        %18 = "ttnn.to_layout"(%17) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x676x32xbf16, #ttnn_layout23>) -> tensor<1x1x676x32xbf16, #ttnn_layout24>
        %19 = "ttnn.to_device"(%18, %0) <{memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout24>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout25>
        %20 = "ttnn.conv2d"(%19, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout25>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %21 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %22 = "ttnn.permute"(%21) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout27>
        %23 = "ttnn.add"(%20, %22) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x1x64xbf16, #ttnn_layout27>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %24 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout28>
        %25 = "ttnn.reshape"(%24) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout28>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %26 = "ttnn.maximum"(%23, %25) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %27 = "ttnn.from_device"(%26) : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout29>
        %28 = "ttnn.to_layout"(%27) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x576x64xbf16, #ttnn_layout29>) -> tensor<1x1x576x64xbf16, #ttnn_layout30>
        %29 = "ttnn.to_device"(%28, %0) <{memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout31>
        %30 = "ttnn.max_pool2d"(%29) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout31>) -> tensor<1x1x144x64xbf16, #ttnn_layout32>
        %31 = "ttnn.to_layout"(%30, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1x1x144x64xbf16, #ttnn_layout32>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout33>
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout33>) -> tensor<1x12x12x64xbf16, #ttnn_layout34>
        %33 = "ttnn.permute"(%32) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout34>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %34 = "ttnn.reshape"(%33) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout35>
        %35 = "ttnn.typecast"(%34) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout35>) -> tensor<1x9216xf32, #ttnn_layout36>
        %36 = "ttnn.matmul"(%35, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout36>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout37>
        %37 = "ttnn.from_device"(%4) : (tensor<1xui32, #ttnn_layout15>) -> tensor<1xui32, #ttnn_layout38>
        %38 = "ttnn.to_layout"(%37) <{layout = #ttnn.layout<tile>}> : (tensor<1xui32, #ttnn_layout38>) -> tensor<1xui32, #ttnn_layout39>
        %39 = "ttnn.to_device"(%38, %0) <{memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout39>, !ttnn.device) -> tensor<1xui32, #ttnn_layout40>
        %40 = "ttnn.reshape"(%39) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout40>) -> tensor<1x1xui32, #ttnn_layout41>
        %41 = "ttnn.typecast"(%40) <{dtype = #tt.supportedDataTypes<si32>}> : (tensor<1x1xui32, #ttnn_layout41>) -> tensor<1x1xsi32, #ttnn_layout42>
        %42 = "ttnn.typecast"(%41) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout42>) -> tensor<1x1xf32, #ttnn_layout43>
        %43 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout37>
        %44 = "ttnn.add"(%42, %43) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        %45 = "ttnn.multiply"(%36, %44) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        %46 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout37>
        %47 = "ttnn.add"(%45, %46) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        %48 = "ttnn.typecast"(%47) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xbf16, #ttnn_layout10>
        %49 = "ttnn.maximum"(%48, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %50 = "ttnn.typecast"(%49) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout37>
        %51 = "ttnn.matmul"(%50, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout37>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout43>
        %52 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %53 = "ttnn.add"(%42, %52) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %54 = "ttnn.multiply"(%51, %53) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %55 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout43>
        %56 = "ttnn.add"(%54, %55) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %57 = "ttnn.max"(%56) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7>
        %58 = "ttnn.reshape"(%57) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43>
        %59 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %60 = "ttnn.add"(%58, %59) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %61 = "ttnn.subtract"(%56, %60) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %62 = "ttnn.exp"(%61) : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %63 = "ttnn.sum"(%62) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7>
        %64 = "ttnn.reshape"(%63) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43>
        %65 = "ttnn.log"(%64) : (tensor<1x1xf32, #ttnn_layout43>) -> tensor<1x1xf32, #ttnn_layout43>
        %66 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %67 = "ttnn.add"(%65, %66) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %68 = "ttnn.subtract"(%61, %67) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %69 = "ttnn.typecast"(%68) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %69 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump After TTNNDeallocate (ttnn-deallocate) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #system_memory>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #system_memory>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #system_memory>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #system_memory>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #system_memory>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<10xbf16, #ttnn_layout1>) -> ()
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<10x128xbf16, #ttnn_layout10>) -> ()
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<128xbf16, #ttnn_layout9>) -> ()
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<128x9216xbf16, #ttnn_layout8>) -> ()
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> ()
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> ()
        %7 = "ttnn.from_device"(%6) : (tensor<1x1x784x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>) -> ()
        %8 = "ttnn.to_layout"(%7) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>) -> tensor<1x1x784x1xbf16, #ttnn_layout19>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>) -> ()
        %9 = "ttnn.to_device"(%8, %0) <{memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout20>
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>) -> ()
        %10 = "ttnn.conv2d"(%9, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout20>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout20>) -> ()
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<32x1x3x3xbf16, #ttnn_layout>) -> ()
        %11 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<32xbf16, #ttnn_layout1>) -> ()
        %12 = "ttnn.permute"(%11) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> ()
        %13 = "ttnn.add"(%10, %12) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x32xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> ()
        %14 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout22>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> ()
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout22>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x26x26x32xbf16, #ttnn_layout22>) -> ()
        %16 = "ttnn.maximum"(%13, %15) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> ()
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> ()
        %17 = "ttnn.from_device"(%16) : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout23>
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> ()
        %18 = "ttnn.to_layout"(%17) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x676x32xbf16, #ttnn_layout23>) -> tensor<1x1x676x32xbf16, #ttnn_layout24>
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout23>) -> ()
        %19 = "ttnn.to_device"(%18, %0) <{memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout24>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout25>
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout24>) -> ()
        %20 = "ttnn.conv2d"(%19, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout25>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout25>) -> ()
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<64x32x3x3xbf16, #ttnn_layout2>) -> ()
        %21 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<64xbf16, #ttnn_layout3>) -> ()
        %22 = "ttnn.permute"(%21) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout27>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> ()
        %23 = "ttnn.add"(%20, %22) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x1x64xbf16, #ttnn_layout27>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout27>) -> ()
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> ()
        %24 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout28>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> ()
        %25 = "ttnn.reshape"(%24) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout28>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x24x24x64xbf16, #ttnn_layout28>) -> ()
        %26 = "ttnn.maximum"(%23, %25) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> ()
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> ()
        %27 = "ttnn.from_device"(%26) : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout29>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> ()
        %28 = "ttnn.to_layout"(%27) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x576x64xbf16, #ttnn_layout29>) -> tensor<1x1x576x64xbf16, #ttnn_layout30>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout29>) -> ()
        %29 = "ttnn.to_device"(%28, %0) <{memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout31>
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout30>) -> ()
        %30 = "ttnn.max_pool2d"(%29) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout31>) -> tensor<1x1x144x64xbf16, #ttnn_layout32>
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout31>) -> ()
        %31 = "ttnn.to_layout"(%30, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1x1x144x64xbf16, #ttnn_layout32>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout33>
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x1x144x64xbf16, #ttnn_layout32>) -> ()
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout33>) -> tensor<1x12x12x64xbf16, #ttnn_layout34>
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x1x144x64xbf16, #ttnn_layout33>) -> ()
        %33 = "ttnn.permute"(%32) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout34>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x12x12x64xbf16, #ttnn_layout34>) -> ()
        %34 = "ttnn.reshape"(%33) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout35>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> ()
        %35 = "ttnn.typecast"(%34) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout35>) -> tensor<1x9216xf32, #ttnn_layout36>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x9216xbf16, #ttnn_layout35>) -> ()
        %36 = "ttnn.matmul"(%35, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout36>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout37>
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x9216xf32, #ttnn_layout36>) -> ()
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<9216x128xf32, #ttnn_layout4>) -> ()
        %37 = "ttnn.from_device"(%4) : (tensor<1xui32, #ttnn_layout15>) -> tensor<1xui32, #ttnn_layout38>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1xui32, #ttnn_layout15>) -> ()
        %38 = "ttnn.to_layout"(%37) <{layout = #ttnn.layout<tile>}> : (tensor<1xui32, #ttnn_layout38>) -> tensor<1xui32, #ttnn_layout39>
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1xui32, #ttnn_layout38>) -> ()
        %39 = "ttnn.to_device"(%38, %0) <{memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout39>, !ttnn.device) -> tensor<1xui32, #ttnn_layout40>
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1xui32, #ttnn_layout39>) -> ()
        %40 = "ttnn.reshape"(%39) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout40>) -> tensor<1x1xui32, #ttnn_layout41>
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1xui32, #ttnn_layout40>) -> ()
        %41 = "ttnn.typecast"(%40) <{dtype = #tt.supportedDataTypes<si32>}> : (tensor<1x1xui32, #ttnn_layout41>) -> tensor<1x1xsi32, #ttnn_layout42>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x1xui32, #ttnn_layout41>) -> ()
        %42 = "ttnn.typecast"(%41) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout42>) -> tensor<1x1xf32, #ttnn_layout43>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x1xsi32, #ttnn_layout42>) -> ()
        %43 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout37>
        %44 = "ttnn.add"(%42, %43) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        %45 = "ttnn.multiply"(%36, %44) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        %46 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout37>
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<128xf32, #ttnn_layout5>) -> ()
        %47 = "ttnn.add"(%45, %46) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        %48 = "ttnn.typecast"(%47) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xbf16, #ttnn_layout10>
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        %49 = "ttnn.maximum"(%48, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x128xbf16, #ttnn_layout10>) -> ()
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x128xbf16, #ttnn_layout10>) -> ()
        %50 = "ttnn.typecast"(%49) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout37>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x128xbf16, #ttnn_layout10>) -> ()
        %51 = "ttnn.matmul"(%50, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout37>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<128x10xf32, #ttnn_layout6>) -> ()
        %52 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %53 = "ttnn.add"(%42, %52) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> ()
        %54 = "ttnn.multiply"(%51, %53) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        %55 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<10xf32, #ttnn_layout7>) -> ()
        %56 = "ttnn.add"(%54, %55) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        %57 = "ttnn.max"(%56) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7>
        %58 = "ttnn.reshape"(%57) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1xf32, #ttnn_layout7>) -> ()
        %59 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %60 = "ttnn.add"(%58, %59) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> ()
        %61 = "ttnn.subtract"(%56, %60) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        %62 = "ttnn.exp"(%61) : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %63 = "ttnn.sum"(%62) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7>
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        %64 = "ttnn.reshape"(%63) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43>
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1xf32, #ttnn_layout7>) -> ()
        %65 = "ttnn.log"(%64) : (tensor<1x1xf32, #ttnn_layout43>) -> tensor<1x1xf32, #ttnn_layout43>
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> ()
        %66 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %67 = "ttnn.add"(%65, %66) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> ()
        %68 = "ttnn.subtract"(%61, %67) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        %69 = "ttnn.typecast"(%68) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xbf16, #ttnn_layout12>
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        return %69 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


TTNN module module
#dram = #ttnn.buffer_type<dram>
#loc1 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":31:0)
#loc2 = loc("convolution")
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [1], [0 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#loc34 = loc(fused[#loc1, #loc2])
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #system_memory>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #system_memory>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #system_memory>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #system_memory>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #system_memory>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]> loc(#loc)
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout> loc(fused[#loc1, #loc2]), %arg1: tensor<32xbf16, #ttnn_layout1> loc(fused[#loc1, #loc2]), %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2> loc(fused[#loc1, #loc2]), %arg3: tensor<64xbf16, #ttnn_layout3> loc(fused[#loc1, #loc2]), %arg4: tensor<9216x128xf32, #ttnn_layout4> loc(fused[#loc1, #loc2]), %arg5: tensor<128xf32, #ttnn_layout5> loc(fused[#loc1, #loc2]), %arg6: tensor<128x10xf32, #ttnn_layout6> loc(fused[#loc1, #loc2]), %arg7: tensor<10xf32, #ttnn_layout7> loc(fused[#loc1, #loc2]), %arg8: tensor<128x9216xbf16, #ttnn_layout8> loc(fused[#loc1, #loc2]), %arg9: tensor<128xbf16, #ttnn_layout9> loc(fused[#loc1, #loc2]), %arg10: tensor<10x128xbf16, #ttnn_layout10> loc(fused[#loc1, #loc2]), %arg11: tensor<10xbf16, #ttnn_layout1> loc(fused[#loc1, #loc2]), %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11> loc(fused[#loc1, #loc2])) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<10xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<10x128xbf16, #ttnn_layout10>) -> () loc(#loc)
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<128xbf16, #ttnn_layout9>) -> () loc(#loc)
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<128x9216xbf16, #ttnn_layout8>) -> () loc(#loc)
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14> loc(#loc)
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10> loc(#loc)
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15> loc(#loc)
        %5 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16> loc(#loc34)
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> () loc(#loc34)
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17> loc(#loc34)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> () loc(#loc34)
        %7 = "ttnn.from_device"(%6) : (tensor<1x1x784x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout18> loc(#loc34)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>) -> () loc(#loc34)
        %8 = "ttnn.to_layout"(%7) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>) -> tensor<1x1x784x1xbf16, #ttnn_layout19> loc(#loc34)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>) -> () loc(#loc34)
        %9 = "ttnn.to_device"(%8, %0) <{memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout20> loc(#loc34)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>) -> () loc(#loc34)
        %10 = "ttnn.conv2d"(%9, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout20>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout21> loc(#loc34)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout20>) -> () loc(#loc34)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<32x1x3x3xbf16, #ttnn_layout>) -> () loc(#loc34)
        %11 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13> loc(#loc34)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<32xbf16, #ttnn_layout1>) -> () loc(#loc34)
        %12 = "ttnn.permute"(%11) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11> loc(#loc34)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> () loc(#loc34)
        %13 = "ttnn.add"(%10, %12) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout21> loc(#loc34)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x32xbf16, #ttnn_layout11>) -> () loc(#loc34)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> () loc(#loc34)
        %14 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout22> loc(#loc35)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> () loc(#loc35)
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout22>) -> tensor<1x1x676x32xbf16, #ttnn_layout21> loc(#loc35)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x26x26x32xbf16, #ttnn_layout22>) -> () loc(#loc35)
        %16 = "ttnn.maximum"(%13, %15) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout21> loc(#loc35)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> () loc(#loc35)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> () loc(#loc35)
        %17 = "ttnn.from_device"(%16) : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout23> loc(#loc36)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> () loc(#loc36)
        %18 = "ttnn.to_layout"(%17) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x676x32xbf16, #ttnn_layout23>) -> tensor<1x1x676x32xbf16, #ttnn_layout24> loc(#loc36)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout23>) -> () loc(#loc36)
        %19 = "ttnn.to_device"(%18, %0) <{memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout24>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout25> loc(#loc36)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout24>) -> () loc(#loc36)
        %20 = "ttnn.conv2d"(%19, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout25>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout26> loc(#loc36)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout25>) -> () loc(#loc36)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<64x32x3x3xbf16, #ttnn_layout2>) -> () loc(#loc36)
        %21 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14> loc(#loc36)
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<64xbf16, #ttnn_layout3>) -> () loc(#loc36)
        %22 = "ttnn.permute"(%21) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout27> loc(#loc36)
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> () loc(#loc36)
        %23 = "ttnn.add"(%20, %22) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x1x64xbf16, #ttnn_layout27>) -> tensor<1x1x576x64xbf16, #ttnn_layout26> loc(#loc36)
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout27>) -> () loc(#loc36)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> () loc(#loc36)
        %24 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout28> loc(#loc37)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> () loc(#loc37)
        %25 = "ttnn.reshape"(%24) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout28>) -> tensor<1x1x576x64xbf16, #ttnn_layout26> loc(#loc37)
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x24x24x64xbf16, #ttnn_layout28>) -> () loc(#loc37)
        %26 = "ttnn.maximum"(%23, %25) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout26> loc(#loc37)
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> () loc(#loc37)
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> () loc(#loc37)
        %27 = "ttnn.from_device"(%26) : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout29> loc(#loc38)
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> () loc(#loc38)
        %28 = "ttnn.to_layout"(%27) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x576x64xbf16, #ttnn_layout29>) -> tensor<1x1x576x64xbf16, #ttnn_layout30> loc(#loc38)
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout29>) -> () loc(#loc38)
        %29 = "ttnn.to_device"(%28, %0) <{memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout31> loc(#loc38)
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout30>) -> () loc(#loc38)
        %30 = "ttnn.max_pool2d"(%29) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout31>) -> tensor<1x1x144x64xbf16, #ttnn_layout32> loc(#loc38)
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout31>) -> () loc(#loc38)
        %31 = "ttnn.to_layout"(%30, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1x1x144x64xbf16, #ttnn_layout32>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout33> loc(#loc38)
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x1x144x64xbf16, #ttnn_layout32>) -> () loc(#loc38)
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout33>) -> tensor<1x12x12x64xbf16, #ttnn_layout34> loc(#loc38)
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x1x144x64xbf16, #ttnn_layout33>) -> () loc(#loc38)
        %33 = "ttnn.permute"(%32) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout34>) -> tensor<1x64x12x12xbf16, #ttnn_layout14> loc(#loc38)
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x12x12x64xbf16, #ttnn_layout34>) -> () loc(#loc38)
        %34 = "ttnn.reshape"(%33) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout35> loc(#loc39)
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> () loc(#loc39)
        %35 = "ttnn.typecast"(%34) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout35>) -> tensor<1x9216xf32, #ttnn_layout36> loc(#loc40)
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x9216xbf16, #ttnn_layout35>) -> () loc(#loc40)
        %36 = "ttnn.matmul"(%35, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout36>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc41)
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x9216xf32, #ttnn_layout36>) -> () loc(#loc41)
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<9216x128xf32, #ttnn_layout4>) -> () loc(#loc41)
        %37 = "ttnn.from_device"(%4) : (tensor<1xui32, #ttnn_layout15>) -> tensor<1xui32, #ttnn_layout38> loc(#loc)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1xui32, #ttnn_layout15>) -> () loc(#loc)
        %38 = "ttnn.to_layout"(%37) <{layout = #ttnn.layout<tile>}> : (tensor<1xui32, #ttnn_layout38>) -> tensor<1xui32, #ttnn_layout39> loc(#loc)
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1xui32, #ttnn_layout38>) -> () loc(#loc)
        %39 = "ttnn.to_device"(%38, %0) <{memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout39>, !ttnn.device) -> tensor<1xui32, #ttnn_layout40> loc(#loc)
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1xui32, #ttnn_layout39>) -> () loc(#loc)
        %40 = "ttnn.reshape"(%39) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout40>) -> tensor<1x1xui32, #ttnn_layout41> loc(#loc42)
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1xui32, #ttnn_layout40>) -> () loc(#loc42)
        %41 = "ttnn.typecast"(%40) <{dtype = #tt.supportedDataTypes<si32>}> : (tensor<1x1xui32, #ttnn_layout41>) -> tensor<1x1xsi32, #ttnn_layout42> loc(#loc42)
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x1xui32, #ttnn_layout41>) -> () loc(#loc42)
        %42 = "ttnn.typecast"(%41) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout42>) -> tensor<1x1xf32, #ttnn_layout43> loc(#loc42)
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x1xsi32, #ttnn_layout42>) -> () loc(#loc42)
        %43 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc42)
        %44 = "ttnn.add"(%42, %43) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc42)
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc42)
        %45 = "ttnn.multiply"(%36, %44) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc42)
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc42)
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc42)
        %46 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc43)
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<128xf32, #ttnn_layout5>) -> () loc(#loc43)
        %47 = "ttnn.add"(%45, %46) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc43)
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc43)
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc43)
        %48 = "ttnn.typecast"(%47) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xbf16, #ttnn_layout10> loc(#loc44)
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc44)
        %49 = "ttnn.maximum"(%48, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10> loc(#loc45)
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x128xbf16, #ttnn_layout10>) -> () loc(#loc45)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x128xbf16, #ttnn_layout10>) -> () loc(#loc45)
        %50 = "ttnn.typecast"(%49) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc46)
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x128xbf16, #ttnn_layout10>) -> () loc(#loc46)
        %51 = "ttnn.matmul"(%50, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout37>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc47)
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc47)
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<128x10xf32, #ttnn_layout6>) -> () loc(#loc47)
        %52 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc48)
        %53 = "ttnn.add"(%42, %52) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc48)
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc48)
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> () loc(#loc48)
        %54 = "ttnn.multiply"(%51, %53) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc48)
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc48)
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc48)
        %55 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc49)
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<10xf32, #ttnn_layout7>) -> () loc(#loc49)
        %56 = "ttnn.add"(%54, %55) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc49)
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc49)
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc49)
        %57 = "ttnn.max"(%56) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7> loc(#loc50)
        %58 = "ttnn.reshape"(%57) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43> loc(#loc50)
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1xf32, #ttnn_layout7>) -> () loc(#loc50)
        %59 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc51)
        %60 = "ttnn.add"(%58, %59) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc51)
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc51)
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> () loc(#loc51)
        %61 = "ttnn.subtract"(%56, %60) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc51)
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc51)
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc51)
        %62 = "ttnn.exp"(%61) : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc52)
        %63 = "ttnn.sum"(%62) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7> loc(#loc53)
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc53)
        %64 = "ttnn.reshape"(%63) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43> loc(#loc53)
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1xf32, #ttnn_layout7>) -> () loc(#loc53)
        %65 = "ttnn.log"(%64) : (tensor<1x1xf32, #ttnn_layout43>) -> tensor<1x1xf32, #ttnn_layout43> loc(#loc54)
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> () loc(#loc54)
        %66 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc55)
        %67 = "ttnn.add"(%65, %66) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc55)
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc55)
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> () loc(#loc55)
        %68 = "ttnn.subtract"(%61, %67) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc55)
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc55)
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc55)
        %69 = "ttnn.typecast"(%68) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xbf16, #ttnn_layout12> loc(#loc56)
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc56)
        return %69 : tensor<1x10xbf16, #ttnn_layout12> loc(#loc34)
      } loc(#loc34)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":32:0)
#loc4 = loc("relu")
#loc5 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":33:0)
#loc6 = loc("convolution_1")
#loc7 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":34:0)
#loc8 = loc("relu_1")
#loc9 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":35:0)
#loc10 = loc("max_pool2d_with_indices")
#loc11 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":37:0)
#loc12 = loc("view")
#loc13 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":38:0)
#loc14 = loc("convert_element_type")
#loc15 = loc("mm")
#loc16 = loc("mul")
#loc17 = loc("add")
#loc18 = loc("convert_element_type_1")
#loc19 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":39:0)
#loc20 = loc("relu_2")
#loc21 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":41:0)
#loc22 = loc("convert_element_type_2")
#loc23 = loc("mm_1")
#loc24 = loc("mul_1")
#loc25 = loc("add_1")
#loc26 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":42:0)
#loc27 = loc("amax")
#loc28 = loc("sub")
#loc29 = loc("exp")
#loc30 = loc("sum_1")
#loc31 = loc("log")
#loc32 = loc("sub_1")
#loc33 = loc("convert_element_type_5")
#loc35 = loc(fused[#loc3, #loc4])
#loc36 = loc(fused[#loc5, #loc6])
#loc37 = loc(fused[#loc7, #loc8])
#loc38 = loc(fused[#loc9, #loc10])
#loc39 = loc(fused[#loc11, #loc12])
#loc40 = loc(fused[#loc13, #loc14])
#loc41 = loc(fused[#loc13, #loc15])
#loc42 = loc(fused[#loc13, #loc16])
#loc43 = loc(fused[#loc13, #loc17])
#loc44 = loc(fused[#loc13, #loc18])
#loc45 = loc(fused[#loc19, #loc20])
#loc46 = loc(fused[#loc21, #loc22])
#loc47 = loc(fused[#loc21, #loc23])
#loc48 = loc(fused[#loc21, #loc24])
#loc49 = loc(fused[#loc21, #loc25])
#loc50 = loc(fused[#loc26, #loc27])
#loc51 = loc(fused[#loc26, #loc28])
#loc52 = loc(fused[#loc26, #loc29])
#loc53 = loc(fused[#loc26, #loc30])
#loc54 = loc(fused[#loc26, #loc31])
#loc55 = loc(fused[#loc26, #loc32])
#loc56 = loc(fused[#loc26, #loc33])

/localdev/hshah/tt-torch/env/venv/lib/python3.10/site-packages/torch/fx/experimental/const_fold.py:264: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer
  new_node = root_const_gm.graph.get_attr(in_node.target)
Torch FX module module
module {
  func.func @main(%arg0: !torch.vtensor<[32,1,3,3],bf16>, %arg1: !torch.vtensor<[32],bf16>, %arg2: !torch.vtensor<[64,32,3,3],bf16>, %arg3: !torch.vtensor<[64],bf16>, %arg4: !torch.vtensor<[9216,128],f32>, %arg5: !torch.vtensor<[128],f32>, %arg6: !torch.vtensor<[128,10],f32>, %arg7: !torch.vtensor<[10],f32>, %arg8: !torch.vtensor<[128,9216],bf16>, %arg9: !torch.vtensor<[128],bf16>, %arg10: !torch.vtensor<[10,128],bf16>, %arg11: !torch.vtensor<[10],bf16>, %arg12: !torch.vtensor<[1,1,28,28],bf16>) -> !torch.vtensor<[1,10],bf16> attributes {torch.assume_strict_symbolic_shapes} {
    %int1 = torch.constant.int 1
    %int1_0 = torch.constant.int 1
    %0 = torch.prim.ListConstruct %int1, %int1_0 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0 = torch.constant.int 0
    %int0_1 = torch.constant.int 0
    %1 = torch.prim.ListConstruct %int0, %int0_1 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2 = torch.constant.int 1
    %int1_3 = torch.constant.int 1
    %2 = torch.prim.ListConstruct %int1_2, %int1_3 : (!torch.int, !torch.int) -> !torch.list<int>
    %false = torch.constant.bool false
    %int0_4 = torch.constant.int 0
    %int0_5 = torch.constant.int 0
    %3 = torch.prim.ListConstruct %int0_4, %int0_5 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6 = torch.constant.int 1
    %4 = torch.aten.convolution %arg12, %arg0, %arg1, %0, %1, %2, %false, %3, %int1_6 : !torch.vtensor<[1,1,28,28],bf16>, !torch.vtensor<[32,1,3,3],bf16>, !torch.vtensor<[32],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,32,26,26],bf16>
    %5 = torch.aten.relu %4 : !torch.vtensor<[1,32,26,26],bf16> -> !torch.vtensor<[1,32,26,26],bf16>
    %int1_7 = torch.constant.int 1
    %int1_8 = torch.constant.int 1
    %6 = torch.prim.ListConstruct %int1_7, %int1_8 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_9 = torch.constant.int 0
    %int0_10 = torch.constant.int 0
    %7 = torch.prim.ListConstruct %int0_9, %int0_10 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_11 = torch.constant.int 1
    %int1_12 = torch.constant.int 1
    %8 = torch.prim.ListConstruct %int1_11, %int1_12 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_13 = torch.constant.bool false
    %int0_14 = torch.constant.int 0
    %int0_15 = torch.constant.int 0
    %9 = torch.prim.ListConstruct %int0_14, %int0_15 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_16 = torch.constant.int 1
    %10 = torch.aten.convolution %5, %arg2, %arg3, %6, %7, %8, %false_13, %9, %int1_16 : !torch.vtensor<[1,32,26,26],bf16>, !torch.vtensor<[64,32,3,3],bf16>, !torch.vtensor<[64],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,24,24],bf16>
    %11 = torch.aten.relu %10 : !torch.vtensor<[1,64,24,24],bf16> -> !torch.vtensor<[1,64,24,24],bf16>
    %int2 = torch.constant.int 2
    %int2_17 = torch.constant.int 2
    %12 = torch.prim.ListConstruct %int2, %int2_17 : (!torch.int, !torch.int) -> !torch.list<int>
    %13 = torch.prim.ListConstruct  : () -> !torch.list<int>
    %int0_18 = torch.constant.int 0
    %int0_19 = torch.constant.int 0
    %14 = torch.prim.ListConstruct %int0_18, %int0_19 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_20 = torch.constant.int 1
    %int1_21 = torch.constant.int 1
    %15 = torch.prim.ListConstruct %int1_20, %int1_21 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_22 = torch.constant.bool false
    %result0, %result1 = torch.aten.max_pool2d_with_indices %11, %12, %13, %14, %15, %false_22 : !torch.vtensor<[1,64,24,24],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,12,12],bf16>, !torch.vtensor<[1,64,12,12],si64>
    %none = torch.constant.none
    %16 = torch.aten.clone %result0, %none : !torch.vtensor<[1,64,12,12],bf16>, !torch.none -> !torch.vtensor<[1,64,12,12],bf16>
    %int1_23 = torch.constant.int 1
    %int9216 = torch.constant.int 9216
    %17 = torch.prim.ListConstruct %int1_23, %int9216 : (!torch.int, !torch.int) -> !torch.list<int>
    %18 = torch.aten.view %16, %17 : !torch.vtensor<[1,64,12,12],bf16>, !torch.list<int> -> !torch.vtensor<[1,9216],bf16>
    %int6 = torch.constant.int 6
    %19 = torch.prims.convert_element_type %18, %int6 : !torch.vtensor<[1,9216],bf16>, !torch.int -> !torch.vtensor<[1,9216],f32>
    %20 = torch.aten.mm %19, %arg4 : !torch.vtensor<[1,9216],f32>, !torch.vtensor<[9216,128],f32> -> !torch.vtensor<[1,128],f32>
    %int1_24 = torch.constant.int 1
    %21 = torch.aten.mul.Scalar %20, %int1_24 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %int1_25 = torch.constant.int 1
    %22 = torch.aten.add.Tensor %21, %arg5, %int1_25 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %int15 = torch.constant.int 15
    %23 = torch.prims.convert_element_type %22, %int15 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],bf16>
    %24 = torch.aten.relu %23 : !torch.vtensor<[1,128],bf16> -> !torch.vtensor<[1,128],bf16>
    %none_26 = torch.constant.none
    %25 = torch.aten.clone %24, %none_26 : !torch.vtensor<[1,128],bf16>, !torch.none -> !torch.vtensor<[1,128],bf16>
    %int6_27 = torch.constant.int 6
    %26 = torch.prims.convert_element_type %25, %int6_27 : !torch.vtensor<[1,128],bf16>, !torch.int -> !torch.vtensor<[1,128],f32>
    %27 = torch.aten.mm %26, %arg6 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128,10],f32> -> !torch.vtensor<[1,10],f32>
    %int1_28 = torch.constant.int 1
    %28 = torch.aten.mul.Scalar %27, %int1_28 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %int1_29 = torch.constant.int 1
    %29 = torch.aten.add.Tensor %28, %arg7, %int1_29 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %int15_30 = torch.constant.int 15
    %30 = torch.prims.convert_element_type %29, %int15_30 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],bf16>
    %int6_31 = torch.constant.int 6
    %31 = torch.prims.convert_element_type %30, %int6_31 : !torch.vtensor<[1,10],bf16>, !torch.int -> !torch.vtensor<[1,10],f32>
    %int1_32 = torch.constant.int 1
    %32 = torch.prim.ListConstruct %int1_32 : (!torch.int) -> !torch.list<int>
    %true = torch.constant.bool true
    %33 = torch.aten.amax %31, %32, %true : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,1],f32>
    %int1_33 = torch.constant.int 1
    %34 = torch.aten.sub.Tensor %31, %33, %int1_33 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %35 = torch.aten.exp %34 : !torch.vtensor<[1,10],f32> -> !torch.vtensor<[1,10],f32>
    %int1_34 = torch.constant.int 1
    %36 = torch.prim.ListConstruct %int1_34 : (!torch.int) -> !torch.list<int>
    %true_35 = torch.constant.bool true
    %none_36 = torch.constant.none
    %37 = torch.aten.sum.dim_IntList %35, %36, %true_35, %none_36 : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[1,1],f32>
    %38 = torch.aten.log %37 : !torch.vtensor<[1,1],f32> -> !torch.vtensor<[1,1],f32>
    %int1_37 = torch.constant.int 1
    %39 = torch.aten.sub.Tensor %34, %38, %int1_37 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %int15_38 = torch.constant.int 15
    %40 = torch.prims.convert_element_type %39, %int15_38 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],bf16>
    return %40 : !torch.vtensor<[1,10],bf16>
  }
}

// -----// IR Dump After ReduceOpVariants (torch-reduce-op-variants) //----- //
func.func @main(%arg0: !torch.vtensor<[32,1,3,3],bf16>, %arg1: !torch.vtensor<[32],bf16>, %arg2: !torch.vtensor<[64,32,3,3],bf16>, %arg3: !torch.vtensor<[64],bf16>, %arg4: !torch.vtensor<[9216,128],f32>, %arg5: !torch.vtensor<[128],f32>, %arg6: !torch.vtensor<[128,10],f32>, %arg7: !torch.vtensor<[10],f32>, %arg8: !torch.vtensor<[128,9216],bf16>, %arg9: !torch.vtensor<[128],bf16>, %arg10: !torch.vtensor<[10,128],bf16>, %arg11: !torch.vtensor<[10],bf16>, %arg12: !torch.vtensor<[1,1,28,28],bf16>) -> !torch.vtensor<[1,10],bf16> attributes {torch.assume_strict_symbolic_shapes} {
  %int1 = torch.constant.int 1
  %int1_0 = torch.constant.int 1
  %0 = torch.prim.ListConstruct %int1, %int1_0 : (!torch.int, !torch.int) -> !torch.list<int>
  %int0 = torch.constant.int 0
  %int0_1 = torch.constant.int 0
  %1 = torch.prim.ListConstruct %int0, %int0_1 : (!torch.int, !torch.int) -> !torch.list<int>
  %int1_2 = torch.constant.int 1
  %int1_3 = torch.constant.int 1
  %2 = torch.prim.ListConstruct %int1_2, %int1_3 : (!torch.int, !torch.int) -> !torch.list<int>
  %false = torch.constant.bool false
  %int0_4 = torch.constant.int 0
  %int0_5 = torch.constant.int 0
  %3 = torch.prim.ListConstruct %int0_4, %int0_5 : (!torch.int, !torch.int) -> !torch.list<int>
  %int1_6 = torch.constant.int 1
  %4 = torch.aten.convolution %arg12, %arg0, %arg1, %0, %1, %2, %false, %3, %int1_6 : !torch.vtensor<[1,1,28,28],bf16>, !torch.vtensor<[32,1,3,3],bf16>, !torch.vtensor<[32],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,32,26,26],bf16>
  %5 = torch.aten.relu %4 : !torch.vtensor<[1,32,26,26],bf16> -> !torch.vtensor<[1,32,26,26],bf16>
  %int1_7 = torch.constant.int 1
  %int1_8 = torch.constant.int 1
  %6 = torch.prim.ListConstruct %int1_7, %int1_8 : (!torch.int, !torch.int) -> !torch.list<int>
  %int0_9 = torch.constant.int 0
  %int0_10 = torch.constant.int 0
  %7 = torch.prim.ListConstruct %int0_9, %int0_10 : (!torch.int, !torch.int) -> !torch.list<int>
  %int1_11 = torch.constant.int 1
  %int1_12 = torch.constant.int 1
  %8 = torch.prim.ListConstruct %int1_11, %int1_12 : (!torch.int, !torch.int) -> !torch.list<int>
  %false_13 = torch.constant.bool false
  %int0_14 = torch.constant.int 0
  %int0_15 = torch.constant.int 0
  %9 = torch.prim.ListConstruct %int0_14, %int0_15 : (!torch.int, !torch.int) -> !torch.list<int>
  %int1_16 = torch.constant.int 1
  %10 = torch.aten.convolution %5, %arg2, %arg3, %6, %7, %8, %false_13, %9, %int1_16 : !torch.vtensor<[1,32,26,26],bf16>, !torch.vtensor<[64,32,3,3],bf16>, !torch.vtensor<[64],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,24,24],bf16>
  %11 = torch.aten.relu %10 : !torch.vtensor<[1,64,24,24],bf16> -> !torch.vtensor<[1,64,24,24],bf16>
  %int2 = torch.constant.int 2
  %int2_17 = torch.constant.int 2
  %12 = torch.prim.ListConstruct %int2, %int2_17 : (!torch.int, !torch.int) -> !torch.list<int>
  %13 = torch.prim.ListConstruct  : () -> !torch.list<int>
  %int0_18 = torch.constant.int 0
  %int0_19 = torch.constant.int 0
  %14 = torch.prim.ListConstruct %int0_18, %int0_19 : (!torch.int, !torch.int) -> !torch.list<int>
  %int1_20 = torch.constant.int 1
  %int1_21 = torch.constant.int 1
  %15 = torch.prim.ListConstruct %int1_20, %int1_21 : (!torch.int, !torch.int) -> !torch.list<int>
  %false_22 = torch.constant.bool false
  %result0, %result1 = torch.aten.max_pool2d_with_indices %11, %12, %13, %14, %15, %false_22 : !torch.vtensor<[1,64,24,24],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,12,12],bf16>, !torch.vtensor<[1,64,12,12],si64>
  %none = torch.constant.none
  %16 = torch.aten.clone %result0, %none : !torch.vtensor<[1,64,12,12],bf16>, !torch.none -> !torch.vtensor<[1,64,12,12],bf16>
  %int1_23 = torch.constant.int 1
  %int9216 = torch.constant.int 9216
  %17 = torch.prim.ListConstruct %int1_23, %int9216 : (!torch.int, !torch.int) -> !torch.list<int>
  %18 = torch.aten.view %16, %17 : !torch.vtensor<[1,64,12,12],bf16>, !torch.list<int> -> !torch.vtensor<[1,9216],bf16>
  %int6 = torch.constant.int 6
  %19 = torch.prims.convert_element_type %18, %int6 : !torch.vtensor<[1,9216],bf16>, !torch.int -> !torch.vtensor<[1,9216],f32>
  %20 = torch.aten.mm %19, %arg4 : !torch.vtensor<[1,9216],f32>, !torch.vtensor<[9216,128],f32> -> !torch.vtensor<[1,128],f32>
  %int1_24 = torch.constant.int 1
  %21 = torch.aten.mul.Scalar %20, %int1_24 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %int1_25 = torch.constant.int 1
  %22 = torch.aten.add.Tensor %21, %arg5, %int1_25 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %int15 = torch.constant.int 15
  %23 = torch.prims.convert_element_type %22, %int15 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],bf16>
  %24 = torch.aten.relu %23 : !torch.vtensor<[1,128],bf16> -> !torch.vtensor<[1,128],bf16>
  %none_26 = torch.constant.none
  %25 = torch.aten.clone %24, %none_26 : !torch.vtensor<[1,128],bf16>, !torch.none -> !torch.vtensor<[1,128],bf16>
  %int6_27 = torch.constant.int 6
  %26 = torch.prims.convert_element_type %25, %int6_27 : !torch.vtensor<[1,128],bf16>, !torch.int -> !torch.vtensor<[1,128],f32>
  %27 = torch.aten.mm %26, %arg6 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128,10],f32> -> !torch.vtensor<[1,10],f32>
  %int1_28 = torch.constant.int 1
  %28 = torch.aten.mul.Scalar %27, %int1_28 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %int1_29 = torch.constant.int 1
  %29 = torch.aten.add.Tensor %28, %arg7, %int1_29 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %int15_30 = torch.constant.int 15
  %30 = torch.prims.convert_element_type %29, %int15_30 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],bf16>
  %int6_31 = torch.constant.int 6
  %31 = torch.prims.convert_element_type %30, %int6_31 : !torch.vtensor<[1,10],bf16>, !torch.int -> !torch.vtensor<[1,10],f32>
  %int1_32 = torch.constant.int 1
  %32 = torch.prim.ListConstruct %int1_32 : (!torch.int) -> !torch.list<int>
  %true = torch.constant.bool true
  %33 = torch.aten.amax %31, %32, %true : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,1],f32>
  %int1_33 = torch.constant.int 1
  %34 = torch.aten.sub.Tensor %31, %33, %int1_33 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %35 = torch.aten.exp %34 : !torch.vtensor<[1,10],f32> -> !torch.vtensor<[1,10],f32>
  %int1_34 = torch.constant.int 1
  %36 = torch.prim.ListConstruct %int1_34 : (!torch.int) -> !torch.list<int>
  %true_35 = torch.constant.bool true
  %none_36 = torch.constant.none
  %37 = torch.aten.sum.dim_IntList %35, %36, %true_35, %none_36 : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[1,1],f32>
  %38 = torch.aten.log %37 : !torch.vtensor<[1,1],f32> -> !torch.vtensor<[1,1],f32>
  %int1_37 = torch.constant.int 1
  %39 = torch.aten.sub.Tensor %34, %38, %int1_37 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %int15_38 = torch.constant.int 15
  %40 = torch.prims.convert_element_type %39, %int15_38 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],bf16>
  return %40 : !torch.vtensor<[1,10],bf16>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !torch.vtensor<[32,1,3,3],bf16>, %arg1: !torch.vtensor<[32],bf16>, %arg2: !torch.vtensor<[64,32,3,3],bf16>, %arg3: !torch.vtensor<[64],bf16>, %arg4: !torch.vtensor<[9216,128],f32>, %arg5: !torch.vtensor<[128],f32>, %arg6: !torch.vtensor<[128,10],f32>, %arg7: !torch.vtensor<[10],f32>, %arg8: !torch.vtensor<[128,9216],bf16>, %arg9: !torch.vtensor<[128],bf16>, %arg10: !torch.vtensor<[10,128],bf16>, %arg11: !torch.vtensor<[10],bf16>, %arg12: !torch.vtensor<[1,1,28,28],bf16>) -> !torch.vtensor<[1,10],bf16> attributes {torch.assume_strict_symbolic_shapes} {
  %true = torch.constant.bool true
  %int15 = torch.constant.int 15
  %int6 = torch.constant.int 6
  %int9216 = torch.constant.int 9216
  %none = torch.constant.none
  %int2 = torch.constant.int 2
  %false = torch.constant.bool false
  %int0 = torch.constant.int 0
  %int1 = torch.constant.int 1
  %0 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %1 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %2 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %3 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %4 = torch.aten.convolution %arg12, %arg0, %arg1, %0, %1, %2, %false, %3, %int1 : !torch.vtensor<[1,1,28,28],bf16>, !torch.vtensor<[32,1,3,3],bf16>, !torch.vtensor<[32],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,32,26,26],bf16>
  %5 = torch.aten.relu %4 : !torch.vtensor<[1,32,26,26],bf16> -> !torch.vtensor<[1,32,26,26],bf16>
  %6 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %7 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %8 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %9 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %10 = torch.aten.convolution %5, %arg2, %arg3, %6, %7, %8, %false, %9, %int1 : !torch.vtensor<[1,32,26,26],bf16>, !torch.vtensor<[64,32,3,3],bf16>, !torch.vtensor<[64],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,24,24],bf16>
  %11 = torch.aten.relu %10 : !torch.vtensor<[1,64,24,24],bf16> -> !torch.vtensor<[1,64,24,24],bf16>
  %12 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
  %13 = torch.prim.ListConstruct  : () -> !torch.list<int>
  %14 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %15 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %16 = torch.aten.max_pool2d %11, %12, %13, %14, %15, %false : !torch.vtensor<[1,64,24,24],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,12,12],bf16>
  %17 = torch.prim.ListConstruct %int1, %int9216 : (!torch.int, !torch.int) -> !torch.list<int>
  %18 = torch.aten.view %16, %17 : !torch.vtensor<[1,64,12,12],bf16>, !torch.list<int> -> !torch.vtensor<[1,9216],bf16>
  %19 = torch.prims.convert_element_type %18, %int6 : !torch.vtensor<[1,9216],bf16>, !torch.int -> !torch.vtensor<[1,9216],f32>
  %20 = torch.aten.mm %19, %arg4 : !torch.vtensor<[1,9216],f32>, !torch.vtensor<[9216,128],f32> -> !torch.vtensor<[1,128],f32>
  %21 = torch.aten.mul.Scalar %20, %int1 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %22 = torch.aten.add.Tensor %21, %arg5, %int1 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %23 = torch.prims.convert_element_type %22, %int15 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],bf16>
  %24 = torch.aten.relu %23 : !torch.vtensor<[1,128],bf16> -> !torch.vtensor<[1,128],bf16>
  %25 = torch.prims.convert_element_type %24, %int6 : !torch.vtensor<[1,128],bf16>, !torch.int -> !torch.vtensor<[1,128],f32>
  %26 = torch.aten.mm %25, %arg6 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128,10],f32> -> !torch.vtensor<[1,10],f32>
  %27 = torch.aten.mul.Scalar %26, %int1 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %28 = torch.aten.add.Tensor %27, %arg7, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %29 = torch.prims.convert_element_type %28, %int15 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],bf16>
  %30 = torch.prims.convert_element_type %29, %int6 : !torch.vtensor<[1,10],bf16>, !torch.int -> !torch.vtensor<[1,10],f32>
  %31 = torch.prim.ListConstruct %int1 : (!torch.int) -> !torch.list<int>
  %32 = torch.aten.amax %30, %31, %true : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,1],f32>
  %33 = torch.aten.sub.Tensor %30, %32, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %34 = torch.aten.exp %33 : !torch.vtensor<[1,10],f32> -> !torch.vtensor<[1,10],f32>
  %35 = torch.prim.ListConstruct %int1 : (!torch.int) -> !torch.list<int>
  %36 = torch.aten.sum.dim_IntList %34, %35, %true, %none : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[1,1],f32>
  %37 = torch.aten.log %36 : !torch.vtensor<[1,1],f32> -> !torch.vtensor<[1,1],f32>
  %38 = torch.aten.sub.Tensor %33, %37, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %39 = torch.prims.convert_element_type %38, %int15 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],bf16>
  return %39 : !torch.vtensor<[1,10],bf16>
}

// -----// IR Dump After DecomposeComplexOps (torch-decompose-complex-ops) //----- //
func.func @main(%arg0: !torch.vtensor<[32,1,3,3],bf16>, %arg1: !torch.vtensor<[32],bf16>, %arg2: !torch.vtensor<[64,32,3,3],bf16>, %arg3: !torch.vtensor<[64],bf16>, %arg4: !torch.vtensor<[9216,128],f32>, %arg5: !torch.vtensor<[128],f32>, %arg6: !torch.vtensor<[128,10],f32>, %arg7: !torch.vtensor<[10],f32>, %arg8: !torch.vtensor<[128,9216],bf16>, %arg9: !torch.vtensor<[128],bf16>, %arg10: !torch.vtensor<[10,128],bf16>, %arg11: !torch.vtensor<[10],bf16>, %arg12: !torch.vtensor<[1,1,28,28],bf16>) -> !torch.vtensor<[1,10],bf16> attributes {torch.assume_strict_symbolic_shapes} {
  %true = torch.constant.bool true
  %int15 = torch.constant.int 15
  %int6 = torch.constant.int 6
  %int9216 = torch.constant.int 9216
  %none = torch.constant.none
  %int2 = torch.constant.int 2
  %false = torch.constant.bool false
  %int0 = torch.constant.int 0
  %int1 = torch.constant.int 1
  %0 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %1 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %2 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %3 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %4 = torch.aten.convolution %arg12, %arg0, %arg1, %0, %1, %2, %false, %3, %int1 : !torch.vtensor<[1,1,28,28],bf16>, !torch.vtensor<[32,1,3,3],bf16>, !torch.vtensor<[32],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,32,26,26],bf16>
  %5 = torch.aten.relu %4 : !torch.vtensor<[1,32,26,26],bf16> -> !torch.vtensor<[1,32,26,26],bf16>
  %6 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %7 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %8 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %9 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %10 = torch.aten.convolution %5, %arg2, %arg3, %6, %7, %8, %false, %9, %int1 : !torch.vtensor<[1,32,26,26],bf16>, !torch.vtensor<[64,32,3,3],bf16>, !torch.vtensor<[64],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,24,24],bf16>
  %11 = torch.aten.relu %10 : !torch.vtensor<[1,64,24,24],bf16> -> !torch.vtensor<[1,64,24,24],bf16>
  %12 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
  %13 = torch.prim.ListConstruct  : () -> !torch.list<int>
  %14 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %15 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %16 = torch.aten.max_pool2d %11, %12, %13, %14, %15, %false : !torch.vtensor<[1,64,24,24],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,12,12],bf16>
  %17 = torch.prim.ListConstruct %int1, %int9216 : (!torch.int, !torch.int) -> !torch.list<int>
  %18 = torch.aten.view %16, %17 : !torch.vtensor<[1,64,12,12],bf16>, !torch.list<int> -> !torch.vtensor<[1,9216],bf16>
  %19 = torch.aten.to.dtype %18, %int6, %false, %false, %none : !torch.vtensor<[1,9216],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,9216],f32>
  %20 = torch.aten.mm %19, %arg4 : !torch.vtensor<[1,9216],f32>, !torch.vtensor<[9216,128],f32> -> !torch.vtensor<[1,128],f32>
  %21 = torch.aten.mul.Scalar %20, %int1 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %22 = torch.aten.add.Tensor %21, %arg5, %int1 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %23 = torch.aten.to.dtype %22, %int15, %false, %false, %none : !torch.vtensor<[1,128],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,128],bf16>
  %24 = torch.aten.relu %23 : !torch.vtensor<[1,128],bf16> -> !torch.vtensor<[1,128],bf16>
  %25 = torch.aten.to.dtype %24, %int6, %false, %false, %none : !torch.vtensor<[1,128],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,128],f32>
  %26 = torch.aten.mm %25, %arg6 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128,10],f32> -> !torch.vtensor<[1,10],f32>
  %27 = torch.aten.mul.Scalar %26, %int1 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %28 = torch.aten.add.Tensor %27, %arg7, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %29 = torch.aten.to.dtype %28, %int15, %false, %false, %none : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],bf16>
  %30 = torch.aten.to.dtype %29, %int6, %false, %false, %none : !torch.vtensor<[1,10],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],f32>
  %values, %indices = torch.aten.max.dim %30, %int1, %true : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool -> !torch.vtensor<[1,1],f32>, !torch.vtensor<[1,1],si64>
  %31 = torch.aten.sub.Tensor %30, %values, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %32 = torch.aten.exp %31 : !torch.vtensor<[1,10],f32> -> !torch.vtensor<[1,10],f32>
  %33 = torch.prim.ListConstruct %int1 : (!torch.int) -> !torch.list<int>
  %34 = torch.aten.sum.dim_IntList %32, %33, %true, %none : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[1,1],f32>
  %35 = torch.aten.log %34 : !torch.vtensor<[1,1],f32> -> !torch.vtensor<[1,1],f32>
  %36 = torch.aten.sub.Tensor %31, %35, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %37 = torch.aten.to.dtype %36, %int15, %false, %false, %none : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],bf16>
  return %37 : !torch.vtensor<[1,10],bf16>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !torch.vtensor<[32,1,3,3],bf16>, %arg1: !torch.vtensor<[32],bf16>, %arg2: !torch.vtensor<[64,32,3,3],bf16>, %arg3: !torch.vtensor<[64],bf16>, %arg4: !torch.vtensor<[9216,128],f32>, %arg5: !torch.vtensor<[128],f32>, %arg6: !torch.vtensor<[128,10],f32>, %arg7: !torch.vtensor<[10],f32>, %arg8: !torch.vtensor<[128,9216],bf16>, %arg9: !torch.vtensor<[128],bf16>, %arg10: !torch.vtensor<[10,128],bf16>, %arg11: !torch.vtensor<[10],bf16>, %arg12: !torch.vtensor<[1,1,28,28],bf16>) -> !torch.vtensor<[1,10],bf16> attributes {torch.assume_strict_symbolic_shapes} {
  %true = torch.constant.bool true
  %int15 = torch.constant.int 15
  %int6 = torch.constant.int 6
  %int9216 = torch.constant.int 9216
  %none = torch.constant.none
  %int2 = torch.constant.int 2
  %false = torch.constant.bool false
  %int0 = torch.constant.int 0
  %int1 = torch.constant.int 1
  %0 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %1 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %2 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %3 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %4 = torch.aten.convolution %arg12, %arg0, %arg1, %0, %1, %2, %false, %3, %int1 : !torch.vtensor<[1,1,28,28],bf16>, !torch.vtensor<[32,1,3,3],bf16>, !torch.vtensor<[32],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,32,26,26],bf16>
  %5 = torch.aten.relu %4 : !torch.vtensor<[1,32,26,26],bf16> -> !torch.vtensor<[1,32,26,26],bf16>
  %6 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %7 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %8 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %9 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %10 = torch.aten.convolution %5, %arg2, %arg3, %6, %7, %8, %false, %9, %int1 : !torch.vtensor<[1,32,26,26],bf16>, !torch.vtensor<[64,32,3,3],bf16>, !torch.vtensor<[64],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,24,24],bf16>
  %11 = torch.aten.relu %10 : !torch.vtensor<[1,64,24,24],bf16> -> !torch.vtensor<[1,64,24,24],bf16>
  %12 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
  %13 = torch.prim.ListConstruct  : () -> !torch.list<int>
  %14 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
  %15 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
  %16 = torch.aten.max_pool2d %11, %12, %13, %14, %15, %false : !torch.vtensor<[1,64,24,24],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,12,12],bf16>
  %17 = torch.prim.ListConstruct %int1, %int9216 : (!torch.int, !torch.int) -> !torch.list<int>
  %18 = torch.aten.view %16, %17 : !torch.vtensor<[1,64,12,12],bf16>, !torch.list<int> -> !torch.vtensor<[1,9216],bf16>
  %19 = torch.aten.to.dtype %18, %int6, %false, %false, %none : !torch.vtensor<[1,9216],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,9216],f32>
  %20 = torch.aten.mm %19, %arg4 : !torch.vtensor<[1,9216],f32>, !torch.vtensor<[9216,128],f32> -> !torch.vtensor<[1,128],f32>
  %21 = torch.aten.mul.Scalar %20, %int1 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %22 = torch.aten.add.Tensor %21, %arg5, %int1 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
  %23 = torch.aten.to.dtype %22, %int15, %false, %false, %none : !torch.vtensor<[1,128],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,128],bf16>
  %24 = torch.aten.relu %23 : !torch.vtensor<[1,128],bf16> -> !torch.vtensor<[1,128],bf16>
  %25 = torch.aten.to.dtype %24, %int6, %false, %false, %none : !torch.vtensor<[1,128],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,128],f32>
  %26 = torch.aten.mm %25, %arg6 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128,10],f32> -> !torch.vtensor<[1,10],f32>
  %27 = torch.aten.mul.Scalar %26, %int1 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %28 = torch.aten.add.Tensor %27, %arg7, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %29 = torch.aten.to.dtype %28, %int15, %false, %false, %none : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],bf16>
  %30 = torch.aten.to.dtype %29, %int6, %false, %false, %none : !torch.vtensor<[1,10],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],f32>
  %values, %indices = torch.aten.max.dim %30, %int1, %true : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool -> !torch.vtensor<[1,1],f32>, !torch.vtensor<[1,1],si64>
  %31 = torch.aten.sub.Tensor %30, %values, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %32 = torch.aten.exp %31 : !torch.vtensor<[1,10],f32> -> !torch.vtensor<[1,10],f32>
  %33 = torch.prim.ListConstruct %int1 : (!torch.int) -> !torch.list<int>
  %34 = torch.aten.sum.dim_IntList %32, %33, %true, %none : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[1,1],f32>
  %35 = torch.aten.log %34 : !torch.vtensor<[1,1],f32> -> !torch.vtensor<[1,1],f32>
  %36 = torch.aten.sub.Tensor %31, %35, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
  %37 = torch.aten.to.dtype %36, %int15, %false, %false, %none : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],bf16>
  return %37 : !torch.vtensor<[1,10],bf16>
}

Torch Backend module module
module {
  func.func @main(%arg0: !torch.vtensor<[32,1,3,3],bf16>, %arg1: !torch.vtensor<[32],bf16>, %arg2: !torch.vtensor<[64,32,3,3],bf16>, %arg3: !torch.vtensor<[64],bf16>, %arg4: !torch.vtensor<[9216,128],f32>, %arg5: !torch.vtensor<[128],f32>, %arg6: !torch.vtensor<[128,10],f32>, %arg7: !torch.vtensor<[10],f32>, %arg8: !torch.vtensor<[128,9216],bf16>, %arg9: !torch.vtensor<[128],bf16>, %arg10: !torch.vtensor<[10,128],bf16>, %arg11: !torch.vtensor<[10],bf16>, %arg12: !torch.vtensor<[1,1,28,28],bf16>) -> !torch.vtensor<[1,10],bf16> attributes {torch.assume_strict_symbolic_shapes} {
    %true = torch.constant.bool true
    %int15 = torch.constant.int 15
    %int6 = torch.constant.int 6
    %int9216 = torch.constant.int 9216
    %none = torch.constant.none
    %int2 = torch.constant.int 2
    %false = torch.constant.bool false
    %int0 = torch.constant.int 0
    %int1 = torch.constant.int 1
    %0 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %2 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %4 = torch.aten.convolution %arg12, %arg0, %arg1, %0, %1, %2, %false, %3, %int1 : !torch.vtensor<[1,1,28,28],bf16>, !torch.vtensor<[32,1,3,3],bf16>, !torch.vtensor<[32],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,32,26,26],bf16>
    %5 = torch.aten.relu %4 : !torch.vtensor<[1,32,26,26],bf16> -> !torch.vtensor<[1,32,26,26],bf16>
    %6 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %7 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %9 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %10 = torch.aten.convolution %5, %arg2, %arg3, %6, %7, %8, %false, %9, %int1 : !torch.vtensor<[1,32,26,26],bf16>, !torch.vtensor<[64,32,3,3],bf16>, !torch.vtensor<[64],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,24,24],bf16>
    %11 = torch.aten.relu %10 : !torch.vtensor<[1,64,24,24],bf16> -> !torch.vtensor<[1,64,24,24],bf16>
    %12 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
    %13 = torch.prim.ListConstruct  : () -> !torch.list<int>
    %14 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16 = torch.aten.max_pool2d %11, %12, %13, %14, %15, %false : !torch.vtensor<[1,64,24,24],bf16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,12,12],bf16>
    %17 = torch.prim.ListConstruct %int1, %int9216 : (!torch.int, !torch.int) -> !torch.list<int>
    %18 = torch.aten.view %16, %17 : !torch.vtensor<[1,64,12,12],bf16>, !torch.list<int> -> !torch.vtensor<[1,9216],bf16>
    %19 = torch.aten.to.dtype %18, %int6, %false, %false, %none : !torch.vtensor<[1,9216],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,9216],f32>
    %20 = torch.aten.mm %19, %arg4 : !torch.vtensor<[1,9216],f32>, !torch.vtensor<[9216,128],f32> -> !torch.vtensor<[1,128],f32>
    %21 = torch.aten.mul.Scalar %20, %int1 : !torch.vtensor<[1,128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %22 = torch.aten.add.Tensor %21, %arg5, %int1 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %23 = torch.aten.to.dtype %22, %int15, %false, %false, %none : !torch.vtensor<[1,128],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,128],bf16>
    %24 = torch.aten.relu %23 : !torch.vtensor<[1,128],bf16> -> !torch.vtensor<[1,128],bf16>
    %25 = torch.aten.to.dtype %24, %int6, %false, %false, %none : !torch.vtensor<[1,128],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,128],f32>
    %26 = torch.aten.mm %25, %arg6 : !torch.vtensor<[1,128],f32>, !torch.vtensor<[128,10],f32> -> !torch.vtensor<[1,10],f32>
    %27 = torch.aten.mul.Scalar %26, %int1 : !torch.vtensor<[1,10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %28 = torch.aten.add.Tensor %27, %arg7, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[10],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %29 = torch.aten.to.dtype %28, %int15, %false, %false, %none : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],bf16>
    %30 = torch.aten.to.dtype %29, %int6, %false, %false, %none : !torch.vtensor<[1,10],bf16>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],f32>
    %values, %indices = torch.aten.max.dim %30, %int1, %true : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool -> !torch.vtensor<[1,1],f32>, !torch.vtensor<[1,1],si64>
    %31 = torch.aten.sub.Tensor %30, %values, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %32 = torch.aten.exp %31 : !torch.vtensor<[1,10],f32> -> !torch.vtensor<[1,10],f32>
    %33 = torch.prim.ListConstruct %int1 : (!torch.int) -> !torch.list<int>
    %34 = torch.aten.sum.dim_IntList %32, %33, %true, %none : !torch.vtensor<[1,10],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[1,1],f32>
    %35 = torch.aten.log %34 : !torch.vtensor<[1,1],f32> -> !torch.vtensor<[1,1],f32>
    %36 = torch.aten.sub.Tensor %31, %35, %int1 : !torch.vtensor<[1,10],f32>, !torch.vtensor<[1,1],f32>, !torch.int -> !torch.vtensor<[1,10],f32>
    %37 = torch.aten.to.dtype %36, %int15, %false, %false, %none : !torch.vtensor<[1,10],f32>, !torch.int, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,10],bf16>
    return %37 : !torch.vtensor<[1,10],bf16>
  }
}

StableHLO module module
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_5 = arith.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %cst_5 : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}

// -----// IR Dump Before ConvertArithToStableHLO (convert-arith-to-stablehlo) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_5 = arith.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %cst_5 : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}


// -----// IR Dump After ConvertArithToStableHLO (convert-arith-to-stablehlo) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %c : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before StablehloLegalizeCompositeToCallPass (stablehlo-legalize-composite-to-call) ('func.func' operation: @main) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %c : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %c : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %c : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x32x26x26xbf16>
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x64x24x24xbf16>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x128xbf16>
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<1> : tensor<1xi64>
    %0 = stablehlo.convolution(%arg12, %arg0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>) -> tensor<1x32x26x26xbf16>
    %1 = stablehlo.reshape %arg1 : (tensor<32xbf16>) -> tensor<32x1x1xbf16>
    %2 = stablehlo.broadcast_in_dim %0, dims = [0, 1, 2, 3] : (tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %3 = stablehlo.broadcast_in_dim %1, dims = [1, 2, 3] : (tensor<32x1x1xbf16>) -> tensor<1x32x26x26xbf16>
    %4 = stablehlo.add %2, %3 : tensor<1x32x26x26xbf16>
    %5 = stablehlo.maximum %4, %cst : tensor<1x32x26x26xbf16>
    %6 = stablehlo.convolution(%5, %arg2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[0, 0], [0, 0]], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>) -> tensor<1x64x24x24xbf16>
    %7 = stablehlo.reshape %arg3 : (tensor<64xbf16>) -> tensor<64x1x1xbf16>
    %8 = stablehlo.broadcast_in_dim %6, dims = [0, 1, 2, 3] : (tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %9 = stablehlo.broadcast_in_dim %7, dims = [1, 2, 3] : (tensor<64x1x1xbf16>) -> tensor<1x64x24x24xbf16>
    %10 = stablehlo.add %8, %9 : tensor<1x64x24x24xbf16>
    %11 = stablehlo.maximum %10, %cst_0 : tensor<1x64x24x24xbf16>
    %12 = "stablehlo.reduce_window"(%11, %cst_1) <{padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> ({
    ^bb0(%arg13: tensor<bf16>, %arg14: tensor<bf16>):
      %49 = stablehlo.maximum %arg13, %arg14 : tensor<bf16>
      stablehlo.return %49 : tensor<bf16>
    }) : (tensor<1x64x24x24xbf16>, tensor<bf16>) -> tensor<1x64x12x12xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x64x12x12xbf16>) -> tensor<1x9216xbf16>
    %14 = stablehlo.convert %13 : (tensor<1x9216xbf16>) -> tensor<1x9216xf32>
    %15 = stablehlo.dot_general %14, %arg4, contracting_dims = [1] x [0] : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %16 = stablehlo.convert %c : (tensor<1xi64>) -> tensor<1xf32>
    %17 = stablehlo.reshape %16 : (tensor<1xf32>) -> tensor<f32>
    %18 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %19 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x128xf32>
    %20 = stablehlo.multiply %18, %19 : tensor<1x128xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<128xf32>) -> tensor<1x128xf32>
    %23 = stablehlo.add %21, %22 : tensor<1x128xf32>
    %24 = stablehlo.convert %23 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %25 = stablehlo.maximum %24, %cst_2 : tensor<1x128xbf16>
    %26 = stablehlo.convert %25 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %27 = stablehlo.dot_general %26, %arg6, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %29 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<1x10xf32>
    %30 = stablehlo.multiply %28, %29 : tensor<1x10xf32>
    %31 = stablehlo.broadcast_in_dim %30, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %32 = stablehlo.broadcast_in_dim %arg7, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>
    %33 = stablehlo.add %31, %32 : tensor<1x10xf32>
    %34 = stablehlo.convert %33 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x10xbf16>) -> tensor<1x10xf32>
    %36 = stablehlo.reduce(%35 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %37 = stablehlo.reshape %36 : (tensor<1xf32>) -> tensor<1x1xf32>
    %38 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %39 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %40 = stablehlo.subtract %38, %39 : tensor<1x10xf32>
    %41 = stablehlo.exponential %40 : tensor<1x10xf32>
    %42 = stablehlo.reduce(%41 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
    %43 = stablehlo.reshape %42 : (tensor<1xf32>) -> tensor<1x1xf32>
    %44 = stablehlo.log %43 : tensor<1x1xf32>
    %45 = stablehlo.broadcast_in_dim %40, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<1x10xf32>
    %46 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32>
    %47 = stablehlo.subtract %45, %46 : tensor<1x10xf32>
    %48 = stablehlo.convert %47 : (tensor<1x10xf32>) -> tensor<1x10xbf16>
    return %48 : tensor<1x10xbf16>
  }
}


// -----// IR Dump After ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0xFF80> : tensor<1xbf16>}> : () -> tensor<1xbf16>
    %3 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %4 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<1xf32>}> : () -> tensor<1xf32>
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1xf32>}> : () -> tensor<1xf32>
    %6 = "ttir.constant"() <{value = dense<1> : tensor<1xi64>}> : () -> tensor<1xi64>
    %7 = ttir.empty() : tensor<1x32x26x26xbf16>
    %8 = "ttir.convolution"(%arg12, %arg0, %7) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %9 = ttir.empty() : tensor<32x1x1xbf16>
    %10 = "ttir.reshape"(%arg1, %9) <{shape = [32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<32x1x1xbf16>) -> tensor<32x1x1xbf16>
    %11 = ttir.empty() : tensor<1x32x26x26xbf16>
    %12 = "ttir.broadcast"(%8, %11) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %13 = ttir.empty() : tensor<1x32x1x1xbf16>
    %14 = "ttir.reshape"(%10, %13) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x1x1xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %15 = ttir.empty() : tensor<1x32x26x26xbf16>
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %17 = ttir.empty() : tensor<1x32x26x26xbf16>
    %18 = "ttir.add"(%12, %16, %17) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %19 = ttir.empty() : tensor<1x32x26x26xbf16>
    %20 = "ttir.maximum"(%18, %0, %19) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %21 = ttir.empty() : tensor<1x64x24x24xbf16>
    %22 = "ttir.convolution"(%20, %arg2, %21) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %23 = ttir.empty() : tensor<64x1x1xbf16>
    %24 = "ttir.reshape"(%arg3, %23) <{shape = [64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<64x1x1xbf16>) -> tensor<64x1x1xbf16>
    %25 = ttir.empty() : tensor<1x64x24x24xbf16>
    %26 = "ttir.broadcast"(%22, %25) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %27 = ttir.empty() : tensor<1x64x1x1xbf16>
    %28 = "ttir.reshape"(%24, %27) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64x1x1xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %29 = ttir.empty() : tensor<1x64x24x24xbf16>
    %30 = "ttir.broadcast"(%28, %29) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %31 = ttir.empty() : tensor<1x64x24x24xbf16>
    %32 = "ttir.add"(%26, %30, %31) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %33 = ttir.empty() : tensor<1x64x24x24xbf16>
    %34 = "ttir.maximum"(%32, %1, %33) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %35 = ttir.empty() : tensor<1x64x12x12xbf16>
    %36 = "ttir.pooling"(%34, %35) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %37 = ttir.empty() : tensor<1x9216xbf16>
    %38 = "ttir.reshape"(%36, %37) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %39 = ttir.empty() : tensor<1x9216xf32>
    %40 = "ttir.typecast"(%38, %39) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %41 = "ttir.dot_general"(%40, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %42 = ttir.empty() : tensor<1xf32>
    %43 = "ttir.typecast"(%6, %42) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32>
    %44 = ttir.empty() : tensor<1xf32>
    %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32]}> : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
    %46 = ttir.empty() : tensor<1x128xf32>
    %47 = "ttir.broadcast"(%41, %46) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %48 = ttir.empty() : tensor<1x1xf32>
    %49 = "ttir.reshape"(%45, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %50 = ttir.empty() : tensor<1x128xf32>
    %51 = "ttir.broadcast"(%49, %50) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %52 = ttir.empty() : tensor<1x128xf32>
    %53 = "ttir.multiply"(%47, %51, %52) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %54 = ttir.empty() : tensor<1x128xf32>
    %55 = "ttir.broadcast"(%53, %54) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %56 = ttir.empty() : tensor<1x128xf32>
    %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %58 = ttir.empty() : tensor<1x128xf32>
    %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %60 = ttir.empty() : tensor<1x128xf32>
    %61 = "ttir.add"(%55, %59, %60) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %62 = ttir.empty() : tensor<1x128xbf16>
    %63 = "ttir.typecast"(%61, %62) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %64 = ttir.empty() : tensor<1x128xbf16>
    %65 = "ttir.maximum"(%63, %3, %64) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %66 = ttir.empty() : tensor<1x128xf32>
    %67 = "ttir.typecast"(%65, %66) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %68 = "ttir.dot_general"(%67, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %69 = ttir.empty() : tensor<1x10xf32>
    %70 = "ttir.broadcast"(%68, %69) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %71 = ttir.empty() : tensor<1x1xf32>
    %72 = "ttir.reshape"(%45, %71) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %73 = ttir.empty() : tensor<1x10xf32>
    %74 = "ttir.broadcast"(%72, %73) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %75 = ttir.empty() : tensor<1x10xf32>
    %76 = "ttir.multiply"(%70, %74, %75) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %77 = ttir.empty() : tensor<1x10xf32>
    %78 = "ttir.broadcast"(%76, %77) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %79 = ttir.empty() : tensor<1x10xf32>
    %80 = "ttir.reshape"(%arg7, %79) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %81 = ttir.empty() : tensor<1x10xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %83 = ttir.empty() : tensor<1x10xf32>
    %84 = "ttir.add"(%78, %82, %83) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %85 = ttir.empty() : tensor<1x10xbf16>
    %86 = "ttir.typecast"(%84, %85) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    %87 = ttir.empty() : tensor<1x10xf32>
    %88 = "ttir.typecast"(%86, %87) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xbf16>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %89 = ttir.empty() : tensor<1xf32>
    %90 = "ttir.max"(%88, %89) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %91 = ttir.empty() : tensor<1x1xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %93 = ttir.empty() : tensor<1x10xf32>
    %94 = "ttir.broadcast"(%88, %93) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %95 = ttir.empty() : tensor<1x10xf32>
    %96 = "ttir.broadcast"(%92, %95) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %97 = ttir.empty() : tensor<1x10xf32>
    %98 = "ttir.subtract"(%94, %96, %97) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %99 = ttir.empty() : tensor<1x10xf32>
    %100 = "ttir.exp"(%98, %99) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %101 = ttir.empty() : tensor<1xf32>
    %102 = "ttir.sum"(%100, %101) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %103 = ttir.empty() : tensor<1x1xf32>
    %104 = "ttir.reshape"(%102, %103) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %105 = ttir.empty() : tensor<1x1xf32>
    %106 = "ttir.log"(%104, %105) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %107 = ttir.empty() : tensor<1x10xf32>
    %108 = "ttir.broadcast"(%98, %107) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %109 = ttir.empty() : tensor<1x10xf32>
    %110 = "ttir.broadcast"(%106, %109) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %111 = ttir.empty() : tensor<1x10xf32>
    %112 = "ttir.subtract"(%108, %110, %111) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %113 = ttir.empty() : tensor<1x10xbf16>
    %114 = "ttir.typecast"(%112, %113) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %114 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before mlir::tt::TTIRTensorAnnotationCleanupPass (shardy-tensor-annotation-cleanup) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0xFF80> : tensor<1xbf16>}> : () -> tensor<1xbf16>
    %3 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %4 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<1xf32>}> : () -> tensor<1xf32>
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1xf32>}> : () -> tensor<1xf32>
    %6 = "ttir.constant"() <{value = dense<1> : tensor<1xi64>}> : () -> tensor<1xi64>
    %7 = ttir.empty() : tensor<1x32x26x26xbf16>
    %8 = "ttir.convolution"(%arg12, %arg0, %7) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %9 = ttir.empty() : tensor<32x1x1xbf16>
    %10 = "ttir.reshape"(%arg1, %9) <{shape = [32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<32x1x1xbf16>) -> tensor<32x1x1xbf16>
    %11 = ttir.empty() : tensor<1x32x26x26xbf16>
    %12 = "ttir.broadcast"(%8, %11) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %13 = ttir.empty() : tensor<1x32x1x1xbf16>
    %14 = "ttir.reshape"(%10, %13) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x1x1xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %15 = ttir.empty() : tensor<1x32x26x26xbf16>
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %17 = ttir.empty() : tensor<1x32x26x26xbf16>
    %18 = "ttir.add"(%12, %16, %17) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %19 = ttir.empty() : tensor<1x32x26x26xbf16>
    %20 = "ttir.maximum"(%18, %0, %19) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %21 = ttir.empty() : tensor<1x64x24x24xbf16>
    %22 = "ttir.convolution"(%20, %arg2, %21) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %23 = ttir.empty() : tensor<64x1x1xbf16>
    %24 = "ttir.reshape"(%arg3, %23) <{shape = [64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<64x1x1xbf16>) -> tensor<64x1x1xbf16>
    %25 = ttir.empty() : tensor<1x64x24x24xbf16>
    %26 = "ttir.broadcast"(%22, %25) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %27 = ttir.empty() : tensor<1x64x1x1xbf16>
    %28 = "ttir.reshape"(%24, %27) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64x1x1xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %29 = ttir.empty() : tensor<1x64x24x24xbf16>
    %30 = "ttir.broadcast"(%28, %29) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %31 = ttir.empty() : tensor<1x64x24x24xbf16>
    %32 = "ttir.add"(%26, %30, %31) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %33 = ttir.empty() : tensor<1x64x24x24xbf16>
    %34 = "ttir.maximum"(%32, %1, %33) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %35 = ttir.empty() : tensor<1x64x12x12xbf16>
    %36 = "ttir.pooling"(%34, %35) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %37 = ttir.empty() : tensor<1x9216xbf16>
    %38 = "ttir.reshape"(%36, %37) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %39 = ttir.empty() : tensor<1x9216xf32>
    %40 = "ttir.typecast"(%38, %39) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %41 = "ttir.dot_general"(%40, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %42 = ttir.empty() : tensor<1xf32>
    %43 = "ttir.typecast"(%6, %42) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32>
    %44 = ttir.empty() : tensor<1xf32>
    %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32]}> : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
    %46 = ttir.empty() : tensor<1x128xf32>
    %47 = "ttir.broadcast"(%41, %46) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %48 = ttir.empty() : tensor<1x1xf32>
    %49 = "ttir.reshape"(%45, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %50 = ttir.empty() : tensor<1x128xf32>
    %51 = "ttir.broadcast"(%49, %50) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %52 = ttir.empty() : tensor<1x128xf32>
    %53 = "ttir.multiply"(%47, %51, %52) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %54 = ttir.empty() : tensor<1x128xf32>
    %55 = "ttir.broadcast"(%53, %54) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %56 = ttir.empty() : tensor<1x128xf32>
    %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %58 = ttir.empty() : tensor<1x128xf32>
    %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %60 = ttir.empty() : tensor<1x128xf32>
    %61 = "ttir.add"(%55, %59, %60) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %62 = ttir.empty() : tensor<1x128xbf16>
    %63 = "ttir.typecast"(%61, %62) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %64 = ttir.empty() : tensor<1x128xbf16>
    %65 = "ttir.maximum"(%63, %3, %64) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %66 = ttir.empty() : tensor<1x128xf32>
    %67 = "ttir.typecast"(%65, %66) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %68 = "ttir.dot_general"(%67, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %69 = ttir.empty() : tensor<1x10xf32>
    %70 = "ttir.broadcast"(%68, %69) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %71 = ttir.empty() : tensor<1x1xf32>
    %72 = "ttir.reshape"(%45, %71) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %73 = ttir.empty() : tensor<1x10xf32>
    %74 = "ttir.broadcast"(%72, %73) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %75 = ttir.empty() : tensor<1x10xf32>
    %76 = "ttir.multiply"(%70, %74, %75) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %77 = ttir.empty() : tensor<1x10xf32>
    %78 = "ttir.broadcast"(%76, %77) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %79 = ttir.empty() : tensor<1x10xf32>
    %80 = "ttir.reshape"(%arg7, %79) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %81 = ttir.empty() : tensor<1x10xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %83 = ttir.empty() : tensor<1x10xf32>
    %84 = "ttir.add"(%78, %82, %83) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %85 = ttir.empty() : tensor<1x10xbf16>
    %86 = "ttir.typecast"(%84, %85) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    %87 = ttir.empty() : tensor<1x10xf32>
    %88 = "ttir.typecast"(%86, %87) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xbf16>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %89 = ttir.empty() : tensor<1xf32>
    %90 = "ttir.max"(%88, %89) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %91 = ttir.empty() : tensor<1x1xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %93 = ttir.empty() : tensor<1x10xf32>
    %94 = "ttir.broadcast"(%88, %93) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %95 = ttir.empty() : tensor<1x10xf32>
    %96 = "ttir.broadcast"(%92, %95) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %97 = ttir.empty() : tensor<1x10xf32>
    %98 = "ttir.subtract"(%94, %96, %97) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %99 = ttir.empty() : tensor<1x10xf32>
    %100 = "ttir.exp"(%98, %99) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %101 = ttir.empty() : tensor<1xf32>
    %102 = "ttir.sum"(%100, %101) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %103 = ttir.empty() : tensor<1x1xf32>
    %104 = "ttir.reshape"(%102, %103) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %105 = ttir.empty() : tensor<1x1xf32>
    %106 = "ttir.log"(%104, %105) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %107 = ttir.empty() : tensor<1x10xf32>
    %108 = "ttir.broadcast"(%98, %107) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %109 = ttir.empty() : tensor<1x10xf32>
    %110 = "ttir.broadcast"(%106, %109) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %111 = ttir.empty() : tensor<1x10xf32>
    %112 = "ttir.subtract"(%108, %110, %111) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %113 = ttir.empty() : tensor<1x10xbf16>
    %114 = "ttir.typecast"(%112, %113) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %114 : tensor<1x10xbf16>
  }
}


TTIR module module
#loc1 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":31:0)
#loc2 = loc("convolution")
#loc36 = loc(fused[#loc1, #loc2])
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16> loc(fused[#loc1, #loc2]), %arg1: tensor<32xbf16> loc(fused[#loc1, #loc2]), %arg2: tensor<64x32x3x3xbf16> loc(fused[#loc1, #loc2]), %arg3: tensor<64xbf16> loc(fused[#loc1, #loc2]), %arg4: tensor<9216x128xf32> loc(fused[#loc1, #loc2]), %arg5: tensor<128xf32> loc(fused[#loc1, #loc2]), %arg6: tensor<128x10xf32> loc(fused[#loc1, #loc2]), %arg7: tensor<10xf32> loc(fused[#loc1, #loc2]), %arg8: tensor<128x9216xbf16> loc(fused[#loc1, #loc2]), %arg9: tensor<128xbf16> loc(fused[#loc1, #loc2]), %arg10: tensor<10x128xbf16> loc(fused[#loc1, #loc2]), %arg11: tensor<10xbf16> loc(fused[#loc1, #loc2]), %arg12: tensor<1x1x28x28xbf16> loc(fused[#loc1, #loc2])) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16> loc(#loc)
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16> loc(#loc)
    %2 = "ttir.constant"() <{value = dense<0xFF80> : tensor<1xbf16>}> : () -> tensor<1xbf16> loc(#loc)
    %3 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16> loc(#loc)
    %4 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<1xf32>}> : () -> tensor<1xf32> loc(#loc)
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1xf32>}> : () -> tensor<1xf32> loc(#loc)
    %6 = "ttir.constant"() <{value = dense<1> : tensor<1xi64>}> : () -> tensor<1xi64> loc(#loc)
    %7 = ttir.empty() : tensor<1x32x26x26xbf16> loc(#loc36)
    %8 = "ttir.convolution"(%arg12, %arg0, %7) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16> loc(#loc36)
    %9 = ttir.empty() : tensor<32x1x1xbf16> loc(#loc36)
    %10 = "ttir.reshape"(%arg1, %9) <{shape = [32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<32x1x1xbf16>) -> tensor<32x1x1xbf16> loc(#loc36)
    %11 = ttir.empty() : tensor<1x32x26x26xbf16> loc(#loc36)
    %12 = "ttir.broadcast"(%8, %11) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16> loc(#loc36)
    %13 = ttir.empty() : tensor<1x32x1x1xbf16> loc(#loc36)
    %14 = "ttir.reshape"(%10, %13) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x1x1xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16> loc(#loc36)
    %15 = ttir.empty() : tensor<1x32x26x26xbf16> loc(#loc36)
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16> loc(#loc36)
    %17 = ttir.empty() : tensor<1x32x26x26xbf16> loc(#loc36)
    %18 = "ttir.add"(%12, %16, %17) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16> loc(#loc36)
    %19 = ttir.empty() : tensor<1x32x26x26xbf16> loc(#loc37)
    %20 = "ttir.maximum"(%18, %0, %19) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16> loc(#loc37)
    %21 = ttir.empty() : tensor<1x64x24x24xbf16> loc(#loc38)
    %22 = "ttir.convolution"(%20, %arg2, %21) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16> loc(#loc38)
    %23 = ttir.empty() : tensor<64x1x1xbf16> loc(#loc38)
    %24 = "ttir.reshape"(%arg3, %23) <{shape = [64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<64x1x1xbf16>) -> tensor<64x1x1xbf16> loc(#loc38)
    %25 = ttir.empty() : tensor<1x64x24x24xbf16> loc(#loc38)
    %26 = "ttir.broadcast"(%22, %25) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16> loc(#loc38)
    %27 = ttir.empty() : tensor<1x64x1x1xbf16> loc(#loc38)
    %28 = "ttir.reshape"(%24, %27) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64x1x1xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16> loc(#loc38)
    %29 = ttir.empty() : tensor<1x64x24x24xbf16> loc(#loc38)
    %30 = "ttir.broadcast"(%28, %29) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16> loc(#loc38)
    %31 = ttir.empty() : tensor<1x64x24x24xbf16> loc(#loc38)
    %32 = "ttir.add"(%26, %30, %31) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16> loc(#loc38)
    %33 = ttir.empty() : tensor<1x64x24x24xbf16> loc(#loc39)
    %34 = "ttir.maximum"(%32, %1, %33) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16> loc(#loc39)
    %35 = ttir.empty() : tensor<1x64x12x12xbf16> loc(#loc40)
    %36 = "ttir.pooling"(%34, %35) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16> loc(#loc40)
    %37 = ttir.empty() : tensor<1x9216xbf16> loc(#loc41)
    %38 = "ttir.reshape"(%36, %37) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16> loc(#loc41)
    %39 = ttir.empty() : tensor<1x9216xf32> loc(#loc42)
    %40 = "ttir.typecast"(%38, %39) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32> loc(#loc42)
    %41 = "ttir.dot_general"(%40, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32> loc(#loc43)
    %42 = ttir.empty() : tensor<1xf32> loc(#loc44)
    %43 = "ttir.typecast"(%6, %42) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32> loc(#loc44)
    %44 = ttir.empty() : tensor<1xf32> loc(#loc44)
    %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32]}> : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32> loc(#loc44)
    %46 = ttir.empty() : tensor<1x128xf32> loc(#loc44)
    %47 = "ttir.broadcast"(%41, %46) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc44)
    %48 = ttir.empty() : tensor<1x1xf32> loc(#loc44)
    %49 = "ttir.reshape"(%45, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc44)
    %50 = ttir.empty() : tensor<1x128xf32> loc(#loc44)
    %51 = "ttir.broadcast"(%49, %50) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc44)
    %52 = ttir.empty() : tensor<1x128xf32> loc(#loc44)
    %53 = "ttir.multiply"(%47, %51, %52) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc44)
    %54 = ttir.empty() : tensor<1x128xf32> loc(#loc45)
    %55 = "ttir.broadcast"(%53, %54) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc45)
    %56 = ttir.empty() : tensor<1x128xf32> loc(#loc45)
    %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc45)
    %58 = ttir.empty() : tensor<1x128xf32> loc(#loc45)
    %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc45)
    %60 = ttir.empty() : tensor<1x128xf32> loc(#loc45)
    %61 = "ttir.add"(%55, %59, %60) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc45)
    %62 = ttir.empty() : tensor<1x128xbf16> loc(#loc46)
    %63 = "ttir.typecast"(%61, %62) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc46)
    %64 = ttir.empty() : tensor<1x128xbf16> loc(#loc47)
    %65 = "ttir.maximum"(%63, %3, %64) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc47)
    %66 = ttir.empty() : tensor<1x128xf32> loc(#loc48)
    %67 = "ttir.typecast"(%65, %66) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc48)
    %68 = "ttir.dot_general"(%67, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32> loc(#loc49)
    %69 = ttir.empty() : tensor<1x10xf32> loc(#loc50)
    %70 = "ttir.broadcast"(%68, %69) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc50)
    %71 = ttir.empty() : tensor<1x1xf32> loc(#loc50)
    %72 = "ttir.reshape"(%45, %71) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc50)
    %73 = ttir.empty() : tensor<1x10xf32> loc(#loc50)
    %74 = "ttir.broadcast"(%72, %73) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc50)
    %75 = ttir.empty() : tensor<1x10xf32> loc(#loc50)
    %76 = "ttir.multiply"(%70, %74, %75) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc50)
    %77 = ttir.empty() : tensor<1x10xf32> loc(#loc51)
    %78 = "ttir.broadcast"(%76, %77) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc51)
    %79 = ttir.empty() : tensor<1x10xf32> loc(#loc51)
    %80 = "ttir.reshape"(%arg7, %79) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc51)
    %81 = ttir.empty() : tensor<1x10xf32> loc(#loc51)
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc51)
    %83 = ttir.empty() : tensor<1x10xf32> loc(#loc51)
    %84 = "ttir.add"(%78, %82, %83) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc51)
    %85 = ttir.empty() : tensor<1x10xbf16> loc(#loc52)
    %86 = "ttir.typecast"(%84, %85) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16> loc(#loc52)
    %87 = ttir.empty() : tensor<1x10xf32> loc(#loc53)
    %88 = "ttir.typecast"(%86, %87) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xbf16>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc53)
    %89 = ttir.empty() : tensor<1xf32> loc(#loc54)
    %90 = "ttir.max"(%88, %89) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32> loc(#loc54)
    %91 = ttir.empty() : tensor<1x1xf32> loc(#loc54)
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc54)
    %93 = ttir.empty() : tensor<1x10xf32> loc(#loc55)
    %94 = "ttir.broadcast"(%88, %93) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc55)
    %95 = ttir.empty() : tensor<1x10xf32> loc(#loc55)
    %96 = "ttir.broadcast"(%92, %95) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc55)
    %97 = ttir.empty() : tensor<1x10xf32> loc(#loc55)
    %98 = "ttir.subtract"(%94, %96, %97) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc55)
    %99 = ttir.empty() : tensor<1x10xf32> loc(#loc56)
    %100 = "ttir.exp"(%98, %99) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc56)
    %101 = ttir.empty() : tensor<1xf32> loc(#loc57)
    %102 = "ttir.sum"(%100, %101) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32> loc(#loc57)
    %103 = ttir.empty() : tensor<1x1xf32> loc(#loc57)
    %104 = "ttir.reshape"(%102, %103) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc57)
    %105 = ttir.empty() : tensor<1x1xf32> loc(#loc58)
    %106 = "ttir.log"(%104, %105) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc58)
    %107 = ttir.empty() : tensor<1x10xf32> loc(#loc59)
    %108 = "ttir.broadcast"(%98, %107) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc59)
    %109 = ttir.empty() : tensor<1x10xf32> loc(#loc59)
    %110 = "ttir.broadcast"(%106, %109) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc59)
    %111 = ttir.empty() : tensor<1x10xf32> loc(#loc59)
    %112 = "ttir.subtract"(%108, %110, %111) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc59)
    %113 = ttir.empty() : tensor<1x10xbf16> loc(#loc60)
    %114 = "ttir.typecast"(%112, %113) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16> loc(#loc60)
    return %114 : tensor<1x10xbf16> loc(#loc36)
  } loc(#loc36)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":32:0)
#loc4 = loc("relu")
#loc5 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":33:0)
#loc6 = loc("convolution_1")
#loc7 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":34:0)
#loc8 = loc("relu_1")
#loc9 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":35:0)
#loc10 = loc("max_pool2d_with_indices")
#loc11 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":37:0)
#loc12 = loc("view")
#loc13 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":38:0)
#loc14 = loc("convert_element_type")
#loc15 = loc("mm")
#loc16 = loc("mul")
#loc17 = loc("add")
#loc18 = loc("convert_element_type_1")
#loc19 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":39:0)
#loc20 = loc("relu_2")
#loc21 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":41:0)
#loc22 = loc("convert_element_type_2")
#loc23 = loc("mm_1")
#loc24 = loc("mul_1")
#loc25 = loc("add_1")
#loc26 = loc("convert_element_type_3")
#loc27 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":42:0)
#loc28 = loc("convert_element_type_4")
#loc29 = loc("amax")
#loc30 = loc("sub")
#loc31 = loc("exp")
#loc32 = loc("sum_1")
#loc33 = loc("log")
#loc34 = loc("sub_1")
#loc35 = loc("convert_element_type_5")
#loc37 = loc(fused[#loc3, #loc4])
#loc38 = loc(fused[#loc5, #loc6])
#loc39 = loc(fused[#loc7, #loc8])
#loc40 = loc(fused[#loc9, #loc10])
#loc41 = loc(fused[#loc11, #loc12])
#loc42 = loc(fused[#loc13, #loc14])
#loc43 = loc(fused[#loc13, #loc15])
#loc44 = loc(fused[#loc13, #loc16])
#loc45 = loc(fused[#loc13, #loc17])
#loc46 = loc(fused[#loc13, #loc18])
#loc47 = loc(fused[#loc19, #loc20])
#loc48 = loc(fused[#loc21, #loc22])
#loc49 = loc(fused[#loc21, #loc23])
#loc50 = loc(fused[#loc21, #loc24])
#loc51 = loc(fused[#loc21, #loc25])
#loc52 = loc(fused[#loc21, #loc26])
#loc53 = loc(fused[#loc27, #loc28])
#loc54 = loc(fused[#loc27, #loc29])
#loc55 = loc(fused[#loc27, #loc30])
#loc56 = loc(fused[#loc27, #loc31])
#loc57 = loc(fused[#loc27, #loc32])
#loc58 = loc(fused[#loc27, #loc33])
#loc59 = loc(fused[#loc27, #loc34])
#loc60 = loc(fused[#loc27, #loc35])

// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0xFF80> : tensor<1xbf16>}> : () -> tensor<1xbf16>
    %3 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %4 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<1xf32>}> : () -> tensor<1xf32>
    %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1xf32>}> : () -> tensor<1xf32>
    %6 = "ttir.constant"() <{value = dense<1> : tensor<1xi64>}> : () -> tensor<1xi64>
    %7 = ttir.empty() : tensor<1x32x26x26xbf16>
    %8 = "ttir.convolution"(%arg12, %arg0, %7) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %9 = ttir.empty() : tensor<32x1x1xbf16>
    %10 = "ttir.reshape"(%arg1, %9) <{shape = [32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<32x1x1xbf16>) -> tensor<32x1x1xbf16>
    %11 = ttir.empty() : tensor<1x32x26x26xbf16>
    %12 = "ttir.broadcast"(%8, %11) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %13 = ttir.empty() : tensor<1x32x1x1xbf16>
    %14 = "ttir.reshape"(%10, %13) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x1x1xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %15 = ttir.empty() : tensor<1x32x26x26xbf16>
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %17 = ttir.empty() : tensor<1x32x26x26xbf16>
    %18 = "ttir.add"(%12, %16, %17) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %19 = ttir.empty() : tensor<1x32x26x26xbf16>
    %20 = "ttir.maximum"(%18, %0, %19) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %21 = ttir.empty() : tensor<1x64x24x24xbf16>
    %22 = "ttir.convolution"(%20, %arg2, %21) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %23 = ttir.empty() : tensor<64x1x1xbf16>
    %24 = "ttir.reshape"(%arg3, %23) <{shape = [64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<64x1x1xbf16>) -> tensor<64x1x1xbf16>
    %25 = ttir.empty() : tensor<1x64x24x24xbf16>
    %26 = "ttir.broadcast"(%22, %25) <{broadcast_dimensions = array<i64: 1, 1, 1, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %27 = ttir.empty() : tensor<1x64x1x1xbf16>
    %28 = "ttir.reshape"(%24, %27) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64x1x1xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %29 = ttir.empty() : tensor<1x64x24x24xbf16>
    %30 = "ttir.broadcast"(%28, %29) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %31 = ttir.empty() : tensor<1x64x24x24xbf16>
    %32 = "ttir.add"(%26, %30, %31) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %33 = ttir.empty() : tensor<1x64x24x24xbf16>
    %34 = "ttir.maximum"(%32, %1, %33) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %35 = ttir.empty() : tensor<1x64x12x12xbf16>
    %36 = "ttir.pooling"(%34, %35) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %37 = ttir.empty() : tensor<1x9216xbf16>
    %38 = "ttir.reshape"(%36, %37) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %39 = ttir.empty() : tensor<1x9216xf32>
    %40 = "ttir.typecast"(%38, %39) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %41 = "ttir.dot_general"(%40, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %42 = ttir.empty() : tensor<1xf32>
    %43 = "ttir.typecast"(%6, %42) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32>
    %44 = ttir.empty() : tensor<1xf32>
    %45 = "ttir.reshape"(%43, %44) <{shape = [1 : i32]}> : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
    %46 = ttir.empty() : tensor<1x128xf32>
    %47 = "ttir.broadcast"(%41, %46) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %48 = ttir.empty() : tensor<1x1xf32>
    %49 = "ttir.reshape"(%45, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %50 = ttir.empty() : tensor<1x128xf32>
    %51 = "ttir.broadcast"(%49, %50) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %52 = ttir.empty() : tensor<1x128xf32>
    %53 = "ttir.multiply"(%47, %51, %52) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %54 = ttir.empty() : tensor<1x128xf32>
    %55 = "ttir.broadcast"(%53, %54) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %56 = ttir.empty() : tensor<1x128xf32>
    %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %58 = ttir.empty() : tensor<1x128xf32>
    %59 = "ttir.broadcast"(%57, %58) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %60 = ttir.empty() : tensor<1x128xf32>
    %61 = "ttir.add"(%55, %59, %60) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %62 = ttir.empty() : tensor<1x128xbf16>
    %63 = "ttir.typecast"(%61, %62) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %64 = ttir.empty() : tensor<1x128xbf16>
    %65 = "ttir.maximum"(%63, %3, %64) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %66 = ttir.empty() : tensor<1x128xf32>
    %67 = "ttir.typecast"(%65, %66) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %68 = "ttir.dot_general"(%67, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %69 = ttir.empty() : tensor<1x10xf32>
    %70 = "ttir.broadcast"(%68, %69) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %71 = ttir.empty() : tensor<1x1xf32>
    %72 = "ttir.reshape"(%45, %71) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %73 = ttir.empty() : tensor<1x10xf32>
    %74 = "ttir.broadcast"(%72, %73) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %75 = ttir.empty() : tensor<1x10xf32>
    %76 = "ttir.multiply"(%70, %74, %75) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %77 = ttir.empty() : tensor<1x10xf32>
    %78 = "ttir.broadcast"(%76, %77) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %79 = ttir.empty() : tensor<1x10xf32>
    %80 = "ttir.reshape"(%arg7, %79) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %81 = ttir.empty() : tensor<1x10xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %83 = ttir.empty() : tensor<1x10xf32>
    %84 = "ttir.add"(%78, %82, %83) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %85 = ttir.empty() : tensor<1x10xbf16>
    %86 = "ttir.typecast"(%84, %85) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    %87 = ttir.empty() : tensor<1x10xf32>
    %88 = "ttir.typecast"(%86, %87) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xbf16>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %89 = ttir.empty() : tensor<1xf32>
    %90 = "ttir.max"(%88, %89) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %91 = ttir.empty() : tensor<1x1xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %93 = ttir.empty() : tensor<1x10xf32>
    %94 = "ttir.broadcast"(%88, %93) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %95 = ttir.empty() : tensor<1x10xf32>
    %96 = "ttir.broadcast"(%92, %95) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %97 = ttir.empty() : tensor<1x10xf32>
    %98 = "ttir.subtract"(%94, %96, %97) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %99 = ttir.empty() : tensor<1x10xf32>
    %100 = "ttir.exp"(%98, %99) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %101 = ttir.empty() : tensor<1xf32>
    %102 = "ttir.sum"(%100, %101) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %103 = ttir.empty() : tensor<1x1xf32>
    %104 = "ttir.reshape"(%102, %103) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %105 = ttir.empty() : tensor<1x1xf32>
    %106 = "ttir.log"(%104, %105) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %107 = ttir.empty() : tensor<1x10xf32>
    %108 = "ttir.broadcast"(%98, %107) <{broadcast_dimensions = array<i64: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %109 = ttir.empty() : tensor<1x10xf32>
    %110 = "ttir.broadcast"(%106, %109) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %111 = ttir.empty() : tensor<1x10xf32>
    %112 = "ttir.subtract"(%108, %110, %111) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %113 = ttir.empty() : tensor<1x10xbf16>
    %114 = "ttir.typecast"(%112, %113) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %114 : tensor<1x10xbf16>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %3 = "ttir.constant"() <{value = dense<1> : tensor<1xi64>}> : () -> tensor<1xi64>
    %4 = ttir.empty() : tensor<1x32x26x26xbf16>
    %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %6 = ttir.empty() : tensor<1x32x1x1xbf16>
    %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %8 = ttir.empty() : tensor<1x32x26x26xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %10 = ttir.empty() : tensor<1x32x26x26xbf16>
    %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %12 = ttir.empty() : tensor<1x32x26x26xbf16>
    %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %14 = ttir.empty() : tensor<1x64x24x24xbf16>
    %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %16 = ttir.empty() : tensor<1x64x1x1xbf16>
    %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %18 = ttir.empty() : tensor<1x64x24x24xbf16>
    %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %20 = ttir.empty() : tensor<1x64x24x24xbf16>
    %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %22 = ttir.empty() : tensor<1x64x24x24xbf16>
    %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %24 = ttir.empty() : tensor<1x64x12x12xbf16>
    %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %26 = ttir.empty() : tensor<1x9216xbf16>
    %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %28 = ttir.empty() : tensor<1x9216xf32>
    %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %31 = ttir.empty() : tensor<1xf32>
    %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32>
    %33 = ttir.empty() : tensor<1x1xf32>
    %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %35 = ttir.empty() : tensor<1x128xf32>
    %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %37 = ttir.empty() : tensor<1x128xf32>
    %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %39 = ttir.empty() : tensor<1x128xf32>
    %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %41 = ttir.empty() : tensor<1x128xf32>
    %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %43 = ttir.empty() : tensor<1x128xbf16>
    %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %45 = ttir.empty() : tensor<1x128xbf16>
    %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %47 = ttir.empty() : tensor<1x128xf32>
    %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %50 = ttir.empty() : tensor<1x1xf32>
    %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %52 = ttir.empty() : tensor<1x10xf32>
    %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %54 = ttir.empty() : tensor<1x10xf32>
    %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %56 = ttir.empty() : tensor<1x10xf32>
    %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %58 = ttir.empty() : tensor<1x10xf32>
    %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %60 = ttir.empty() : tensor<1xf32>
    %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %62 = ttir.empty() : tensor<1x1xf32>
    %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %64 = ttir.empty() : tensor<1x10xf32>
    %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %66 = ttir.empty() : tensor<1x10xf32>
    %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %68 = ttir.empty() : tensor<1x10xf32>
    %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %70 = ttir.empty() : tensor<1xf32>
    %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %72 = ttir.empty() : tensor<1x1xf32>
    %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %74 = ttir.empty() : tensor<1x1xf32>
    %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %76 = ttir.empty() : tensor<1x10xf32>
    %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %78 = ttir.empty() : tensor<1x10xf32>
    %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %80 = ttir.empty() : tensor<1x10xbf16>
    %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %81 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before ElementTypeNormalization (ttir-element-type-normalization) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %3 = "ttir.constant"() <{value = dense<1> : tensor<1xi64>}> : () -> tensor<1xi64>
    %4 = ttir.empty() : tensor<1x32x26x26xbf16>
    %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %6 = ttir.empty() : tensor<1x32x1x1xbf16>
    %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %8 = ttir.empty() : tensor<1x32x26x26xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %10 = ttir.empty() : tensor<1x32x26x26xbf16>
    %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %12 = ttir.empty() : tensor<1x32x26x26xbf16>
    %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %14 = ttir.empty() : tensor<1x64x24x24xbf16>
    %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %16 = ttir.empty() : tensor<1x64x1x1xbf16>
    %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %18 = ttir.empty() : tensor<1x64x24x24xbf16>
    %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %20 = ttir.empty() : tensor<1x64x24x24xbf16>
    %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %22 = ttir.empty() : tensor<1x64x24x24xbf16>
    %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %24 = ttir.empty() : tensor<1x64x12x12xbf16>
    %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %26 = ttir.empty() : tensor<1x9216xbf16>
    %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %28 = ttir.empty() : tensor<1x9216xf32>
    %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %31 = ttir.empty() : tensor<1xf32>
    %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32>
    %33 = ttir.empty() : tensor<1x1xf32>
    %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %35 = ttir.empty() : tensor<1x128xf32>
    %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %37 = ttir.empty() : tensor<1x128xf32>
    %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %39 = ttir.empty() : tensor<1x128xf32>
    %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %41 = ttir.empty() : tensor<1x128xf32>
    %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %43 = ttir.empty() : tensor<1x128xbf16>
    %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %45 = ttir.empty() : tensor<1x128xbf16>
    %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %47 = ttir.empty() : tensor<1x128xf32>
    %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %50 = ttir.empty() : tensor<1x1xf32>
    %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %52 = ttir.empty() : tensor<1x10xf32>
    %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %54 = ttir.empty() : tensor<1x10xf32>
    %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %56 = ttir.empty() : tensor<1x10xf32>
    %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %58 = ttir.empty() : tensor<1x10xf32>
    %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %60 = ttir.empty() : tensor<1xf32>
    %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %62 = ttir.empty() : tensor<1x1xf32>
    %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %64 = ttir.empty() : tensor<1x10xf32>
    %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %66 = ttir.empty() : tensor<1x10xf32>
    %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %68 = ttir.empty() : tensor<1x10xf32>
    %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %70 = ttir.empty() : tensor<1xf32>
    %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %72 = ttir.empty() : tensor<1x1xf32>
    %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %74 = ttir.empty() : tensor<1x1xf32>
    %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %76 = ttir.empty() : tensor<1x10xf32>
    %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %78 = ttir.empty() : tensor<1x10xf32>
    %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %80 = ttir.empty() : tensor<1x10xbf16>
    %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %81 : tensor<1x10xbf16>
  }
}


// -----// IR Dump After ElementTypeNormalization (ttir-element-type-normalization) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
    %4 = ttir.empty() : tensor<1x32x26x26xbf16>
    %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %6 = ttir.empty() : tensor<1x32x1x1xbf16>
    %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %8 = ttir.empty() : tensor<1x32x26x26xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %10 = ttir.empty() : tensor<1x32x26x26xbf16>
    %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %12 = ttir.empty() : tensor<1x32x26x26xbf16>
    %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %14 = ttir.empty() : tensor<1x64x24x24xbf16>
    %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %16 = ttir.empty() : tensor<1x64x1x1xbf16>
    %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %18 = ttir.empty() : tensor<1x64x24x24xbf16>
    %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %20 = ttir.empty() : tensor<1x64x24x24xbf16>
    %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %22 = ttir.empty() : tensor<1x64x24x24xbf16>
    %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %24 = ttir.empty() : tensor<1x64x12x12xbf16>
    %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %26 = ttir.empty() : tensor<1x9216xbf16>
    %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %28 = ttir.empty() : tensor<1x9216xf32>
    %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %31 = ttir.empty() : tensor<1xf32>
    %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
    %33 = ttir.empty() : tensor<1x1xf32>
    %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %35 = ttir.empty() : tensor<1x128xf32>
    %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %37 = ttir.empty() : tensor<1x128xf32>
    %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %39 = ttir.empty() : tensor<1x128xf32>
    %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %41 = ttir.empty() : tensor<1x128xf32>
    %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %43 = ttir.empty() : tensor<1x128xbf16>
    %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %45 = ttir.empty() : tensor<1x128xbf16>
    %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %47 = ttir.empty() : tensor<1x128xf32>
    %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %50 = ttir.empty() : tensor<1x1xf32>
    %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %52 = ttir.empty() : tensor<1x10xf32>
    %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %54 = ttir.empty() : tensor<1x10xf32>
    %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %56 = ttir.empty() : tensor<1x10xf32>
    %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %58 = ttir.empty() : tensor<1x10xf32>
    %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %60 = ttir.empty() : tensor<1xf32>
    %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %62 = ttir.empty() : tensor<1x1xf32>
    %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %64 = ttir.empty() : tensor<1x10xf32>
    %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %66 = ttir.empty() : tensor<1x10xf32>
    %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %68 = ttir.empty() : tensor<1x10xf32>
    %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %70 = ttir.empty() : tensor<1xf32>
    %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %72 = ttir.empty() : tensor<1x1xf32>
    %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %74 = ttir.empty() : tensor<1x1xf32>
    %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %76 = ttir.empty() : tensor<1x10xf32>
    %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %78 = ttir.empty() : tensor<1x10xf32>
    %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %80 = ttir.empty() : tensor<1x10xbf16>
    %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %81 : tensor<1x10xbf16>
  }
}


// -----// IR Dump Before TTWrapDeviceModulePass (tt-wrap-device-module) ('builtin.module' operation) //----- //
module {
  func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
    %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
    %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
    %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
    %4 = ttir.empty() : tensor<1x32x26x26xbf16>
    %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %6 = ttir.empty() : tensor<1x32x1x1xbf16>
    %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
    %8 = ttir.empty() : tensor<1x32x26x26xbf16>
    %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %10 = ttir.empty() : tensor<1x32x26x26xbf16>
    %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %12 = ttir.empty() : tensor<1x32x26x26xbf16>
    %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
    %14 = ttir.empty() : tensor<1x64x24x24xbf16>
    %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %16 = ttir.empty() : tensor<1x64x1x1xbf16>
    %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
    %18 = ttir.empty() : tensor<1x64x24x24xbf16>
    %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %20 = ttir.empty() : tensor<1x64x24x24xbf16>
    %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %22 = ttir.empty() : tensor<1x64x24x24xbf16>
    %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
    %24 = ttir.empty() : tensor<1x64x12x12xbf16>
    %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
    %26 = ttir.empty() : tensor<1x9216xbf16>
    %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
    %28 = ttir.empty() : tensor<1x9216xf32>
    %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
    %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
    %31 = ttir.empty() : tensor<1xf32>
    %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
    %33 = ttir.empty() : tensor<1x1xf32>
    %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %35 = ttir.empty() : tensor<1x128xf32>
    %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %37 = ttir.empty() : tensor<1x128xf32>
    %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %39 = ttir.empty() : tensor<1x128xf32>
    %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %41 = ttir.empty() : tensor<1x128xf32>
    %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %43 = ttir.empty() : tensor<1x128xbf16>
    %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %45 = ttir.empty() : tensor<1x128xbf16>
    %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %47 = ttir.empty() : tensor<1x128xf32>
    %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
    %50 = ttir.empty() : tensor<1x1xf32>
    %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %52 = ttir.empty() : tensor<1x10xf32>
    %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %54 = ttir.empty() : tensor<1x10xf32>
    %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %56 = ttir.empty() : tensor<1x10xf32>
    %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %58 = ttir.empty() : tensor<1x10xf32>
    %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %60 = ttir.empty() : tensor<1xf32>
    %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %62 = ttir.empty() : tensor<1x1xf32>
    %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %64 = ttir.empty() : tensor<1x10xf32>
    %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %66 = ttir.empty() : tensor<1x10xf32>
    %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %68 = ttir.empty() : tensor<1x10xf32>
    %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %70 = ttir.empty() : tensor<1xf32>
    %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
    %72 = ttir.empty() : tensor<1x1xf32>
    %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %74 = ttir.empty() : tensor<1x1xf32>
    %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %76 = ttir.empty() : tensor<1x10xf32>
    %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %78 = ttir.empty() : tensor<1x10xf32>
    %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
    %80 = ttir.empty() : tensor<1x10xbf16>
    %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
    return %81 : tensor<1x10xbf16>
  }
}


// -----// IR Dump After TTWrapDeviceModulePass (tt-wrap-device-module) ('builtin.module' operation) //----- //
module {
  tt.device_module {
    builtin.module {
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRHoistTransform (ttir-cpu-hoist-transform) ('builtin.module' operation) //----- //
module {
  tt.device_module {
    builtin.module {
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTRegisterDevicePass (tt-register-device) ('builtin.module' operation) //----- //
module {
  tt.device_module {
    builtin.module {
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After TTRegisterDevicePass (tt-register-device) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = "ttir.convolution"(%arg12, %arg0, %4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1x28x28xbf16>, tensor<32x1x3x3xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %6 = ttir.empty() : tensor<1x32x1x1xbf16>
        %7 = "ttir.reshape"(%arg1, %6) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %8 = ttir.empty() : tensor<1x32x26x26xbf16>
        %9 = "ttir.broadcast"(%7, %8) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x26x26xbf16>
        %11 = "ttir.add"(%5, %9, %10) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.maximum"(%11, %0, %12) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x64x24x24xbf16>
        %15 = "ttir.convolution"(%13, %arg2, %14) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x32x26x26xbf16>, tensor<64x32x3x3xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %16 = ttir.empty() : tensor<1x64x1x1xbf16>
        %17 = "ttir.reshape"(%arg3, %16) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = "ttir.broadcast"(%17, %18) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = "ttir.add"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %22 = ttir.empty() : tensor<1x64x24x24xbf16>
        %23 = "ttir.maximum"(%21, %1, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x12x12xbf16>
        %25 = "ttir.pooling"(%23, %24) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 2, 2>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %26 = ttir.empty() : tensor<1x9216xbf16>
        %27 = "ttir.reshape"(%25, %26) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %28 = ttir.empty() : tensor<1x9216xf32>
        %29 = "ttir.typecast"(%27, %28) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %30 = "ttir.dot_general"(%29, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x9216xf32>, tensor<9216x128xf32>) -> tensor<1x128xf32>
        %31 = ttir.empty() : tensor<1xf32>
        %32 = "ttir.typecast"(%3, %31) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %33 = ttir.empty() : tensor<1x1xf32>
        %34 = "ttir.reshape"(%32, %33) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %35 = ttir.empty() : tensor<1x128xf32>
        %36 = "ttir.broadcast"(%34, %35) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %37 = ttir.empty() : tensor<1x128xf32>
        %38 = "ttir.multiply"(%30, %36, %37) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %39 = ttir.empty() : tensor<1x128xf32>
        %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %41 = ttir.empty() : tensor<1x128xf32>
        %42 = "ttir.add"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %43 = ttir.empty() : tensor<1x128xbf16>
        %44 = "ttir.typecast"(%42, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %45 = ttir.empty() : tensor<1x128xbf16>
        %46 = "ttir.maximum"(%44, %2, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %47 = ttir.empty() : tensor<1x128xf32>
        %48 = "ttir.typecast"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %49 = "ttir.dot_general"(%48, %arg6) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.reshape"(%32, %50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x10xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %54 = ttir.empty() : tensor<1x10xf32>
        %55 = "ttir.multiply"(%49, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %56 = ttir.empty() : tensor<1x10xf32>
        %57 = "ttir.reshape"(%arg7, %56) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %58 = ttir.empty() : tensor<1x10xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %60 = ttir.empty() : tensor<1xf32>
        %61 = "ttir.max"(%59, %60) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %62 = ttir.empty() : tensor<1x1xf32>
        %63 = "ttir.reshape"(%61, %62) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %64 = ttir.empty() : tensor<1x10xf32>
        %65 = "ttir.broadcast"(%63, %64) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.subtract"(%59, %65, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.exp"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1xf32>
        %71 = "ttir.sum"(%69, %70) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %72 = ttir.empty() : tensor<1x1xf32>
        %73 = "ttir.reshape"(%71, %72) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %74 = ttir.empty() : tensor<1x1xf32>
        %75 = "ttir.log"(%73, %74) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.broadcast"(%75, %76) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.subtract"(%67, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xbf16>
        %81 = "ttir.typecast"(%79, %80) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %81 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<32x1x3x3xbf16>
        %8 = "ttir.permute"(%arg0, %7) <{permutation = array<i64: 0, 1, 2, 3>}> : (tensor<32x1x3x3xbf16>, tensor<32x1x3x3xbf16>) -> tensor<32x1x3x3xbf16>
        %9 = ttir.empty() : tensor<1x26x26x32xbf16>
        %10 = "ttir.conv2d"(%6, %8, %9) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x28x28x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %11 = "ttir.permute"(%10, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x1x1xbf16>
        %13 = "ttir.reshape"(%arg1, %12) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %14 = ttir.empty() : tensor<1x32x26x26xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %16 = ttir.empty() : tensor<1x32x26x26xbf16>
        %17 = "ttir.add"(%11, %15, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %18 = ttir.empty() : tensor<1x32x26x26xbf16>
        %19 = "ttir.maximum"(%17, %0, %18) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = ttir.empty() : tensor<1x26x26x32xbf16>
        %22 = "ttir.permute"(%19, %21) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %23 = ttir.empty() : tensor<64x32x3x3xbf16>
        %24 = "ttir.permute"(%arg2, %23) <{permutation = array<i64: 0, 1, 2, 3>}> : (tensor<64x32x3x3xbf16>, tensor<64x32x3x3xbf16>) -> tensor<64x32x3x3xbf16>
        %25 = ttir.empty() : tensor<1x24x24x64xbf16>
        %26 = "ttir.conv2d"(%22, %24, %25) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x26x26x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %27 = "ttir.permute"(%26, %20) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %28 = ttir.empty() : tensor<1x64x1x1xbf16>
        %29 = "ttir.reshape"(%arg3, %28) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %30 = ttir.empty() : tensor<1x64x24x24xbf16>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %32 = ttir.empty() : tensor<1x64x24x24xbf16>
        %33 = "ttir.add"(%27, %31, %32) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %34 = ttir.empty() : tensor<1x64x24x24xbf16>
        %35 = "ttir.maximum"(%33, %1, %34) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %36 = ttir.empty() : tensor<1x64x12x12xbf16>
        %37 = ttir.empty() : tensor<1x24x24x64xbf16>
        %38 = "ttir.permute"(%35, %37) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %39 = ttir.empty() : tensor<1x12x12x64xbf16>
        %40 = "ttir.max_pool2d"(%38, %39) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x24x24x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %41 = ttir.empty() : tensor<1x64x12x12xbf16>
        %42 = "ttir.permute"(%40, %41) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %43 = ttir.empty() : tensor<1x9216xbf16>
        %44 = "ttir.reshape"(%42, %43) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %45 = ttir.empty() : tensor<1x9216xf32>
        %46 = "ttir.typecast"(%44, %45) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %47 = ttir.empty() : tensor<1x9216xf32>
        %48 = "ttir.permute"(%46, %47) <{permutation = array<i64: 0, 1>}> : (tensor<1x9216xf32>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %49 = ttir.empty() : tensor<9216x128xf32>
        %50 = "ttir.permute"(%arg4, %49) <{permutation = array<i64: 0, 1>}> : (tensor<9216x128xf32>, tensor<9216x128xf32>) -> tensor<9216x128xf32>
        %51 = ttir.empty() : tensor<1x9216xf32>
        %52 = "ttir.reshape"(%48, %51) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x9216xf32>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %53 = ttir.empty() : tensor<9216x128xf32>
        %54 = "ttir.reshape"(%50, %53) <{shape = [9216 : i32, 128 : i32]}> : (tensor<9216x128xf32>, tensor<9216x128xf32>) -> tensor<9216x128xf32>
        %55 = ttir.empty() : tensor<1x128xf32>
        %56 = "ttir.matmul"(%52, %54, %55) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %57 = ttir.empty() : tensor<1x128xf32>
        %58 = "ttir.reshape"(%56, %57) <{shape = [1 : i32, 128 : i32]}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %59 = ttir.empty() : tensor<1xf32>
        %60 = "ttir.typecast"(%3, %59) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %61 = ttir.empty() : tensor<1x1xf32>
        %62 = "ttir.reshape"(%60, %61) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %63 = ttir.empty() : tensor<1x128xf32>
        %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %65 = ttir.empty() : tensor<1x128xf32>
        %66 = "ttir.multiply"(%58, %64, %65) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %67 = ttir.empty() : tensor<1x128xf32>
        %68 = "ttir.reshape"(%arg5, %67) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %69 = ttir.empty() : tensor<1x128xf32>
        %70 = "ttir.add"(%66, %68, %69) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %71 = ttir.empty() : tensor<1x128xbf16>
        %72 = "ttir.typecast"(%70, %71) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %73 = ttir.empty() : tensor<1x128xbf16>
        %74 = "ttir.maximum"(%72, %2, %73) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %75 = ttir.empty() : tensor<1x128xf32>
        %76 = "ttir.typecast"(%74, %75) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %77 = ttir.empty() : tensor<1x128xf32>
        %78 = "ttir.permute"(%76, %77) <{permutation = array<i64: 0, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %79 = ttir.empty() : tensor<128x10xf32>
        %80 = "ttir.permute"(%arg6, %79) <{permutation = array<i64: 0, 1>}> : (tensor<128x10xf32>, tensor<128x10xf32>) -> tensor<128x10xf32>
        %81 = ttir.empty() : tensor<1x128xf32>
        %82 = "ttir.reshape"(%78, %81) <{shape = [1 : i32, 128 : i32]}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %83 = ttir.empty() : tensor<128x10xf32>
        %84 = "ttir.reshape"(%80, %83) <{shape = [128 : i32, 10 : i32]}> : (tensor<128x10xf32>, tensor<128x10xf32>) -> tensor<128x10xf32>
        %85 = ttir.empty() : tensor<1x10xf32>
        %86 = "ttir.matmul"(%82, %84, %85) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %87 = ttir.empty() : tensor<1x10xf32>
        %88 = "ttir.reshape"(%86, %87) <{shape = [1 : i32, 10 : i32]}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %89 = ttir.empty() : tensor<1x1xf32>
        %90 = "ttir.reshape"(%60, %89) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %91 = ttir.empty() : tensor<1x10xf32>
        %92 = "ttir.broadcast"(%90, %91) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %93 = ttir.empty() : tensor<1x10xf32>
        %94 = "ttir.multiply"(%88, %92, %93) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %95 = ttir.empty() : tensor<1x10xf32>
        %96 = "ttir.reshape"(%arg7, %95) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %97 = ttir.empty() : tensor<1x10xf32>
        %98 = "ttir.add"(%94, %96, %97) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %99 = ttir.empty() : tensor<1xf32>
        %100 = "ttir.max"(%98, %99) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %101 = ttir.empty() : tensor<1x1xf32>
        %102 = "ttir.reshape"(%100, %101) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %103 = ttir.empty() : tensor<1x10xf32>
        %104 = "ttir.broadcast"(%102, %103) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %105 = ttir.empty() : tensor<1x10xf32>
        %106 = "ttir.subtract"(%98, %104, %105) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %107 = ttir.empty() : tensor<1x10xf32>
        %108 = "ttir.exp"(%106, %107) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %109 = ttir.empty() : tensor<1xf32>
        %110 = "ttir.sum"(%108, %109) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %111 = ttir.empty() : tensor<1x1xf32>
        %112 = "ttir.reshape"(%110, %111) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %113 = ttir.empty() : tensor<1x1xf32>
        %114 = "ttir.log"(%112, %113) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %115 = ttir.empty() : tensor<1x10xf32>
        %116 = "ttir.broadcast"(%114, %115) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %117 = ttir.empty() : tensor<1x10xf32>
        %118 = "ttir.subtract"(%106, %116, %117) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %119 = ttir.empty() : tensor<1x10xbf16>
        %120 = "ttir.typecast"(%118, %119) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %120 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<32x1x3x3xbf16>
        %8 = "ttir.permute"(%arg0, %7) <{permutation = array<i64: 0, 1, 2, 3>}> : (tensor<32x1x3x3xbf16>, tensor<32x1x3x3xbf16>) -> tensor<32x1x3x3xbf16>
        %9 = ttir.empty() : tensor<1x26x26x32xbf16>
        %10 = "ttir.conv2d"(%6, %8, %9) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x28x28x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %11 = "ttir.permute"(%10, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %12 = ttir.empty() : tensor<1x32x1x1xbf16>
        %13 = "ttir.reshape"(%arg1, %12) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %14 = ttir.empty() : tensor<1x32x26x26xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %16 = ttir.empty() : tensor<1x32x26x26xbf16>
        %17 = "ttir.add"(%11, %15, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %18 = ttir.empty() : tensor<1x32x26x26xbf16>
        %19 = "ttir.maximum"(%17, %0, %18) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %20 = ttir.empty() : tensor<1x64x24x24xbf16>
        %21 = ttir.empty() : tensor<1x26x26x32xbf16>
        %22 = "ttir.permute"(%19, %21) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %23 = ttir.empty() : tensor<64x32x3x3xbf16>
        %24 = "ttir.permute"(%arg2, %23) <{permutation = array<i64: 0, 1, 2, 3>}> : (tensor<64x32x3x3xbf16>, tensor<64x32x3x3xbf16>) -> tensor<64x32x3x3xbf16>
        %25 = ttir.empty() : tensor<1x24x24x64xbf16>
        %26 = "ttir.conv2d"(%22, %24, %25) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x26x26x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %27 = "ttir.permute"(%26, %20) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %28 = ttir.empty() : tensor<1x64x1x1xbf16>
        %29 = "ttir.reshape"(%arg3, %28) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %30 = ttir.empty() : tensor<1x64x24x24xbf16>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %32 = ttir.empty() : tensor<1x64x24x24xbf16>
        %33 = "ttir.add"(%27, %31, %32) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %34 = ttir.empty() : tensor<1x64x24x24xbf16>
        %35 = "ttir.maximum"(%33, %1, %34) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %36 = ttir.empty() : tensor<1x64x12x12xbf16>
        %37 = ttir.empty() : tensor<1x24x24x64xbf16>
        %38 = "ttir.permute"(%35, %37) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %39 = ttir.empty() : tensor<1x12x12x64xbf16>
        %40 = "ttir.max_pool2d"(%38, %39) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x24x24x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %41 = ttir.empty() : tensor<1x64x12x12xbf16>
        %42 = "ttir.permute"(%40, %41) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %43 = ttir.empty() : tensor<1x9216xbf16>
        %44 = "ttir.reshape"(%42, %43) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %45 = ttir.empty() : tensor<1x9216xf32>
        %46 = "ttir.typecast"(%44, %45) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %47 = ttir.empty() : tensor<1x9216xf32>
        %48 = "ttir.permute"(%46, %47) <{permutation = array<i64: 0, 1>}> : (tensor<1x9216xf32>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %49 = ttir.empty() : tensor<9216x128xf32>
        %50 = "ttir.permute"(%arg4, %49) <{permutation = array<i64: 0, 1>}> : (tensor<9216x128xf32>, tensor<9216x128xf32>) -> tensor<9216x128xf32>
        %51 = ttir.empty() : tensor<1x9216xf32>
        %52 = "ttir.reshape"(%48, %51) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x9216xf32>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %53 = ttir.empty() : tensor<9216x128xf32>
        %54 = "ttir.reshape"(%50, %53) <{shape = [9216 : i32, 128 : i32]}> : (tensor<9216x128xf32>, tensor<9216x128xf32>) -> tensor<9216x128xf32>
        %55 = ttir.empty() : tensor<1x128xf32>
        %56 = "ttir.matmul"(%52, %54, %55) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %57 = ttir.empty() : tensor<1x128xf32>
        %58 = "ttir.reshape"(%56, %57) <{shape = [1 : i32, 128 : i32]}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %59 = ttir.empty() : tensor<1xf32>
        %60 = "ttir.typecast"(%3, %59) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %61 = ttir.empty() : tensor<1x1xf32>
        %62 = "ttir.reshape"(%60, %61) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %63 = ttir.empty() : tensor<1x128xf32>
        %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %65 = ttir.empty() : tensor<1x128xf32>
        %66 = "ttir.multiply"(%58, %64, %65) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %67 = ttir.empty() : tensor<1x128xf32>
        %68 = "ttir.reshape"(%arg5, %67) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %69 = ttir.empty() : tensor<1x128xf32>
        %70 = "ttir.add"(%66, %68, %69) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %71 = ttir.empty() : tensor<1x128xbf16>
        %72 = "ttir.typecast"(%70, %71) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %73 = ttir.empty() : tensor<1x128xbf16>
        %74 = "ttir.maximum"(%72, %2, %73) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %75 = ttir.empty() : tensor<1x128xf32>
        %76 = "ttir.typecast"(%74, %75) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %77 = ttir.empty() : tensor<1x128xf32>
        %78 = "ttir.permute"(%76, %77) <{permutation = array<i64: 0, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %79 = ttir.empty() : tensor<128x10xf32>
        %80 = "ttir.permute"(%arg6, %79) <{permutation = array<i64: 0, 1>}> : (tensor<128x10xf32>, tensor<128x10xf32>) -> tensor<128x10xf32>
        %81 = ttir.empty() : tensor<1x128xf32>
        %82 = "ttir.reshape"(%78, %81) <{shape = [1 : i32, 128 : i32]}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %83 = ttir.empty() : tensor<128x10xf32>
        %84 = "ttir.reshape"(%80, %83) <{shape = [128 : i32, 10 : i32]}> : (tensor<128x10xf32>, tensor<128x10xf32>) -> tensor<128x10xf32>
        %85 = ttir.empty() : tensor<1x10xf32>
        %86 = "ttir.matmul"(%82, %84, %85) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %87 = ttir.empty() : tensor<1x10xf32>
        %88 = "ttir.reshape"(%86, %87) <{shape = [1 : i32, 10 : i32]}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %89 = ttir.empty() : tensor<1x1xf32>
        %90 = "ttir.reshape"(%60, %89) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %91 = ttir.empty() : tensor<1x10xf32>
        %92 = "ttir.broadcast"(%90, %91) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %93 = ttir.empty() : tensor<1x10xf32>
        %94 = "ttir.multiply"(%88, %92, %93) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %95 = ttir.empty() : tensor<1x10xf32>
        %96 = "ttir.reshape"(%arg7, %95) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %97 = ttir.empty() : tensor<1x10xf32>
        %98 = "ttir.add"(%94, %96, %97) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %99 = ttir.empty() : tensor<1xf32>
        %100 = "ttir.max"(%98, %99) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %101 = ttir.empty() : tensor<1x1xf32>
        %102 = "ttir.reshape"(%100, %101) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %103 = ttir.empty() : tensor<1x10xf32>
        %104 = "ttir.broadcast"(%102, %103) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %105 = ttir.empty() : tensor<1x10xf32>
        %106 = "ttir.subtract"(%98, %104, %105) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %107 = ttir.empty() : tensor<1x10xf32>
        %108 = "ttir.exp"(%106, %107) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %109 = ttir.empty() : tensor<1xf32>
        %110 = "ttir.sum"(%108, %109) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %111 = ttir.empty() : tensor<1x1xf32>
        %112 = "ttir.reshape"(%110, %111) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %113 = ttir.empty() : tensor<1x1xf32>
        %114 = "ttir.log"(%112, %113) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %115 = ttir.empty() : tensor<1x10xf32>
        %116 = "ttir.broadcast"(%114, %115) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %117 = ttir.empty() : tensor<1x10xf32>
        %118 = "ttir.subtract"(%106, %116, %117) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %119 = ttir.empty() : tensor<1x10xbf16>
        %120 = "ttir.typecast"(%118, %119) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %120 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<1x26x26x32xbf16>
        %8 = "ttir.conv2d"(%6, %arg0, %7) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x28x28x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %9 = "ttir.permute"(%8, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x32x26x26xbf16>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %16 = ttir.empty() : tensor<1x32x26x26xbf16>
        %17 = "ttir.maximum"(%15, %0, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = ttir.empty() : tensor<1x26x26x32xbf16>
        %20 = "ttir.permute"(%17, %19) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %21 = ttir.empty() : tensor<1x24x24x64xbf16>
        %22 = "ttir.conv2d"(%20, %arg2, %21) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x26x26x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %23 = "ttir.permute"(%22, %18) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %26 = ttir.empty() : tensor<1x64x24x24xbf16>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %28 = ttir.empty() : tensor<1x64x24x24xbf16>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %30 = ttir.empty() : tensor<1x64x24x24xbf16>
        %31 = "ttir.maximum"(%29, %1, %30) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %32 = ttir.empty() : tensor<1x24x24x64xbf16>
        %33 = "ttir.permute"(%31, %32) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %34 = ttir.empty() : tensor<1x12x12x64xbf16>
        %35 = "ttir.max_pool2d"(%33, %34) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x24x24x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %36 = ttir.empty() : tensor<1x64x12x12xbf16>
        %37 = "ttir.permute"(%35, %36) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %38 = ttir.empty() : tensor<1x9216xbf16>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %40 = ttir.empty() : tensor<1x9216xf32>
        %41 = "ttir.typecast"(%39, %40) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %42 = ttir.empty() : tensor<1x128xf32>
        %43 = "ttir.matmul"(%41, %arg4, %42) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %44 = ttir.empty() : tensor<1xf32>
        %45 = "ttir.typecast"(%3, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %46 = ttir.empty() : tensor<1x1xf32>
        %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %48 = ttir.empty() : tensor<1x128xf32>
        %49 = "ttir.broadcast"(%47, %48) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %50 = ttir.empty() : tensor<1x128xf32>
        %51 = "ttir.multiply"(%43, %49, %50) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %52 = ttir.empty() : tensor<1x128xf32>
        %53 = "ttir.reshape"(%arg5, %52) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %54 = ttir.empty() : tensor<1x128xf32>
        %55 = "ttir.add"(%51, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %56 = ttir.empty() : tensor<1x128xbf16>
        %57 = "ttir.typecast"(%55, %56) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %58 = ttir.empty() : tensor<1x128xbf16>
        %59 = "ttir.maximum"(%57, %2, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %60 = ttir.empty() : tensor<1x128xf32>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %62 = ttir.empty() : tensor<1x10xf32>
        %63 = "ttir.matmul"(%61, %arg6, %62) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %64 = ttir.empty() : tensor<1x1xf32>
        %65 = "ttir.reshape"(%45, %64) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.broadcast"(%65, %66) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.multiply"(%63, %67, %68) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.reshape"(%arg7, %70) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.add"(%69, %71, %72) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1xf32>
        %75 = "ttir.max"(%73, %74) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %76 = ttir.empty() : tensor<1x1xf32>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.broadcast"(%77, %78) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xf32>
        %81 = "ttir.subtract"(%73, %79, %80) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %82 = ttir.empty() : tensor<1x10xf32>
        %83 = "ttir.exp"(%81, %82) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %84 = ttir.empty() : tensor<1xf32>
        %85 = "ttir.sum"(%83, %84) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %86 = ttir.empty() : tensor<1x1xf32>
        %87 = "ttir.reshape"(%85, %86) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %88 = ttir.empty() : tensor<1x1xf32>
        %89 = "ttir.log"(%87, %88) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %90 = ttir.empty() : tensor<1x10xf32>
        %91 = "ttir.broadcast"(%89, %90) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %92 = ttir.empty() : tensor<1x10xf32>
        %93 = "ttir.subtract"(%81, %91, %92) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %94 = ttir.empty() : tensor<1x10xbf16>
        %95 = "ttir.typecast"(%93, %94) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %95 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<1x26x26x32xbf16>
        %8 = "ttir.conv2d"(%6, %arg0, %7) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x28x28x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %9 = "ttir.permute"(%8, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x32x26x26xbf16>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %16 = ttir.empty() : tensor<1x32x26x26xbf16>
        %17 = "ttir.maximum"(%15, %0, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = ttir.empty() : tensor<1x26x26x32xbf16>
        %20 = "ttir.permute"(%17, %19) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %21 = ttir.empty() : tensor<1x24x24x64xbf16>
        %22 = "ttir.conv2d"(%20, %arg2, %21) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x26x26x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %23 = "ttir.permute"(%22, %18) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %26 = ttir.empty() : tensor<1x64x24x24xbf16>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %28 = ttir.empty() : tensor<1x64x24x24xbf16>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %30 = ttir.empty() : tensor<1x64x24x24xbf16>
        %31 = "ttir.maximum"(%29, %1, %30) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %32 = ttir.empty() : tensor<1x24x24x64xbf16>
        %33 = "ttir.permute"(%31, %32) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %34 = ttir.empty() : tensor<1x12x12x64xbf16>
        %35 = "ttir.max_pool2d"(%33, %34) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x24x24x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %36 = ttir.empty() : tensor<1x64x12x12xbf16>
        %37 = "ttir.permute"(%35, %36) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %38 = ttir.empty() : tensor<1x9216xbf16>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %40 = ttir.empty() : tensor<1x9216xf32>
        %41 = "ttir.typecast"(%39, %40) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %42 = ttir.empty() : tensor<1x128xf32>
        %43 = "ttir.matmul"(%41, %arg4, %42) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %44 = ttir.empty() : tensor<1xf32>
        %45 = "ttir.typecast"(%3, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %46 = ttir.empty() : tensor<1x1xf32>
        %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %48 = ttir.empty() : tensor<1x128xf32>
        %49 = "ttir.broadcast"(%47, %48) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %50 = ttir.empty() : tensor<1x128xf32>
        %51 = "ttir.multiply"(%43, %49, %50) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %52 = ttir.empty() : tensor<1x128xf32>
        %53 = "ttir.reshape"(%arg5, %52) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %54 = ttir.empty() : tensor<1x128xf32>
        %55 = "ttir.add"(%51, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %56 = ttir.empty() : tensor<1x128xbf16>
        %57 = "ttir.typecast"(%55, %56) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %58 = ttir.empty() : tensor<1x128xbf16>
        %59 = "ttir.maximum"(%57, %2, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %60 = ttir.empty() : tensor<1x128xf32>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %62 = ttir.empty() : tensor<1x10xf32>
        %63 = "ttir.matmul"(%61, %arg6, %62) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %64 = ttir.empty() : tensor<1x1xf32>
        %65 = "ttir.reshape"(%45, %64) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.broadcast"(%65, %66) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.multiply"(%63, %67, %68) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.reshape"(%arg7, %70) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.add"(%69, %71, %72) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1xf32>
        %75 = "ttir.max"(%73, %74) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %76 = ttir.empty() : tensor<1x1xf32>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.broadcast"(%77, %78) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xf32>
        %81 = "ttir.subtract"(%73, %79, %80) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %82 = ttir.empty() : tensor<1x10xf32>
        %83 = "ttir.exp"(%81, %82) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %84 = ttir.empty() : tensor<1xf32>
        %85 = "ttir.sum"(%83, %84) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %86 = ttir.empty() : tensor<1x1xf32>
        %87 = "ttir.reshape"(%85, %86) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %88 = ttir.empty() : tensor<1x1xf32>
        %89 = "ttir.log"(%87, %88) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %90 = ttir.empty() : tensor<1x10xf32>
        %91 = "ttir.broadcast"(%89, %90) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %92 = ttir.empty() : tensor<1x10xf32>
        %93 = "ttir.subtract"(%81, %91, %92) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %94 = ttir.empty() : tensor<1x10xbf16>
        %95 = "ttir.typecast"(%93, %94) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %95 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<1x26x26x32xbf16>
        %8 = "ttir.conv2d"(%6, %arg0, %7) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x28x28x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %9 = "ttir.permute"(%8, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x32x26x26xbf16>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %16 = ttir.empty() : tensor<1x32x26x26xbf16>
        %17 = "ttir.maximum"(%15, %0, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = ttir.empty() : tensor<1x26x26x32xbf16>
        %20 = "ttir.permute"(%17, %19) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %21 = ttir.empty() : tensor<1x24x24x64xbf16>
        %22 = "ttir.conv2d"(%20, %arg2, %21) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x26x26x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %23 = "ttir.permute"(%22, %18) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %26 = ttir.empty() : tensor<1x64x24x24xbf16>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %28 = ttir.empty() : tensor<1x64x24x24xbf16>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %30 = ttir.empty() : tensor<1x64x24x24xbf16>
        %31 = "ttir.maximum"(%29, %1, %30) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %32 = ttir.empty() : tensor<1x24x24x64xbf16>
        %33 = "ttir.permute"(%31, %32) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %34 = ttir.empty() : tensor<1x12x12x64xbf16>
        %35 = "ttir.max_pool2d"(%33, %34) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x24x24x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %36 = ttir.empty() : tensor<1x64x12x12xbf16>
        %37 = "ttir.permute"(%35, %36) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %38 = ttir.empty() : tensor<1x9216xbf16>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %40 = ttir.empty() : tensor<1x9216xf32>
        %41 = "ttir.typecast"(%39, %40) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %42 = ttir.empty() : tensor<1x128xf32>
        %43 = "ttir.matmul"(%41, %arg4, %42) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %44 = ttir.empty() : tensor<1xf32>
        %45 = "ttir.typecast"(%3, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %46 = ttir.empty() : tensor<1x1xf32>
        %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %48 = ttir.empty() : tensor<1x128xf32>
        %49 = "ttir.broadcast"(%47, %48) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %50 = ttir.empty() : tensor<1x128xf32>
        %51 = "ttir.multiply"(%43, %49, %50) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %52 = ttir.empty() : tensor<1x128xf32>
        %53 = "ttir.reshape"(%arg5, %52) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %54 = ttir.empty() : tensor<1x128xf32>
        %55 = "ttir.add"(%51, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %56 = ttir.empty() : tensor<1x128xbf16>
        %57 = "ttir.typecast"(%55, %56) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %58 = ttir.empty() : tensor<1x128xbf16>
        %59 = "ttir.maximum"(%57, %2, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %60 = ttir.empty() : tensor<1x128xf32>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %62 = ttir.empty() : tensor<1x10xf32>
        %63 = "ttir.matmul"(%61, %arg6, %62) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %64 = ttir.empty() : tensor<1x1xf32>
        %65 = "ttir.reshape"(%45, %64) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.broadcast"(%65, %66) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.multiply"(%63, %67, %68) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.reshape"(%arg7, %70) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.add"(%69, %71, %72) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1xf32>
        %75 = "ttir.max"(%73, %74) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %76 = ttir.empty() : tensor<1x1xf32>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.broadcast"(%77, %78) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xf32>
        %81 = "ttir.subtract"(%73, %79, %80) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %82 = ttir.empty() : tensor<1x10xf32>
        %83 = "ttir.exp"(%81, %82) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %84 = ttir.empty() : tensor<1xf32>
        %85 = "ttir.sum"(%83, %84) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %86 = ttir.empty() : tensor<1x1xf32>
        %87 = "ttir.reshape"(%85, %86) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %88 = ttir.empty() : tensor<1x1xf32>
        %89 = "ttir.log"(%87, %88) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %90 = ttir.empty() : tensor<1x10xf32>
        %91 = "ttir.broadcast"(%89, %90) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %92 = ttir.empty() : tensor<1x10xf32>
        %93 = "ttir.subtract"(%81, %91, %92) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %94 = ttir.empty() : tensor<1x10xbf16>
        %95 = "ttir.typecast"(%93, %94) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %95 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRFlattenSlidingWindow (ttir-flatten-sliding-window) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<1x26x26x32xbf16>
        %8 = "ttir.conv2d"(%6, %arg0, %7) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x28x28x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %9 = "ttir.permute"(%8, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x32x26x26xbf16>
        %13 = "ttir.broadcast"(%11, %12) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %14 = ttir.empty() : tensor<1x32x26x26xbf16>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %16 = ttir.empty() : tensor<1x32x26x26xbf16>
        %17 = "ttir.maximum"(%15, %0, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %18 = ttir.empty() : tensor<1x64x24x24xbf16>
        %19 = ttir.empty() : tensor<1x26x26x32xbf16>
        %20 = "ttir.permute"(%17, %19) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %21 = ttir.empty() : tensor<1x24x24x64xbf16>
        %22 = "ttir.conv2d"(%20, %arg2, %21) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x26x26x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %23 = "ttir.permute"(%22, %18) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %26 = ttir.empty() : tensor<1x64x24x24xbf16>
        %27 = "ttir.broadcast"(%25, %26) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %28 = ttir.empty() : tensor<1x64x24x24xbf16>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %30 = ttir.empty() : tensor<1x64x24x24xbf16>
        %31 = "ttir.maximum"(%29, %1, %30) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %32 = ttir.empty() : tensor<1x24x24x64xbf16>
        %33 = "ttir.permute"(%31, %32) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %34 = ttir.empty() : tensor<1x12x12x64xbf16>
        %35 = "ttir.max_pool2d"(%33, %34) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x24x24x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %36 = ttir.empty() : tensor<1x64x12x12xbf16>
        %37 = "ttir.permute"(%35, %36) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %38 = ttir.empty() : tensor<1x9216xbf16>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %40 = ttir.empty() : tensor<1x9216xf32>
        %41 = "ttir.typecast"(%39, %40) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %42 = ttir.empty() : tensor<1x128xf32>
        %43 = "ttir.matmul"(%41, %arg4, %42) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %44 = ttir.empty() : tensor<1xf32>
        %45 = "ttir.typecast"(%3, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %46 = ttir.empty() : tensor<1x1xf32>
        %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %48 = ttir.empty() : tensor<1x128xf32>
        %49 = "ttir.broadcast"(%47, %48) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %50 = ttir.empty() : tensor<1x128xf32>
        %51 = "ttir.multiply"(%43, %49, %50) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %52 = ttir.empty() : tensor<1x128xf32>
        %53 = "ttir.reshape"(%arg5, %52) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %54 = ttir.empty() : tensor<1x128xf32>
        %55 = "ttir.add"(%51, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %56 = ttir.empty() : tensor<1x128xbf16>
        %57 = "ttir.typecast"(%55, %56) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %58 = ttir.empty() : tensor<1x128xbf16>
        %59 = "ttir.maximum"(%57, %2, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %60 = ttir.empty() : tensor<1x128xf32>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %62 = ttir.empty() : tensor<1x10xf32>
        %63 = "ttir.matmul"(%61, %arg6, %62) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %64 = ttir.empty() : tensor<1x1xf32>
        %65 = "ttir.reshape"(%45, %64) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.broadcast"(%65, %66) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.multiply"(%63, %67, %68) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.reshape"(%arg7, %70) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.add"(%69, %71, %72) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1xf32>
        %75 = "ttir.max"(%73, %74) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %76 = ttir.empty() : tensor<1x1xf32>
        %77 = "ttir.reshape"(%75, %76) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.broadcast"(%77, %78) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1x10xf32>
        %81 = "ttir.subtract"(%73, %79, %80) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %82 = ttir.empty() : tensor<1x10xf32>
        %83 = "ttir.exp"(%81, %82) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %84 = ttir.empty() : tensor<1xf32>
        %85 = "ttir.sum"(%83, %84) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %86 = ttir.empty() : tensor<1x1xf32>
        %87 = "ttir.reshape"(%85, %86) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %88 = ttir.empty() : tensor<1x1xf32>
        %89 = "ttir.log"(%87, %88) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %90 = ttir.empty() : tensor<1x10xf32>
        %91 = "ttir.broadcast"(%89, %90) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %92 = ttir.empty() : tensor<1x10xf32>
        %93 = "ttir.subtract"(%81, %91, %92) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %94 = ttir.empty() : tensor<1x10xbf16>
        %95 = "ttir.typecast"(%93, %94) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %95 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRFlattenSlidingWindow (ttir-flatten-sliding-window) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<1x26x26x32xbf16>
        %8 = ttir.empty() : tensor<1x1x784x1xbf16>
        %9 = "ttir.reshape"(%6, %8) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16>, tensor<1x1x784x1xbf16>) -> tensor<1x1x784x1xbf16>
        %10 = ttir.empty() : tensor<1x1x676x32xbf16>
        %11 = "ttir.conv2d"(%9, %arg0, %10) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %12 = ttir.empty() : tensor<1x26x26x32xbf16>
        %13 = "ttir.reshape"(%11, %12) <{shape = [1 : i32, 26 : i32, 26 : i32, 32 : i32]}> : (tensor<1x1x676x32xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %14 = "ttir.permute"(%13, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %15 = ttir.empty() : tensor<1x32x1x1xbf16>
        %16 = "ttir.reshape"(%arg1, %15) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %17 = ttir.empty() : tensor<1x32x26x26xbf16>
        %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %19 = ttir.empty() : tensor<1x32x26x26xbf16>
        %20 = "ttir.add"(%14, %18, %19) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %21 = ttir.empty() : tensor<1x32x26x26xbf16>
        %22 = "ttir.maximum"(%20, %0, %21) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %23 = ttir.empty() : tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x26x26x32xbf16>
        %25 = "ttir.permute"(%22, %24) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %26 = ttir.empty() : tensor<1x24x24x64xbf16>
        %27 = ttir.empty() : tensor<1x1x676x32xbf16>
        %28 = "ttir.reshape"(%25, %27) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %29 = ttir.empty() : tensor<1x1x576x64xbf16>
        %30 = "ttir.conv2d"(%28, %arg2, %29) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %31 = ttir.empty() : tensor<1x24x24x64xbf16>
        %32 = "ttir.reshape"(%30, %31) <{shape = [1 : i32, 24 : i32, 24 : i32, 64 : i32]}> : (tensor<1x1x576x64xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %33 = "ttir.permute"(%32, %23) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %34 = ttir.empty() : tensor<1x64x1x1xbf16>
        %35 = "ttir.reshape"(%arg3, %34) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %36 = ttir.empty() : tensor<1x64x24x24xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %38 = ttir.empty() : tensor<1x64x24x24xbf16>
        %39 = "ttir.add"(%33, %37, %38) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %40 = ttir.empty() : tensor<1x64x24x24xbf16>
        %41 = "ttir.maximum"(%39, %1, %40) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %42 = ttir.empty() : tensor<1x24x24x64xbf16>
        %43 = "ttir.permute"(%41, %42) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %44 = ttir.empty() : tensor<1x12x12x64xbf16>
        %45 = ttir.empty() : tensor<1x1x576x64xbf16>
        %46 = "ttir.reshape"(%43, %45) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %47 = ttir.empty() : tensor<1x1x144x64xbf16>
        %48 = "ttir.max_pool2d"(%46, %47) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16>, tensor<1x1x144x64xbf16>) -> tensor<1x1x144x64xbf16>
        %49 = ttir.empty() : tensor<1x12x12x64xbf16>
        %50 = "ttir.reshape"(%48, %49) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %51 = ttir.empty() : tensor<1x64x12x12xbf16>
        %52 = "ttir.permute"(%50, %51) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %53 = ttir.empty() : tensor<1x9216xbf16>
        %54 = "ttir.reshape"(%52, %53) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %55 = ttir.empty() : tensor<1x9216xf32>
        %56 = "ttir.typecast"(%54, %55) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %57 = ttir.empty() : tensor<1x128xf32>
        %58 = "ttir.matmul"(%56, %arg4, %57) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %59 = ttir.empty() : tensor<1xf32>
        %60 = "ttir.typecast"(%3, %59) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %61 = ttir.empty() : tensor<1x1xf32>
        %62 = "ttir.reshape"(%60, %61) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %63 = ttir.empty() : tensor<1x128xf32>
        %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %65 = ttir.empty() : tensor<1x128xf32>
        %66 = "ttir.multiply"(%58, %64, %65) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %67 = ttir.empty() : tensor<1x128xf32>
        %68 = "ttir.reshape"(%arg5, %67) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %69 = ttir.empty() : tensor<1x128xf32>
        %70 = "ttir.add"(%66, %68, %69) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %71 = ttir.empty() : tensor<1x128xbf16>
        %72 = "ttir.typecast"(%70, %71) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %73 = ttir.empty() : tensor<1x128xbf16>
        %74 = "ttir.maximum"(%72, %2, %73) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %75 = ttir.empty() : tensor<1x128xf32>
        %76 = "ttir.typecast"(%74, %75) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %77 = ttir.empty() : tensor<1x10xf32>
        %78 = "ttir.matmul"(%76, %arg6, %77) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %79 = ttir.empty() : tensor<1x1xf32>
        %80 = "ttir.reshape"(%60, %79) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %81 = ttir.empty() : tensor<1x10xf32>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %83 = ttir.empty() : tensor<1x10xf32>
        %84 = "ttir.multiply"(%78, %82, %83) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %85 = ttir.empty() : tensor<1x10xf32>
        %86 = "ttir.reshape"(%arg7, %85) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %87 = ttir.empty() : tensor<1x10xf32>
        %88 = "ttir.add"(%84, %86, %87) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %89 = ttir.empty() : tensor<1xf32>
        %90 = "ttir.max"(%88, %89) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %91 = ttir.empty() : tensor<1x1xf32>
        %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %93 = ttir.empty() : tensor<1x10xf32>
        %94 = "ttir.broadcast"(%92, %93) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %95 = ttir.empty() : tensor<1x10xf32>
        %96 = "ttir.subtract"(%88, %94, %95) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %97 = ttir.empty() : tensor<1x10xf32>
        %98 = "ttir.exp"(%96, %97) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %99 = ttir.empty() : tensor<1xf32>
        %100 = "ttir.sum"(%98, %99) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %101 = ttir.empty() : tensor<1x1xf32>
        %102 = "ttir.reshape"(%100, %101) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %103 = ttir.empty() : tensor<1x1xf32>
        %104 = "ttir.log"(%102, %103) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %105 = ttir.empty() : tensor<1x10xf32>
        %106 = "ttir.broadcast"(%104, %105) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %107 = ttir.empty() : tensor<1x10xf32>
        %108 = "ttir.subtract"(%96, %106, %107) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %109 = ttir.empty() : tensor<1x10xbf16>
        %110 = "ttir.typecast"(%108, %109) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %110 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIREraseInverseOps (ttir-erase-inverse-ops) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x32x26x26xbf16>
        %5 = ttir.empty() : tensor<1x28x28x1xbf16>
        %6 = "ttir.permute"(%arg12, %5) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %7 = ttir.empty() : tensor<1x26x26x32xbf16>
        %8 = ttir.empty() : tensor<1x1x784x1xbf16>
        %9 = "ttir.reshape"(%6, %8) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16>, tensor<1x1x784x1xbf16>) -> tensor<1x1x784x1xbf16>
        %10 = ttir.empty() : tensor<1x1x676x32xbf16>
        %11 = "ttir.conv2d"(%9, %arg0, %10) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %12 = ttir.empty() : tensor<1x26x26x32xbf16>
        %13 = "ttir.reshape"(%11, %12) <{shape = [1 : i32, 26 : i32, 26 : i32, 32 : i32]}> : (tensor<1x1x676x32xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %14 = "ttir.permute"(%13, %4) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x26x26x32xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %15 = ttir.empty() : tensor<1x32x1x1xbf16>
        %16 = "ttir.reshape"(%arg1, %15) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %17 = ttir.empty() : tensor<1x32x26x26xbf16>
        %18 = "ttir.broadcast"(%16, %17) <{broadcast_dimensions = array<i64: 1, 1, 26, 26>}> : (tensor<1x32x1x1xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %19 = ttir.empty() : tensor<1x32x26x26xbf16>
        %20 = "ttir.add"(%14, %18, %19) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %21 = ttir.empty() : tensor<1x32x26x26xbf16>
        %22 = "ttir.maximum"(%20, %0, %21) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>, tensor<1x32x26x26xbf16>) -> tensor<1x32x26x26xbf16>
        %23 = ttir.empty() : tensor<1x64x24x24xbf16>
        %24 = ttir.empty() : tensor<1x26x26x32xbf16>
        %25 = "ttir.permute"(%22, %24) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %26 = ttir.empty() : tensor<1x24x24x64xbf16>
        %27 = ttir.empty() : tensor<1x1x676x32xbf16>
        %28 = "ttir.reshape"(%25, %27) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %29 = ttir.empty() : tensor<1x1x576x64xbf16>
        %30 = "ttir.conv2d"(%28, %arg2, %29) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %31 = ttir.empty() : tensor<1x24x24x64xbf16>
        %32 = "ttir.reshape"(%30, %31) <{shape = [1 : i32, 24 : i32, 24 : i32, 64 : i32]}> : (tensor<1x1x576x64xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %33 = "ttir.permute"(%32, %23) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x24x24x64xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %34 = ttir.empty() : tensor<1x64x1x1xbf16>
        %35 = "ttir.reshape"(%arg3, %34) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %36 = ttir.empty() : tensor<1x64x24x24xbf16>
        %37 = "ttir.broadcast"(%35, %36) <{broadcast_dimensions = array<i64: 1, 1, 24, 24>}> : (tensor<1x64x1x1xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %38 = ttir.empty() : tensor<1x64x24x24xbf16>
        %39 = "ttir.add"(%33, %37, %38) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %40 = ttir.empty() : tensor<1x64x24x24xbf16>
        %41 = "ttir.maximum"(%39, %1, %40) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>, tensor<1x64x24x24xbf16>) -> tensor<1x64x24x24xbf16>
        %42 = ttir.empty() : tensor<1x24x24x64xbf16>
        %43 = "ttir.permute"(%41, %42) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %44 = ttir.empty() : tensor<1x12x12x64xbf16>
        %45 = ttir.empty() : tensor<1x1x576x64xbf16>
        %46 = "ttir.reshape"(%43, %45) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %47 = ttir.empty() : tensor<1x1x144x64xbf16>
        %48 = "ttir.max_pool2d"(%46, %47) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16>, tensor<1x1x144x64xbf16>) -> tensor<1x1x144x64xbf16>
        %49 = ttir.empty() : tensor<1x12x12x64xbf16>
        %50 = "ttir.reshape"(%48, %49) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %51 = ttir.empty() : tensor<1x64x12x12xbf16>
        %52 = "ttir.permute"(%50, %51) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %53 = ttir.empty() : tensor<1x9216xbf16>
        %54 = "ttir.reshape"(%52, %53) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %55 = ttir.empty() : tensor<1x9216xf32>
        %56 = "ttir.typecast"(%54, %55) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %57 = ttir.empty() : tensor<1x128xf32>
        %58 = "ttir.matmul"(%56, %arg4, %57) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %59 = ttir.empty() : tensor<1xf32>
        %60 = "ttir.typecast"(%3, %59) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1xsi32>, tensor<1xf32>) -> tensor<1xf32>
        %61 = ttir.empty() : tensor<1x1xf32>
        %62 = "ttir.reshape"(%60, %61) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %63 = ttir.empty() : tensor<1x128xf32>
        %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %65 = ttir.empty() : tensor<1x128xf32>
        %66 = "ttir.multiply"(%58, %64, %65) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %67 = ttir.empty() : tensor<1x128xf32>
        %68 = "ttir.reshape"(%arg5, %67) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %69 = ttir.empty() : tensor<1x128xf32>
        %70 = "ttir.add"(%66, %68, %69) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %71 = ttir.empty() : tensor<1x128xbf16>
        %72 = "ttir.typecast"(%70, %71) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %73 = ttir.empty() : tensor<1x128xbf16>
        %74 = "ttir.maximum"(%72, %2, %73) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %75 = ttir.empty() : tensor<1x128xf32>
        %76 = "ttir.typecast"(%74, %75) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %77 = ttir.empty() : tensor<1x10xf32>
        %78 = "ttir.matmul"(%76, %arg6, %77) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %79 = ttir.empty() : tensor<1x1xf32>
        %80 = "ttir.reshape"(%60, %79) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %81 = ttir.empty() : tensor<1x10xf32>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %83 = ttir.empty() : tensor<1x10xf32>
        %84 = "ttir.multiply"(%78, %82, %83) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %85 = ttir.empty() : tensor<1x10xf32>
        %86 = "ttir.reshape"(%arg7, %85) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %87 = ttir.empty() : tensor<1x10xf32>
        %88 = "ttir.add"(%84, %86, %87) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %89 = ttir.empty() : tensor<1xf32>
        %90 = "ttir.max"(%88, %89) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %91 = ttir.empty() : tensor<1x1xf32>
        %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %93 = ttir.empty() : tensor<1x10xf32>
        %94 = "ttir.broadcast"(%92, %93) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %95 = ttir.empty() : tensor<1x10xf32>
        %96 = "ttir.subtract"(%88, %94, %95) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %97 = ttir.empty() : tensor<1x10xf32>
        %98 = "ttir.exp"(%96, %97) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %99 = ttir.empty() : tensor<1xf32>
        %100 = "ttir.sum"(%98, %99) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %101 = ttir.empty() : tensor<1x1xf32>
        %102 = "ttir.reshape"(%100, %101) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %103 = ttir.empty() : tensor<1x1xf32>
        %104 = "ttir.log"(%102, %103) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %105 = ttir.empty() : tensor<1x10xf32>
        %106 = "ttir.broadcast"(%104, %105) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %107 = ttir.empty() : tensor<1x10xf32>
        %108 = "ttir.subtract"(%96, %106, %107) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %109 = ttir.empty() : tensor<1x10xbf16>
        %110 = "ttir.typecast"(%108, %109) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %110 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIREraseInverseOps (ttir-erase-inverse-ops) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x28x28x1xbf16>
        %5 = "ttir.permute"(%arg12, %4) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %6 = ttir.empty() : tensor<1x1x784x1xbf16>
        %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16>, tensor<1x1x784x1xbf16>) -> tensor<1x1x784x1xbf16>
        %8 = ttir.empty() : tensor<1x1x676x32xbf16>
        %9 = "ttir.conv2d"(%7, %arg0, %8) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x1x1x32xbf16>
        %13 = "ttir.permute"(%11, %12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16>, tensor<1x1x1x32xbf16>) -> tensor<1x1x1x32xbf16>
        %14 = ttir.empty() : tensor<1x1x676x32xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 1, 1, 676, 1>}> : (tensor<1x1x1x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %16 = ttir.empty() : tensor<1x1x676x32xbf16>
        %17 = "ttir.add"(%9, %15, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %18 = ttir.empty() : tensor<1x26x26x32xbf16>
        %19 = "ttir.permute"(%0, %18) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %20 = ttir.empty() : tensor<1x1x676x32xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %22 = ttir.empty() : tensor<1x1x676x32xbf16>
        %23 = "ttir.maximum"(%17, %21, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %24 = ttir.empty() : tensor<1x1x576x64xbf16>
        %25 = "ttir.conv2d"(%23, %arg2, %24) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %26 = ttir.empty() : tensor<1x64x1x1xbf16>
        %27 = "ttir.reshape"(%arg3, %26) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %28 = ttir.empty() : tensor<1x1x1x64xbf16>
        %29 = "ttir.permute"(%27, %28) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x1x1x64xbf16>) -> tensor<1x1x1x64xbf16>
        %30 = ttir.empty() : tensor<1x1x576x64xbf16>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 1, 1, 576, 1>}> : (tensor<1x1x1x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %32 = ttir.empty() : tensor<1x1x576x64xbf16>
        %33 = "ttir.add"(%25, %31, %32) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %34 = ttir.empty() : tensor<1x24x24x64xbf16>
        %35 = "ttir.permute"(%1, %34) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %36 = ttir.empty() : tensor<1x1x576x64xbf16>
        %37 = "ttir.reshape"(%35, %36) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %38 = ttir.empty() : tensor<1x1x576x64xbf16>
        %39 = "ttir.maximum"(%33, %37, %38) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %40 = ttir.empty() : tensor<1x1x144x64xbf16>
        %41 = "ttir.max_pool2d"(%39, %40) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16>, tensor<1x1x144x64xbf16>) -> tensor<1x1x144x64xbf16>
        %42 = ttir.empty() : tensor<1x12x12x64xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %44 = ttir.empty() : tensor<1x64x12x12xbf16>
        %45 = "ttir.permute"(%43, %44) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %46 = ttir.empty() : tensor<1x9216xbf16>
        %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %48 = ttir.empty() : tensor<1x9216xf32>
        %49 = "ttir.typecast"(%47, %48) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %50 = ttir.empty() : tensor<1x128xf32>
        %51 = "ttir.matmul"(%49, %arg4, %50) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %52 = ttir.empty() : tensor<1x1xsi32>
        %53 = "ttir.reshape"(%3, %52) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32>, tensor<1x1xsi32>) -> tensor<1x1xsi32>
        %54 = ttir.empty() : tensor<1x1xf32>
        %55 = "ttir.typecast"(%53, %54) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xsi32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %56 = ttir.empty() : tensor<1x128xf32>
        %57 = "ttir.broadcast"(%55, %56) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %58 = ttir.empty() : tensor<1x128xf32>
        %59 = "ttir.multiply"(%51, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %60 = ttir.empty() : tensor<1x128xf32>
        %61 = "ttir.reshape"(%arg5, %60) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %62 = ttir.empty() : tensor<1x128xf32>
        %63 = "ttir.add"(%59, %61, %62) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %64 = ttir.empty() : tensor<1x128xbf16>
        %65 = "ttir.typecast"(%63, %64) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %66 = ttir.empty() : tensor<1x128xbf16>
        %67 = "ttir.maximum"(%65, %2, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %68 = ttir.empty() : tensor<1x128xf32>
        %69 = "ttir.typecast"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.matmul"(%69, %arg6, %70) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.broadcast"(%55, %72) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1x10xf32>
        %75 = "ttir.multiply"(%71, %73, %74) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.reshape"(%arg7, %76) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.add"(%75, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1xf32>
        %81 = "ttir.max"(%79, %80) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %82 = ttir.empty() : tensor<1x1xf32>
        %83 = "ttir.reshape"(%81, %82) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %84 = ttir.empty() : tensor<1x10xf32>
        %85 = "ttir.broadcast"(%83, %84) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %86 = ttir.empty() : tensor<1x10xf32>
        %87 = "ttir.subtract"(%79, %85, %86) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %88 = ttir.empty() : tensor<1x10xf32>
        %89 = "ttir.exp"(%87, %88) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %90 = ttir.empty() : tensor<1xf32>
        %91 = "ttir.sum"(%89, %90) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %92 = ttir.empty() : tensor<1x1xf32>
        %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %94 = ttir.empty() : tensor<1x1xf32>
        %95 = "ttir.log"(%93, %94) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %96 = ttir.empty() : tensor<1x10xf32>
        %97 = "ttir.broadcast"(%95, %96) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %98 = ttir.empty() : tensor<1x10xf32>
        %99 = "ttir.subtract"(%87, %97, %98) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %100 = ttir.empty() : tensor<1x10xbf16>
        %101 = "ttir.typecast"(%99, %100) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %101 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x28x28x1xbf16>
        %5 = "ttir.permute"(%arg12, %4) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %6 = ttir.empty() : tensor<1x1x784x1xbf16>
        %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16>, tensor<1x1x784x1xbf16>) -> tensor<1x1x784x1xbf16>
        %8 = ttir.empty() : tensor<1x1x676x32xbf16>
        %9 = "ttir.conv2d"(%7, %arg0, %8) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x1x1x32xbf16>
        %13 = "ttir.permute"(%11, %12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16>, tensor<1x1x1x32xbf16>) -> tensor<1x1x1x32xbf16>
        %14 = ttir.empty() : tensor<1x1x676x32xbf16>
        %15 = "ttir.broadcast"(%13, %14) <{broadcast_dimensions = array<i64: 1, 1, 676, 1>}> : (tensor<1x1x1x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %16 = ttir.empty() : tensor<1x1x676x32xbf16>
        %17 = "ttir.add"(%9, %15, %16) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %18 = ttir.empty() : tensor<1x26x26x32xbf16>
        %19 = "ttir.permute"(%0, %18) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %20 = ttir.empty() : tensor<1x1x676x32xbf16>
        %21 = "ttir.reshape"(%19, %20) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %22 = ttir.empty() : tensor<1x1x676x32xbf16>
        %23 = "ttir.maximum"(%17, %21, %22) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %24 = ttir.empty() : tensor<1x1x576x64xbf16>
        %25 = "ttir.conv2d"(%23, %arg2, %24) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %26 = ttir.empty() : tensor<1x64x1x1xbf16>
        %27 = "ttir.reshape"(%arg3, %26) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %28 = ttir.empty() : tensor<1x1x1x64xbf16>
        %29 = "ttir.permute"(%27, %28) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x1x1x64xbf16>) -> tensor<1x1x1x64xbf16>
        %30 = ttir.empty() : tensor<1x1x576x64xbf16>
        %31 = "ttir.broadcast"(%29, %30) <{broadcast_dimensions = array<i64: 1, 1, 576, 1>}> : (tensor<1x1x1x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %32 = ttir.empty() : tensor<1x1x576x64xbf16>
        %33 = "ttir.add"(%25, %31, %32) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %34 = ttir.empty() : tensor<1x24x24x64xbf16>
        %35 = "ttir.permute"(%1, %34) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %36 = ttir.empty() : tensor<1x1x576x64xbf16>
        %37 = "ttir.reshape"(%35, %36) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %38 = ttir.empty() : tensor<1x1x576x64xbf16>
        %39 = "ttir.maximum"(%33, %37, %38) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %40 = ttir.empty() : tensor<1x1x144x64xbf16>
        %41 = "ttir.max_pool2d"(%39, %40) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16>, tensor<1x1x144x64xbf16>) -> tensor<1x1x144x64xbf16>
        %42 = ttir.empty() : tensor<1x12x12x64xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %44 = ttir.empty() : tensor<1x64x12x12xbf16>
        %45 = "ttir.permute"(%43, %44) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %46 = ttir.empty() : tensor<1x9216xbf16>
        %47 = "ttir.reshape"(%45, %46) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %48 = ttir.empty() : tensor<1x9216xf32>
        %49 = "ttir.typecast"(%47, %48) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %50 = ttir.empty() : tensor<1x128xf32>
        %51 = "ttir.matmul"(%49, %arg4, %50) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %52 = ttir.empty() : tensor<1x1xsi32>
        %53 = "ttir.reshape"(%3, %52) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32>, tensor<1x1xsi32>) -> tensor<1x1xsi32>
        %54 = ttir.empty() : tensor<1x1xf32>
        %55 = "ttir.typecast"(%53, %54) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xsi32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %56 = ttir.empty() : tensor<1x128xf32>
        %57 = "ttir.broadcast"(%55, %56) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %58 = ttir.empty() : tensor<1x128xf32>
        %59 = "ttir.multiply"(%51, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %60 = ttir.empty() : tensor<1x128xf32>
        %61 = "ttir.reshape"(%arg5, %60) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %62 = ttir.empty() : tensor<1x128xf32>
        %63 = "ttir.add"(%59, %61, %62) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %64 = ttir.empty() : tensor<1x128xbf16>
        %65 = "ttir.typecast"(%63, %64) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %66 = ttir.empty() : tensor<1x128xbf16>
        %67 = "ttir.maximum"(%65, %2, %66) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %68 = ttir.empty() : tensor<1x128xf32>
        %69 = "ttir.typecast"(%67, %68) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.matmul"(%69, %arg6, %70) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.broadcast"(%55, %72) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1x10xf32>
        %75 = "ttir.multiply"(%71, %73, %74) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %76 = ttir.empty() : tensor<1x10xf32>
        %77 = "ttir.reshape"(%arg7, %76) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %78 = ttir.empty() : tensor<1x10xf32>
        %79 = "ttir.add"(%75, %77, %78) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %80 = ttir.empty() : tensor<1xf32>
        %81 = "ttir.max"(%79, %80) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %82 = ttir.empty() : tensor<1x1xf32>
        %83 = "ttir.reshape"(%81, %82) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %84 = ttir.empty() : tensor<1x10xf32>
        %85 = "ttir.broadcast"(%83, %84) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %86 = ttir.empty() : tensor<1x10xf32>
        %87 = "ttir.subtract"(%79, %85, %86) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %88 = ttir.empty() : tensor<1x10xf32>
        %89 = "ttir.exp"(%87, %88) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %90 = ttir.empty() : tensor<1xf32>
        %91 = "ttir.sum"(%89, %90) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %92 = ttir.empty() : tensor<1x1xf32>
        %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %94 = ttir.empty() : tensor<1x1xf32>
        %95 = "ttir.log"(%93, %94) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %96 = ttir.empty() : tensor<1x10xf32>
        %97 = "ttir.broadcast"(%95, %96) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %98 = ttir.empty() : tensor<1x10xf32>
        %99 = "ttir.subtract"(%87, %97, %98) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %100 = ttir.empty() : tensor<1x10xbf16>
        %101 = "ttir.typecast"(%99, %100) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %101 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x28x28x1xbf16>
        %5 = "ttir.permute"(%arg12, %4) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %6 = ttir.empty() : tensor<1x1x784x1xbf16>
        %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16>, tensor<1x1x784x1xbf16>) -> tensor<1x1x784x1xbf16>
        %8 = ttir.empty() : tensor<1x1x676x32xbf16>
        %9 = "ttir.conv2d"(%7, %arg0, %8) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x1x1x32xbf16>
        %13 = "ttir.permute"(%11, %12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16>, tensor<1x1x1x32xbf16>) -> tensor<1x1x1x32xbf16>
        %14 = ttir.empty() : tensor<1x1x676x32xbf16>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x1x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %16 = ttir.empty() : tensor<1x26x26x32xbf16>
        %17 = "ttir.permute"(%0, %16) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %18 = ttir.empty() : tensor<1x1x676x32xbf16>
        %19 = "ttir.reshape"(%17, %18) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %20 = ttir.empty() : tensor<1x1x676x32xbf16>
        %21 = "ttir.maximum"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %22 = ttir.empty() : tensor<1x1x576x64xbf16>
        %23 = "ttir.conv2d"(%21, %arg2, %22) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %26 = ttir.empty() : tensor<1x1x1x64xbf16>
        %27 = "ttir.permute"(%25, %26) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x1x1x64xbf16>) -> tensor<1x1x1x64xbf16>
        %28 = ttir.empty() : tensor<1x1x576x64xbf16>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x1x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %30 = ttir.empty() : tensor<1x24x24x64xbf16>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %32 = ttir.empty() : tensor<1x1x576x64xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %34 = ttir.empty() : tensor<1x1x576x64xbf16>
        %35 = "ttir.maximum"(%29, %33, %34) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %36 = ttir.empty() : tensor<1x1x144x64xbf16>
        %37 = "ttir.max_pool2d"(%35, %36) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16>, tensor<1x1x144x64xbf16>) -> tensor<1x1x144x64xbf16>
        %38 = ttir.empty() : tensor<1x12x12x64xbf16>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %40 = ttir.empty() : tensor<1x64x12x12xbf16>
        %41 = "ttir.permute"(%39, %40) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %42 = ttir.empty() : tensor<1x9216xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %44 = ttir.empty() : tensor<1x9216xf32>
        %45 = "ttir.typecast"(%43, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %46 = ttir.empty() : tensor<1x128xf32>
        %47 = "ttir.matmul"(%45, %arg4, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %48 = ttir.empty() : tensor<1x1xsi32>
        %49 = "ttir.reshape"(%3, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32>, tensor<1x1xsi32>) -> tensor<1x1xsi32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.typecast"(%49, %50) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xsi32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x128xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %54 = ttir.empty() : tensor<1x128xf32>
        %55 = "ttir.multiply"(%47, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %56 = ttir.empty() : tensor<1x128xf32>
        %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %58 = ttir.empty() : tensor<1x128xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %60 = ttir.empty() : tensor<1x128xbf16>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %62 = ttir.empty() : tensor<1x128xbf16>
        %63 = "ttir.maximum"(%61, %2, %62) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %64 = ttir.empty() : tensor<1x128xf32>
        %65 = "ttir.typecast"(%63, %64) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.matmul"(%65, %arg6, %66) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.broadcast"(%51, %68) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.multiply"(%67, %69, %70) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.reshape"(%arg7, %72) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1x10xf32>
        %75 = "ttir.add"(%71, %73, %74) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %76 = ttir.empty() : tensor<1xf32>
        %77 = "ttir.max"(%75, %76) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %78 = ttir.empty() : tensor<1x1xf32>
        %79 = "ttir.reshape"(%77, %78) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %80 = ttir.empty() : tensor<1x10xf32>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %82 = ttir.empty() : tensor<1x10xf32>
        %83 = "ttir.subtract"(%75, %81, %82) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %84 = ttir.empty() : tensor<1x10xf32>
        %85 = "ttir.exp"(%83, %84) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %86 = ttir.empty() : tensor<1xf32>
        %87 = "ttir.sum"(%85, %86) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %88 = ttir.empty() : tensor<1x1xf32>
        %89 = "ttir.reshape"(%87, %88) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %90 = ttir.empty() : tensor<1x1xf32>
        %91 = "ttir.log"(%89, %90) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %92 = ttir.empty() : tensor<1x10xf32>
        %93 = "ttir.broadcast"(%91, %92) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %94 = ttir.empty() : tensor<1x10xf32>
        %95 = "ttir.subtract"(%83, %93, %94) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %96 = ttir.empty() : tensor<1x10xbf16>
        %97 = "ttir.typecast"(%95, %96) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %97 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump Before TTNNLayout (ttnn-layout) ('builtin.module' operation) //----- //
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16>, %arg1: tensor<32xbf16>, %arg2: tensor<64x32x3x3xbf16>, %arg3: tensor<64xbf16>, %arg4: tensor<9216x128xf32>, %arg5: tensor<128xf32>, %arg6: tensor<128x10xf32>, %arg7: tensor<10xf32>, %arg8: tensor<128x9216xbf16>, %arg9: tensor<128xbf16>, %arg10: tensor<10x128xbf16>, %arg11: tensor<10xbf16>, %arg12: tensor<1x1x28x28xbf16>) -> tensor<1x10xbf16> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32>
        %4 = ttir.empty() : tensor<1x28x28x1xbf16>
        %5 = "ttir.permute"(%arg12, %4) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16>, tensor<1x28x28x1xbf16>) -> tensor<1x28x28x1xbf16>
        %6 = ttir.empty() : tensor<1x1x784x1xbf16>
        %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16>, tensor<1x1x784x1xbf16>) -> tensor<1x1x784x1xbf16>
        %8 = ttir.empty() : tensor<1x1x676x32xbf16>
        %9 = "ttir.conv2d"(%7, %arg0, %8) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16>, tensor<32x1x3x3xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x1x1xbf16>
        %12 = ttir.empty() : tensor<1x1x1x32xbf16>
        %13 = "ttir.permute"(%11, %12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16>, tensor<1x1x1x32xbf16>) -> tensor<1x1x1x32xbf16>
        %14 = ttir.empty() : tensor<1x1x676x32xbf16>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x1x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %16 = ttir.empty() : tensor<1x26x26x32xbf16>
        %17 = "ttir.permute"(%0, %16) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16>, tensor<1x26x26x32xbf16>) -> tensor<1x26x26x32xbf16>
        %18 = ttir.empty() : tensor<1x1x676x32xbf16>
        %19 = "ttir.reshape"(%17, %18) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %20 = ttir.empty() : tensor<1x1x676x32xbf16>
        %21 = "ttir.maximum"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>, tensor<1x1x676x32xbf16>) -> tensor<1x1x676x32xbf16>
        %22 = ttir.empty() : tensor<1x1x576x64xbf16>
        %23 = "ttir.conv2d"(%21, %arg2, %22) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x1x1xbf16>
        %26 = ttir.empty() : tensor<1x1x1x64xbf16>
        %27 = "ttir.permute"(%25, %26) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16>, tensor<1x1x1x64xbf16>) -> tensor<1x1x1x64xbf16>
        %28 = ttir.empty() : tensor<1x1x576x64xbf16>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x1x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %30 = ttir.empty() : tensor<1x24x24x64xbf16>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16>, tensor<1x24x24x64xbf16>) -> tensor<1x24x24x64xbf16>
        %32 = ttir.empty() : tensor<1x1x576x64xbf16>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %34 = ttir.empty() : tensor<1x1x576x64xbf16>
        %35 = "ttir.maximum"(%29, %33, %34) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>, tensor<1x1x576x64xbf16>) -> tensor<1x1x576x64xbf16>
        %36 = ttir.empty() : tensor<1x1x144x64xbf16>
        %37 = "ttir.max_pool2d"(%35, %36) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16>, tensor<1x1x144x64xbf16>) -> tensor<1x1x144x64xbf16>
        %38 = ttir.empty() : tensor<1x12x12x64xbf16>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16>, tensor<1x12x12x64xbf16>) -> tensor<1x12x12x64xbf16>
        %40 = ttir.empty() : tensor<1x64x12x12xbf16>
        %41 = "ttir.permute"(%39, %40) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16>, tensor<1x64x12x12xbf16>) -> tensor<1x64x12x12xbf16>
        %42 = ttir.empty() : tensor<1x9216xbf16>
        %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16>, tensor<1x9216xbf16>) -> tensor<1x9216xbf16>
        %44 = ttir.empty() : tensor<1x9216xf32>
        %45 = "ttir.typecast"(%43, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16>, tensor<1x9216xf32>) -> tensor<1x9216xf32>
        %46 = ttir.empty() : tensor<1x128xf32>
        %47 = "ttir.matmul"(%45, %arg4, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32>, tensor<9216x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %48 = ttir.empty() : tensor<1x1xsi32>
        %49 = "ttir.reshape"(%3, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32>, tensor<1x1xsi32>) -> tensor<1x1xsi32>
        %50 = ttir.empty() : tensor<1x1xf32>
        %51 = "ttir.typecast"(%49, %50) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xsi32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %52 = ttir.empty() : tensor<1x128xf32>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %54 = ttir.empty() : tensor<1x128xf32>
        %55 = "ttir.multiply"(%47, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %56 = ttir.empty() : tensor<1x128xf32>
        %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %58 = ttir.empty() : tensor<1x128xf32>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %60 = ttir.empty() : tensor<1x128xbf16>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %62 = ttir.empty() : tensor<1x128xbf16>
        %63 = "ttir.maximum"(%61, %2, %62) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
        %64 = ttir.empty() : tensor<1x128xf32>
        %65 = "ttir.typecast"(%63, %64) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
        %66 = ttir.empty() : tensor<1x10xf32>
        %67 = "ttir.matmul"(%65, %arg6, %66) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32>, tensor<128x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %68 = ttir.empty() : tensor<1x10xf32>
        %69 = "ttir.broadcast"(%51, %68) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %70 = ttir.empty() : tensor<1x10xf32>
        %71 = "ttir.multiply"(%67, %69, %70) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %72 = ttir.empty() : tensor<1x10xf32>
        %73 = "ttir.reshape"(%arg7, %72) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %74 = ttir.empty() : tensor<1x10xf32>
        %75 = "ttir.add"(%71, %73, %74) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %76 = ttir.empty() : tensor<1xf32>
        %77 = "ttir.max"(%75, %76) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %78 = ttir.empty() : tensor<1x1xf32>
        %79 = "ttir.reshape"(%77, %78) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %80 = ttir.empty() : tensor<1x10xf32>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %82 = ttir.empty() : tensor<1x10xf32>
        %83 = "ttir.subtract"(%75, %81, %82) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %84 = ttir.empty() : tensor<1x10xf32>
        %85 = "ttir.exp"(%83, %84) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %86 = ttir.empty() : tensor<1xf32>
        %87 = "ttir.sum"(%85, %86) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32>, tensor<1xf32>) -> tensor<1xf32>
        %88 = ttir.empty() : tensor<1x1xf32>
        %89 = "ttir.reshape"(%87, %88) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %90 = ttir.empty() : tensor<1x1xf32>
        %91 = "ttir.log"(%89, %90) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %92 = ttir.empty() : tensor<1x10xf32>
        %93 = "ttir.broadcast"(%91, %92) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %94 = ttir.empty() : tensor<1x10xf32>
        %95 = "ttir.subtract"(%83, %93, %94) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
        %96 = ttir.empty() : tensor<1x10xbf16>
        %97 = "ttir.typecast"(%95, %96) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32>, tensor<1x10xbf16>) -> tensor<1x10xbf16>
        return %97 : tensor<1x10xbf16>
      }
    }
  }
}


// -----// IR Dump After TTNNLayout (ttnn-layout) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16, #ttnn_layout10>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32, #ttnn_layout15>
        %4 = ttir.empty() : tensor<1x28x28x1xbf16, #ttnn_layout16>
        %5 = "ttir.permute"(%arg12, %4) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>, tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = ttir.empty() : tensor<1x1x784x1xbf16, #ttnn_layout17>
        %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>, tensor<1x1x784x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %8 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %9 = "ttir.conv2d"(%7, %arg0, %8) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>, tensor<32x1x3x3xbf16, #ttnn_layout>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16, #ttnn_layout13>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>, tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %12 = ttir.empty() : tensor<1x1x1x32xbf16, #ttnn_layout11>
        %13 = "ttir.permute"(%11, %12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %14 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x1x32xbf16, #ttnn_layout11>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %16 = ttir.empty() : tensor<1x26x26x32xbf16, #ttnn_layout19>
        %17 = "ttir.permute"(%0, %16) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>, tensor<1x26x26x32xbf16, #ttnn_layout19>) -> tensor<1x26x26x32xbf16, #ttnn_layout19>
        %18 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %19 = "ttir.reshape"(%17, %18) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout19>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %20 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %21 = "ttir.maximum"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %22 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %23 = "ttir.conv2d"(%21, %arg2, %22) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<64x32x3x3xbf16, #ttnn_layout2>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16, #ttnn_layout14>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>, tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %26 = ttir.empty() : tensor<1x1x1x64xbf16, #ttnn_layout21>
        %27 = "ttir.permute"(%25, %26) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>, tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x1x64xbf16, #ttnn_layout21>
        %28 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x1x64xbf16, #ttnn_layout21>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %30 = ttir.empty() : tensor<1x24x24x64xbf16, #ttnn_layout22>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>, tensor<1x24x24x64xbf16, #ttnn_layout22>) -> tensor<1x24x24x64xbf16, #ttnn_layout22>
        %32 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout22>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %34 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %35 = "ttir.maximum"(%29, %33, %34) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %36 = ttir.empty() : tensor<1x1x144x64xbf16, #ttnn_layout23>
        %37 = "ttir.max_pool2d"(%35, %36) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x144x64xbf16, #ttnn_layout23>) -> tensor<1x1x144x64xbf16, #ttnn_layout23>
        %38 = ttir.empty() : tensor<1x12x12x64xbf16, #ttnn_layout24>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout23>, tensor<1x12x12x64xbf16, #ttnn_layout24>) -> tensor<1x12x12x64xbf16, #ttnn_layout24>
        %40 = ttir.empty() : tensor<1x64x12x12xbf16, #ttnn_layout14>
        %41 = "ttir.permute"(%39, %40) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout24>, tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %42 = ttir.empty() : tensor<1x9216xbf16, #ttnn_layout25>
        %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>, tensor<1x9216xbf16, #ttnn_layout25>) -> tensor<1x9216xbf16, #ttnn_layout25>
        %44 = ttir.empty() : tensor<1x9216xf32, #ttnn_layout26>
        %45 = "ttir.typecast"(%43, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16, #ttnn_layout25>, tensor<1x9216xf32, #ttnn_layout26>) -> tensor<1x9216xf32, #ttnn_layout26>
        %46 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %47 = "ttir.matmul"(%45, %arg4, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout26>, tensor<9216x128xf32, #ttnn_layout4>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %48 = ttir.empty() : tensor<1x1xsi32, #ttnn_layout28>
        %49 = "ttir.reshape"(%3, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn_layout15>, tensor<1x1xsi32, #ttnn_layout28>) -> tensor<1x1xsi32, #ttnn_layout28>
        %50 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %51 = "ttir.typecast"(%49, %50) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xsi32, #ttnn_layout28>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %52 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %54 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %55 = "ttir.multiply"(%47, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %56 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %58 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %60 = ttir.empty() : tensor<1x128xbf16, #ttnn_layout10>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %62 = ttir.empty() : tensor<1x128xbf16, #ttnn_layout10>
        %63 = "ttir.maximum"(%61, %2, %62) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %64 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %65 = "ttir.typecast"(%63, %64) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %66 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %67 = "ttir.matmul"(%65, %arg6, %66) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<128x10xf32, #ttnn_layout6>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %68 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %69 = "ttir.broadcast"(%51, %68) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %70 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %71 = "ttir.multiply"(%67, %69, %70) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %72 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %73 = "ttir.reshape"(%arg7, %72) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %74 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %75 = "ttir.add"(%71, %73, %74) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %76 = ttir.empty() : tensor<1xf32, #ttnn_layout7>
        %77 = "ttir.max"(%75, %76) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1xf32, #ttnn_layout7>) -> tensor<1xf32, #ttnn_layout7>
        %78 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %79 = "ttir.reshape"(%77, %78) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %80 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %82 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %83 = "ttir.subtract"(%75, %81, %82) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %84 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %85 = "ttir.exp"(%83, %84) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %86 = ttir.empty() : tensor<1xf32, #ttnn_layout7>
        %87 = "ttir.sum"(%85, %86) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1xf32, #ttnn_layout7>) -> tensor<1xf32, #ttnn_layout7>
        %88 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %89 = "ttir.reshape"(%87, %88) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %90 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %91 = "ttir.log"(%89, %90) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %92 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %93 = "ttir.broadcast"(%91, %92) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %94 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %95 = "ttir.subtract"(%83, %93, %94) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %96 = ttir.empty() : tensor<1x10xbf16, #ttnn_layout12>
        %97 = "ttir.typecast"(%95, %96) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xbf16, #ttnn_layout12>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %97 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump Before ConvertTTIRToTTNN (convert-ttir-to-ttnn) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x32x26x26xbf16>}> : () -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x24x24xbf16>}> : () -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128xbf16>}> : () -> tensor<1x128xbf16, #ttnn_layout10>
        %3 = "ttir.constant"() <{value = dense<1> : tensor<1xsi32>}> : () -> tensor<1xsi32, #ttnn_layout15>
        %4 = ttir.empty() : tensor<1x28x28x1xbf16, #ttnn_layout16>
        %5 = "ttir.permute"(%arg12, %4) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>, tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = ttir.empty() : tensor<1x1x784x1xbf16, #ttnn_layout17>
        %7 = "ttir.reshape"(%5, %6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>, tensor<1x1x784x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %8 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %9 = "ttir.conv2d"(%7, %arg0, %8) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 28, input_width = 28>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>, tensor<32x1x3x3xbf16, #ttnn_layout>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %10 = ttir.empty() : tensor<1x32x1x1xbf16, #ttnn_layout13>
        %11 = "ttir.reshape"(%arg1, %10) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>, tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %12 = ttir.empty() : tensor<1x1x1x32xbf16, #ttnn_layout11>
        %13 = "ttir.permute"(%11, %12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %14 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %15 = "ttir.add"(%9, %13, %14) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x1x32xbf16, #ttnn_layout11>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %16 = ttir.empty() : tensor<1x26x26x32xbf16, #ttnn_layout19>
        %17 = "ttir.permute"(%0, %16) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>, tensor<1x26x26x32xbf16, #ttnn_layout19>) -> tensor<1x26x26x32xbf16, #ttnn_layout19>
        %18 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %19 = "ttir.reshape"(%17, %18) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout19>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %20 = ttir.empty() : tensor<1x1x676x32xbf16, #ttnn_layout18>
        %21 = "ttir.maximum"(%15, %19, %20) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %22 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %23 = "ttir.conv2d"(%21, %arg2, %22) <{dilation = array<i32: 1, 1>, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 26, input_width = 26>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<64x32x3x3xbf16, #ttnn_layout2>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %24 = ttir.empty() : tensor<1x64x1x1xbf16, #ttnn_layout14>
        %25 = "ttir.reshape"(%arg3, %24) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>, tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %26 = ttir.empty() : tensor<1x1x1x64xbf16, #ttnn_layout21>
        %27 = "ttir.permute"(%25, %26) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>, tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x1x64xbf16, #ttnn_layout21>
        %28 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %29 = "ttir.add"(%23, %27, %28) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x1x64xbf16, #ttnn_layout21>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %30 = ttir.empty() : tensor<1x24x24x64xbf16, #ttnn_layout22>
        %31 = "ttir.permute"(%1, %30) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>, tensor<1x24x24x64xbf16, #ttnn_layout22>) -> tensor<1x24x24x64xbf16, #ttnn_layout22>
        %32 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %33 = "ttir.reshape"(%31, %32) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout22>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %34 = ttir.empty() : tensor<1x1x576x64xbf16, #ttnn_layout20>
        %35 = "ttir.maximum"(%29, %33, %34) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %36 = ttir.empty() : tensor<1x1x144x64xbf16, #ttnn_layout23>
        %37 = "ttir.max_pool2d"(%35, %36) <{ceil_mode = false, dilation_height = 1 : si32, dilation_width = 1 : si32, flattened_compat_info = #ttir<flattened_compat batch_size = 1, input_height = 24, input_width = 24>, kernel_height = 2 : si32, kernel_width = 2 : si32, padding_bottom = 0 : si32, padding_left = 0 : si32, padding_right = 0 : si32, padding_top = 0 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x144x64xbf16, #ttnn_layout23>) -> tensor<1x1x144x64xbf16, #ttnn_layout23>
        %38 = ttir.empty() : tensor<1x12x12x64xbf16, #ttnn_layout24>
        %39 = "ttir.reshape"(%37, %38) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout23>, tensor<1x12x12x64xbf16, #ttnn_layout24>) -> tensor<1x12x12x64xbf16, #ttnn_layout24>
        %40 = ttir.empty() : tensor<1x64x12x12xbf16, #ttnn_layout14>
        %41 = "ttir.permute"(%39, %40) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout24>, tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %42 = ttir.empty() : tensor<1x9216xbf16, #ttnn_layout25>
        %43 = "ttir.reshape"(%41, %42) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>, tensor<1x9216xbf16, #ttnn_layout25>) -> tensor<1x9216xbf16, #ttnn_layout25>
        %44 = ttir.empty() : tensor<1x9216xf32, #ttnn_layout26>
        %45 = "ttir.typecast"(%43, %44) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x9216xbf16, #ttnn_layout25>, tensor<1x9216xf32, #ttnn_layout26>) -> tensor<1x9216xf32, #ttnn_layout26>
        %46 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %47 = "ttir.matmul"(%45, %arg4, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout26>, tensor<9216x128xf32, #ttnn_layout4>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %48 = ttir.empty() : tensor<1x1xsi32, #ttnn_layout28>
        %49 = "ttir.reshape"(%3, %48) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn_layout15>, tensor<1x1xsi32, #ttnn_layout28>) -> tensor<1x1xsi32, #ttnn_layout28>
        %50 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %51 = "ttir.typecast"(%49, %50) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xsi32, #ttnn_layout28>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %52 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %53 = "ttir.broadcast"(%51, %52) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %54 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %55 = "ttir.multiply"(%47, %53, %54) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %56 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %57 = "ttir.reshape"(%arg5, %56) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %58 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %59 = "ttir.add"(%55, %57, %58) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %60 = ttir.empty() : tensor<1x128xbf16, #ttnn_layout10>
        %61 = "ttir.typecast"(%59, %60) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %62 = ttir.empty() : tensor<1x128xbf16, #ttnn_layout10>
        %63 = "ttir.maximum"(%61, %2, %62) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %64 = ttir.empty() : tensor<1x128xf32, #ttnn_layout27>
        %65 = "ttir.typecast"(%63, %64) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %66 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %67 = "ttir.matmul"(%65, %arg6, %66) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<128x10xf32, #ttnn_layout6>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %68 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %69 = "ttir.broadcast"(%51, %68) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %70 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %71 = "ttir.multiply"(%67, %69, %70) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %72 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %73 = "ttir.reshape"(%arg7, %72) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %74 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %75 = "ttir.add"(%71, %73, %74) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %76 = ttir.empty() : tensor<1xf32, #ttnn_layout7>
        %77 = "ttir.max"(%75, %76) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1xf32, #ttnn_layout7>) -> tensor<1xf32, #ttnn_layout7>
        %78 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %79 = "ttir.reshape"(%77, %78) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %80 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %81 = "ttir.broadcast"(%79, %80) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %82 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %83 = "ttir.subtract"(%75, %81, %82) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %84 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %85 = "ttir.exp"(%83, %84) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %86 = ttir.empty() : tensor<1xf32, #ttnn_layout7>
        %87 = "ttir.sum"(%85, %86) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1xf32, #ttnn_layout7>) -> tensor<1xf32, #ttnn_layout7>
        %88 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %89 = "ttir.reshape"(%87, %88) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %90 = ttir.empty() : tensor<1x1xf32, #ttnn_layout29>
        %91 = "ttir.log"(%89, %90) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %92 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %93 = "ttir.broadcast"(%91, %92) <{broadcast_dimensions = array<i64: 1, 10>}> : (tensor<1x1xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %94 = ttir.empty() : tensor<1x10xf32, #ttnn_layout29>
        %95 = "ttir.subtract"(%83, %93, %94) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %96 = ttir.empty() : tensor<1x10xbf16, #ttnn_layout12>
        %97 = "ttir.typecast"(%95, %96) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xbf16, #ttnn_layout12>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %97 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump After ConvertTTIRToTTNN (convert-ttir-to-ttnn) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xsi32, #ttnn_layout15>
        %5 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<28x1>>, <interleaved>>, shape = #ttnn.shape<1x28x28x1>}> : (!ttnn.device) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %7 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<25x1>>, <interleaved>>, shape = #ttnn.shape<1x1x784x1>}> : (!ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %8 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %9 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %10 = "ttnn.conv2d"(%8, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %11 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<32x1>>, <interleaved>>, shape = #ttnn.shape<1x32x1x1>}> : (!ttnn.device) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %13 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1x1x32>}> : (!ttnn.device) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %14 = "ttnn.permute"(%12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %15 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %16 = "ttnn.add"(%10, %14) : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %17 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<26x1>>, <interleaved>>, shape = #ttnn.shape<1x26x26x32>}> : (!ttnn.device) -> tensor<1x26x26x32xbf16, #ttnn_layout19>
        %18 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout19>
        %19 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %20 = "ttnn.reshape"(%18) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout19>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %21 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %22 = "ttnn.maximum"(%16, %20) : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %23 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %24 = "ttnn.conv2d"(%22, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %25 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<64x1>>, <interleaved>>, shape = #ttnn.shape<1x64x1x1>}> : (!ttnn.device) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %26 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %27 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x2>>, <interleaved>>, shape = #ttnn.shape<1x1x1x64>}> : (!ttnn.device) -> tensor<1x1x1x64xbf16, #ttnn_layout21>
        %28 = "ttnn.permute"(%26) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout21>
        %29 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %30 = "ttnn.add"(%24, %28) : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %31 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<24x2>>, <interleaved>>, shape = #ttnn.shape<1x24x24x64>}> : (!ttnn.device) -> tensor<1x24x24x64xbf16, #ttnn_layout22>
        %32 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout22>
        %33 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %34 = "ttnn.reshape"(%32) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout22>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %35 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %36 = "ttnn.maximum"(%30, %34) : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %37 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<5x2>>, <interleaved>>, shape = #ttnn.shape<1x1x144x64>}> : (!ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout23>
        %38 = "ttnn.max_pool2d"(%36) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x144x64xbf16, #ttnn_layout23>
        %39 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<12x2>>, <interleaved>>, shape = #ttnn.shape<1x12x12x64>}> : (!ttnn.device) -> tensor<1x12x12x64xbf16, #ttnn_layout24>
        %40 = "ttnn.reshape"(%38) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout23>) -> tensor<1x12x12x64xbf16, #ttnn_layout24>
        %41 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<64x1>>, <interleaved>>, shape = #ttnn.shape<1x64x12x12>}> : (!ttnn.device) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %42 = "ttnn.permute"(%40) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout24>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %43 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x288>>, <interleaved>>, shape = #ttnn.shape<1x9216>}> : (!ttnn.device) -> tensor<1x9216xbf16, #ttnn_layout25>
        %44 = "ttnn.reshape"(%42) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout25>
        %45 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x288>>, <interleaved>>, shape = #ttnn.shape<1x9216>}> : (!ttnn.device) -> tensor<1x9216xf32, #ttnn_layout26>
        %46 = "ttnn.typecast"(%44) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout25>) -> tensor<1x9216xf32, #ttnn_layout26>
        %47 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %48 = "ttnn.matmul"(%46, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout26>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout27>
        %49 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xsi32, #ttnn_layout28>
        %50 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn_layout15>) -> tensor<1x1xsi32, #ttnn_layout28>
        %51 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %52 = "ttnn.typecast"(%50) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout28>) -> tensor<1x1xf32, #ttnn_layout29>
        %53 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %54 = "ttnn.repeat"(%52) <{repeat_dims = #ttnn.shape<1x128>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x128xf32, #ttnn_layout27>
        %55 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %56 = "ttnn.multiply"(%48, %54) : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %57 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %58 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout27>
        %59 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %60 = "ttnn.add"(%56, %58) : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %61 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %62 = "ttnn.typecast"(%60) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xbf16, #ttnn_layout10>
        %63 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %64 = "ttnn.maximum"(%62, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %65 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %66 = "ttnn.typecast"(%64) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout27>
        %67 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %68 = "ttnn.matmul"(%66, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout29>
        %69 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %70 = "ttnn.repeat"(%52) <{repeat_dims = #ttnn.shape<1x10>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %71 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %72 = "ttnn.multiply"(%68, %70) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %73 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %74 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout29>
        %75 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %76 = "ttnn.add"(%72, %74) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %77 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xf32, #ttnn_layout7>
        %78 = "ttnn.max"(%76) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1xf32, #ttnn_layout7>
        %79 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %80 = "ttnn.reshape"(%78) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout29>
        %81 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %82 = "ttnn.repeat"(%80) <{repeat_dims = #ttnn.shape<1x10>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %83 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %84 = "ttnn.subtract"(%76, %82) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %85 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %86 = "ttnn.exp"(%84) : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %87 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xf32, #ttnn_layout7>
        %88 = "ttnn.sum"(%86) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1xf32, #ttnn_layout7>
        %89 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %90 = "ttnn.reshape"(%88) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout29>
        %91 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %92 = "ttnn.log"(%90) : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %93 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %94 = "ttnn.repeat"(%92) <{repeat_dims = #ttnn.shape<1x10>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %95 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %96 = "ttnn.subtract"(%84, %94) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %97 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xbf16, #ttnn_layout12>
        %98 = "ttnn.typecast"(%96) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %98 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump Before TTNNWorkarounds (ttnn-workaround) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xsi32, #ttnn_layout15>
        %5 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<28x1>>, <interleaved>>, shape = #ttnn.shape<1x28x28x1>}> : (!ttnn.device) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %7 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<25x1>>, <interleaved>>, shape = #ttnn.shape<1x1x784x1>}> : (!ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %8 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %9 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %10 = "ttnn.conv2d"(%8, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %11 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<32x1>>, <interleaved>>, shape = #ttnn.shape<1x32x1x1>}> : (!ttnn.device) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %12 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %13 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1x1x32>}> : (!ttnn.device) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %14 = "ttnn.permute"(%12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %15 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %16 = "ttnn.add"(%10, %14) : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %17 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<26x1>>, <interleaved>>, shape = #ttnn.shape<1x26x26x32>}> : (!ttnn.device) -> tensor<1x26x26x32xbf16, #ttnn_layout19>
        %18 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout19>
        %19 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %20 = "ttnn.reshape"(%18) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout19>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %21 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<22x1>>, <interleaved>>, shape = #ttnn.shape<1x1x676x32>}> : (!ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %22 = "ttnn.maximum"(%16, %20) : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<1x1x676x32xbf16, #ttnn_layout18>) -> tensor<1x1x676x32xbf16, #ttnn_layout18>
        %23 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %24 = "ttnn.conv2d"(%22, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout18>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %25 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<64x1>>, <interleaved>>, shape = #ttnn.shape<1x64x1x1>}> : (!ttnn.device) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %26 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %27 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x2>>, <interleaved>>, shape = #ttnn.shape<1x1x1x64>}> : (!ttnn.device) -> tensor<1x1x1x64xbf16, #ttnn_layout21>
        %28 = "ttnn.permute"(%26) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout21>
        %29 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %30 = "ttnn.add"(%24, %28) : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x1x64xbf16, #ttnn_layout21>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %31 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<24x2>>, <interleaved>>, shape = #ttnn.shape<1x24x24x64>}> : (!ttnn.device) -> tensor<1x24x24x64xbf16, #ttnn_layout22>
        %32 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout22>
        %33 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %34 = "ttnn.reshape"(%32) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout22>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %35 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<18x2>>, <interleaved>>, shape = #ttnn.shape<1x1x576x64>}> : (!ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %36 = "ttnn.maximum"(%30, %34) : (tensor<1x1x576x64xbf16, #ttnn_layout20>, tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x576x64xbf16, #ttnn_layout20>
        %37 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<5x2>>, <interleaved>>, shape = #ttnn.shape<1x1x144x64>}> : (!ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout23>
        %38 = "ttnn.max_pool2d"(%36) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout20>) -> tensor<1x1x144x64xbf16, #ttnn_layout23>
        %39 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<12x2>>, <interleaved>>, shape = #ttnn.shape<1x12x12x64>}> : (!ttnn.device) -> tensor<1x12x12x64xbf16, #ttnn_layout24>
        %40 = "ttnn.reshape"(%38) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout23>) -> tensor<1x12x12x64xbf16, #ttnn_layout24>
        %41 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<64x1>>, <interleaved>>, shape = #ttnn.shape<1x64x12x12>}> : (!ttnn.device) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %42 = "ttnn.permute"(%40) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout24>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %43 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x288>>, <interleaved>>, shape = #ttnn.shape<1x9216>}> : (!ttnn.device) -> tensor<1x9216xbf16, #ttnn_layout25>
        %44 = "ttnn.reshape"(%42) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout25>
        %45 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x288>>, <interleaved>>, shape = #ttnn.shape<1x9216>}> : (!ttnn.device) -> tensor<1x9216xf32, #ttnn_layout26>
        %46 = "ttnn.typecast"(%44) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout25>) -> tensor<1x9216xf32, #ttnn_layout26>
        %47 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %48 = "ttnn.matmul"(%46, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout26>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout27>
        %49 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xsi32, #ttnn_layout28>
        %50 = "ttnn.reshape"(%4) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xsi32, #ttnn_layout15>) -> tensor<1x1xsi32, #ttnn_layout28>
        %51 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %52 = "ttnn.typecast"(%50) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout28>) -> tensor<1x1xf32, #ttnn_layout29>
        %53 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %54 = "ttnn.repeat"(%52) <{repeat_dims = #ttnn.shape<1x128>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x128xf32, #ttnn_layout27>
        %55 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %56 = "ttnn.multiply"(%48, %54) : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %57 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %58 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout27>
        %59 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %60 = "ttnn.add"(%56, %58) : (tensor<1x128xf32, #ttnn_layout27>, tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xf32, #ttnn_layout27>
        %61 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %62 = "ttnn.typecast"(%60) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout27>) -> tensor<1x128xbf16, #ttnn_layout10>
        %63 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %64 = "ttnn.maximum"(%62, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %65 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x128>}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout27>
        %66 = "ttnn.typecast"(%64) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout27>
        %67 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %68 = "ttnn.matmul"(%66, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout27>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout29>
        %69 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %70 = "ttnn.repeat"(%52) <{repeat_dims = #ttnn.shape<1x10>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %71 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %72 = "ttnn.multiply"(%68, %70) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %73 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %74 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout29>
        %75 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %76 = "ttnn.add"(%72, %74) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %77 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xf32, #ttnn_layout7>
        %78 = "ttnn.max"(%76) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1xf32, #ttnn_layout7>
        %79 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %80 = "ttnn.reshape"(%78) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout29>
        %81 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %82 = "ttnn.repeat"(%80) <{repeat_dims = #ttnn.shape<1x10>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %83 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %84 = "ttnn.subtract"(%76, %82) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %85 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %86 = "ttnn.exp"(%84) : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %87 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1>}> : (!ttnn.device) -> tensor<1xf32, #ttnn_layout7>
        %88 = "ttnn.sum"(%86) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1xf32, #ttnn_layout7>
        %89 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %90 = "ttnn.reshape"(%88) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout29>
        %91 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn_layout29>
        %92 = "ttnn.log"(%90) : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x1xf32, #ttnn_layout29>
        %93 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %94 = "ttnn.repeat"(%92) <{repeat_dims = #ttnn.shape<1x10>}> : (tensor<1x1xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %95 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout29>
        %96 = "ttnn.subtract"(%84, %94) : (tensor<1x10xf32, #ttnn_layout29>, tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xf32, #ttnn_layout29>
        %97 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>, shape = #ttnn.shape<1x10>}> : (!ttnn.device) -> tensor<1x10xbf16, #ttnn_layout12>
        %98 = "ttnn.typecast"(%96) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout29>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %98 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump After TTNNWorkarounds (ttnn-workaround) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.to_layout"(%4, %0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout15>, !ttnn.device) -> tensor<1xsi32, #ttnn_layout16>
        %6 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout17>
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        %8 = "ttnn.to_layout"(%7, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout19>
        %9 = "ttnn.conv2d"(%8, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %10 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %11 = "ttnn.permute"(%10) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %12 = "ttnn.add"(%9, %11) : (tensor<1x1x676x32xbf16, #ttnn_layout20>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %13 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout21>
        %14 = "ttnn.reshape"(%13) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %15 = "ttnn.maximum"(%12, %14) : (tensor<1x1x676x32xbf16, #ttnn_layout20>, tensor<1x1x676x32xbf16, #ttnn_layout20>) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %16 = "ttnn.to_layout"(%15, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout20>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout22>
        %17 = "ttnn.conv2d"(%16, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout22>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %18 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %19 = "ttnn.permute"(%18) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout24>
        %20 = "ttnn.add"(%17, %19) : (tensor<1x1x576x64xbf16, #ttnn_layout23>, tensor<1x1x1x64xbf16, #ttnn_layout24>) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %21 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout25>
        %22 = "ttnn.reshape"(%21) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout25>) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %23 = "ttnn.maximum"(%20, %22) : (tensor<1x1x576x64xbf16, #ttnn_layout23>, tensor<1x1x576x64xbf16, #ttnn_layout23>) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %24 = "ttnn.to_layout"(%23, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %25 = "ttnn.max_pool2d"(%24) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x144x64xbf16, #ttnn_layout27>
        %26 = "ttnn.to_layout"(%25, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<5x2>>, <interleaved>>}> : (tensor<1x1x144x64xbf16, #ttnn_layout27>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout28>
        %27 = "ttnn.reshape"(%26) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout28>) -> tensor<1x12x12x64xbf16, #ttnn_layout29>
        %28 = "ttnn.permute"(%27) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout29>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %29 = "ttnn.reshape"(%28) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout30>
        %30 = "ttnn.typecast"(%29) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout30>) -> tensor<1x9216xf32, #ttnn_layout31>
        %31 = "ttnn.matmul"(%30, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout31>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout32>
        %32 = "ttnn.to_layout"(%5, %0) <{dtype = #tt.supportedDataTypes<u32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xsi32, #ttnn_layout16>, !ttnn.device) -> tensor<1xui32, #ttnn_layout33>
        %33 = "ttnn.reshape"(%32) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout33>) -> tensor<1x1xui32, #ttnn_layout34>
        %34 = "ttnn.to_layout"(%33, %0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1x1xui32, #ttnn_layout34>, !ttnn.device) -> tensor<1x1xsi32, #ttnn_layout35>
        %35 = "ttnn.typecast"(%34) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout35>) -> tensor<1x1xf32, #ttnn_layout36>
        %36 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout32>
        %37 = "ttnn.add"(%35, %36) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xf32, #ttnn_layout32>
        %38 = "ttnn.multiply"(%31, %37) : (tensor<1x128xf32, #ttnn_layout32>, tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xf32, #ttnn_layout32>
        %39 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout32>
        %40 = "ttnn.add"(%38, %39) : (tensor<1x128xf32, #ttnn_layout32>, tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xf32, #ttnn_layout32>
        %41 = "ttnn.typecast"(%40) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xbf16, #ttnn_layout10>
        %42 = "ttnn.maximum"(%41, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %43 = "ttnn.typecast"(%42) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout32>
        %44 = "ttnn.matmul"(%43, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout32>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout36>
        %45 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout36>
        %46 = "ttnn.add"(%35, %45) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %47 = "ttnn.multiply"(%44, %46) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %48 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout36>
        %49 = "ttnn.add"(%47, %48) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %50 = "ttnn.max"(%49) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1xf32, #ttnn_layout7>
        %51 = "ttnn.reshape"(%50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout36>
        %52 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout36>
        %53 = "ttnn.add"(%51, %52) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %54 = "ttnn.subtract"(%49, %53) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %55 = "ttnn.exp"(%54) : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %56 = "ttnn.sum"(%55) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1xf32, #ttnn_layout7>
        %57 = "ttnn.reshape"(%56) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout36>
        %58 = "ttnn.log"(%57) : (tensor<1x1xf32, #ttnn_layout36>) -> tensor<1x1xf32, #ttnn_layout36>
        %59 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout36>
        %60 = "ttnn.add"(%58, %59) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %61 = "ttnn.subtract"(%54, %60) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %62 = "ttnn.typecast"(%61) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %62 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.to_layout"(%4, %0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout15>, !ttnn.device) -> tensor<1xsi32, #ttnn_layout16>
        %6 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout17>
        %7 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        %8 = "ttnn.to_layout"(%7, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout19>
        %9 = "ttnn.conv2d"(%8, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %10 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %11 = "ttnn.permute"(%10) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %12 = "ttnn.add"(%9, %11) : (tensor<1x1x676x32xbf16, #ttnn_layout20>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %13 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout21>
        %14 = "ttnn.reshape"(%13) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %15 = "ttnn.maximum"(%12, %14) : (tensor<1x1x676x32xbf16, #ttnn_layout20>, tensor<1x1x676x32xbf16, #ttnn_layout20>) -> tensor<1x1x676x32xbf16, #ttnn_layout20>
        %16 = "ttnn.to_layout"(%15, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout20>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout22>
        %17 = "ttnn.conv2d"(%16, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout22>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %18 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %19 = "ttnn.permute"(%18) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout24>
        %20 = "ttnn.add"(%17, %19) : (tensor<1x1x576x64xbf16, #ttnn_layout23>, tensor<1x1x1x64xbf16, #ttnn_layout24>) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %21 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout25>
        %22 = "ttnn.reshape"(%21) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout25>) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %23 = "ttnn.maximum"(%20, %22) : (tensor<1x1x576x64xbf16, #ttnn_layout23>, tensor<1x1x576x64xbf16, #ttnn_layout23>) -> tensor<1x1x576x64xbf16, #ttnn_layout23>
        %24 = "ttnn.to_layout"(%23, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %25 = "ttnn.max_pool2d"(%24) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x144x64xbf16, #ttnn_layout27>
        %26 = "ttnn.to_layout"(%25, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<5x2>>, <interleaved>>}> : (tensor<1x1x144x64xbf16, #ttnn_layout27>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout28>
        %27 = "ttnn.reshape"(%26) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout28>) -> tensor<1x12x12x64xbf16, #ttnn_layout29>
        %28 = "ttnn.permute"(%27) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout29>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %29 = "ttnn.reshape"(%28) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout30>
        %30 = "ttnn.typecast"(%29) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout30>) -> tensor<1x9216xf32, #ttnn_layout31>
        %31 = "ttnn.matmul"(%30, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout31>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout32>
        %32 = "ttnn.to_layout"(%5, %0) <{dtype = #tt.supportedDataTypes<u32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xsi32, #ttnn_layout16>, !ttnn.device) -> tensor<1xui32, #ttnn_layout33>
        %33 = "ttnn.reshape"(%32) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout33>) -> tensor<1x1xui32, #ttnn_layout34>
        %34 = "ttnn.to_layout"(%33, %0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1x1xui32, #ttnn_layout34>, !ttnn.device) -> tensor<1x1xsi32, #ttnn_layout35>
        %35 = "ttnn.typecast"(%34) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout35>) -> tensor<1x1xf32, #ttnn_layout36>
        %36 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout32>
        %37 = "ttnn.add"(%35, %36) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xf32, #ttnn_layout32>
        %38 = "ttnn.multiply"(%31, %37) : (tensor<1x128xf32, #ttnn_layout32>, tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xf32, #ttnn_layout32>
        %39 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout32>
        %40 = "ttnn.add"(%38, %39) : (tensor<1x128xf32, #ttnn_layout32>, tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xf32, #ttnn_layout32>
        %41 = "ttnn.typecast"(%40) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout32>) -> tensor<1x128xbf16, #ttnn_layout10>
        %42 = "ttnn.maximum"(%41, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %43 = "ttnn.typecast"(%42) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout32>
        %44 = "ttnn.matmul"(%43, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout32>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout36>
        %45 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout36>
        %46 = "ttnn.add"(%35, %45) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %47 = "ttnn.multiply"(%44, %46) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %48 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout36>
        %49 = "ttnn.add"(%47, %48) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %50 = "ttnn.max"(%49) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1xf32, #ttnn_layout7>
        %51 = "ttnn.reshape"(%50) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout36>
        %52 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout36>
        %53 = "ttnn.add"(%51, %52) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %54 = "ttnn.subtract"(%49, %53) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %55 = "ttnn.exp"(%54) : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %56 = "ttnn.sum"(%55) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1xf32, #ttnn_layout7>
        %57 = "ttnn.reshape"(%56) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout36>
        %58 = "ttnn.log"(%57) : (tensor<1x1xf32, #ttnn_layout36>) -> tensor<1x1xf32, #ttnn_layout36>
        %59 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout36>
        %60 = "ttnn.add"(%58, %59) : (tensor<1x1xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %61 = "ttnn.subtract"(%54, %60) : (tensor<1x10xf32, #ttnn_layout36>, tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xf32, #ttnn_layout36>
        %62 = "ttnn.typecast"(%61) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout36>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %62 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %7 = "ttnn.to_layout"(%6, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        %8 = "ttnn.conv2d"(%7, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %9 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %10 = "ttnn.permute"(%9) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %11 = "ttnn.add"(%8, %10) : (tensor<1x1x676x32xbf16, #ttnn_layout19>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %12 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout20>
        %13 = "ttnn.reshape"(%12) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout20>) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %14 = "ttnn.maximum"(%11, %13) : (tensor<1x1x676x32xbf16, #ttnn_layout19>, tensor<1x1x676x32xbf16, #ttnn_layout19>) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %15 = "ttnn.to_layout"(%14, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %16 = "ttnn.conv2d"(%15, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %17 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %18 = "ttnn.permute"(%17) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout23>
        %19 = "ttnn.add"(%16, %18) : (tensor<1x1x576x64xbf16, #ttnn_layout22>, tensor<1x1x1x64xbf16, #ttnn_layout23>) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %20 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout24>
        %21 = "ttnn.reshape"(%20) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout24>) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %22 = "ttnn.maximum"(%19, %21) : (tensor<1x1x576x64xbf16, #ttnn_layout22>, tensor<1x1x576x64xbf16, #ttnn_layout22>) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %23 = "ttnn.to_layout"(%22, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout25>
        %24 = "ttnn.max_pool2d"(%23) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout25>) -> tensor<1x1x144x64xbf16, #ttnn_layout26>
        %25 = "ttnn.to_layout"(%24, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<5x2>>, <interleaved>>}> : (tensor<1x1x144x64xbf16, #ttnn_layout26>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout27>
        %26 = "ttnn.reshape"(%25) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout27>) -> tensor<1x12x12x64xbf16, #ttnn_layout28>
        %27 = "ttnn.permute"(%26) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout28>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %28 = "ttnn.reshape"(%27) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout29>
        %29 = "ttnn.typecast"(%28) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout29>) -> tensor<1x9216xf32, #ttnn_layout30>
        %30 = "ttnn.matmul"(%29, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout30>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout31>
        %31 = "ttnn.to_layout"(%4, %0) <{dtype = #tt.supportedDataTypes<u32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout15>, !ttnn.device) -> tensor<1xui32, #ttnn_layout32>
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout32>) -> tensor<1x1xui32, #ttnn_layout33>
        %33 = "ttnn.to_layout"(%32, %0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1x1xui32, #ttnn_layout33>, !ttnn.device) -> tensor<1x1xsi32, #ttnn_layout34>
        %34 = "ttnn.typecast"(%33) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout34>) -> tensor<1x1xf32, #ttnn_layout35>
        %35 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout31>
        %36 = "ttnn.add"(%34, %35) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xf32, #ttnn_layout31>
        %37 = "ttnn.multiply"(%30, %36) : (tensor<1x128xf32, #ttnn_layout31>, tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xf32, #ttnn_layout31>
        %38 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout31>
        %39 = "ttnn.add"(%37, %38) : (tensor<1x128xf32, #ttnn_layout31>, tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xf32, #ttnn_layout31>
        %40 = "ttnn.typecast"(%39) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xbf16, #ttnn_layout10>
        %41 = "ttnn.maximum"(%40, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %42 = "ttnn.typecast"(%41) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout31>
        %43 = "ttnn.matmul"(%42, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout31>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout35>
        %44 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout35>
        %45 = "ttnn.add"(%34, %44) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %46 = "ttnn.multiply"(%43, %45) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %47 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout35>
        %48 = "ttnn.add"(%46, %47) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %49 = "ttnn.max"(%48) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1xf32, #ttnn_layout7>
        %50 = "ttnn.reshape"(%49) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout35>
        %51 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout35>
        %52 = "ttnn.add"(%50, %51) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %53 = "ttnn.subtract"(%48, %52) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %54 = "ttnn.exp"(%53) : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %55 = "ttnn.sum"(%54) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1xf32, #ttnn_layout7>
        %56 = "ttnn.reshape"(%55) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout35>
        %57 = "ttnn.log"(%56) : (tensor<1x1xf32, #ttnn_layout35>) -> tensor<1x1xf32, #ttnn_layout35>
        %58 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout35>
        %59 = "ttnn.add"(%57, %58) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %60 = "ttnn.subtract"(%53, %59) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %61 = "ttnn.typecast"(%60) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %61 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump Before TTNNDecomposeLayouts (ttnn-decompose-layouts) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %7 = "ttnn.to_layout"(%6, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        %8 = "ttnn.conv2d"(%7, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %9 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %10 = "ttnn.permute"(%9) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %11 = "ttnn.add"(%8, %10) : (tensor<1x1x676x32xbf16, #ttnn_layout19>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %12 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout20>
        %13 = "ttnn.reshape"(%12) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout20>) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %14 = "ttnn.maximum"(%11, %13) : (tensor<1x1x676x32xbf16, #ttnn_layout19>, tensor<1x1x676x32xbf16, #ttnn_layout19>) -> tensor<1x1x676x32xbf16, #ttnn_layout19>
        %15 = "ttnn.to_layout"(%14, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %16 = "ttnn.conv2d"(%15, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %17 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %18 = "ttnn.permute"(%17) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout23>
        %19 = "ttnn.add"(%16, %18) : (tensor<1x1x576x64xbf16, #ttnn_layout22>, tensor<1x1x1x64xbf16, #ttnn_layout23>) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %20 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout24>
        %21 = "ttnn.reshape"(%20) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout24>) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %22 = "ttnn.maximum"(%19, %21) : (tensor<1x1x576x64xbf16, #ttnn_layout22>, tensor<1x1x576x64xbf16, #ttnn_layout22>) -> tensor<1x1x576x64xbf16, #ttnn_layout22>
        %23 = "ttnn.to_layout"(%22, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout25>
        %24 = "ttnn.max_pool2d"(%23) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout25>) -> tensor<1x1x144x64xbf16, #ttnn_layout26>
        %25 = "ttnn.to_layout"(%24, %0) <{dtype = #tt.supportedDataTypes<bf16>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<5x2>>, <interleaved>>}> : (tensor<1x1x144x64xbf16, #ttnn_layout26>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout27>
        %26 = "ttnn.reshape"(%25) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout27>) -> tensor<1x12x12x64xbf16, #ttnn_layout28>
        %27 = "ttnn.permute"(%26) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout28>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %28 = "ttnn.reshape"(%27) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout29>
        %29 = "ttnn.typecast"(%28) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout29>) -> tensor<1x9216xf32, #ttnn_layout30>
        %30 = "ttnn.matmul"(%29, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout30>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout31>
        %31 = "ttnn.to_layout"(%4, %0) <{dtype = #tt.supportedDataTypes<u32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout15>, !ttnn.device) -> tensor<1xui32, #ttnn_layout32>
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout32>) -> tensor<1x1xui32, #ttnn_layout33>
        %33 = "ttnn.to_layout"(%32, %0) <{dtype = #tt.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1x1xui32, #ttnn_layout33>, !ttnn.device) -> tensor<1x1xsi32, #ttnn_layout34>
        %34 = "ttnn.typecast"(%33) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout34>) -> tensor<1x1xf32, #ttnn_layout35>
        %35 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout31>
        %36 = "ttnn.add"(%34, %35) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xf32, #ttnn_layout31>
        %37 = "ttnn.multiply"(%30, %36) : (tensor<1x128xf32, #ttnn_layout31>, tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xf32, #ttnn_layout31>
        %38 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout31>
        %39 = "ttnn.add"(%37, %38) : (tensor<1x128xf32, #ttnn_layout31>, tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xf32, #ttnn_layout31>
        %40 = "ttnn.typecast"(%39) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout31>) -> tensor<1x128xbf16, #ttnn_layout10>
        %41 = "ttnn.maximum"(%40, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %42 = "ttnn.typecast"(%41) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout31>
        %43 = "ttnn.matmul"(%42, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout31>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout35>
        %44 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout35>
        %45 = "ttnn.add"(%34, %44) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %46 = "ttnn.multiply"(%43, %45) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %47 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout35>
        %48 = "ttnn.add"(%46, %47) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %49 = "ttnn.max"(%48) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1xf32, #ttnn_layout7>
        %50 = "ttnn.reshape"(%49) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout35>
        %51 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout35>
        %52 = "ttnn.add"(%50, %51) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %53 = "ttnn.subtract"(%48, %52) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %54 = "ttnn.exp"(%53) : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %55 = "ttnn.sum"(%54) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1xf32, #ttnn_layout7>
        %56 = "ttnn.reshape"(%55) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout35>
        %57 = "ttnn.log"(%56) : (tensor<1x1xf32, #ttnn_layout35>) -> tensor<1x1xf32, #ttnn_layout35>
        %58 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout35>
        %59 = "ttnn.add"(%57, %58) : (tensor<1x1xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %60 = "ttnn.subtract"(%53, %59) : (tensor<1x10xf32, #ttnn_layout35>, tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xf32, #ttnn_layout35>
        %61 = "ttnn.typecast"(%60) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout35>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %61 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump After TTNNDecomposeLayouts (ttnn-decompose-layouts) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #system_memory>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #system_memory>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #system_memory>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #system_memory>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #system_memory>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %7 = "ttnn.from_device"(%6) : (tensor<1x1x784x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        %8 = "ttnn.to_layout"(%7) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>) -> tensor<1x1x784x1xbf16, #ttnn_layout19>
        %9 = "ttnn.to_device"(%8, %0) <{memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout20>
        %10 = "ttnn.conv2d"(%9, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout20>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %11 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %12 = "ttnn.permute"(%11) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %13 = "ttnn.add"(%10, %12) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %14 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout22>
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout22>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %16 = "ttnn.maximum"(%13, %15) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %17 = "ttnn.from_device"(%16) : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout23>
        %18 = "ttnn.to_layout"(%17) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x676x32xbf16, #ttnn_layout23>) -> tensor<1x1x676x32xbf16, #ttnn_layout24>
        %19 = "ttnn.to_device"(%18, %0) <{memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout24>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout25>
        %20 = "ttnn.conv2d"(%19, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout25>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %21 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %22 = "ttnn.permute"(%21) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout27>
        %23 = "ttnn.add"(%20, %22) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x1x64xbf16, #ttnn_layout27>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %24 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout28>
        %25 = "ttnn.reshape"(%24) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout28>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %26 = "ttnn.maximum"(%23, %25) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %27 = "ttnn.from_device"(%26) : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout29>
        %28 = "ttnn.to_layout"(%27) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x576x64xbf16, #ttnn_layout29>) -> tensor<1x1x576x64xbf16, #ttnn_layout30>
        %29 = "ttnn.to_device"(%28, %0) <{memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout31>
        %30 = "ttnn.max_pool2d"(%29) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout31>) -> tensor<1x1x144x64xbf16, #ttnn_layout32>
        %31 = "ttnn.to_layout"(%30, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1x1x144x64xbf16, #ttnn_layout32>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout33>
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout33>) -> tensor<1x12x12x64xbf16, #ttnn_layout34>
        %33 = "ttnn.permute"(%32) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout34>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %34 = "ttnn.reshape"(%33) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout35>
        %35 = "ttnn.typecast"(%34) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout35>) -> tensor<1x9216xf32, #ttnn_layout36>
        %36 = "ttnn.matmul"(%35, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout36>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout37>
        %37 = "ttnn.from_device"(%4) : (tensor<1xui32, #ttnn_layout15>) -> tensor<1xui32, #ttnn_layout38>
        %38 = "ttnn.to_layout"(%37) <{layout = #ttnn.layout<tile>}> : (tensor<1xui32, #ttnn_layout38>) -> tensor<1xui32, #ttnn_layout39>
        %39 = "ttnn.to_device"(%38, %0) <{memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout39>, !ttnn.device) -> tensor<1xui32, #ttnn_layout40>
        %40 = "ttnn.reshape"(%39) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout40>) -> tensor<1x1xui32, #ttnn_layout41>
        %41 = "ttnn.typecast"(%40) <{dtype = #tt.supportedDataTypes<si32>}> : (tensor<1x1xui32, #ttnn_layout41>) -> tensor<1x1xsi32, #ttnn_layout42>
        %42 = "ttnn.typecast"(%41) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout42>) -> tensor<1x1xf32, #ttnn_layout43>
        %43 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout37>
        %44 = "ttnn.add"(%42, %43) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        %45 = "ttnn.multiply"(%36, %44) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        %46 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout37>
        %47 = "ttnn.add"(%45, %46) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        %48 = "ttnn.typecast"(%47) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xbf16, #ttnn_layout10>
        %49 = "ttnn.maximum"(%48, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %50 = "ttnn.typecast"(%49) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout37>
        %51 = "ttnn.matmul"(%50, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout37>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout43>
        %52 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %53 = "ttnn.add"(%42, %52) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %54 = "ttnn.multiply"(%51, %53) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %55 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout43>
        %56 = "ttnn.add"(%54, %55) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %57 = "ttnn.max"(%56) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7>
        %58 = "ttnn.reshape"(%57) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43>
        %59 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %60 = "ttnn.add"(%58, %59) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %61 = "ttnn.subtract"(%56, %60) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %62 = "ttnn.exp"(%61) : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %63 = "ttnn.sum"(%62) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7>
        %64 = "ttnn.reshape"(%63) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43>
        %65 = "ttnn.log"(%64) : (tensor<1x1xf32, #ttnn_layout43>) -> tensor<1x1xf32, #ttnn_layout43>
        %66 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %67 = "ttnn.add"(%65, %66) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %68 = "ttnn.subtract"(%61, %67) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %69 = "ttnn.typecast"(%68) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %69 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump Before TTNNDeallocate (ttnn-deallocate) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #system_memory>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #system_memory>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #system_memory>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #system_memory>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #system_memory>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        %7 = "ttnn.from_device"(%6) : (tensor<1x1x784x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        %8 = "ttnn.to_layout"(%7) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>) -> tensor<1x1x784x1xbf16, #ttnn_layout19>
        %9 = "ttnn.to_device"(%8, %0) <{memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout20>
        %10 = "ttnn.conv2d"(%9, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout20>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %11 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        %12 = "ttnn.permute"(%11) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        %13 = "ttnn.add"(%10, %12) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %14 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout22>
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout22>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %16 = "ttnn.maximum"(%13, %15) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        %17 = "ttnn.from_device"(%16) : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout23>
        %18 = "ttnn.to_layout"(%17) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x676x32xbf16, #ttnn_layout23>) -> tensor<1x1x676x32xbf16, #ttnn_layout24>
        %19 = "ttnn.to_device"(%18, %0) <{memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout24>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout25>
        %20 = "ttnn.conv2d"(%19, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout25>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %21 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        %22 = "ttnn.permute"(%21) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout27>
        %23 = "ttnn.add"(%20, %22) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x1x64xbf16, #ttnn_layout27>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %24 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout28>
        %25 = "ttnn.reshape"(%24) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout28>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %26 = "ttnn.maximum"(%23, %25) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        %27 = "ttnn.from_device"(%26) : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout29>
        %28 = "ttnn.to_layout"(%27) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x576x64xbf16, #ttnn_layout29>) -> tensor<1x1x576x64xbf16, #ttnn_layout30>
        %29 = "ttnn.to_device"(%28, %0) <{memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout31>
        %30 = "ttnn.max_pool2d"(%29) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout31>) -> tensor<1x1x144x64xbf16, #ttnn_layout32>
        %31 = "ttnn.to_layout"(%30, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1x1x144x64xbf16, #ttnn_layout32>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout33>
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout33>) -> tensor<1x12x12x64xbf16, #ttnn_layout34>
        %33 = "ttnn.permute"(%32) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout34>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        %34 = "ttnn.reshape"(%33) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout35>
        %35 = "ttnn.typecast"(%34) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout35>) -> tensor<1x9216xf32, #ttnn_layout36>
        %36 = "ttnn.matmul"(%35, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout36>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout37>
        %37 = "ttnn.from_device"(%4) : (tensor<1xui32, #ttnn_layout15>) -> tensor<1xui32, #ttnn_layout38>
        %38 = "ttnn.to_layout"(%37) <{layout = #ttnn.layout<tile>}> : (tensor<1xui32, #ttnn_layout38>) -> tensor<1xui32, #ttnn_layout39>
        %39 = "ttnn.to_device"(%38, %0) <{memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout39>, !ttnn.device) -> tensor<1xui32, #ttnn_layout40>
        %40 = "ttnn.reshape"(%39) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout40>) -> tensor<1x1xui32, #ttnn_layout41>
        %41 = "ttnn.typecast"(%40) <{dtype = #tt.supportedDataTypes<si32>}> : (tensor<1x1xui32, #ttnn_layout41>) -> tensor<1x1xsi32, #ttnn_layout42>
        %42 = "ttnn.typecast"(%41) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout42>) -> tensor<1x1xf32, #ttnn_layout43>
        %43 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout37>
        %44 = "ttnn.add"(%42, %43) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        %45 = "ttnn.multiply"(%36, %44) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        %46 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout37>
        %47 = "ttnn.add"(%45, %46) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        %48 = "ttnn.typecast"(%47) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xbf16, #ttnn_layout10>
        %49 = "ttnn.maximum"(%48, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        %50 = "ttnn.typecast"(%49) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout37>
        %51 = "ttnn.matmul"(%50, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout37>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout43>
        %52 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %53 = "ttnn.add"(%42, %52) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %54 = "ttnn.multiply"(%51, %53) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %55 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout43>
        %56 = "ttnn.add"(%54, %55) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %57 = "ttnn.max"(%56) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7>
        %58 = "ttnn.reshape"(%57) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43>
        %59 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %60 = "ttnn.add"(%58, %59) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %61 = "ttnn.subtract"(%56, %60) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %62 = "ttnn.exp"(%61) : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %63 = "ttnn.sum"(%62) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7>
        %64 = "ttnn.reshape"(%63) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43>
        %65 = "ttnn.log"(%64) : (tensor<1x1xf32, #ttnn_layout43>) -> tensor<1x1xf32, #ttnn_layout43>
        %66 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %67 = "ttnn.add"(%65, %66) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %68 = "ttnn.subtract"(%61, %67) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %69 = "ttnn.typecast"(%68) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xbf16, #ttnn_layout12>
        return %69 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


// -----// IR Dump After TTNNDeallocate (ttnn-deallocate) ('builtin.module' operation) //----- //
#dram = #ttnn.buffer_type<dram>
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #system_memory>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #system_memory>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #system_memory>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #system_memory>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #system_memory>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]>
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout>, %arg1: tensor<32xbf16, #ttnn_layout1>, %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2>, %arg3: tensor<64xbf16, #ttnn_layout3>, %arg4: tensor<9216x128xf32, #ttnn_layout4>, %arg5: tensor<128xf32, #ttnn_layout5>, %arg6: tensor<128x10xf32, #ttnn_layout6>, %arg7: tensor<10xf32, #ttnn_layout7>, %arg8: tensor<128x9216xbf16, #ttnn_layout8>, %arg9: tensor<128xbf16, #ttnn_layout9>, %arg10: tensor<10x128xbf16, #ttnn_layout10>, %arg11: tensor<10xbf16, #ttnn_layout1>, %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<10xbf16, #ttnn_layout1>) -> ()
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<10x128xbf16, #ttnn_layout10>) -> ()
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<128xbf16, #ttnn_layout9>) -> ()
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<128x9216xbf16, #ttnn_layout8>) -> ()
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13>
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14>
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10>
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15>
        %5 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16>
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> ()
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> ()
        %7 = "ttnn.from_device"(%6) : (tensor<1x1x784x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout18>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>) -> ()
        %8 = "ttnn.to_layout"(%7) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>) -> tensor<1x1x784x1xbf16, #ttnn_layout19>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>) -> ()
        %9 = "ttnn.to_device"(%8, %0) <{memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout20>
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>) -> ()
        %10 = "ttnn.conv2d"(%9, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout20>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout20>) -> ()
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<32x1x3x3xbf16, #ttnn_layout>) -> ()
        %11 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13>
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<32xbf16, #ttnn_layout1>) -> ()
        %12 = "ttnn.permute"(%11) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11>
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> ()
        %13 = "ttnn.add"(%10, %12) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x32xbf16, #ttnn_layout11>) -> ()
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> ()
        %14 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout22>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> ()
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout22>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x26x26x32xbf16, #ttnn_layout22>) -> ()
        %16 = "ttnn.maximum"(%13, %15) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout21>
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> ()
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> ()
        %17 = "ttnn.from_device"(%16) : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout23>
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> ()
        %18 = "ttnn.to_layout"(%17) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x676x32xbf16, #ttnn_layout23>) -> tensor<1x1x676x32xbf16, #ttnn_layout24>
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout23>) -> ()
        %19 = "ttnn.to_device"(%18, %0) <{memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout24>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout25>
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout24>) -> ()
        %20 = "ttnn.conv2d"(%19, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout25>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout25>) -> ()
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<64x32x3x3xbf16, #ttnn_layout2>) -> ()
        %21 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<64xbf16, #ttnn_layout3>) -> ()
        %22 = "ttnn.permute"(%21) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout27>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> ()
        %23 = "ttnn.add"(%20, %22) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x1x64xbf16, #ttnn_layout27>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout27>) -> ()
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> ()
        %24 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout28>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> ()
        %25 = "ttnn.reshape"(%24) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout28>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x24x24x64xbf16, #ttnn_layout28>) -> ()
        %26 = "ttnn.maximum"(%23, %25) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout26>
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> ()
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> ()
        %27 = "ttnn.from_device"(%26) : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout29>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> ()
        %28 = "ttnn.to_layout"(%27) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x576x64xbf16, #ttnn_layout29>) -> tensor<1x1x576x64xbf16, #ttnn_layout30>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout29>) -> ()
        %29 = "ttnn.to_device"(%28, %0) <{memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout31>
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout30>) -> ()
        %30 = "ttnn.max_pool2d"(%29) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout31>) -> tensor<1x1x144x64xbf16, #ttnn_layout32>
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout31>) -> ()
        %31 = "ttnn.to_layout"(%30, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1x1x144x64xbf16, #ttnn_layout32>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout33>
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x1x144x64xbf16, #ttnn_layout32>) -> ()
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout33>) -> tensor<1x12x12x64xbf16, #ttnn_layout34>
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x1x144x64xbf16, #ttnn_layout33>) -> ()
        %33 = "ttnn.permute"(%32) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout34>) -> tensor<1x64x12x12xbf16, #ttnn_layout14>
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x12x12x64xbf16, #ttnn_layout34>) -> ()
        %34 = "ttnn.reshape"(%33) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout35>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> ()
        %35 = "ttnn.typecast"(%34) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout35>) -> tensor<1x9216xf32, #ttnn_layout36>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x9216xbf16, #ttnn_layout35>) -> ()
        %36 = "ttnn.matmul"(%35, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout36>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout37>
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x9216xf32, #ttnn_layout36>) -> ()
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<9216x128xf32, #ttnn_layout4>) -> ()
        %37 = "ttnn.from_device"(%4) : (tensor<1xui32, #ttnn_layout15>) -> tensor<1xui32, #ttnn_layout38>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1xui32, #ttnn_layout15>) -> ()
        %38 = "ttnn.to_layout"(%37) <{layout = #ttnn.layout<tile>}> : (tensor<1xui32, #ttnn_layout38>) -> tensor<1xui32, #ttnn_layout39>
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1xui32, #ttnn_layout38>) -> ()
        %39 = "ttnn.to_device"(%38, %0) <{memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout39>, !ttnn.device) -> tensor<1xui32, #ttnn_layout40>
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1xui32, #ttnn_layout39>) -> ()
        %40 = "ttnn.reshape"(%39) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout40>) -> tensor<1x1xui32, #ttnn_layout41>
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1xui32, #ttnn_layout40>) -> ()
        %41 = "ttnn.typecast"(%40) <{dtype = #tt.supportedDataTypes<si32>}> : (tensor<1x1xui32, #ttnn_layout41>) -> tensor<1x1xsi32, #ttnn_layout42>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x1xui32, #ttnn_layout41>) -> ()
        %42 = "ttnn.typecast"(%41) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout42>) -> tensor<1x1xf32, #ttnn_layout43>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x1xsi32, #ttnn_layout42>) -> ()
        %43 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout37>
        %44 = "ttnn.add"(%42, %43) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        %45 = "ttnn.multiply"(%36, %44) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        %46 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout37>
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<128xf32, #ttnn_layout5>) -> ()
        %47 = "ttnn.add"(%45, %46) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37>
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        %48 = "ttnn.typecast"(%47) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xbf16, #ttnn_layout10>
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        %49 = "ttnn.maximum"(%48, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10>
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x128xbf16, #ttnn_layout10>) -> ()
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x128xbf16, #ttnn_layout10>) -> ()
        %50 = "ttnn.typecast"(%49) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout37>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x128xbf16, #ttnn_layout10>) -> ()
        %51 = "ttnn.matmul"(%50, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout37>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> ()
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<128x10xf32, #ttnn_layout6>) -> ()
        %52 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %53 = "ttnn.add"(%42, %52) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> ()
        %54 = "ttnn.multiply"(%51, %53) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        %55 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<10xf32, #ttnn_layout7>) -> ()
        %56 = "ttnn.add"(%54, %55) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        %57 = "ttnn.max"(%56) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7>
        %58 = "ttnn.reshape"(%57) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1xf32, #ttnn_layout7>) -> ()
        %59 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %60 = "ttnn.add"(%58, %59) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> ()
        %61 = "ttnn.subtract"(%56, %60) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        %62 = "ttnn.exp"(%61) : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        %63 = "ttnn.sum"(%62) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7>
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        %64 = "ttnn.reshape"(%63) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43>
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1xf32, #ttnn_layout7>) -> ()
        %65 = "ttnn.log"(%64) : (tensor<1x1xf32, #ttnn_layout43>) -> tensor<1x1xf32, #ttnn_layout43>
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> ()
        %66 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43>
        %67 = "ttnn.add"(%65, %66) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> ()
        %68 = "ttnn.subtract"(%61, %67) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43>
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        %69 = "ttnn.typecast"(%68) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xbf16, #ttnn_layout12>
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> ()
        return %69 : tensor<1x10xbf16, #ttnn_layout12>
      }
    }
  }
}


TTNN module module
#dram = #ttnn.buffer_type<dram>
#loc1 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":31:0)
#loc2 = loc("convolution")
#system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 104992, dram_unreserved_base = 32, dram_unreserved_end = 1073206944, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  16x25,  17x19,  17x20,  17x22,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [3 : i32], [ 0x0x0x0], [<[0, 8, 0], [1, 0, 0]>]>
#system_memory = #ttnn.buffer_type<system_memory>
#loc34 = loc(fused[#loc1, #loc2])
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3 + d1 * 3 + d2, d3), <1x1>, memref<96x3xbf16, #system_memory>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 3 + d2, d3), <1x1>, memref<6144x3xbf16, #system_memory>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<288x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 32 + d2, d3), <1x1>, memref<32x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #system_memory>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x1xbf16, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 832 + d1 * 32 + d2, d3), <1x1>, memref<26x1x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 704 + d1 * 704 + d2, d3), <1x1>, memref<22x1x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #system_memory>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 676 + d1 * 676 + d2, d3), <1x1>, memref<676x32xbf16, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<18x2x!tt.tile<32x32, bf16>, #system_memory>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #system_memory>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 576 + d1 * 576 + d2, d3), <1x1>, memref<576x64xbf16, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 144 + d1 * 144 + d2, d3), <1x1>, memref<144x64xbf16, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 160 + d1 * 160 + d2, d3), <1x1>, memref<5x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x288x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #system_memory>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #system_memory>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #dram>, <interleaved>>
module {
  tt.device_module {
    builtin.module attributes {tt.system_desc = #system_desc} {
      tt.device @default_device = <workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = , chipIds = [0]> loc(#loc)
      func.func @main(%arg0: tensor<32x1x3x3xbf16, #ttnn_layout> loc(fused[#loc1, #loc2]), %arg1: tensor<32xbf16, #ttnn_layout1> loc(fused[#loc1, #loc2]), %arg2: tensor<64x32x3x3xbf16, #ttnn_layout2> loc(fused[#loc1, #loc2]), %arg3: tensor<64xbf16, #ttnn_layout3> loc(fused[#loc1, #loc2]), %arg4: tensor<9216x128xf32, #ttnn_layout4> loc(fused[#loc1, #loc2]), %arg5: tensor<128xf32, #ttnn_layout5> loc(fused[#loc1, #loc2]), %arg6: tensor<128x10xf32, #ttnn_layout6> loc(fused[#loc1, #loc2]), %arg7: tensor<10xf32, #ttnn_layout7> loc(fused[#loc1, #loc2]), %arg8: tensor<128x9216xbf16, #ttnn_layout8> loc(fused[#loc1, #loc2]), %arg9: tensor<128xbf16, #ttnn_layout9> loc(fused[#loc1, #loc2]), %arg10: tensor<10x128xbf16, #ttnn_layout10> loc(fused[#loc1, #loc2]), %arg11: tensor<10xbf16, #ttnn_layout1> loc(fused[#loc1, #loc2]), %arg12: tensor<1x1x28x28xbf16, #ttnn_layout11> loc(fused[#loc1, #loc2])) -> tensor<1x10xbf16, #ttnn_layout12> {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<10xbf16, #ttnn_layout1>) -> () loc(#loc)
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<10x128xbf16, #ttnn_layout10>) -> () loc(#loc)
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<128xbf16, #ttnn_layout9>) -> () loc(#loc)
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<128x9216xbf16, #ttnn_layout8>) -> () loc(#loc)
        %1 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x32x26x26xbf16, #ttnn_layout13> loc(#loc)
        %2 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x64x24x24xbf16, #ttnn_layout14> loc(#loc)
        %3 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xbf16, #ttnn_layout10> loc(#loc)
        %4 = "ttnn.full"(%0) <{fillValue = 1.000000e+00 : f32}> : (!ttnn.device) -> tensor<1xui32, #ttnn_layout15> loc(#loc)
        %5 = "ttnn.permute"(%arg12) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> tensor<1x28x28x1xbf16, #ttnn_layout16> loc(#loc34)
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<1x1x28x28xbf16, #ttnn_layout11>) -> () loc(#loc34)
        %6 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 784 : i32, 1 : i32]}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> tensor<1x1x784x1xbf16, #ttnn_layout17> loc(#loc34)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x28x28x1xbf16, #ttnn_layout16>) -> () loc(#loc34)
        %7 = "ttnn.from_device"(%6) : (tensor<1x1x784x1xbf16, #ttnn_layout17>) -> tensor<1x1x784x1xbf16, #ttnn_layout18> loc(#loc34)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout17>) -> () loc(#loc34)
        %8 = "ttnn.to_layout"(%7) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>) -> tensor<1x1x784x1xbf16, #ttnn_layout19> loc(#loc34)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout18>) -> () loc(#loc34)
        %9 = "ttnn.to_device"(%8, %0) <{memory_config = #ttnn.memory_config<#dram, <<784x1>>, <interleaved>>}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<1x1x784x1xbf16, #ttnn_layout20> loc(#loc34)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout19>) -> () loc(#loc34)
        %10 = "ttnn.conv2d"(%9, %arg0, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 1 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 32 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x1xbf16, #ttnn_layout20>, tensor<32x1x3x3xbf16, #ttnn_layout>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout21> loc(#loc34)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x1x784x1xbf16, #ttnn_layout20>) -> () loc(#loc34)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<32x1x3x3xbf16, #ttnn_layout>) -> () loc(#loc34)
        %11 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32xbf16, #ttnn_layout1>) -> tensor<1x32x1x1xbf16, #ttnn_layout13> loc(#loc34)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<32xbf16, #ttnn_layout1>) -> () loc(#loc34)
        %12 = "ttnn.permute"(%11) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> tensor<1x1x1x32xbf16, #ttnn_layout11> loc(#loc34)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x32x1x1xbf16, #ttnn_layout13>) -> () loc(#loc34)
        %13 = "ttnn.add"(%10, %12) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x1x32xbf16, #ttnn_layout11>) -> tensor<1x1x676x32xbf16, #ttnn_layout21> loc(#loc34)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x1x32xbf16, #ttnn_layout11>) -> () loc(#loc34)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> () loc(#loc34)
        %14 = "ttnn.permute"(%1) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> tensor<1x26x26x32xbf16, #ttnn_layout22> loc(#loc35)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x32x26x26xbf16, #ttnn_layout13>) -> () loc(#loc35)
        %15 = "ttnn.reshape"(%14) <{shape = [1 : i32, 1 : i32, 676 : i32, 32 : i32]}> : (tensor<1x26x26x32xbf16, #ttnn_layout22>) -> tensor<1x1x676x32xbf16, #ttnn_layout21> loc(#loc35)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x26x26x32xbf16, #ttnn_layout22>) -> () loc(#loc35)
        %16 = "ttnn.maximum"(%13, %15) : (tensor<1x1x676x32xbf16, #ttnn_layout21>, tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout21> loc(#loc35)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> () loc(#loc35)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> () loc(#loc35)
        %17 = "ttnn.from_device"(%16) : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> tensor<1x1x676x32xbf16, #ttnn_layout23> loc(#loc36)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout21>) -> () loc(#loc36)
        %18 = "ttnn.to_layout"(%17) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x676x32xbf16, #ttnn_layout23>) -> tensor<1x1x676x32xbf16, #ttnn_layout24> loc(#loc36)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout23>) -> () loc(#loc36)
        %19 = "ttnn.to_device"(%18, %0) <{memory_config = #ttnn.memory_config<#dram, <<676x32>>, <interleaved>>}> : (tensor<1x1x676x32xbf16, #ttnn_layout24>, !ttnn.device) -> tensor<1x1x676x32xbf16, #ttnn_layout25> loc(#loc36)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout24>) -> () loc(#loc36)
        %20 = "ttnn.conv2d"(%19, %arg2, %0) <{batch_size = 1 : i32, dilation = array<i32: 1, 1>, groups = 1 : i32, in_channels = 32 : i32, input_height = 26 : i32, input_width = 26 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x676x32xbf16, #ttnn_layout25>, tensor<64x32x3x3xbf16, #ttnn_layout2>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout26> loc(#loc36)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x676x32xbf16, #ttnn_layout25>) -> () loc(#loc36)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<64x32x3x3xbf16, #ttnn_layout2>) -> () loc(#loc36)
        %21 = "ttnn.reshape"(%arg3) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout3>) -> tensor<1x64x1x1xbf16, #ttnn_layout14> loc(#loc36)
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<64xbf16, #ttnn_layout3>) -> () loc(#loc36)
        %22 = "ttnn.permute"(%21) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> tensor<1x1x1x64xbf16, #ttnn_layout27> loc(#loc36)
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout14>) -> () loc(#loc36)
        %23 = "ttnn.add"(%20, %22) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x1x64xbf16, #ttnn_layout27>) -> tensor<1x1x576x64xbf16, #ttnn_layout26> loc(#loc36)
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x1x1x64xbf16, #ttnn_layout27>) -> () loc(#loc36)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> () loc(#loc36)
        %24 = "ttnn.permute"(%2) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> tensor<1x24x24x64xbf16, #ttnn_layout28> loc(#loc37)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x64x24x24xbf16, #ttnn_layout14>) -> () loc(#loc37)
        %25 = "ttnn.reshape"(%24) <{shape = [1 : i32, 1 : i32, 576 : i32, 64 : i32]}> : (tensor<1x24x24x64xbf16, #ttnn_layout28>) -> tensor<1x1x576x64xbf16, #ttnn_layout26> loc(#loc37)
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x24x24x64xbf16, #ttnn_layout28>) -> () loc(#loc37)
        %26 = "ttnn.maximum"(%23, %25) : (tensor<1x1x576x64xbf16, #ttnn_layout26>, tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout26> loc(#loc37)
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> () loc(#loc37)
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> () loc(#loc37)
        %27 = "ttnn.from_device"(%26) : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> tensor<1x1x576x64xbf16, #ttnn_layout29> loc(#loc38)
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout26>) -> () loc(#loc38)
        %28 = "ttnn.to_layout"(%27) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x576x64xbf16, #ttnn_layout29>) -> tensor<1x1x576x64xbf16, #ttnn_layout30> loc(#loc38)
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout29>) -> () loc(#loc38)
        %29 = "ttnn.to_device"(%28, %0) <{memory_config = #ttnn.memory_config<#dram, <<576x64>>, <interleaved>>}> : (tensor<1x1x576x64xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x576x64xbf16, #ttnn_layout31> loc(#loc38)
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout30>) -> () loc(#loc38)
        %30 = "ttnn.max_pool2d"(%29) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, input_height = 24 : si32, input_width = 24 : si32, kernel_size = array<i32: 2, 2>, padding = array<i32: 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x576x64xbf16, #ttnn_layout31>) -> tensor<1x1x144x64xbf16, #ttnn_layout32> loc(#loc38)
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x1x576x64xbf16, #ttnn_layout31>) -> () loc(#loc38)
        %31 = "ttnn.to_layout"(%30, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1x1x144x64xbf16, #ttnn_layout32>, !ttnn.device) -> tensor<1x1x144x64xbf16, #ttnn_layout33> loc(#loc38)
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x1x144x64xbf16, #ttnn_layout32>) -> () loc(#loc38)
        %32 = "ttnn.reshape"(%31) <{shape = [1 : i32, 12 : i32, 12 : i32, 64 : i32]}> : (tensor<1x1x144x64xbf16, #ttnn_layout33>) -> tensor<1x12x12x64xbf16, #ttnn_layout34> loc(#loc38)
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x1x144x64xbf16, #ttnn_layout33>) -> () loc(#loc38)
        %33 = "ttnn.permute"(%32) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x12x12x64xbf16, #ttnn_layout34>) -> tensor<1x64x12x12xbf16, #ttnn_layout14> loc(#loc38)
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x12x12x64xbf16, #ttnn_layout34>) -> () loc(#loc38)
        %34 = "ttnn.reshape"(%33) <{shape = [1 : i32, 9216 : i32]}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> tensor<1x9216xbf16, #ttnn_layout35> loc(#loc39)
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x64x12x12xbf16, #ttnn_layout14>) -> () loc(#loc39)
        %35 = "ttnn.typecast"(%34) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x9216xbf16, #ttnn_layout35>) -> tensor<1x9216xf32, #ttnn_layout36> loc(#loc40)
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x9216xbf16, #ttnn_layout35>) -> () loc(#loc40)
        %36 = "ttnn.matmul"(%35, %arg4) <{transpose_a = false, transpose_b = false}> : (tensor<1x9216xf32, #ttnn_layout36>, tensor<9216x128xf32, #ttnn_layout4>) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc41)
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x9216xf32, #ttnn_layout36>) -> () loc(#loc41)
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<9216x128xf32, #ttnn_layout4>) -> () loc(#loc41)
        %37 = "ttnn.from_device"(%4) : (tensor<1xui32, #ttnn_layout15>) -> tensor<1xui32, #ttnn_layout38> loc(#loc)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1xui32, #ttnn_layout15>) -> () loc(#loc)
        %38 = "ttnn.to_layout"(%37) <{layout = #ttnn.layout<tile>}> : (tensor<1xui32, #ttnn_layout38>) -> tensor<1xui32, #ttnn_layout39> loc(#loc)
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1xui32, #ttnn_layout38>) -> () loc(#loc)
        %39 = "ttnn.to_device"(%38, %0) <{memory_config = #ttnn.memory_config<#dram, <<1x1>>, <interleaved>>}> : (tensor<1xui32, #ttnn_layout39>, !ttnn.device) -> tensor<1xui32, #ttnn_layout40> loc(#loc)
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1xui32, #ttnn_layout39>) -> () loc(#loc)
        %40 = "ttnn.reshape"(%39) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xui32, #ttnn_layout40>) -> tensor<1x1xui32, #ttnn_layout41> loc(#loc42)
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1xui32, #ttnn_layout40>) -> () loc(#loc42)
        %41 = "ttnn.typecast"(%40) <{dtype = #tt.supportedDataTypes<si32>}> : (tensor<1x1xui32, #ttnn_layout41>) -> tensor<1x1xsi32, #ttnn_layout42> loc(#loc42)
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x1xui32, #ttnn_layout41>) -> () loc(#loc42)
        %42 = "ttnn.typecast"(%41) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x1xsi32, #ttnn_layout42>) -> tensor<1x1xf32, #ttnn_layout43> loc(#loc42)
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x1xsi32, #ttnn_layout42>) -> () loc(#loc42)
        %43 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc42)
        %44 = "ttnn.add"(%42, %43) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc42)
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc42)
        %45 = "ttnn.multiply"(%36, %44) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc42)
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc42)
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc42)
        %46 = "ttnn.reshape"(%arg5) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout5>) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc43)
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<128xf32, #ttnn_layout5>) -> () loc(#loc43)
        %47 = "ttnn.add"(%45, %46) : (tensor<1x128xf32, #ttnn_layout37>, tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc43)
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc43)
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc43)
        %48 = "ttnn.typecast"(%47) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x128xf32, #ttnn_layout37>) -> tensor<1x128xbf16, #ttnn_layout10> loc(#loc44)
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc44)
        %49 = "ttnn.maximum"(%48, %3) : (tensor<1x128xbf16, #ttnn_layout10>, tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xbf16, #ttnn_layout10> loc(#loc45)
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x128xbf16, #ttnn_layout10>) -> () loc(#loc45)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x128xbf16, #ttnn_layout10>) -> () loc(#loc45)
        %50 = "ttnn.typecast"(%49) <{dtype = #tt.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn_layout10>) -> tensor<1x128xf32, #ttnn_layout37> loc(#loc46)
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x128xbf16, #ttnn_layout10>) -> () loc(#loc46)
        %51 = "ttnn.matmul"(%50, %arg6) <{transpose_a = false, transpose_b = false}> : (tensor<1x128xf32, #ttnn_layout37>, tensor<128x10xf32, #ttnn_layout6>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc47)
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x128xf32, #ttnn_layout37>) -> () loc(#loc47)
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<128x10xf32, #ttnn_layout6>) -> () loc(#loc47)
        %52 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc48)
        %53 = "ttnn.add"(%42, %52) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc48)
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc48)
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> () loc(#loc48)
        %54 = "ttnn.multiply"(%51, %53) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc48)
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc48)
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc48)
        %55 = "ttnn.reshape"(%arg7) <{shape = [1 : i32, 10 : i32]}> : (tensor<10xf32, #ttnn_layout7>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc49)
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<10xf32, #ttnn_layout7>) -> () loc(#loc49)
        %56 = "ttnn.add"(%54, %55) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc49)
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc49)
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc49)
        %57 = "ttnn.max"(%56) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7> loc(#loc50)
        %58 = "ttnn.reshape"(%57) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43> loc(#loc50)
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1xf32, #ttnn_layout7>) -> () loc(#loc50)
        %59 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc51)
        %60 = "ttnn.add"(%58, %59) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc51)
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc51)
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> () loc(#loc51)
        %61 = "ttnn.subtract"(%56, %60) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc51)
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc51)
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc51)
        %62 = "ttnn.exp"(%61) : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc52)
        %63 = "ttnn.sum"(%62) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1xf32, #ttnn_layout7> loc(#loc53)
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc53)
        %64 = "ttnn.reshape"(%63) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn_layout7>) -> tensor<1x1xf32, #ttnn_layout43> loc(#loc53)
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1xf32, #ttnn_layout7>) -> () loc(#loc53)
        %65 = "ttnn.log"(%64) : (tensor<1x1xf32, #ttnn_layout43>) -> tensor<1x1xf32, #ttnn_layout43> loc(#loc54)
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> () loc(#loc54)
        %66 = "ttnn.full"(%0) <{fillValue = 0.000000e+00 : f32}> : (!ttnn.device) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc55)
        %67 = "ttnn.add"(%65, %66) : (tensor<1x1xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc55)
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc55)
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x1xf32, #ttnn_layout43>) -> () loc(#loc55)
        %68 = "ttnn.subtract"(%61, %67) : (tensor<1x10xf32, #ttnn_layout43>, tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xf32, #ttnn_layout43> loc(#loc55)
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc55)
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc55)
        %69 = "ttnn.typecast"(%68) <{dtype = #tt.supportedDataTypes<bf16>}> : (tensor<1x10xf32, #ttnn_layout43>) -> tensor<1x10xbf16, #ttnn_layout12> loc(#loc56)
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x10xf32, #ttnn_layout43>) -> () loc(#loc56)
        return %69 : tensor<1x10xbf16, #ttnn_layout12> loc(#loc34)
      } loc(#loc34)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":32:0)
#loc4 = loc("relu")
#loc5 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":33:0)
#loc6 = loc("convolution_1")
#loc7 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":34:0)
#loc8 = loc("relu_1")
#loc9 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":35:0)
#loc10 = loc("max_pool2d_with_indices")
#loc11 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":37:0)
#loc12 = loc("view")
#loc13 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":38:0)
#loc14 = loc("convert_element_type")
#loc15 = loc("mm")
#loc16 = loc("mul")
#loc17 = loc("add")
#loc18 = loc("convert_element_type_1")
#loc19 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":39:0)
#loc20 = loc("relu_2")
#loc21 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":41:0)
#loc22 = loc("convert_element_type_2")
#loc23 = loc("mm_1")
#loc24 = loc("mul_1")
#loc25 = loc("add_1")
#loc26 = loc("/localdev/hshah/tt-torch/demos/mnist/mnist_data_parallel_async.py":42:0)
#loc27 = loc("amax")
#loc28 = loc("sub")
#loc29 = loc("exp")
#loc30 = loc("sum_1")
#loc31 = loc("log")
#loc32 = loc("sub_1")
#loc33 = loc("convert_element_type_5")
#loc35 = loc(fused[#loc3, #loc4])
#loc36 = loc(fused[#loc5, #loc6])
#loc37 = loc(fused[#loc7, #loc8])
#loc38 = loc(fused[#loc9, #loc10])
#loc39 = loc(fused[#loc11, #loc12])
#loc40 = loc(fused[#loc13, #loc14])
#loc41 = loc(fused[#loc13, #loc15])
#loc42 = loc(fused[#loc13, #loc16])
#loc43 = loc(fused[#loc13, #loc17])
#loc44 = loc(fused[#loc13, #loc18])
#loc45 = loc(fused[#loc19, #loc20])
#loc46 = loc(fused[#loc21, #loc22])
#loc47 = loc(fused[#loc21, #loc23])
#loc48 = loc(fused[#loc21, #loc24])
#loc49 = loc(fused[#loc21, #loc25])
#loc50 = loc(fused[#loc26, #loc27])
#loc51 = loc(fused[#loc26, #loc28])
#loc52 = loc(fused[#loc26, #loc29])
#loc53 = loc(fused[#loc26, #loc30])
#loc54 = loc(fused[#loc26, #loc31])
#loc55 = loc(fused[#loc26, #loc32])
#loc56 = loc(fused[#loc26, #loc33])

/localdev/hshah/tt-torch/env/venv/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
EXECUTOR DEBUG - TIME FOR ASYNC EXECUTE:  15.941543102264404
DEBUG - TIME FOR RUNTIME TENSOR:  19.008643627166748
EXECUTOR DEBUG - TIME FOR ASYNC EXECUTE:  0.03165245056152344
DEBUG - TIME FOR RUNTIME TENSOR:  1.1364338397979736
DEBUG - TIME FOR TO_HOST:  0.00021338462829589844
DEBUG - TIME FOR TO_HOST:  0.00010442733764648438

Test set: Average loss: 2.3125, Accuracy: 0/2 (0%)
