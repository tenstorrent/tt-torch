name: Run Op-By-Op Model Tests (Nightly)

on:
  workflow_call:
    inputs:
      docker-image:
        description: 'Docker image to use for build'
        required: true
        type: string
      use-shared-runners:
        description: 'Use shared runners instead of custom runners'
        required: false
        default: true
        type: boolean
  workflow_run:
    workflows: [Build]
    types: [completed]

permissions:
  packages: write
  checks: write

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      runner_name_format: ${{ steps.set-config.outputs.runner_name_format }}
      container_image: ${{ steps.set-config.outputs.container_image }}
    steps:
    - name: Set configuration
      id: set-config
      run: |
        # Set runner name format based on use-shared-runners
        if [[ "${{ inputs.use-shared-runners }}" == "true" ]]; then
          echo "runner_name_format=tt-beta-ubuntu-2204-{0}-large-stable" >> $GITHUB_OUTPUT
          echo "container_image=harbor.ci.tenstorrent.net/${{ inputs.docker-image }}" >> $GITHUB_OUTPUT
        else
          echo "runner_name_format={0}" >> $GITHUB_OUTPUT
          echo "container_image=${{ inputs.docker-image }}" >> $GITHUB_OUTPUT
        fi

  tests:
    needs: setup
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        runs-on: [ n150 ]
        build: [
          {
            name: "qwen", tests: "
              tests/models/Qwen/test_qwen2_token_classification.py::test_qwen2_token_classification[op_by_op_torch-eval]
              "
          },
          {
            name: "autoencoder", tests: "
              tests/models/autoencoder_conv/test_autoencoder_conv.py::test_autoencoder_conv[op_by_op_torch-eval]
              tests/models/autoencoder_conv/test_autoencoder_conv_v2.py::test_autoencoder_conv_v2[op_by_op_torch-eval]
              "
          },
          {
            name: "falcon", tests: "
              tests/models/falcon/test_falcon.py::test_falcon[op_by_op_torch-eval]
              "
          },
          {
            name: "openpose", tests: "
              tests/models/openpose/test_openpose.py::test_openpose[op_by_op_torch-eval]
              "
          },
          {
            name: "stable-diffusion-pipe", tests: "
              tests/models/stable_diffusion/test_stable_diffusion.py::test_stable_diffusion[op_by_op_torch-eval]
              "
          },
          {
            name: "t5", tests: "
              tests/models/speecht5_tts/test_speecht5_tts.py::test_speecht5_tts[op_by_op_torch-eval]
              "
          },
          {
            name: "unet", tests: "
              tests/models/vgg19_unet/test_vgg19_unet.py::test_vgg19_unet[op_by_op_torch-eval]
              "
          },
          {
            name: "RMBG", tests: "
              tests/models/RMBG/test_RMBG.py::test_RMBG[op_by_op_torch-eval]
              "
          },
          {
            name: "timm", tests: "
              tests/models/timm/test_timm_image_classification.py::test_timm_image_classification[single_device-op_by_op_torch-eval-ghostnetv2_100.in1k]
              "
          },
          {
            name: "xglm", tests: "
              tests/models/xglm/test_xglm.py::test_xglm[op_by_op_torch-xglm-564M-eval]
              tests/models/xglm/test_xglm.py::test_xglm[op_by_op_torch-xglm-1.7B-eval]
              "
          },
          {
            name: "vision-misc", tests: "
              tests/models/glpn_kitti/test_glpn_kitti.py::test_glpn_kitti[op_by_op_torch-eval]
              tests/models/hand_landmark/test_hand_landmark.py::test_hand_landmark[op_by_op_torch-eval]
              tests/models/segment_anything/test_segment_anything.py::test_segment_anything[op_by_op_torch-eval]
              tests/models/vilt/test_vilt.py::test_vilt[op_by_op_torch-eval]
              "
          },
          {
            name: "torchvision_1", tests: "
              tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-op_by_op_torch-eval-googlenet]
              tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-op_by_op_torch-eval-regnet_y_128gf]
              "
          },
          {
            name: "deepseek", tests: "
              tests/models/deepseek/test_deepseek.py::test_deepseek[op_by_op_torch-eval]
              tests/models/deepseek/test_deepseek_qwen.py::test_deepseek_qwen[op_by_op_torch-eval]
              "
          },
          {
            name: "bi_lstm_crf", tests: "
              tests/models/bi_lstm_crf/test_bi_lstm_crf.py::test_bi_lstm_crf[op_by_op_torch-lstm-eval]
              tests/models/bi_lstm_crf/test_bi_lstm_crf.py::test_bi_lstm_crf[op_by_op_torch-gru-eval]
              "
          },
          {
            name: "gliner", tests: "
              tests/models/gliner/test_gliner.py::test_gliner[op_by_op_torch-eval]
              "
          },
          {
            name: "YOLOv10", tests: "
              tests/models/yolov10/test_yolov10.py::test_yolov10[op_by_op_torch-eval]
              "
          },
          {
            name: "OFT", tests: "
              tests/models/oft/test_oft.py::test_oft[op_by_op_torch-eval]
              "
          },
          {

            name: "mistral", tests: "
              tests/models/mistral/test_mistral.py::test_mistral[op_by_op_torch-7b-eval]
              tests/models/mistral/test_mistral.py::test_mistral[op_by_op_torch-ministral_8b_instruct-eval]
              "
          },
          {
            name: "pixtral", tests: "
              tests/models/mistral/test_pixtral.py::test_pixtral[op_by_op_torch-eval]
            "
          },
          {
            name: "stable-diffusion-v3.5", tests: "
              tests/models/stable_diffusion/test_stable_diffusion_3_5.py::test_stable_diffusion_3_5[op_by_op_torch-SD3.5-medium-eval]
              tests/models/stable_diffusion/test_stable_diffusion_3_5.py::test_stable_diffusion_3_5[op_by_op_torch-SD3.5-large-eval]
              tests/models/stable_diffusion/test_stable_diffusion_transformer.py::test_stable_diffusion_transformer[op_by_op_torch-SD3.5-large-transformer-eval]
              "
          },
          {
            name: "detr", tests: "
              tests/models/detr/test_detr_onnx.py::test_detr_onnx[op_by_op_stablehlo-eval]
              "
          },
          {
            name: "d-fine", tests: "
              tests/models/d_fine/test_d_fine.py::test_d_fine[op_by_op_torch-nano-eval]
              tests/models/d_fine/test_d_fine.py::test_d_fine[op_by_op_torch-small-eval]
              tests/models/d_fine/test_d_fine.py::test_d_fine[op_by_op_torch-medium-eval]
              tests/models/d_fine/test_d_fine.py::test_d_fine[op_by_op_torch-large-eval]
              tests/models/d_fine/test_d_fine.py::test_d_fine[op_by_op_torch-xlarge-eval]
              "
          },
          {
            # only testing 4k variant as they're identical aside from token size
            name: "phi3", tests: "
              tests/models/phi/test_phi_3.py::test_phi_3[op_by_op_torch-mini_4k_instruct-eval]
            "
          },
          {
            name: "phi3p5_moe", tests: "
            tests/models/phi/test_phi_3p5_moe.py::test_phi_3p5_moe[op_by_op_torch-eval]
            "
          },
          {
            # Only testing few variants since runtime on these is long. Increase when more mahcines or priority increases.
            name: "centernet", tests: "
            tests/models/centernet/test_centernet_onnx.py::test_centernet_onnx[op_by_op_stablehlo-centernet-hpe-dla1x-eval]
            tests/models/centernet/test_centernet_onnx.py::test_centernet_onnx[op_by_op_stablehlo-centernet-3d_bb-ddd_3dop-eval]
            "
          },
          {
            name: "phi3p5_vision", tests: "
              tests/models/phi/test_phi_3p5_vision.py::test_phi_3p5_vision[op_by_op_torch-eval]
            "
          },
          {
            name: "phi4", tests: "
              tests/models/phi/test_phi_4.py::test_phi_4[op_by_op_torch-eval]
            "
          },
          {
            name: "llama", tests: "
              tests/models/llama/test_llama_7b.py::test_llama_7b[op_by_op_torch-eval]
            "
          },
          {
            name: "flux", tests: "
              tests/models/flux/test_flux.py::test_flux[op_by_op_torch-schnell-eval]
              tests/models/flux/test_flux.py::test_flux[op_by_op_torch-dev-eval]
            "
          },
        ]
    runs-on: ${{ format(needs.setup.outputs.runner_name_format, matrix.runs-on) }}

    name: "tests (${{ matrix.runs-on }}, ${{ matrix.build.name }})"

    container:
      # Use container image from the setup job
      image: ${{ needs.setup.outputs.container_image }}
      options: --user root --device /dev/tenstorrent --shm-size=4gb --pid=host
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
        - /mnt/dockercache:/mnt/dockercache

    steps:
    - uses: actions/checkout@v4

    - name: Fetch job id
      id: fetch-job-id
      uses: tenstorrent/tt-github-actions/.github/actions/job_id@main
      with:
        job_name: "tests (${{ matrix.runs-on }}, ${{ matrix.build.name }})"

    - name: Set reusable strings
      id: strings
      shell: bash
      env:
        JOB_ID: ${{ steps.fetch-job-id.outputs.job_id }}
      run: |
        echo "work-dir=$(pwd)" >> "$GITHUB_OUTPUT"
        echo "install-dir=$(pwd)/install" >> "$GITHUB_OUTPUT"
        echo "dist-dir=$(pwd)/dist" >> "$GITHUB_OUTPUT"
        echo "test-output-dir=$(pwd)/results/models/tests/" >> "$GITHUB_OUTPUT"

        # Set cache root based on use-shared-runners
        if [[ "${{ inputs.use-shared-runners }}" == "true" ]]; then
            echo "IRD_LF_CACHE=http://large-file-cache.large-file-cache.svc.cluster.local/" >> $GITHUB_ENV
        else
            echo "DOCKER_CACHE_ROOT=/mnt/dockercache" >> $GITHUB_ENV
        fi

    - name: Sample memory
      shell: bash
      run: |
        echo "Memory sample at $(date)"
        ps -aux --sort -%mem | head -n 10

    - name: Git safe dir
      run: git config --global --add safe.directory ${{ steps.strings.outputs.work-dir }}

    - name: Use build artifacts
      uses: tenstorrent/tt-forge/.github/actions/download-artifact@main
      with:
        name: install-artifacts
        path: install

    - name: install tt-torch
      shell: bash
      run: |
        source env/activate
        mkdir -p ${{ steps.strings.outputs.dist-dir }}
        mv install/wheels/* ${{ steps.strings.outputs.dist-dir }}
        pip install ${{ steps.strings.outputs.dist-dir }}/*.whl

    - name: Run Model Tests
      env:
        HF_HOME: ${{ env.DOCKER_CACHE_ROOT }}/huggingface
        TORCH_HOME: ${{ env.DOCKER_CACHE_ROOT }}/torch
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      shell: bash
      run: |
        source env/activate

        # Make sure we don't stop on first failure
        set +e

        tests_list=$(echo "${{ matrix.build.tests }}" | xargs -n1 echo)
        total_tests=$(echo "$tests_list" | wc -l)

        failures=0
        counter=0
        rm -f pytest.log
        rm -f full_job_output.log

        for test_case in $tests_list; do
          counter=$((counter + 1))

          echo "====== BEGIN LOG: $test_case ======" >> full_job_output.log
          pytest -svv -rf "$test_case" > test.log 2>&1
          exit_code=$?
          cat test.log >> full_job_output.log
          sed -n '/=========================== short test summary info ============================/,$p' test.log >>pytest.log


          echo "====== END LOG: $test_case ========" >> full_job_output.log
          echo >> full_job_output.log

          if [ $exit_code -eq 0 ]; then
            echo "[ $counter / $total_tests ] $test_case PASSED"
          else
            echo "[ $counter / $total_tests ] $test_case FAILED"
            failures=$((failures + 1))
          fi
        done

        # If any test failed, exit nonzero to mark the job as failed
        if [ $failures -ne 0 ]; then
          echo "Total failures: $failures"
          exit 1
        fi

    - name: Tar results
      if: success() || failure()
      shell: bash
      run: |
        TEST_DIR="${{ steps.strings.outputs.test-output-dir }}"
        OUTPUT_TAR="${{ matrix.build.name }}_${{ steps.fetch-job-id.outputs.job_id }}.tar"

        if [ ! -d "$TEST_DIR" ]; then
          echo "WARNING: Test output dir '$TEST_DIR' does not exist. Please check if test ran properly. Skipping tar."
        else
          cd "$TEST_DIR"
          tar cvf "$OUTPUT_TAR" .
        fi

    - name: Upload test folder to archive
      if: success() || failure()
      uses: actions/upload-artifact@v4
      with:
        name: test-reports-nightly-${{ matrix.build.name }}.tar
        path: ${{ steps.strings.outputs.test-output-dir }}/${{ matrix.build.name }}_${{ steps.fetch-job-id.outputs.job_id }}.tar

    - name: Upload full logs
      if: success() || failure()
      uses: actions/upload-artifact@v4
      with:
        name: full-logs-nightly-${{ matrix.build.name }}
        path: full_job_output.log

    - name: Upload logs for bisect
      if: success() || failure()
      uses: actions/upload-artifact@v4
      with:
        name: test-log-${{ matrix.runs-on }}-${{ matrix.build.name }}-${{ steps.fetch-job-id.outputs.job_id }}
        path: pytest.log
