name: Run Full Model Execution Tests (Nightly)

on:
  workflow_dispatch:
  workflow_call:
    inputs:
      docker-image:
        description: 'Docker image to use for build'
        required: true
        type: string
      run-codecov:
        description: 'Run code coverage reports'
        required: false
        type: string # properly a boolean but autocast to string when passed in using 'with' inputs
        default: 'true'
  workflow_run:
    workflows: [Build] # backref to run-build as dependency
    types: [completed]

env:
  DOCKER_CACHE_ROOT: /mnt/dockercache

jobs:
  tests:
    timeout-minutes: 240
    strategy:
      fail-fast: false
      matrix:
        build: [
          {
            # Approximately 70 minutes.
            runs-on: tt-beta-ubuntu-2204-n150-large-stable, name: "eval_1", tests: "
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification_generality[full-eval-vgg19]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification_generality[full-eval-vgg19_bn]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification_generality[full-eval-vgg16]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification_generality[full-eval-vgg16_bn]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification_generality[full-eval-vgg13_bn]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification_generality[full-eval-vgg11_bn]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification_generality[full-eval-vgg11]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification_generality[full-eval-vgg13]
                  tests/models/dpr/test_dpr.py::test_dpr[full-eval]
                  tests/models/albert/test_albert_masked_lm.py::test_albert_masked_lm[full-albert/albert-xlarge-v2-eval]
                  tests/models/albert/test_albert_masked_lm.py::test_albert_masked_lm[full-albert/albert-large-v2-eval]
                  tests/models/albert/test_albert_masked_lm.py::test_albert_masked_lm[full-albert/albert-base-v2-eval]
                  tests/models/albert/test_albert_sequence_classification.py::test_albert_sequence_classification[full-textattack/albert-base-v2-imdb-eval]
                  tests/models/albert/test_albert_token_classification.py::test_albert_token_classification[full-albert/albert-base-v2-eval]
                  tests/models/mobilenet_ssd/test_mobilenet_ssd.py::test_mobilenet_ssd[full-eval]
                  tests/models/roberta/test_roberta.py::test_roberta[full-eval]
                  tests/models/hardnet/test_hardnet.py::test_hardnet[full-eval]
                  tests/models/timm/test_timm_image_classification.py::test_timm_image_classification_generality[full-eval-tf_efficientnet_lite0.in1k]
                  tests/models/timm/test_timm_image_classification.py::test_timm_image_classification_generality[full-eval-tf_efficientnet_lite1.in1k]
                  tests/models/timm/test_timm_image_classification.py::test_timm_image_classification_generality[full-eval-tf_efficientnet_lite2.in1k]
                  tests/models/timm/test_timm_image_classification.py::test_timm_image_classification_generality[full-eval-tf_efficientnet_lite3.in1k]
                  tests/models/timm/test_timm_image_classification.py::test_timm_image_classification_generality[full-eval-tf_efficientnet_lite4.in1k]
                  tests/models/segformer/test_segformer.py::test_segformer[full-eval]
                  tests/models/torchvision/test_torchvision_object_detection.py::test_torchvision_object_detection[full-ssdlite320_mobilenet_v3_large-eval]
                  tests/models/squeeze_bert/test_squeeze_bert.py::test_squeeze_bert[full-eval]
                  tests/models/beit/test_beit_image_classification.py::test_beit_image_classification[full-microsoft/beit-base-patch16-224-eval]
                  tests/models/beit/test_beit_image_classification.py::test_beit_image_classification[full-microsoft/beit-large-patch16-224-eval]
                  tests/models/deit/test_deit.py::test_deit[full-facebook/deit-base-patch16-224-eval]
                  tests/models/timm/test_timm_image_classification.py::test_timm_image_classification_generality[full-eval-mixer_b16_224.goog_in21k]
                  tests/models/mgp-str-base/test_mgp_str_base.py::test_mgp_str_base[full-eval]
                  tests/models/detr/test_detr.py::test_detr[full-eval]
                  tests/models/vit/test_vit.py::test_vit[full-eval]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification_generality[full-eval-vit_h_14]
                  tests/models/musicgen_small/test_musicgen_small.py::test_musicgen_small[full-eval]
                  tests/models/yolos/test_yolos.py::test_yolos[full-eval]
                  tests/models/whisper/test_whisper.py::test_whisper[full-eval]
                  tests/models/bert/test_bert_turkish.py::test_bert_turkish[full-eval]
            "
          },
          {
            # Approximately 65 minutes.
            runs-on: tt-beta-ubuntu-2204-n150-large-stable, name: "eval_2", tests: "
                  tests/models/bloom/test_bloom.py::test_bloom[full-eval]
                  tests/models/timm/test_timm_image_classification.py::test_timm_image_classification_generality[full-eval-hrnet_w18.ms_aug_in1k]
                  tests/models/timm/test_timm_image_classification.py::test_timm_image_classification_generality[full-eval-ghostnet_100.in1k]
                  tests/models/timm/test_timm_image_classification.py::test_timm_image_classification_generality[full-eval-xception71.tf_in1k]
                  tests/models/timm/test_timm_image_classification.py::test_timm_image_classification_generality[full-eval-mobilenetv1_100.ra4_e3600_r224_in1k]
                  tests/models/timm/test_timm_image_classification.py::test_timm_image_classification_generality[full-eval-dla34.in1k]
                  tests/models/yolov5/test_yolov5.py::test_yolov5[full-eval]
                  tests/models/albert/test_albert_question_answering.py::test_albert_question_answering[full-twmkn9/albert-base-v2-squad2-eval]
                  tests/models/phi/test_phi_1_1p5_2.py::test_phi[full-microsoft/phi-1-eval]
                  tests/models/clip/test_clip.py::test_clip[full-eval]
                  tests/models/gpt2/test_gpt2.py::test_gpt2[full-eval]
                  tests/models/xglm/test_xglm.py::test_xglm[full-eval]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification_red[full-eval-swin_b]
            "
          },
          {
            # Approximately 45 minutes.
            runs-on: tt-beta-ubuntu-2204-n150-large-stable, name: "eval_3", tests: "
                  tests/models/phi/test_phi_1_1p5_2.py::test_phi[full-microsoft/phi-1.5-eval]
            "
          },
          {
            # Approximately 50 minutes.
            runs-on: tt-beta-ubuntu-2204-n150-large-stable, name: "eval_5", tests: "
                  tests/models/Qwen/test_qwen2_casual_lm.py::test_qwen2_casual_lm[full-Qwen/Qwen2.5-1.5B-eval]
                  tests/models/opt/test_opt.py::test_opt[full-eval]
            "
          },
          {
            # Approximately 40 minutes.
            runs-on: tt-beta-ubuntu-2204-n150-large-stable, name: "eval_6", tests: "
                  tests/models/stable_diffusion/test_stable_diffusion_unet.py::test_stable_diffusion_unet[full-eval]
            "
          },
          {
            # Approximately 5 minutes.
            runs-on: tt-beta-ubuntu-2204-n150-large-stable, name: "eval_7_bert_qrtn", tests: "
                  tests/models/bert/test_bert.py::test_bert[full-eval]
            "
          },
          # Approximately 180 minutes.
          {
            runs-on: tt-beta-ubuntu-2204-n150-large-stable, name: "eval_8", tests: "
                  tests/models/mamba/test_mamba.py::test_mamba[full-state-spaces/mamba-790m-hf-eval]
            "
          }
        ]
    runs-on:
      - ${{ matrix.build.runs-on }}

    name: "test execution_nightly (${{ matrix.build.runs-on }}, ${{ matrix.build.name }})"

    container:
      image: ${{ inputs.docker-image }}
      options: --user root --device /dev/tenstorrent/0 --shm-size=4gb
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
        - /mnt/dockercache:/mnt/dockercache
    steps:
    - uses: actions/checkout@v4
      with:
        submodules: recursive
        lfs: true

    - name: Fetch job id
      id: fetch-job-id
      uses: tenstorrent/tt-github-actions/.github/actions/job_id@main
      with:
        job_name: "test execution_nightly (${{ matrix.build.runs-on }}, ${{ matrix.build.name }})" # reference above tests.name

    - name: Set reusable strings
      id: strings
      shell: bash
      env:
        JOB_ID: ${{ steps.fetch-job-id.outputs.job_id }}

      run: |
        echo "work-dir=$(pwd)" >> "$GITHUB_OUTPUT"
        echo "install-dir=$(pwd)/install" >> "$GITHUB_OUTPUT"
        echo "dist-dir=$(pwd)/dist" >> "$GITHUB_OUTPUT"
        echo "test_report_path_models=report_models_$JOB_ID.xml" >> "$GITHUB_OUTPUT"
        echo "test_report_dir=$(pwd)/test-reports" >> "$GITHUB_OUTPUT"

    - name: Use build artifacts
      uses: actions/download-artifact@v4
      with:
        name: install-artifacts
        path: ${{ steps.strings.outputs.install-dir }}

    - name: 'Untar install directory'
      shell: bash
      working-directory: ${{ steps.strings.outputs.install-dir }}
      run: |
        tar xvf artifact.tar
        mkdir -p ${{ steps.strings.outputs.dist-dir }}
        mv wheels/* ${{ steps.strings.outputs.dist-dir }}

    - name: install tt-torch
      shell: bash
      run: |
        source env/activate
        pip install ${{ steps.strings.outputs.dist-dir }}/*.whl

    - name: Run Full Model Execution Tests
      env:
        HF_HOME: ${{ env.DOCKER_CACHE_ROOT }}/huggingface
        TORCH_HOME: ${{ env.DOCKER_CACHE_ROOT }}/torch
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      shell: bash
      run: |
        source env/activate

        # Create a directory for test reports
        mkdir -p ${{ steps.strings.outputs.test_report_dir }}
        rm -f pytest.log

        # prevent script termination on failure
        set +e

        failure=0

        # Split the test string into individual tests and run them one by one
        while read -r test; do
          if [[ -n "$test" ]]; then
            echo "Running test: $test"
            report_file="${{ steps.strings.outputs.test_report_dir }}/$(echo $test | tr '/' '_' | tr ':' '_')__${{ steps.strings.outputs.test_report_path_models }}"

            echo "JUnitXML logged to $report_file"

            pytest --durations=0 -v "$test" \
              --junit-xml="$report_file" \
              --cov=tt_torch --cov-report term --cov-report xml:coverage.xml --cov-append \
              2>&1 | tee -a pytest.log
            # Check the return code of pytest
            if [[ $? -ne 0 ]]; then
              echo "Test failed: $test"
              failure=1
            fi
          fi
        # execute while loop with process substitution so it doesn't run in a subshell and value of $failure is preserved
        done < <(echo "${{ matrix.build.tests }}" | tr ' ' '\n')

        set -e

        # Exit with the appropriate code
        if [[ $failure -ne 0 ]]; then
          echo "One or more tests failed."
          exit 1
        else
          echo "All tests passed."
          exit 0
        fi

    - name: Upload Test Log
      uses: actions/upload-artifact@v4
      if: success() || failure()
      with:
        name: test-log-${{ matrix.build.runs-on }}-${{ matrix.build.name }}-${{ steps.fetch-job-id.outputs.job_id }}
        path: pytest.log

    - name: Upload Test Report Models
      uses: actions/upload-artifact@v4
      if: success() || failure()
      with:
        name: test-reports-models-${{ matrix.build.runs-on }}-${{ matrix.build.name }}-${{ steps.fetch-job-id.outputs.job_id }}
        path: ${{ steps.strings.outputs.test_report_dir }}

    - name: Upload coverage reports to Codecov
      if: ${{ (success() || failure()) && inputs.run-codecov == 'true' }}
      continue-on-error: true
      uses: codecov/codecov-action@v5
      with:
        files: coverage.info,.coverage,coverage.xml
        # disable_search: true
        token: ${{ secrets.CODECOV_TOKEN }}

    - name: Upload test results to Codecov
      if: ${{ (success() || failure()) && inputs.run-codecov == 'true' }}
      continue-on-error: true
      uses: codecov/test-results-action@v1
      with:
        files: ${{ steps.strings.outputs.test_report_path_models }}
        disable_search: true
        token: ${{ secrets.CODECOV_TOKEN }}
