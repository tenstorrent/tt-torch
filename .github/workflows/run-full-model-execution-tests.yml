name: Run Full Model Execution Tests

on:
  workflow_dispatch:
  workflow_call:
    inputs:
      docker-image:
        description: 'Docker image to use for build'
        required: true
        type: string
      run-codecov:
        description: 'Run code coverage reports'
        required: false
        type: string # properly a boolean but autocast to string when passed in using 'with' inputs
        default: 'true'
      run-dump-mlir:
        description: 'Dump MLIR files'
        required: false
        type: string
        default: 'true'
      force-legacy-backend:
        description: 'Force legacy backend'
        required: false
        type: boolean
        default: false
  workflow_run:
    workflows: [Build] # backref to run-build as dependency
    types: [completed]

env:
  DOCKER_CACHE_ROOT: /mnt/dockercache

permissions:
  packages: write
  checks: write

jobs:
  tests:
    timeout-minutes: 120
    strategy:
      fail-fast: false
      matrix:
        runner: [wormhole, blackhole]
        build: [
          {
            wh-runner: wormhole_b0, bh-runner: p150, name: "eval_1", tests: "
                  tests/models/distilbert/test_distilbert.py::test_distilbert[full-distilbert-base-uncased-eval]
                  tests/models/mlpmixer/test_mlpmixer.py::test_mlpmixer[full-eval]
                  tests/models/yolov3/test_yolov3.py::test_yolov3[full-eval]
                  tests/models/yolov4/test_yolov4.py::test_yolov4[full-eval]
                  tests/models/vovnet/test_vovnet_onnx.py::test_vovnet_onnx[full-eval]
                  tests/models/llama/test_llama3_generative.py::test_llama3_generate
            "
          },
          {
            wh-runner: wormhole_b0, bh-runner: p150, name: "eval_2", tests: "
                  tests/models/mnist/test_mnist.py::test_mnist_train[single_device-full-eval]
                  tests/models/MobileNetV2/test_MobileNetV2.py::test_MobileNetV2[full-eval]
                  tests/models/openpose/test_openpose_v2.py::test_openpose_v2[full-eval]
                  tests/models/resnet/test_resnet.py::test_resnet[single_device-full-eval]
                  tests/models/resnet50/test_resnet50.py::test_resnet[single_device-full-eval]
                  tests/models/distilbert/test_distilbert.py::test_distilbert_multiloop[full-distilbert-base-uncased-eval-64]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-vit_l_16]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-vit_b_16]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-vit_l_32]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-vit_b_32]
            "
          },
          {
            wh-runner: wormhole_b0, bh-runner: p150, name: "eval_3", tests: "
                  tests/models/albert/test_albert_masked_lm.py::test_albert_masked_lm[single_device-full-albert-xxlarge-v2-eval]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_y_400mf]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_y_800mf]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_y_1_6gf]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_y_3_2gf]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_y_8gf]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_y_16gf]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_y_32gf]
            "
          },
          {
            wh-runner: wormhole_b0, bh-runner: p150, name: "eval_4", tests: "
                  tests/models/llama/test_llama_3b.py::test_llama_3b[full-eval]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_x_400mf]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_x_800mf]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_x_1_6gf]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_x_3_2gf]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_x_8gf]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_x_16gf]
                  tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-regnet_x_32gf]
                  tests/models/mistral/test_mistral.py::test_mistral[full-ministral3b-eval]
            "
          },
          {
            wh-runner: wormhole_b0, bh-runner: p150, name: "eval_5", tests: "
                tests/models/perceiver_io/test_perceiver_io.py::test_perceiver_io[full-eval]
                tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-mobilenet_v2]
                tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-mobilenet_v3_small]
                tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-mobilenet_v3_large]
                tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-resnet18]
                tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-resnet34]
                tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-resnet50]
                tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-resnet101]
                tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-resnet152]
                tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-resnext50_32x4d]
                tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-resnext101_32x8d]
                tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-resnext101_64x4d]
                tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-wide_resnet50_2]
                tests/models/torchvision/test_torchvision_image_classification.py::test_torchvision_image_classification[single_device-full-eval-wide_resnet101_2]
            "
          },
          # This test group needs to be moved. This will be done once multichip tests are refactored: #780
          {
            wh-runner: n300, name: "eval_6", tests: "
                tests/torch/test_basic_async.py
                tests/torch/test_basic_multichip.py
                tests/models/mnist/test_mnist.py::test_mnist_train[data_parallel-full-eval]
                tests/models/resnet50/test_resnet50.py::test_resnet[data_parallel-full-eval]
                tests/models/resnet/test_resnet.py::test_resnet[data_parallel-full-eval]
                tests/models/mnist/test_mnist_n300.py::test_mnist_train[full-eval]
                tests/models/autoencoder_linear/test_autoencoder_linear_n300.py::test_autoencoder_linear[full-eval]
                tests/models/MobileNetV2/test_MobileNetV2_n300.py::test_MobileNetV2[full-eval]
                tests/models/EfficientNet/test_EfficientNet_n300.py::test_EfficientNet[full-efficientnet-b0-eval]
                tests/models/yolov4/test_yolov4_n300.py::test_yolov4[full-eval]
                tests/models/vit/test_vit_n300.py::test_vit[full-base-eval]
                tests/models/vit/test_vit_n300.py::test_vit[full-large-eval]
                tests/models/segformer/test_segformer_n300.py::test_segformer[full-eval]
                tests/models/hardnet/test_hardnet_n300.py::test_hardnet[full-eval]
            "
          },
          {
            wh-runner: high-memory-n300, name: "eval_6_highmem_quarantine", tests: "
              tests/models/llama/test_llama_7b_pipeline_parallel.py::test_llama_7b_pipeline_parallel[huggyllama/llama-7b-eval]
              tests/models/falcon/test_falcon3_7b_10b_pipeline_parallel.py::test_falcon_pipeline_parallel[full-tiiuae/Falcon3-7B-Base-eval]
              tests/models/falcon/test_falcon3_7b_10b_pipeline_parallel.py::test_falcon_pipeline_parallel[full-tiiuae/Falcon3-10B-Base-eval]
              tests/models/llama/test_llama_7b_generative_pipeline_parallel.py::test_llama_7b_generative_pipeline_parallel
            "
          },
          {
            wh-runner: wormhole_b0, bh-runner: p150, name: "tt-xla-eager", tests: "
              tests/experimental/test_torch_xla_basic.py
              tests/experimental/models/Qwen/test_qwen2_causal_lm.py::test_qwen2_causal_lm_eager
              tests/experimental/models/llama/test_llama_3b.py::test_llama_3b_eager
              tests/experimental/models/mistral/test_mistral.py::test_mistral3b_eager
              tests/experimental/models/resnet50/test_resnet50.py::test_resnet_eager
            "
          },
        ]
    runs-on: ${{ matrix.runner == 'wormhole' && matrix.build.wh-runner || matrix.runner == 'blackhole' && matrix.build.bh-runner  || 'wormhole_b0'}}
    name: "test exec ${{ matrix.runner == 'wormhole' && 'wh' || matrix.runner == 'blackhole' && 'bh' || 'unk' }} (${{ matrix.build.name }})"

    container:
      image: ${{ inputs.docker-image }}
      options: --user root --device /dev/tenstorrent/0 --shm-size=4gb
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
        - /mnt/dockercache:/mnt/dockercache
    steps:
    - uses: actions/checkout@v4

    - name: Check for runner # Check if we need to skip testing when a wh-runner is not set for wormhole run or a bh-runner on blackhole run
      run: |
        if [ "${{ matrix.runner }}" = "wormhole" ] && [ -n "${{ matrix.build.wh-runner }}" ]; then
          echo "RUN_TEST=wormhole" >> $GITHUB_ENV
        elif [ "${{ matrix.runner }}" = "blackhole" ] && [ -n "${{ matrix.build.bh-runner }}" ]; then
          echo "RUN_TEST=blackhole" >> $GITHUB_ENV
        else
          echo "RUN_TEST=skip" >> $GITHUB_ENV
        fi

    - name: Fetch job id
      id: fetch-job-id
      if: ${{ env.RUN_TEST != 'skip' }}
      uses: tenstorrent/tt-github-actions/.github/actions/job_id@main
      with:
        job_name: "test exec ${{ matrix.runner == 'wormhole' && 'wh' || matrix.runner == 'blackhole' && 'bh' || 'unk' }} (${{ matrix.build.name }})" # reference above tests.name

    - name: Set reusable strings
      if: ${{ env.RUN_TEST != 'skip' }}
      id: strings
      shell: bash
      env:
        JOB_ID: ${{ steps.fetch-job-id.outputs.job_id }}

      run: |
        echo "work-dir=$(pwd)" >> "$GITHUB_OUTPUT"
        echo "install-dir=$(pwd)/install" >> "$GITHUB_OUTPUT"
        echo "dist-dir=$(pwd)/dist" >> "$GITHUB_OUTPUT"
        echo "test_report_path_models=report_models_$JOB_ID.xml" >> "$GITHUB_OUTPUT"
        echo "mlir_dir=$(pwd)/model_mlir" >> "$GITHUB_OUTPUT" # Define the model_mlir directory

    - name: Use build artifacts
      if: ${{ env.RUN_TEST != 'skip' }}
      uses: tenstorrent/tt-forge/.github/actions/download-artifact@main
      with:
        name: install-artifacts
        path: install

    - name: install tt-torch
      if: ${{ env.RUN_TEST != 'skip' }}
      shell: bash
      run: |
        source env/activate
        mkdir -p ${{ steps.strings.outputs.dist-dir }}
        mv install/wheels/* ${{ steps.strings.outputs.dist-dir }}
        pip install ${{ steps.strings.outputs.dist-dir }}/*.whl

    - name: Run Full Model Execution Tests
      if: ${{ env.RUN_TEST != 'skip' }}
      env:
        HF_HOME: ${{ env.DOCKER_CACHE_ROOT }}/huggingface
        TORCH_HOME: ${{ env.DOCKER_CACHE_ROOT }}/torch
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
        TT_TORCH_FORCE_LEGACY_BACKEND: ${{ inputs.force-legacy-backend && '1' || '0' }}
      shell: bash
      run: |
        source env/activate

        TT_TORCH_SAVE_MLIR=STABLEHLO,TTIR,TTNN pytest --forked --durations=0 -v -rf ${{matrix.build.tests}} \
           --junit-xml=${{ steps.strings.outputs.test_report_path_models }} \
           --cov=tt_torch --cov-report term --cov-report xml:coverage.xml --cov-append | tee pytest.log

    - name: Upload Test Log
      uses: actions/upload-artifact@v4
      if: ${{ (success() || failure()) && env.RUN_TEST != 'skip' }}
      with:
        name: test-log-${{ matrix.runner }}-${{ matrix.build.name }}-${{ steps.fetch-job-id.outputs.job_id }}
        path: pytest.log

    - name: Upload Test Report Models
      uses: actions/upload-artifact@v4
      if: ${{ (success() || failure()) && env.RUN_TEST != 'skip' }}
      with:
        name: test-reports-models-${{ matrix.runner }}-${{ matrix.build.name }}-${{ steps.fetch-job-id.outputs.job_id }}
        path: ${{ steps.strings.outputs.test_report_path_models }}

    - name: Upload coverage reports to Codecov
      if: ${{ (success() || failure()) && inputs.run-codecov == 'true' && env.RUN_TEST != 'skip' }}
      continue-on-error: true
      uses: codecov/codecov-action@v5
      with:
        files: coverage.info,.coverage,coverage.xml
        # disable_search: true
        token: ${{ secrets.CODECOV_TOKEN }}

    - name: Upload test results to Codecov
      if: ${{ (success() || failure()) && inputs.run-codecov == 'true' && env.RUN_TEST != 'skip' }}
      continue-on-error: true
      uses: codecov/test-results-action@v1
      with:
        files: ${{ steps.strings.outputs.test_report_path_models }}
        disable_search: true
        token: ${{ secrets.CODECOV_TOKEN }}

    - name: Upload MLIR files
      uses: actions/upload-artifact@v4
      if: ${{ (success() || failure()) && inputs.run-dump-mlir == 'true' && env.RUN_TEST != 'skip' }}
      with:
        name: model-mlir-execute-push-${{ matrix.runner }}-${{ matrix.build.name }}-${{ steps.fetch-job-id.outputs.job_id }}
        path: ${{ steps.strings.outputs.mlir_dir }}

  merge_mlir_artifacts:
    needs: [tests]
    if: ${{ always() && inputs.run-dump-mlir == 'true' }}  # Runs always if MLIR dump is enabled
    runs-on: ubuntu-latest
    name: Merge MLIR Artifacts
    steps:
      - name: Merge Artifacts
        uses: actions/upload-artifact/merge@v4
        with:
          name: model-mlir-execute-push
          pattern: model-mlir-execute-push-*
          delete-merged: true
