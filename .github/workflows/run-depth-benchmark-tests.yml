name: Run Model Compile Depth Benchmark Tests

on:
  workflow_dispatch:
  workflow_call:
    inputs:
      docker-image:
        description: 'Docker image to use for build'
        required: true
        type: string
      matrix-json-splits:
        description: 'JSON string describing build matrix group configuration, an array of (id, name, runs-on) objects'
        required: true
        type: string
  workflow_run:
    workflows: [Build] # backref to run-build as dependency
    types: [completed]

jobs:
  tests:
    timeout-minutes: 500
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        group: ${{ fromJson(inputs.matrix-json-splits) }}

    runs-on:
      - ${{ matrix.group.runs-on }}

    # use short names because github actions GUI crops long names including model names
    name: "bmk (${{ matrix.group.name }})"

    container:
      image: ${{ inputs.docker-image }}
      options: --user root --device /dev/tenstorrent/0 --shm-size=4gb
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
        - /mnt/dockercache:/mnt/dockercache
    steps:
    - uses: actions/checkout@v4
      with:
        submodules: recursive
        lfs: true

    - name: Fetch job id
      id: fetch-job-id
      uses: tenstorrent/tt-github-actions/.github/actions/job_id@main
      with:
        job_name: "bmk (${{ matrix.group.name }})" # reference above tests.name

    - name: Set reusable strings
      id: strings
      shell: bash
      env:
        JOB_ID: ${{ steps.fetch-job-id.outputs.job_id }}

      run: |
        echo "work-dir=$(pwd)" >> "$GITHUB_OUTPUT"
        echo "install-dir=$(pwd)/install" >> "$GITHUB_OUTPUT"
        echo "dist-dir=$(pwd)/dist" >> "$GITHUB_OUTPUT"
        echo "test_report_path_models=report_models_$JOB_ID.xml" >> "$GITHUB_OUTPUT"

        # hardcoded output filename
        echo "matrix-json=benchmark_test_matrix.json" >> "$GITHUB_OUTPUT"

    - name: Use build artifacts
      uses: actions/download-artifact@v4
      with:
        name: install-artifacts
        path: ${{ steps.strings.outputs.install-dir }}

    - name: 'Untar install directory'
      shell: bash
      working-directory: ${{ steps.strings.outputs.install-dir }}
      run: |
        tar xvf artifact.tar
        mkdir -p ${{ steps.strings.outputs.dist-dir }}
        mv wheels/* ${{ steps.strings.outputs.dist-dir }}

    - name: install tt-torch
      shell: bash
      run: |
        source env/activate
        pip install ${{ steps.strings.outputs.dist-dir }}/*.whl

    - name: Download test matrix JSON file
      uses: actions/download-artifact@v4
      with:
        name: benchmark-test-matrix-json
        merge-multiple: true
        path: ${{ steps.strings.outputs.work-dir }}

    - name: List directory
      shell: bash
      run: |
        ls -l ${{ steps.strings.outputs.work-dir }}

    - name: Run Execution Benchmark Tests
      env:
        DOCKER_CACHE_ROOT: /mnt/dockercache
        HF_HOME: ${{ env.DOCKER_CACHE_ROOT }}/huggingface
        TORCH_HOME: ${{ env.DOCKER_CACHE_ROOT }}/torch
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      shell: bash
      run: |
        source env/activate
        set +e
        mkdir -p ${{ steps.strings.outputs.work-dir }}/pytest-logs

        echo "Matrix group ${{ matrix.group.name }} runs-on ${{ matrix.group.runs-on }} with index ${{ matrix.group.group-id }}"

        # TODO: remove this debug statement
        # Assume the matrix json was downloaded into work-dir (so this cat is from work-dir/benchmark_test_matrix.json)
        cat ${{ steps.strings.outputs.matrix-json }}

        # Read the JSON file and extract the per-matrix testlist assigned to this runner (from its group-id)
        testlist=$(cat ${{ steps.strings.outputs.matrix-json }} | jq ".[${{ matrix.group.group-id }}]")

        # Print all tests to be run in this group
        echo "Tests to be run in matrix with group ${{ matrix.group.group-id }}"
        echo "$testlist" | jq -c '.tests[] | {name: .["full-test-name"], duration: .["test-duration"]}'


        counter=0
        total_tests=$(echo $testlist | jq -r '.[]' | wc -l)

        # Given a test with known runtime, allow it to run up to timeout_buffer X its expected runtime before killing it.
        timeout_buffer=5
        timeout_logname="${{ steps.strings.outputs.work-dir }}/pytest-logs/group_${{matrix.group.group-id}}_timeout.log"

        # iterate over all tests in testlist
        echo "$testlist" | jq -c '.tests[]' | while read -r test; do

            # Extract the test name and duration from JSON object "test"

            raw_test_name=$(echo "$test" | jq -r '.["full-test-name"]')
            test_name=$(echo "$raw_test_name" | sed 's/[^a-zA-Z0-9]/_/g')

            test_duration=$(echo "$test" | jq -r '.["test-duration"]')

            # Bash only supports integer arithmetic, so to compute fp timeout * int buffer we need to pipe it to
            # something that supports fp arithmetic like python

            buffered_duration=-1

            # Determine buffered duration and quarantine status
            if [ "$test_duration" = "-1" ]; then
                buffered_duration=-1
                echo "Timeout: $buffered_duration seconds (test_duration is -1, no timeout applied for quarantined tests.)"
            else
                buffered_duration=$(python -c "print(int($test_duration * $timeout_buffer))")
                echo "Timeout: $buffered_duration seconds (expected $test_duration x $timeout_buffer)"
            fi
            counter=$((counter + 1))

            echo "========================================"
            echo "Running test $counter of $total_tests: $raw_test_name"
            echo "========================================"

            # use GNU timeout to autokill test after test_duration seconds
            # sends sigterm by default

            if [ "$buffered_duration" != "-1" ]; then
                timeout "${buffered_duration}s" pytest -svv "$raw_test_name" \
                    --junit-xml=${{ steps.strings.outputs.test_report_path_models }}_subtest_${test_name}.xml \
                    --crashsafe | tee ${{ steps.strings.outputs.work-dir }}/pytest-logs/${test_name}.log
            else
                pytest -svv "$raw_test_name" \
                    --junit-xml=${{ steps.strings.outputs.test_report_path_models }}_subtest_${test_name}.xml \
                    --crashsafe | tee ${{ steps.strings.outputs.work-dir }}/pytest-logs/${test_name}.log
            fi

            # Check the exit status of the timeout command for timeout detection.
            exit_status=$?
            if [ $exit_status -eq 124 ]; then
                echo "Test $raw_test_name timed out after $test_duration seconds."
                # Perform conditional actions for timeout, e.g., log or mark as failed
                echo "$raw_test_name" >> timeout_logname
            else
                echo "Test $raw_test_name completed successfully."
            fi
            echo "Wrote junitxml report ${{ steps.strings.outputs.test_report_path_models }}_subtest_${test_name}.xml"
            echo "wrote log ${{ steps.strings.outputs.work-dir }}/pytest-logs/${test_name}.log"
        done

        set -e
        exit 0 # force successful exit

    - name: Upload Pytest Logs
      uses: actions/upload-artifact@v4
      with:
        name: pytest-logs-${{ matrix.group.group-id }}-${{ matrix.group.name }}
        path: ${{ steps.strings.outputs.work-dir }}/pytest-logs

    - name: Postprocess and Fuse Test Reports
      shell: bash
      run: |
        source env/activate
        python tt_torch/tools/postprocess_crashsafe_reports.py "${{ steps.strings.outputs.test_report_path_models }}_subtest*_crashsafe.xml" "${{ steps.strings.outputs.test_report_path_models }}"

    - name: Upload Test Report Models
      uses: actions/upload-artifact@v4
      if: success() || failure()
      with:
        name: test-reports-models-benchmark-${{ matrix.group.runs-on }}-${{ matrix.group.name }}-${{ steps.fetch-job-id.outputs.job_id }}
        path: ${{ steps.strings.outputs.test_report_path_models }}
