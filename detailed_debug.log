WARNING:root:Defaulting to PJRT_DEVICE=CPU
2025-09-05 12:36:54.523244: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
Torch-XLA SPMD Tensor Parallelism for GPT-OSS 20B Model
==================================================
Setting up XLA environment...
XLA environment configured.
Created device mesh: (1, 8) with 8 devices
Loading model...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 213.27it/s]
Some weights of the model checkpoint at openai/gpt-oss-20b were not used when initializing GptOssForCausalLM: ['model.layers.0.mlp.experts.down_proj_blocks', 'model.layers.0.mlp.experts.down_proj_scales', 'model.layers.0.mlp.experts.gate_up_proj_blocks', 'model.layers.0.mlp.experts.gate_up_proj_scales', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.experts.down_proj_bias', 'model.layers.1.mlp.experts.down_proj_blocks', 'model.layers.1.mlp.experts.down_proj_scales', 'model.layers.1.mlp.experts.gate_up_proj_bias', 'model.layers.1.mlp.experts.gate_up_proj_blocks', 'model.layers.1.mlp.experts.gate_up_proj_scales', 'model.layers.1.mlp.router.bias', 'model.layers.1.mlp.router.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.sinks', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.experts.down_proj_bias', 'model.layers.10.mlp.experts.down_proj_blocks', 'model.layers.10.mlp.experts.down_proj_scales', 'model.layers.10.mlp.experts.gate_up_proj_bias', 'model.layers.10.mlp.experts.gate_up_proj_blocks', 'model.layers.10.mlp.experts.gate_up_proj_scales', 'model.layers.10.mlp.router.bias', 'model.layers.10.mlp.router.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.sinks', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.experts.down_proj_bias', 'model.layers.11.mlp.experts.down_proj_blocks', 'model.layers.11.mlp.experts.down_proj_scales', 'model.layers.11.mlp.experts.gate_up_proj_bias', 'model.layers.11.mlp.experts.gate_up_proj_blocks', 'model.layers.11.mlp.experts.gate_up_proj_scales', 'model.layers.11.mlp.router.bias', 'model.layers.11.mlp.router.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.sinks', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.experts.down_proj_bias', 'model.layers.12.mlp.experts.down_proj_blocks', 'model.layers.12.mlp.experts.down_proj_scales', 'model.layers.12.mlp.experts.gate_up_proj_bias', 'model.layers.12.mlp.experts.gate_up_proj_blocks', 'model.layers.12.mlp.experts.gate_up_proj_scales', 'model.layers.12.mlp.router.bias', 'model.layers.12.mlp.router.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.sinks', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.experts.down_proj_bias', 'model.layers.13.mlp.experts.down_proj_blocks', 'model.layers.13.mlp.experts.down_proj_scales', 'model.layers.13.mlp.experts.gate_up_proj_bias', 'model.layers.13.mlp.experts.gate_up_proj_blocks', 'model.layers.13.mlp.experts.gate_up_proj_scales', 'model.layers.13.mlp.router.bias', 'model.layers.13.mlp.router.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.sinks', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.experts.down_proj_bias', 'model.layers.14.mlp.experts.down_proj_blocks', 'model.layers.14.mlp.experts.down_proj_scales', 'model.layers.14.mlp.experts.gate_up_proj_bias', 'model.layers.14.mlp.experts.gate_up_proj_blocks', 'model.layers.14.mlp.experts.gate_up_proj_scales', 'model.layers.14.mlp.router.bias', 'model.layers.14.mlp.router.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.sinks', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.experts.down_proj_bias', 'model.layers.15.mlp.experts.down_proj_blocks', 'model.layers.15.mlp.experts.down_proj_scales', 'model.layers.15.mlp.experts.gate_up_proj_bias', 'model.layers.15.mlp.experts.gate_up_proj_blocks', 'model.layers.15.mlp.experts.gate_up_proj_scales', 'model.layers.15.mlp.router.bias', 'model.layers.15.mlp.router.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.sinks', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.experts.down_proj_bias', 'model.layers.16.mlp.experts.down_proj_blocks', 'model.layers.16.mlp.experts.down_proj_scales', 'model.layers.16.mlp.experts.gate_up_proj_bias', 'model.layers.16.mlp.experts.gate_up_proj_blocks', 'model.layers.16.mlp.experts.gate_up_proj_scales', 'model.layers.16.mlp.router.bias', 'model.layers.16.mlp.router.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.sinks', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.experts.down_proj_bias', 'model.layers.17.mlp.experts.down_proj_blocks', 'model.layers.17.mlp.experts.down_proj_scales', 'model.layers.17.mlp.experts.gate_up_proj_bias', 'model.layers.17.mlp.experts.gate_up_proj_blocks', 'model.layers.17.mlp.experts.gate_up_proj_scales', 'model.layers.17.mlp.router.bias', 'model.layers.17.mlp.router.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.sinks', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.mlp.experts.down_proj_bias', 'model.layers.18.mlp.experts.down_proj_blocks', 'model.layers.18.mlp.experts.down_proj_scales', 'model.layers.18.mlp.experts.gate_up_proj_bias', 'model.layers.18.mlp.experts.gate_up_proj_blocks', 'model.layers.18.mlp.experts.gate_up_proj_scales', 'model.layers.18.mlp.router.bias', 'model.layers.18.mlp.router.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.sinks', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.mlp.experts.down_proj_bias', 'model.layers.19.mlp.experts.down_proj_blocks', 'model.layers.19.mlp.experts.down_proj_scales', 'model.layers.19.mlp.experts.gate_up_proj_bias', 'model.layers.19.mlp.experts.gate_up_proj_blocks', 'model.layers.19.mlp.experts.gate_up_proj_scales', 'model.layers.19.mlp.router.bias', 'model.layers.19.mlp.router.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.sinks', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.experts.down_proj_bias', 'model.layers.2.mlp.experts.down_proj_blocks', 'model.layers.2.mlp.experts.down_proj_scales', 'model.layers.2.mlp.experts.gate_up_proj_bias', 'model.layers.2.mlp.experts.gate_up_proj_blocks', 'model.layers.2.mlp.experts.gate_up_proj_scales', 'model.layers.2.mlp.router.bias', 'model.layers.2.mlp.router.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.sinks', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.mlp.experts.down_proj_bias', 'model.layers.20.mlp.experts.down_proj_blocks', 'model.layers.20.mlp.experts.down_proj_scales', 'model.layers.20.mlp.experts.gate_up_proj_bias', 'model.layers.20.mlp.experts.gate_up_proj_blocks', 'model.layers.20.mlp.experts.gate_up_proj_scales', 'model.layers.20.mlp.router.bias', 'model.layers.20.mlp.router.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.sinks', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.mlp.experts.down_proj_bias', 'model.layers.21.mlp.experts.down_proj_blocks', 'model.layers.21.mlp.experts.down_proj_scales', 'model.layers.21.mlp.experts.gate_up_proj_bias', 'model.layers.21.mlp.experts.gate_up_proj_blocks', 'model.layers.21.mlp.experts.gate_up_proj_scales', 'model.layers.21.mlp.router.bias', 'model.layers.21.mlp.router.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.sinks', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.mlp.experts.down_proj_bias', 'model.layers.22.mlp.experts.down_proj_blocks', 'model.layers.22.mlp.experts.down_proj_scales', 'model.layers.22.mlp.experts.gate_up_proj_bias', 'model.layers.22.mlp.experts.gate_up_proj_blocks', 'model.layers.22.mlp.experts.gate_up_proj_scales', 'model.layers.22.mlp.router.bias', 'model.layers.22.mlp.router.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.sinks', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.mlp.experts.down_proj_bias', 'model.layers.23.mlp.experts.down_proj_blocks', 'model.layers.23.mlp.experts.down_proj_scales', 'model.layers.23.mlp.experts.gate_up_proj_bias', 'model.layers.23.mlp.experts.gate_up_proj_blocks', 'model.layers.23.mlp.experts.gate_up_proj_scales', 'model.layers.23.mlp.router.bias', 'model.layers.23.mlp.router.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.sinks', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.experts.down_proj_bias', 'model.layers.3.mlp.experts.down_proj_blocks', 'model.layers.3.mlp.experts.down_proj_scales', 'model.layers.3.mlp.experts.gate_up_proj_bias', 'model.layers.3.mlp.experts.gate_up_proj_blocks', 'model.layers.3.mlp.experts.gate_up_proj_scales', 'model.layers.3.mlp.router.bias', 'model.layers.3.mlp.router.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.sinks', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.experts.down_proj_bias', 'model.layers.4.mlp.experts.down_proj_blocks', 'model.layers.4.mlp.experts.down_proj_scales', 'model.layers.4.mlp.experts.gate_up_proj_bias', 'model.layers.4.mlp.experts.gate_up_proj_blocks', 'model.layers.4.mlp.experts.gate_up_proj_scales', 'model.layers.4.mlp.router.bias', 'model.layers.4.mlp.router.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.sinks', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.experts.down_proj_bias', 'model.layers.5.mlp.experts.down_proj_blocks', 'model.layers.5.mlp.experts.down_proj_scales', 'model.layers.5.mlp.experts.gate_up_proj_bias', 'model.layers.5.mlp.experts.gate_up_proj_blocks', 'model.layers.5.mlp.experts.gate_up_proj_scales', 'model.layers.5.mlp.router.bias', 'model.layers.5.mlp.router.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.sinks', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.experts.down_proj_bias', 'model.layers.6.mlp.experts.down_proj_blocks', 'model.layers.6.mlp.experts.down_proj_scales', 'model.layers.6.mlp.experts.gate_up_proj_bias', 'model.layers.6.mlp.experts.gate_up_proj_blocks', 'model.layers.6.mlp.experts.gate_up_proj_scales', 'model.layers.6.mlp.router.bias', 'model.layers.6.mlp.router.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.sinks', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.experts.down_proj_bias', 'model.layers.7.mlp.experts.down_proj_blocks', 'model.layers.7.mlp.experts.down_proj_scales', 'model.layers.7.mlp.experts.gate_up_proj_bias', 'model.layers.7.mlp.experts.gate_up_proj_blocks', 'model.layers.7.mlp.experts.gate_up_proj_scales', 'model.layers.7.mlp.router.bias', 'model.layers.7.mlp.router.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.sinks', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.experts.down_proj_bias', 'model.layers.8.mlp.experts.down_proj_blocks', 'model.layers.8.mlp.experts.down_proj_scales', 'model.layers.8.mlp.experts.gate_up_proj_bias', 'model.layers.8.mlp.experts.gate_up_proj_blocks', 'model.layers.8.mlp.experts.gate_up_proj_scales', 'model.layers.8.mlp.router.bias', 'model.layers.8.mlp.router.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.sinks', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.experts.down_proj_bias', 'model.layers.9.mlp.experts.down_proj_blocks', 'model.layers.9.mlp.experts.down_proj_scales', 'model.layers.9.mlp.experts.gate_up_proj_bias', 'model.layers.9.mlp.experts.gate_up_proj_blocks', 'model.layers.9.mlp.experts.gate_up_proj_scales', 'model.layers.9.mlp.router.bias', 'model.layers.9.mlp.router.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.sinks', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing GptOssForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GptOssForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GptOssForCausalLM were not initialized from the model checkpoint at openai/gpt-oss-20b and are newly initialized: ['model.layers.0.mlp.experts.down_proj', 'model.layers.0.mlp.experts.gate_up_proj']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<512x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg3: !vhlo.tensor_v1<1x13x!vhlo.i64_v1>, %arg4: !vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg7: !vhlo.tensor_v1<32x!vhlo.f32_v1>, %arg8: !vhlo.tensor_v1<512x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<32x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>, %arg15: !vhlo.tensor_v1<64x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg17: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg18: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg19: !vhlo.tensor_v1<4096x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>, %arg21: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>, %arg23: !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg25: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg26: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg27: !vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>, %arg28: !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>, %arg29: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg30: !vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x201088x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>>}> : () -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<ui8>>}> : () -> !vhlo.tensor_v1<!vhlo.ui8_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF80> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.47222231E-4> : tensor<1x13xf32>>}> : () -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>>}> : () -> !vhlo.tensor_v1<1x1x13x!vhlo.f32_v1>
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<ui8>>}> : () -> !vhlo.tensor_v1<!vhlo.ui8_v1>
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %9 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %10 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %12 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>
    %13 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %14 = "vhlo.broadcast_in_dim_v1"(%4) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %15 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %16 = "vhlo.convert_v1"(%arg5) : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x!vhlo.f32_v1>
    %17 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %18 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<1x13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %19 = "vhlo.convert_v1"(%18) : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.ui32_v1>
    %20 = "vhlo.gather_v2"(%arg4, %19) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 2880]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %21 = "vhlo.reshape_v1"(%20) : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %22 = "vhlo.convert_v1"(%21) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %23 = "vhlo.power_v1"(%22, %14) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %24 = "vhlo.reduce_v1"(%23, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg32: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %25 = "vhlo.multiply_v1"(%24, %5) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %26 = "vhlo.reshape_v1"(%25) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %27 = "vhlo.broadcast_in_dim_v1"(%arg2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %28 = "vhlo.add_v1"(%26, %27) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %29 = "vhlo.rsqrt_v2"(%28) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %31 = "vhlo.broadcast_in_dim_v1"(%30) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %32 = "vhlo.multiply_v1"(%22, %31) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %33 = "vhlo.multiply_v1"(%17, %32) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %34 = "vhlo.convert_v1"(%33) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %36 = "vhlo.transpose_v1"(%arg20) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,4096]{0,1}">} : (!vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>
    %37 = "vhlo.dot_general_v2"(%35, %36) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>
    %38 = "vhlo.reshape_v1"(%37) : (!vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%arg19) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>
    %40 = "vhlo.add_v1"(%38, %39) : (!vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>
    %41 = "vhlo.reshape_v1"(%40) : (!vhlo.tensor_v1<1x13x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>
    %42 = "vhlo.transpose_v1"(%41) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,13,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %43 = "vhlo.slice_v1"(%42) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 13, 32]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %44 = "vhlo.convert_v1"(%43) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %45 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x1x!vhlo.f32_v1>
    %46 = "vhlo.dot_general_v2"(%45, %6) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x32x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x13x!vhlo.f32_v1>
    %47 = "vhlo.transpose_v1"(%46) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,13,32]{1,2,0}">} : (!vhlo.tensor_v1<1x32x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %48 = "vhlo.cosine_v2"(%47) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,13,32]{1,2,0}">} : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %49 = "vhlo.broadcast_in_dim_v1"(%arg6) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %50 = "vhlo.multiply_v1"(%48, %49) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %51 = "vhlo.convert_v1"(%50) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>
    %52 = "vhlo.reshape_v1"(%51) : (!vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>
    %53 = "vhlo.convert_v1"(%52) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>
    %54 = "vhlo.reshape_v1"(%53) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %55 = "vhlo.broadcast_in_dim_v1"(%54) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %56 = "vhlo.multiply_v1"(%44, %55) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %58 = "vhlo.slice_v1"(%42) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 13, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 32]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %59 = "vhlo.convert_v1"(%58) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %60 = "vhlo.sine_v2"(%47) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,13,32]{1,2,0}">} : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %61 = "vhlo.multiply_v1"(%60, %49) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %62 = "vhlo.convert_v1"(%61) : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>
    %63 = "vhlo.reshape_v1"(%62) : (!vhlo.tensor_v1<1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>
    %64 = "vhlo.convert_v1"(%63) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>
    %65 = "vhlo.reshape_v1"(%64) : (!vhlo.tensor_v1<1x1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>
    %66 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %67 = "vhlo.multiply_v1"(%59, %66) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %68 = "vhlo.convert_v1"(%67) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %69 = "vhlo.subtract_v1"(%57, %68) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %70 = "vhlo.multiply_v1"(%59, %55) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %71 = "vhlo.convert_v1"(%70) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %72 = "vhlo.multiply_v1"(%44, %66) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>
    %73 = "vhlo.convert_v1"(%72) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %74 = "vhlo.add_v1"(%71, %73) : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>
    %75 = "vhlo.concatenate_v1"(%69, %74) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %76 = "vhlo.reshape_v1"(%75) : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>
    %77 = "vhlo.transpose_v1"(%arg9) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,512]{0,1}">} : (!vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>
    %78 = "vhlo.dot_general_v2"(%35, %77) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x512x!vhlo.bf16_v1>
    %79 = "vhlo.reshape_v1"(%78) : (!vhlo.tensor_v1<13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %80 = "vhlo.broadcast_in_dim_v1"(%arg8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %81 = "vhlo.add_v1"(%79, %80) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %82 = "vhlo.reshape_v1"(%81) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>
    %83 = "vhlo.transpose_v1"(%82) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,13,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>
    %84 = "vhlo.slice_v1"(%83) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 13, 32]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %85 = "vhlo.convert_v1"(%84) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %86 = "vhlo.broadcast_in_dim_v1"(%54) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %87 = "vhlo.multiply_v1"(%85, %86) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %88 = "vhlo.convert_v1"(%87) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %89 = "vhlo.slice_v1"(%83) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 13, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 32]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %90 = "vhlo.convert_v1"(%89) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %91 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %92 = "vhlo.multiply_v1"(%90, %91) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %93 = "vhlo.convert_v1"(%92) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %94 = "vhlo.subtract_v1"(%88, %93) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %95 = "vhlo.multiply_v1"(%90, %86) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %96 = "vhlo.convert_v1"(%95) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %97 = "vhlo.multiply_v1"(%85, %91) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>
    %98 = "vhlo.convert_v1"(%97) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %99 = "vhlo.add_v1"(%96, %98) : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>
    %100 = "vhlo.concatenate_v1"(%94, %99) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>
    %101 = "vhlo.broadcast_in_dim_v1"(%100) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>
    %102 = "vhlo.reshape_v1"(%101) : (!vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %103 = "vhlo.transpose_v1"(%102) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,64,13]{2,3,1,0}">} : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x64x13x!vhlo.bf16_v1>
    %104 = "vhlo.reshape_v1"(%103) : (!vhlo.tensor_v1<1x64x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x64x13x!vhlo.bf16_v1>
    %105 = "vhlo.dot_general_v2"(%76, %104) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<64x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>
    %106 = "vhlo.reshape_v1"(%105) : (!vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %107 = "vhlo.convert_v1"(%106) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>
    %108 = "vhlo.broadcast_in_dim_v1"(%arg18) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>
    %109 = "vhlo.multiply_v1"(%107, %108) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>
    %110 = "vhlo.convert_v1"(%109) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %111 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.i64_v1>
    %112 = "vhlo.broadcast_in_dim_v1"(%arg17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %113 = "vhlo.subtract_v1"(%1, %112) : (!vhlo.tensor_v1<13x!vhlo.i64_v1>, !vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %114 = "vhlo.broadcast_in_dim_v1"(%113) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.i64_v1>
    %115 = "vhlo.compare_v1"(%111, %114) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<13x13x!vhlo.i64_v1>, !vhlo.tensor_v1<13x13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %116 = "vhlo.convert_v1"(%115) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %117 = "vhlo.and_v1"(%116, %13) : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %118 = "vhlo.compare_v1"(%117, %15) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 NE>}> : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %119 = "vhlo.convert_v1"(%118) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %120 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.i64_v1>
    %121 = "vhlo.compare_v1"(%111, %120) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LE>}> : (!vhlo.tensor_v1<13x13x!vhlo.i64_v1>, !vhlo.tensor_v1<13x13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %122 = "vhlo.convert_v1"(%121) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %123 = "vhlo.and_v1"(%119, %122) : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>
    %124 = "vhlo.compare_v1"(%123, %15) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 NE>}> : (!vhlo.tensor_v1<13x13x!vhlo.ui8_v1>, !vhlo.tensor_v1<13x13x!vhlo.ui8_v1>) -> !vhlo.tensor_v1<13x13x!vhlo.bool_v1>
    %125 = "vhlo.reshape_v1"(%124) : (!vhlo.tensor_v1<13x13x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bool_v1>
    %126 = "vhlo.reshape_v1"(%arg16) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.bf16_v1>
    %127 = "vhlo.broadcast_in_dim_v1"(%126) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>
    %128 = "vhlo.select_v1"(%125, %12, %127) : (!vhlo.tensor_v1<1x1x13x13x!vhlo.bool_v1>, !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>
    %129 = "vhlo.reshape_v1"(%128) : (!vhlo.tensor_v1<1x1x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x13x!vhlo.bf16_v1>
    %130 = "vhlo.broadcast_in_dim_v1"(%129) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %131 = "vhlo.add_v1"(%110, %130) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %132 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.bf16_v1>
    %133 = "vhlo.broadcast_in_dim_v1"(%132) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x1x!vhlo.bf16_v1>
    %134 = "vhlo.concatenate_v1"(%131, %133) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %135 = "vhlo.transpose_v1"(%arg1) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,512]{0,1}">} : (!vhlo.tensor_v1<512x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>
    %136 = "vhlo.dot_general_v2"(%35, %135) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x512x!vhlo.bf16_v1>
    %137 = "vhlo.reshape_v1"(%136) : (!vhlo.tensor_v1<13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %138 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %139 = "vhlo.add_v1"(%137, %138) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>
    %140 = "vhlo.reshape_v1"(%139) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>
    %141 = "vhlo.transpose_v1"(%140) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,13,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>
    %142 = "vhlo.convert_v1"(%arg30) : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x!vhlo.f32_v1>
    %143 = "vhlo.broadcast_in_dim_v1"(%142) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %144 = "vhlo.reduce_v1"(%134, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.maximum_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>
    %145 = "vhlo.broadcast_in_dim_v1"(%144) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %146 = "vhlo.subtract_v1"(%134, %145) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %147 = "vhlo.reduce_v1"(%146, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.maximum_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>
    %148 = "vhlo.broadcast_in_dim_v1"(%147) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %149 = "vhlo.subtract_v1"(%146, %148) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %150 = "vhlo.exponential_v2"(%149) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %151 = "vhlo.reduce_v1"(%150, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>
    %152 = "vhlo.broadcast_in_dim_v1"(%151) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %153 = "vhlo.divide_v1"(%150, %152) : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>
    %154 = "vhlo.slice_v1"(%153) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 13, 13]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x13x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>
    %155 = "vhlo.reshape_v1"(%154) : (!vhlo.tensor_v1<1x64x13x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>
    %156 = "vhlo.broadcast_in_dim_v1"(%141) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>
    %157 = "vhlo.reshape_v1"(%156) : (!vhlo.tensor_v1<1x8x8x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>
    %158 = "vhlo.dot_general_v2"(%155, %157) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<64x13x13x!vhlo.bf16_v1>, !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>
    %159 = "vhlo.reshape_v1"(%158) : (!vhlo.tensor_v1<64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>
    %160 = "vhlo.transpose_v1"(%159) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,13,64,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x64x13x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>
    %161 = "vhlo.reshape_v1"(%160) : (!vhlo.tensor_v1<1x13x64x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>
    %162 = "vhlo.transpose_v1"(%arg14) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[4096,2880]{0,1}">} : (!vhlo.tensor_v1<2880x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>
    %163 = "vhlo.dot_general_v2"(%161, %162) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x4096x!vhlo.bf16_v1>, !vhlo.tensor_v1<4096x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %164 = "vhlo.reshape_v1"(%163) : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %165 = "vhlo.broadcast_in_dim_v1"(%arg13) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %166 = "vhlo.add_v1"(%164, %165) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %167 = "vhlo.add_v1"(%21, %166) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %168 = "vhlo.broadcast_in_dim_v1"(%arg29) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %169 = "vhlo.convert_v1"(%arg21) : (!vhlo.tensor_v1<2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x!vhlo.f32_v1>
    %170 = "vhlo.broadcast_in_dim_v1"(%169) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %171 = "vhlo.convert_v1"(%167) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %172 = "vhlo.power_v1"(%171, %14) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %173 = "vhlo.reduce_v1"(%172, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg32: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %174 = "vhlo.multiply_v1"(%173, %5) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %175 = "vhlo.reshape_v1"(%174) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %176 = "vhlo.add_v1"(%175, %27) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %177 = "vhlo.rsqrt_v2"(%176) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %178 = "vhlo.reshape_v1"(%177) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %179 = "vhlo.broadcast_in_dim_v1"(%178) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %180 = "vhlo.multiply_v1"(%171, %179) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %181 = "vhlo.multiply_v1"(%170, %180) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %182 = "vhlo.convert_v1"(%181) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %183 = "vhlo.reshape_v1"(%182) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %184 = "vhlo.concatenate_v1"(%183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183, %183) <{dimension = #vhlo.integer_v1<0 : i64>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<416x2880x!vhlo.bf16_v1>
    %185 = "vhlo.reshape_v1"(%184) : (!vhlo.tensor_v1<416x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %186 = "vhlo.dot_general_v2"(%185, %arg28) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>
    %187 = "vhlo.broadcast_in_dim_v1"(%arg27) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>
    %188 = "vhlo.add_v1"(%186, %187) : (!vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>
    %189 = "vhlo.slice_v1"(%188) <{limit_indices = #vhlo.tensor_v1<dense<[32, 13, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 1]> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %190 = "vhlo.broadcast_in_dim_v1"(%arg25) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %191 = "vhlo.clamp_v1"(%168, %189, %190) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %192 = "vhlo.add_v1"(%191, %11) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %193 = "vhlo.convert_v1"(%192) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %194 = "vhlo.broadcast_in_dim_v1"(%arg26) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %195 = "vhlo.slice_v1"(%188) <{limit_indices = #vhlo.tensor_v1<dense<[32, 13, 5760]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x13x5760x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %196 = "vhlo.clamp_v1"(%194, %195, %190) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %197 = "vhlo.convert_v1"(%196) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %198 = "vhlo.broadcast_in_dim_v1"(%arg24) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %199 = "vhlo.multiply_v1"(%197, %198) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %200 = "vhlo.convert_v1"(%199) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %201 = "vhlo.logistic_v2"(%200) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %202 = "vhlo.convert_v1"(%201) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %203 = "vhlo.multiply_v1"(%197, %202) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %204 = "vhlo.convert_v1"(%203) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %205 = "vhlo.convert_v1"(%204) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %206 = "vhlo.multiply_v1"(%193, %205) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>
    %207 = "vhlo.convert_v1"(%206) : (!vhlo.tensor_v1<32x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %208 = "vhlo.dot_general_v2"(%207, %arg23) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x2880x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %209 = "vhlo.broadcast_in_dim_v1"(%arg22) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %210 = "vhlo.add_v1"(%208, %209) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>
    %211 = "vhlo.reshape_v1"(%210) : (!vhlo.tensor_v1<32x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>
    %212 = "vhlo.convert_v1"(%211) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>
    %213 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<13x!vhlo.i64_v1>
    %214 = "vhlo.broadcast_in_dim_v1"(%213) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>
    %215 = "vhlo.transpose_v1"(%arg12) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,32]{0,1}">} : (!vhlo.tensor_v1<32x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>
    %216 = "vhlo.dot_general_v2"(%183, %215) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %217 = "vhlo.broadcast_in_dim_v1"(%arg11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %218 = "vhlo.add_v1"(%216, %217) : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %219 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %220 = "vhlo.broadcast_in_dim_v1"(%219) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.i32_v1>
    %221:2 = "vhlo.sort_v1"(%218, %220) <{dimension = #vhlo.integer_v1<1 : i64>, is_stable = #vhlo.bool_v1<false>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg33: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg34: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %260 = "vhlo.compare_v1"(%arg31, %arg32) <{compare_type = #vhlo<comparison_type_v1 TOTALORDER>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x32x!vhlo.i32_v1>) -> (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x32x!vhlo.i32_v1>)
    %222 = "vhlo.slice_v1"(%221#1) <{limit_indices = #vhlo.tensor_v1<dense<[13, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<13x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.i32_v1>
    %223 = "vhlo.convert_v1"(%222) : (!vhlo.tensor_v1<13x4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.i64_v1>
    %224 = "vhlo.reshape_v1"(%223) : (!vhlo.tensor_v1<13x4x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>
    %225 = "vhlo.concatenate_v1"(%214, %224) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>, !vhlo.tensor_v1<13x4x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<13x4x2x!vhlo.i64_v1>
    %226 = "vhlo.slice_v1"(%221#0) <{limit_indices = #vhlo.tensor_v1<dense<[13, 4]> : tensor<2xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<2xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %227 = "vhlo.reduce_v1"(%226, %3) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.maximum_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x!vhlo.bf16_v1>
    %228 = "vhlo.broadcast_in_dim_v1"(%227) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %229 = "vhlo.subtract_v1"(%226, %228) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %230 = "vhlo.exponential_v2"(%229) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %231 = "vhlo.reduce_v1"(%230, %9) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x!vhlo.bf16_v1>
    %232 = "vhlo.broadcast_in_dim_v1"(%231) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %233 = "vhlo.divide_v1"(%230, %232) : (!vhlo.tensor_v1<13x4x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>
    %234 = "vhlo.scatter_v2"(%10, %225, %233) <{index_vector_dim = #vhlo.integer_v1<2 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x4x2x!vhlo.i64_v1>, !vhlo.tensor_v1<13x4x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x32x!vhlo.bf16_v1>
    %235 = "vhlo.transpose_v1"(%234) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[32,13]{0,1}">} : (!vhlo.tensor_v1<13x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x13x!vhlo.bf16_v1>
    %236 = "vhlo.reshape_v1"(%235) : (!vhlo.tensor_v1<32x13x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x1x!vhlo.bf16_v1>
    %237 = "vhlo.convert_v1"(%236) : (!vhlo.tensor_v1<32x1x13x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x1x13x1x!vhlo.f32_v1>
    %238 = "vhlo.reshape_v1"(%237) : (!vhlo.tensor_v1<32x1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x!vhlo.f32_v1>
    %239 = "vhlo.broadcast_in_dim_v1"(%238) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<32x1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>
    %240 = "vhlo.multiply_v1"(%212, %239) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>
    %241 = "vhlo.convert_v1"(%240) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>
    %242 = "vhlo.reduce_v1"(%241, %9) <{dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg32: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<32x1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %243 = "vhlo.add_v1"(%167, %242) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %244 = "vhlo.convert_v1"(%243) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %245 = "vhlo.power_v1"(%244, %14) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %246 = "vhlo.reduce_v1"(%245, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg31: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg32: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %260 = "vhlo.add_v1"(%arg31, %arg32) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %247 = "vhlo.multiply_v1"(%246, %5) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %248 = "vhlo.reshape_v1"(%247) : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %249 = "vhlo.add_v1"(%248, %27) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %250 = "vhlo.rsqrt_v2"(%249) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>
    %251 = "vhlo.reshape_v1"(%250) : (!vhlo.tensor_v1<1x13x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x!vhlo.f32_v1>
    %252 = "vhlo.broadcast_in_dim_v1"(%251) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x13x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %253 = "vhlo.multiply_v1"(%244, %252) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %254 = "vhlo.multiply_v1"(%143, %253) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>, !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>
    %255 = "vhlo.convert_v1"(%254) : (!vhlo.tensor_v1<1x13x2880x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>
    %256 = "vhlo.reshape_v1"(%255) : (!vhlo.tensor_v1<1x13x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>
    %257 = "vhlo.transpose_v1"(%arg10) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[2880,201088]{0,1}">} : (!vhlo.tensor_v1<201088x2880x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2880x201088x!vhlo.bf16_v1>
    %258 = "vhlo.dot_general_v2"(%256, %257) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<13x2880x!vhlo.bf16_v1>, !vhlo.tensor_v1<2880x201088x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>
    %259 = "vhlo.reshape_v1"(%258) : (!vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x13x201088x!vhlo.bf16_v1>
    "vhlo.return_v1"(%139, %140, %141, %100, %258, %259) : (!vhlo.tensor_v1<1x13x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x8x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x13x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<13x201088x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x13x201088x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
// -----// IR Dump Before StablehloAggressiveSimplificationPass (stablehlo-aggressive-simplification) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.reshape %arg3 : (tensor<1x13xi64>) -> tensor<13xi64>
    %9 = stablehlo.convert %8 : (tensor<13xi64>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %46 = stablehlo.multiply %34, %45 : tensor<1x64x13x32xf32>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %50 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %51 = stablehlo.multiply %50, %39 : tensor<1x13x32xf32>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
    %54 = stablehlo.convert %53 : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
    %55 = stablehlo.reshape %54 : (tensor<1x1x13x32xf32>) -> tensor<1x13x32xf32>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %57 = stablehlo.multiply %49, %56 : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.subtract %47, %58 : tensor<1x64x13x32xbf16>
    %60 = stablehlo.multiply %49, %45 : tensor<1x64x13x32xf32>
    %61 = stablehlo.convert %60 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %62 = stablehlo.multiply %34, %56 : tensor<1x64x13x32xf32>
    %63 = stablehlo.convert %62 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %64 = stablehlo.add %61, %63 : tensor<1x64x13x32xbf16>
    %65 = stablehlo.concatenate %59, %64, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %66 = stablehlo.reshape %65 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %67 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %68 = stablehlo.dot_general %25, %67, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %69 = stablehlo.reshape %68 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %70 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %71 = stablehlo.add %69, %70 : tensor<1x13x512xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %73 = stablehlo.transpose %72, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %74 = stablehlo.slice %73 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %76 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x8x13x32xf32>
    %78 = stablehlo.convert %77 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %79 = stablehlo.slice %73 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.convert %79 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %82 = stablehlo.multiply %80, %81 : tensor<1x8x13x32xf32>
    %83 = stablehlo.convert %82 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %84 = stablehlo.subtract %78, %83 : tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %80, %76 : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.multiply %75, %81 : tensor<1x8x13x32xf32>
    %88 = stablehlo.convert %87 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %89 = stablehlo.add %86, %88 : tensor<1x8x13x32xbf16>
    %90 = stablehlo.concatenate %84, %89, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %91 = stablehlo.broadcast_in_dim %90, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %92 = stablehlo.reshape %91 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %93 = stablehlo.transpose %92, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %94 = stablehlo.reshape %93 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %95 = stablehlo.dot_general %66, %94, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %96 = stablehlo.reshape %95 : (tensor<64x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.convert %96 : (tensor<1x64x13x13xbf16>) -> tensor<1x64x13x13xf32>
    %98 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %99 = stablehlo.multiply %97, %98 : tensor<1x64x13x13xf32>
    %100 = stablehlo.convert %99 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %101 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %102 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %103 = stablehlo.subtract %c, %102 : tensor<13xi64>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %105 = stablehlo.compare  GT, %101, %104 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %106 = stablehlo.convert %105 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %107 = stablehlo.and %106, %3 : tensor<13x13xui8>
    %108 = stablehlo.compare  NE, %107, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %109 = stablehlo.convert %108 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %110 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %111 = stablehlo.compare  LE, %101, %110 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %112 = stablehlo.convert %111 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %113 = stablehlo.and %109, %112 : tensor<13x13xui8>
    %114 = stablehlo.compare  NE, %113, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %115 = stablehlo.reshape %114 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %116 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %118 = stablehlo.select %115, %2, %117 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %121 = stablehlo.add %100, %120 : tensor<1x64x13x13xbf16>
    %122 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %123 = stablehlo.broadcast_in_dim %122, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %124 = stablehlo.concatenate %121, %123, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %125 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %126 = stablehlo.dot_general %25, %125, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %127 = stablehlo.reshape %126 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %129 = stablehlo.add %127, %128 : tensor<1x13x512xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %132 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %134 = stablehlo.reduce(%124 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %136 = stablehlo.subtract %124, %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.subtract %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.exponential %139 : tensor<1x64x13x14xbf16>
    %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %143 = stablehlo.divide %140, %142 : tensor<1x64x13x14xbf16>
    %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %149 = stablehlo.reshape %148 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %152 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = stablehlo.concatenate %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, %173, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %175 = stablehlo.reshape %174 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.dot_general %175, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %177 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %178 = stablehlo.add %176, %177 : tensor<32x13x5760xbf16>
    %179 = stablehlo.slice %178 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %180 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.clamp %158, %179, %180 : tensor<32x13x2880xbf16>
    %182 = stablehlo.add %181, %1 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %185 = stablehlo.slice %178 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %186 = stablehlo.clamp %184, %185, %180 : tensor<32x13x2880xbf16>
    %187 = stablehlo.convert %186 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %188 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %187, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.logistic %190 : tensor<32x13x2880xbf16>
    %192 = stablehlo.convert %191 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %193 = stablehlo.multiply %187, %192 : tensor<32x13x2880xf32>
    %194 = stablehlo.convert %193 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.convert %194 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %196 = stablehlo.multiply %183, %195 : tensor<32x13x2880xf32>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %198 = stablehlo.dot_general %197, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %199 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %200 = stablehlo.add %198, %199 : tensor<32x13x2880xbf16>
    %201 = stablehlo.reshape %200 : (tensor<32x13x2880xbf16>) -> tensor<32x1x13x2880xbf16>
    %202 = stablehlo.convert %201 : (tensor<32x1x13x2880xbf16>) -> tensor<32x1x13x2880xf32>
    %203 = stablehlo.iota dim = 0 : tensor<13xi64>
    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %205 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %206 = stablehlo.dot_general %173, %205, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %207 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %208 = stablehlo.add %206, %207 : tensor<13x32xbf16>
    %209 = stablehlo.iota dim = 0 : tensor<32xi32>
    %210 = stablehlo.broadcast_in_dim %209, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %211:2 = "stablehlo.sort"(%208, %210) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %250 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %250 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %212 = stablehlo.slice %211#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %213 = stablehlo.convert %212 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %214 = stablehlo.reshape %213 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %215 = stablehlo.concatenate %204, %214, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %216 = stablehlo.slice %211#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.subtract %216, %218 : tensor<13x4xbf16>
    %220 = stablehlo.exponential %219 : tensor<13x4xbf16>
    %221 = stablehlo.reduce(%220 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %222 = stablehlo.broadcast_in_dim %221, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %223 = stablehlo.divide %220, %222 : tensor<13x4xbf16>
    %224 = "stablehlo.scatter"(%0, %215, %223) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %225 = stablehlo.transpose %224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xbf16>) -> tensor<32x13xbf16>
    %226 = stablehlo.reshape %225 : (tensor<32x13xbf16>) -> tensor<32x1x13x1xbf16>
    %227 = stablehlo.convert %226 : (tensor<32x1x13x1xbf16>) -> tensor<32x1x13x1xf32>
    %228 = stablehlo.reshape %227 : (tensor<32x1x13x1xf32>) -> tensor<32x1x13xf32>
    %229 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %230 = stablehlo.multiply %202, %229 : tensor<32x1x13x2880xf32>
    %231 = stablehlo.convert %230 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %232 = stablehlo.reduce(%231 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %233 = stablehlo.add %157, %232 : tensor<1x13x2880xbf16>
    %234 = stablehlo.convert %233 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %235 = stablehlo.power %234, %4 : tensor<1x13x2880xf32>
    %236 = stablehlo.reduce(%235 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %237 = stablehlo.multiply %236, %cst_3 : tensor<1x13xf32>
    %238 = stablehlo.reshape %237 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %239 = stablehlo.add %238, %17 : tensor<1x13x1xf32>
    %240 = stablehlo.rsqrt %239 : tensor<1x13x1xf32>
    %241 = stablehlo.reshape %240 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %243 = stablehlo.multiply %234, %242 : tensor<1x13x2880xf32>
    %244 = stablehlo.multiply %133, %243 : tensor<1x13x2880xf32>
    %245 = stablehlo.convert %244 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %246 = stablehlo.reshape %245 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %247 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %248 = stablehlo.dot_general %246, %247, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %249 = stablehlo.reshape %248 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %129, %130, %131, %90, %248, %249 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After StablehloAggressiveSimplificationPass (stablehlo-aggressive-simplification) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before mlir::sdy::(anonymous namespace)::SdyRoundTripImportShardyAttrsPass (sdy-round-trip-import-shardy-attrs) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg1: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg2: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<1x13xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg4: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg5: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<32xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<512xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg9: tensor<512x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg10: tensor<201088x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg11: tensor<32xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg12: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg13: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg14: tensor<2880x4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8]<=[8]}"}, %arg15: tensor<64xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[8]<=[8]}"}, %arg16: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg17: tensor<i64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg18: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg19: tensor<4096xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg20: tensor<4096x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg21: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg22: tensor<32x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg23: tensor<32x2880x2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg24: tensor<f32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg25: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg26: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg27: tensor<32x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg28: tensor<32x2880x5760xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}, {}]>"}, mhlo.sharding = "{devices=[8,1,1]<=[8]}"}, %arg29: tensor<bf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, []>"}, mhlo.sharding = "{replicated}"}, %arg30: tensor<2880xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After mlir::sdy::(anonymous namespace)::SdyRoundTripImportShardyAttrsPass (sdy-round-trip-import-shardy-attrs) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before mlir::sdy::(anonymous namespace)::SdyRoundTripShardMapImportPass (sdy-round-trip-shard-map-import) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before mlir::sdy::(anonymous namespace)::ImportSdyCustomCallsPass (sdy-import-sdy-custom-calls) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before mlir::sdy::(anonymous namespace)::ImportUninlineableFuncCallsPass (xla-sdy-import-uninlineable-func-calls) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = stablehlo.broadcast_in_dim %arg19, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %30 = stablehlo.add %28, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x4096xbf16>
    %31 = stablehlo.reshape %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %32 = stablehlo.transpose %31, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %33 = stablehlo.slice %32 [0:1, 0:64, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %34 = stablehlo.convert %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %35 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %36 = stablehlo.dot_general %35, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %37 = stablehlo.transpose %36, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %38 = stablehlo.cosine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %39 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %40 = stablehlo.multiply %38, %39 : tensor<1x13x32xf32>
    %41 = stablehlo.convert %40 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %44 = stablehlo.multiply %34, %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %45 = stablehlo.convert %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %46 = stablehlo.slice %32 [0:1, 0:64, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %48 = stablehlo.sine %37 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %49 = stablehlo.multiply %48, %39 : tensor<1x13x32xf32>
    %50 = stablehlo.convert %49 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %53 = stablehlo.multiply %47, %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %54 = stablehlo.convert %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %55 = stablehlo.subtract %45, %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
    %56 = stablehlo.multiply %47, %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %57 = stablehlo.convert %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %58 = stablehlo.multiply %34, %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %59 = stablehlo.convert %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %60 = stablehlo.add %57, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
    %61 = stablehlo.concatenate %55, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %62 = stablehlo.reshape %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %63 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %64 = stablehlo.dot_general %25, %63, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %65 = stablehlo.reshape %64 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %66 = stablehlo.broadcast_in_dim %arg8, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %67 = stablehlo.add %65, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
    %68 = stablehlo.reshape %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %70 = stablehlo.slice %69 [0:1, 0:8, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %71 = stablehlo.convert %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %72 = stablehlo.broadcast_in_dim %42, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %73 = stablehlo.multiply %71, %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %74 = stablehlo.convert %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %75 = stablehlo.slice %69 [0:1, 0:8, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %76 = stablehlo.convert %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %77 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %78 = stablehlo.multiply %76, %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %80 = stablehlo.subtract %74, %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
    %81 = stablehlo.multiply %76, %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %71, %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.add %82, %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
    %86 = stablehlo.concatenate %80, %85, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %87 = stablehlo.broadcast_in_dim %86, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %88 = stablehlo.reshape %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %91 = stablehlo.dot_general %62, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %92 = stablehlo.convert %91 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %93 = stablehlo.reshape %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %94 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %95 = stablehlo.multiply %93, %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xf32>
    %96 = stablehlo.convert %95 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %98 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %99 = stablehlo.subtract %c, %98 : tensor<13xi64>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %101 = stablehlo.compare  GT, %97, %100 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %102 = stablehlo.convert %101 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %103 = stablehlo.and %102, %3 : tensor<13x13xui8>
    %104 = stablehlo.compare  NE, %103, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %106 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %107 = stablehlo.compare  LE, %97, %106 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %109 = stablehlo.and %105, %108 : tensor<13x13xui8>
    %110 = stablehlo.compare  NE, %109, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %111 = stablehlo.reshape %110 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %112 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %114 = stablehlo.select %111, %2, %113 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %115 = stablehlo.reshape %114 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %117 = stablehlo.add %96, %116 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xbf16>
    %118 = stablehlo.reshape %arg15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %120 = stablehlo.concatenate %117, %119, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %121 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %122 = stablehlo.dot_general %25, %121, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %123 = stablehlo.reshape %122 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %124 = stablehlo.broadcast_in_dim %arg0, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %125 = stablehlo.add %123, %124 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
    %126 = stablehlo.reshape %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %128 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %129 = stablehlo.broadcast_in_dim %128, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %130 = stablehlo.reduce(%120 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %132 = stablehlo.subtract %120, %131 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %133 = stablehlo.reduce(%132 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %132, %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %136 = stablehlo.exponential %135 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %137 = stablehlo.reduce(%136 init: %cst_7) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %139 = stablehlo.divide %136, %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %140 = stablehlo.slice %139 [0:1, 0:64, 0:13, 0:13] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %141 = stablehlo.reshape %140 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %142 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %143 = stablehlo.reshape %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %144 = stablehlo.dot_general %141, %143, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %145 = stablehlo.reshape %144 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %146 = stablehlo.transpose %145, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %147 = stablehlo.reshape %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %148 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %149 = stablehlo.dot_general %147, %148, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %150 = stablehlo.reshape %149 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %151 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %152 = stablehlo.add %150, %151 : tensor<1x13x2880xbf16>
    %153 = stablehlo.add %11, %152 : tensor<1x13x2880xbf16>
    %154 = stablehlo.broadcast_in_dim %arg29, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %155 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %156 = stablehlo.broadcast_in_dim %155, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %157 = stablehlo.convert %153 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %158 = stablehlo.power %157, %4 : tensor<1x13x2880xf32>
    %159 = stablehlo.reduce(%158 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %160 = stablehlo.multiply %159, %cst_3 : tensor<1x13xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %162 = stablehlo.add %161, %17 : tensor<1x13x1xf32>
    %163 = stablehlo.rsqrt %162 : tensor<1x13x1xf32>
    %164 = stablehlo.reshape %163 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %166 = stablehlo.multiply %157, %165 : tensor<1x13x2880xf32>
    %167 = stablehlo.multiply %156, %166 : tensor<1x13x2880xf32>
    %168 = stablehlo.convert %167 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %170 = stablehlo.concatenate %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, %169, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %171 = stablehlo.reshape %170 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %172 = stablehlo.dot_general %171, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %173 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %174 = stablehlo.add %172, %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x5760xbf16>
    %175 = stablehlo.slice %174 [0:32, 0:13, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %176 = stablehlo.broadcast_in_dim %arg25, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %177 = stablehlo.clamp %154, %175, %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %178 = stablehlo.add %177, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %179 = stablehlo.convert %178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %180 = stablehlo.broadcast_in_dim %arg26, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %181 = stablehlo.slice %174 [0:32, 0:13, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %182 = stablehlo.clamp %180, %181, %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %183 = stablehlo.convert %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %184 = stablehlo.broadcast_in_dim %arg24, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x13x2880xf32>
    %185 = stablehlo.multiply %183, %184 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %186 = stablehlo.convert %185 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %187 = stablehlo.logistic %186 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %188 = stablehlo.convert %187 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %189 = stablehlo.multiply %183, %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %190 = stablehlo.convert %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %192 = stablehlo.multiply %179, %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %193 = stablehlo.convert %192 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %194 = stablehlo.dot_general %193, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %195 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %196 = stablehlo.add %194, %195 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %197 = stablehlo.convert %196 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %198 = stablehlo.reshape %197 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %199 = stablehlo.iota dim = 0 : tensor<13xi64>
    %200 = stablehlo.broadcast_in_dim %199, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %201 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %202 = stablehlo.dot_general %169, %201, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %203 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %204 = stablehlo.add %202, %203 : tensor<13x32xbf16>
    %205 = stablehlo.iota dim = 0 : tensor<32xi32>
    %206 = stablehlo.broadcast_in_dim %205, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %207:2 = "stablehlo.sort"(%204, %206) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %245 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %245 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %208 = stablehlo.slice %207#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %209 = stablehlo.convert %208 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %210 = stablehlo.reshape %209 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %211 = stablehlo.concatenate %200, %210, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %212 = stablehlo.slice %207#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %213 = stablehlo.reduce(%212 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %214 = stablehlo.broadcast_in_dim %213, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %215 = stablehlo.subtract %212, %214 : tensor<13x4xbf16>
    %216 = stablehlo.exponential %215 : tensor<13x4xbf16>
    %217 = stablehlo.reduce(%216 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %218 = stablehlo.broadcast_in_dim %217, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %219 = stablehlo.divide %216, %218 : tensor<13x4xbf16>
    %220 = "stablehlo.scatter"(%0, %211, %219) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %221 = stablehlo.convert %220 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %223 = stablehlo.reshape %222 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %224 = stablehlo.broadcast_in_dim %223, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %225 = stablehlo.multiply %198, %224 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x13x2880xf32>
    %226 = stablehlo.convert %225 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %227 = stablehlo.reduce(%226 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %228 = stablehlo.add %153, %227 : tensor<1x13x2880xbf16>
    %229 = stablehlo.convert %228 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %230 = stablehlo.power %229, %4 : tensor<1x13x2880xf32>
    %231 = stablehlo.reduce(%230 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %232 = stablehlo.multiply %231, %cst_3 : tensor<1x13xf32>
    %233 = stablehlo.reshape %232 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %234 = stablehlo.add %233, %17 : tensor<1x13x1xf32>
    %235 = stablehlo.rsqrt %234 : tensor<1x13x1xf32>
    %236 = stablehlo.reshape %235 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %237 = stablehlo.broadcast_in_dim %236, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %238 = stablehlo.multiply %229, %237 : tensor<1x13x2880xf32>
    %239 = stablehlo.multiply %129, %238 : tensor<1x13x2880xf32>
    %240 = stablehlo.convert %239 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %241 = stablehlo.reshape %240 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %242 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %243 = stablehlo.dot_general %241, %242, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %244 = stablehlo.reshape %243 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %125, %126, %127, %86, %243, %244 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After InsertExplicitReshardsPass (sdy-insert-explicit-reshards) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = sdy.reshard %arg19 <@mesh, [{"_axis_0"}]> : tensor<4096xbf16>
    %30 = stablehlo.broadcast_in_dim %29, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %31 = stablehlo.add %28, %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x4096xbf16>
    %32 = stablehlo.reshape %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %33 = stablehlo.transpose %32, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %34 = stablehlo.slice %33 [0:1, 0:64, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %35 = stablehlo.convert %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %36 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %37 = stablehlo.dot_general %36, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %38 = stablehlo.transpose %37, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %39 = stablehlo.cosine %38 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %40 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<1x13x32xf32>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %44 = stablehlo.broadcast_in_dim %43, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %45 = stablehlo.multiply %35, %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %46 = stablehlo.convert %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.slice %33 [0:1, 0:64, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.convert %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %49 = stablehlo.sine %38 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %50 = stablehlo.multiply %49, %40 : tensor<1x13x32xf32>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %53 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %54 = stablehlo.multiply %48, %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %55 = stablehlo.convert %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %56 = stablehlo.subtract %46, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
    %57 = stablehlo.multiply %48, %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.multiply %35, %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %60 = stablehlo.convert %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %61 = stablehlo.add %58, %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
    %62 = stablehlo.concatenate %56, %61, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %63 = stablehlo.reshape %62 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %64 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %65 = stablehlo.dot_general %25, %64, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %66 = stablehlo.reshape %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %67 = sdy.reshard %arg8 <@mesh, [{"_axis_0"}]> : tensor<512xbf16>
    %68 = stablehlo.broadcast_in_dim %67, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %69 = stablehlo.add %66, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
    %70 = stablehlo.reshape %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %71 = stablehlo.transpose %70, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %72 = stablehlo.slice %71 [0:1, 0:8, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %73 = stablehlo.convert %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %74 = stablehlo.broadcast_in_dim %43, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %75 = stablehlo.multiply %73, %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %76 = stablehlo.convert %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %77 = stablehlo.slice %71 [0:1, 0:8, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %78 = stablehlo.convert %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %79 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %80 = stablehlo.multiply %78, %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %81 = stablehlo.convert %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %82 = stablehlo.subtract %76, %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %78, %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %73, %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.add %84, %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
    %88 = stablehlo.concatenate %82, %87, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %89 = stablehlo.broadcast_in_dim %88, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %92 = stablehlo.reshape %91 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %93 = stablehlo.dot_general %63, %92, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %94 = stablehlo.convert %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %95 = stablehlo.reshape %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %96 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %97 = stablehlo.multiply %95, %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xf32>
    %98 = stablehlo.convert %97 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %99 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %100 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %101 = stablehlo.subtract %c, %100 : tensor<13xi64>
    %102 = stablehlo.broadcast_in_dim %101, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %103 = stablehlo.compare  GT, %99, %102 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %104 = stablehlo.convert %103 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %105 = stablehlo.and %104, %3 : tensor<13x13xui8>
    %106 = stablehlo.compare  NE, %105, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %107 = stablehlo.convert %106 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %108 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %109 = stablehlo.compare  LE, %99, %108 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %110 = stablehlo.convert %109 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %111 = stablehlo.and %107, %110 : tensor<13x13xui8>
    %112 = stablehlo.compare  NE, %111, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %113 = stablehlo.reshape %112 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %114 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %115 = stablehlo.broadcast_in_dim %114, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %116 = stablehlo.select %113, %2, %115 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %117 = stablehlo.reshape %116 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %118 = stablehlo.broadcast_in_dim %117, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %119 = stablehlo.add %98, %118 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xbf16>
    %120 = stablehlo.reshape %arg15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %121 = stablehlo.broadcast_in_dim %120, dims = [0, 1, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %122 = stablehlo.concatenate %119, %121, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %123 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %124 = stablehlo.dot_general %25, %123, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %125 = stablehlo.reshape %124 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %126 = sdy.reshard %arg0 <@mesh, [{"_axis_0"}]> : tensor<512xbf16>
    %127 = stablehlo.broadcast_in_dim %126, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.add %125, %127 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
    %129 = stablehlo.reshape %128 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %130 = stablehlo.transpose %129, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %131 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %132 = stablehlo.broadcast_in_dim %131, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %133 = stablehlo.reduce(%122 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %122, %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %136 = stablehlo.reduce(%135 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %138 = stablehlo.subtract %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %139 = stablehlo.exponential %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_7) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %141 = stablehlo.broadcast_in_dim %140, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %142 = stablehlo.divide %139, %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %143 = stablehlo.slice %142 [0:1, 0:64, 0:13, 0:13] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %145 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %147 = stablehlo.dot_general %144, %146, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %149 = stablehlo.transpose %148, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %150 = stablehlo.reshape %149 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %151 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %152 = stablehlo.dot_general %150, %151, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %153 = sdy.all_reduce {"_axis_0"} %152 out_sharding=<@mesh, [{}, {}]> : tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %175 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %176 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %177 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %178 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %179 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %180 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %181 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %182 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %183 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %184 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %185 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %186 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %187 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %188 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %189 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %190 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %191 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %192 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %193 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %194 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %195 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %196 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %197 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %198 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %199 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %200 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %201 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %202 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %203 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %204 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %205 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %206 = stablehlo.concatenate %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196, %197, %198, %199, %200, %201, %202, %203, %204, %205, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %207 = stablehlo.reshape %206 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %208 = stablehlo.dot_general %207, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %209 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %210 = stablehlo.add %208, %209 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x5760xbf16>
    %211 = stablehlo.slice %210 [0:32, 0:13, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %212 = stablehlo.broadcast_in_dim %arg25, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %213 = stablehlo.clamp %158, %211, %212 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %214 = stablehlo.add %213, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %215 = stablehlo.convert %214 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %216 = stablehlo.broadcast_in_dim %arg26, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %217 = stablehlo.slice %210 [0:32, 0:13, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %218 = stablehlo.clamp %216, %217, %212 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %219 = stablehlo.convert %218 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %220 = stablehlo.broadcast_in_dim %arg24, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x13x2880xf32>
    %221 = stablehlo.multiply %219, %220 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %222 = stablehlo.convert %221 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %223 = stablehlo.logistic %222 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %224 = stablehlo.convert %223 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %225 = stablehlo.multiply %219, %224 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %226 = stablehlo.convert %225 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %227 = stablehlo.convert %226 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %228 = stablehlo.multiply %215, %227 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %229 = stablehlo.convert %228 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %230 = stablehlo.dot_general %229, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %231 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %232 = stablehlo.add %230, %231 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %233 = stablehlo.convert %232 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %234 = stablehlo.reshape %233 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %235 = stablehlo.iota dim = 0 : tensor<13xi64>
    %236 = stablehlo.broadcast_in_dim %235, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %237 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %238 = stablehlo.dot_general %173, %237, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %239 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %240 = stablehlo.add %238, %239 : tensor<13x32xbf16>
    %241 = stablehlo.iota dim = 0 : tensor<32xi32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %243:2 = "stablehlo.sort"(%240, %242) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %282 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %282 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %244 = stablehlo.slice %243#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %245 = stablehlo.convert %244 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %246 = stablehlo.reshape %245 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %247 = stablehlo.concatenate %236, %246, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %248 = stablehlo.slice %243#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %249 = stablehlo.reduce(%248 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %250 = stablehlo.broadcast_in_dim %249, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %251 = stablehlo.subtract %248, %250 : tensor<13x4xbf16>
    %252 = stablehlo.exponential %251 : tensor<13x4xbf16>
    %253 = stablehlo.reduce(%252 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %254 = stablehlo.broadcast_in_dim %253, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %255 = stablehlo.divide %252, %254 : tensor<13x4xbf16>
    %256 = "stablehlo.scatter"(%0, %247, %255) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %257 = stablehlo.convert %256 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %258 = stablehlo.transpose %257, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %259 = stablehlo.reshape %258 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %260 = stablehlo.broadcast_in_dim %259, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %261 = stablehlo.multiply %234, %260 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x13x2880xf32>
    %262 = stablehlo.convert %261 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %263 = stablehlo.reduce(%262 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %264 = sdy.all_reduce {"_axis_0"} %263 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x13x2880xbf16>
    %265 = stablehlo.add %157, %264 : tensor<1x13x2880xbf16>
    %266 = stablehlo.convert %265 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %267 = stablehlo.power %266, %4 : tensor<1x13x2880xf32>
    %268 = stablehlo.reduce(%267 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %269 = stablehlo.multiply %268, %cst_3 : tensor<1x13xf32>
    %270 = stablehlo.reshape %269 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %271 = stablehlo.add %270, %17 : tensor<1x13x1xf32>
    %272 = stablehlo.rsqrt %271 : tensor<1x13x1xf32>
    %273 = stablehlo.reshape %272 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %274 = stablehlo.broadcast_in_dim %273, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %275 = stablehlo.multiply %266, %274 : tensor<1x13x2880xf32>
    %276 = stablehlo.multiply %132, %275 : tensor<1x13x2880xf32>
    %277 = stablehlo.convert %276 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %278 = stablehlo.reshape %277 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %279 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %280 = stablehlo.dot_general %278, %279, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %281 = stablehlo.reshape %280 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %128, %129, %130, %88, %280, %281 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
    %c_0 = stablehlo.constant dense<0> : tensor<ui8>
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
    %c_5 = stablehlo.constant dense<1> : tensor<ui8>
    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<13x32xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
    %3 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
    %5 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
    %6 = stablehlo.convert %arg5 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %8 = stablehlo.convert %arg3 : (tensor<1x13xi64>) -> tensor<1x13xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x13xui32>) -> tensor<13xui32>
    %10 = "stablehlo.gather"(%arg4, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
    %11 = stablehlo.reshape %10 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %13 = stablehlo.power %12, %4 : tensor<1x13x2880xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %15 = stablehlo.multiply %14, %cst_3 : tensor<1x13xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x13x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x13x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x13x2880xf32>
    %23 = stablehlo.multiply %7, %22 : tensor<1x13x2880xf32>
    %24 = stablehlo.convert %23 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %26 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
    %27 = stablehlo.dot_general %25, %26, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
    %28 = stablehlo.reshape %27 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
    %29 = sdy.reshard %arg19 <@mesh, [{"_axis_0"}]> : tensor<4096xbf16>
    %30 = stablehlo.broadcast_in_dim %29, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
    %31 = stablehlo.add %28, %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x4096xbf16>
    %32 = stablehlo.reshape %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
    %33 = stablehlo.transpose %32, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
    %34 = stablehlo.slice %33 [0:1, 0:64, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %35 = stablehlo.convert %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %36 = stablehlo.reshape %arg7 : (tensor<32xf32>) -> tensor<1x32x1xf32>
    %37 = stablehlo.dot_general %36, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
    %38 = stablehlo.transpose %37, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
    %39 = stablehlo.cosine %38 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %40 = stablehlo.broadcast_in_dim %arg6, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
    %41 = stablehlo.multiply %39, %40 : tensor<1x13x32xf32>
    %42 = stablehlo.convert %41 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %43 = stablehlo.convert %42 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %44 = stablehlo.broadcast_in_dim %43, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %45 = stablehlo.multiply %35, %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %46 = stablehlo.convert %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %47 = stablehlo.slice %33 [0:1, 0:64, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
    %48 = stablehlo.convert %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
    %49 = stablehlo.sine %38 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
    %50 = stablehlo.multiply %49, %40 : tensor<1x13x32xf32>
    %51 = stablehlo.convert %50 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
    %52 = stablehlo.convert %51 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
    %53 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
    %54 = stablehlo.multiply %48, %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %55 = stablehlo.convert %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %56 = stablehlo.subtract %46, %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
    %57 = stablehlo.multiply %48, %44 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %58 = stablehlo.convert %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %59 = stablehlo.multiply %35, %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
    %60 = stablehlo.convert %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
    %61 = stablehlo.add %58, %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
    %62 = stablehlo.concatenate %56, %61, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
    %63 = stablehlo.reshape %62 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %64 = stablehlo.transpose %arg9, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %65 = stablehlo.dot_general %25, %64, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %66 = stablehlo.reshape %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %67 = sdy.reshard %arg8 <@mesh, [{"_axis_0"}]> : tensor<512xbf16>
    %68 = stablehlo.broadcast_in_dim %67, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %69 = stablehlo.add %66, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
    %70 = stablehlo.reshape %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %71 = stablehlo.transpose %70, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %72 = stablehlo.slice %71 [0:1, 0:8, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %73 = stablehlo.convert %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %74 = stablehlo.broadcast_in_dim %43, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %75 = stablehlo.multiply %73, %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %76 = stablehlo.convert %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %77 = stablehlo.slice %71 [0:1, 0:8, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
    %78 = stablehlo.convert %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
    %79 = stablehlo.broadcast_in_dim %52, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
    %80 = stablehlo.multiply %78, %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %81 = stablehlo.convert %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %82 = stablehlo.subtract %76, %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
    %83 = stablehlo.multiply %78, %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %84 = stablehlo.convert %83 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %85 = stablehlo.multiply %73, %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
    %86 = stablehlo.convert %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
    %87 = stablehlo.add %84, %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
    %88 = stablehlo.concatenate %82, %87, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
    %89 = stablehlo.broadcast_in_dim %88, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %91 = stablehlo.transpose %90, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
    %92 = stablehlo.reshape %91 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
    %93 = stablehlo.dot_general %63, %92, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
    %94 = stablehlo.convert %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
    %95 = stablehlo.reshape %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
    %96 = stablehlo.broadcast_in_dim %arg18, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x64x13x13xf32>
    %97 = stablehlo.multiply %95, %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xf32>
    %98 = stablehlo.convert %97 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
    %99 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
    %100 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<i64>) -> tensor<13xi64>
    %101 = stablehlo.subtract %c, %100 : tensor<13xi64>
    %102 = stablehlo.broadcast_in_dim %101, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %103 = stablehlo.compare  GT, %99, %102 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %104 = stablehlo.convert %103 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %105 = stablehlo.and %104, %3 : tensor<13x13xui8>
    %106 = stablehlo.compare  NE, %105, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %107 = stablehlo.convert %106 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %108 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
    %109 = stablehlo.compare  LE, %99, %108 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
    %110 = stablehlo.convert %109 : (tensor<13x13xi1>) -> tensor<13x13xui8>
    %111 = stablehlo.and %107, %110 : tensor<13x13xui8>
    %112 = stablehlo.compare  NE, %111, %5 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
    %113 = stablehlo.reshape %112 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
    %114 = stablehlo.reshape %arg16 : (tensor<bf16>) -> tensor<1x1xbf16>
    %115 = stablehlo.broadcast_in_dim %114, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
    %116 = stablehlo.select %113, %2, %115 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
    %117 = stablehlo.reshape %116 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
    %118 = stablehlo.broadcast_in_dim %117, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
    %119 = stablehlo.add %98, %118 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xbf16>
    %120 = stablehlo.reshape %arg15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
    %121 = stablehlo.broadcast_in_dim %120, dims = [0, 1, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
    %122 = stablehlo.concatenate %119, %121, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
    %123 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
    %124 = stablehlo.dot_general %25, %123, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
    %125 = stablehlo.reshape %124 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
    %126 = sdy.reshard %arg0 <@mesh, [{"_axis_0"}]> : tensor<512xbf16>
    %127 = stablehlo.broadcast_in_dim %126, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
    %128 = stablehlo.add %125, %127 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
    %129 = stablehlo.reshape %128 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
    %130 = stablehlo.transpose %129, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
    %131 = stablehlo.convert %arg30 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %132 = stablehlo.broadcast_in_dim %131, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %133 = stablehlo.reduce(%122 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %135 = stablehlo.subtract %122, %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %136 = stablehlo.reduce(%135 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %138 = stablehlo.subtract %135, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %139 = stablehlo.exponential %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_7) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
    %141 = stablehlo.broadcast_in_dim %140, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
    %142 = stablehlo.divide %139, %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
    %143 = stablehlo.slice %142 [0:1, 0:64, 0:13, 0:13] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
    %145 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
    %146 = stablehlo.reshape %145 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
    %147 = stablehlo.dot_general %144, %146, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
    %148 = stablehlo.reshape %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
    %149 = stablehlo.transpose %148, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
    %150 = stablehlo.reshape %149 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
    %151 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
    %152 = stablehlo.dot_general %150, %151, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
    %153 = sdy.all_reduce {"_axis_0"} %152 out_sharding=<@mesh, [{}, {}]> : tensor<13x2880xbf16>
    %154 = stablehlo.reshape %153 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
    %155 = stablehlo.broadcast_in_dim %arg13, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
    %156 = stablehlo.add %154, %155 : tensor<1x13x2880xbf16>
    %157 = stablehlo.add %11, %156 : tensor<1x13x2880xbf16>
    %158 = stablehlo.broadcast_in_dim %arg29, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %159 = stablehlo.convert %arg21 : (tensor<2880xbf16>) -> tensor<2880xf32>
    %160 = stablehlo.broadcast_in_dim %159, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
    %161 = stablehlo.convert %157 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %162 = stablehlo.power %161, %4 : tensor<1x13x2880xf32>
    %163 = stablehlo.reduce(%162 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %164 = stablehlo.multiply %163, %cst_3 : tensor<1x13xf32>
    %165 = stablehlo.reshape %164 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %166 = stablehlo.add %165, %17 : tensor<1x13x1xf32>
    %167 = stablehlo.rsqrt %166 : tensor<1x13x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %170 = stablehlo.multiply %161, %169 : tensor<1x13x2880xf32>
    %171 = stablehlo.multiply %160, %170 : tensor<1x13x2880xf32>
    %172 = stablehlo.convert %171 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %174 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %175 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %176 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %177 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %178 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %179 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %180 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %181 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %182 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %183 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %184 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %185 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %186 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %187 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %188 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %189 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %190 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %191 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %192 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %193 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %194 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %195 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %196 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %197 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %198 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %199 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %200 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %201 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %202 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %203 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %204 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %205 = sdy.reshard %173 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
    %206 = stablehlo.concatenate %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196, %197, %198, %199, %200, %201, %202, %203, %204, %205, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
    %207 = stablehlo.reshape %206 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
    %208 = stablehlo.dot_general %207, %arg28, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
    %209 = stablehlo.broadcast_in_dim %arg27, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
    %210 = stablehlo.add %208, %209 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x5760xbf16>
    %211 = stablehlo.slice %210 [0:32, 0:13, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %212 = stablehlo.broadcast_in_dim %arg25, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %213 = stablehlo.clamp %158, %211, %212 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %214 = stablehlo.add %213, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %215 = stablehlo.convert %214 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %216 = stablehlo.broadcast_in_dim %arg26, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
    %217 = stablehlo.slice %210 [0:32, 0:13, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
    %218 = stablehlo.clamp %216, %217, %212 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %219 = stablehlo.convert %218 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %220 = stablehlo.broadcast_in_dim %arg24, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x13x2880xf32>
    %221 = stablehlo.multiply %219, %220 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %222 = stablehlo.convert %221 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %223 = stablehlo.logistic %222 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %224 = stablehlo.convert %223 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %225 = stablehlo.multiply %219, %224 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %226 = stablehlo.convert %225 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %227 = stablehlo.convert %226 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %228 = stablehlo.multiply %215, %227 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
    %229 = stablehlo.convert %228 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
    %230 = stablehlo.dot_general %229, %arg23, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
    %231 = stablehlo.broadcast_in_dim %arg22, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
    %232 = stablehlo.add %230, %231 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
    %233 = stablehlo.convert %232 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
    %234 = stablehlo.reshape %233 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
    %235 = stablehlo.iota dim = 0 : tensor<13xi64>
    %236 = stablehlo.broadcast_in_dim %235, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
    %237 = stablehlo.transpose %arg12, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
    %238 = stablehlo.dot_general %173, %237, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
    %239 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
    %240 = stablehlo.add %238, %239 : tensor<13x32xbf16>
    %241 = stablehlo.iota dim = 0 : tensor<32xi32>
    %242 = stablehlo.broadcast_in_dim %241, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
    %243:2 = "stablehlo.sort"(%240, %242) <{dimension = 1 : i64}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>, %arg33: tensor<i32>, %arg34: tensor<i32>):
      %282 = stablehlo.compare  GT, %arg31, %arg32,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
      stablehlo.return %282 : tensor<i1>
    }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
    %244 = stablehlo.slice %243#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
    %245 = stablehlo.convert %244 : (tensor<13x4xi32>) -> tensor<13x4xi64>
    %246 = stablehlo.reshape %245 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
    %247 = stablehlo.concatenate %236, %246, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
    %248 = stablehlo.slice %243#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
    %249 = stablehlo.reduce(%248 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %250 = stablehlo.broadcast_in_dim %249, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %251 = stablehlo.subtract %248, %250 : tensor<13x4xbf16>
    %252 = stablehlo.exponential %251 : tensor<13x4xbf16>
    %253 = stablehlo.reduce(%252 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
    %254 = stablehlo.broadcast_in_dim %253, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
    %255 = stablehlo.divide %252, %254 : tensor<13x4xbf16>
    %256 = "stablehlo.scatter"(%0, %247, %255) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      stablehlo.return %arg32 : tensor<bf16>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
    %257 = stablehlo.convert %256 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>) -> tensor<13x32xf32>
    %258 = stablehlo.transpose %257, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
    %259 = stablehlo.reshape %258 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
    %260 = stablehlo.broadcast_in_dim %259, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
    %261 = stablehlo.multiply %234, %260 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x13x2880xf32>
    %262 = stablehlo.convert %261 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
    %263 = stablehlo.reduce(%262 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
    %264 = sdy.all_reduce {"_axis_0"} %263 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x13x2880xbf16>
    %265 = stablehlo.add %157, %264 : tensor<1x13x2880xbf16>
    %266 = stablehlo.convert %265 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
    %267 = stablehlo.power %266, %4 : tensor<1x13x2880xf32>
    %268 = stablehlo.reduce(%267 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
    %269 = stablehlo.multiply %268, %cst_3 : tensor<1x13xf32>
    %270 = stablehlo.reshape %269 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
    %271 = stablehlo.add %270, %17 : tensor<1x13x1xf32>
    %272 = stablehlo.rsqrt %271 : tensor<1x13x1xf32>
    %273 = stablehlo.reshape %272 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
    %274 = stablehlo.broadcast_in_dim %273, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
    %275 = stablehlo.multiply %266, %274 : tensor<1x13x2880xf32>
    %276 = stablehlo.multiply %132, %275 : tensor<1x13x2880xf32>
    %277 = stablehlo.convert %276 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
    %278 = stablehlo.reshape %277 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
    %279 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
    %280 = stablehlo.dot_general %278, %279, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
    %281 = stablehlo.reshape %280 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
    return %128, %129, %130, %88, %280, %281 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:6 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg25, %arg26, %arg27, %arg28, %arg29, %arg30) in_shardings=[<@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, []>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, [{}]>] out_shardings=[<@mesh, [{?}, {?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {"_axis_0", ?}]>] manual_axes={} (%arg31: tensor<512xbf16>, %arg32: tensor<512x2880xbf16>, %arg33: tensor<f32>, %arg34: tensor<1x13xi64>, %arg35: tensor<201088x2880xbf16>, %arg36: tensor<2880xbf16>, %arg37: tensor<f32>, %arg38: tensor<32xf32>, %arg39: tensor<512xbf16>, %arg40: tensor<512x2880xbf16>, %arg41: tensor<201088x2880xbf16>, %arg42: tensor<32xbf16>, %arg43: tensor<32x2880xbf16>, %arg44: tensor<2880xbf16>, %arg45: tensor<2880x4096xbf16>, %arg46: tensor<64xbf16>, %arg47: tensor<bf16>, %arg48: tensor<i64>, %arg49: tensor<f32>, %arg50: tensor<4096xbf16>, %arg51: tensor<4096x2880xbf16>, %arg52: tensor<2880xbf16>, %arg53: tensor<32x2880xbf16>, %arg54: tensor<32x2880x2880xbf16>, %arg55: tensor<f32>, %arg56: tensor<bf16>, %arg57: tensor<bf16>, %arg58: tensor<32x5760xbf16>, %arg59: tensor<32x2880x5760xbf16>, %arg60: tensor<bf16>, %arg61: tensor<2880xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
      %c_0 = stablehlo.constant dense<0> : tensor<ui8>
      %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
      %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
      %c_5 = stablehlo.constant dense<1> : tensor<ui8>
      %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<13x32xbf16>
      %2 = stablehlo.broadcast_in_dim %cst_6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %3 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
      %4 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
      %5 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
      %6 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
      %7 = stablehlo.convert %arg36 : (tensor<2880xbf16>) -> tensor<2880xf32>
      %8 = stablehlo.broadcast_in_dim %7, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %9 = stablehlo.convert %arg34 : (tensor<1x13xi64>) -> tensor<1x13xui32>
      %10 = stablehlo.reshape %9 : (tensor<1x13xui32>) -> tensor<13xui32>
      %11 = "stablehlo.gather"(%arg35, %10) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
      %12 = stablehlo.reshape %11 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %13 = stablehlo.convert %12 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %14 = stablehlo.power %13, %5 : tensor<1x13x2880xf32>
      %15 = stablehlo.reduce(%14 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %16 = stablehlo.multiply %15, %cst_3 : tensor<1x13xf32>
      %17 = stablehlo.reshape %16 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %18 = stablehlo.broadcast_in_dim %arg33, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
      %19 = stablehlo.add %17, %18 : tensor<1x13x1xf32>
      %20 = stablehlo.rsqrt %19 : tensor<1x13x1xf32>
      %21 = stablehlo.reshape %20 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %23 = stablehlo.multiply %13, %22 : tensor<1x13x2880xf32>
      %24 = stablehlo.multiply %8, %23 : tensor<1x13x2880xf32>
      %25 = stablehlo.convert %24 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %26 = stablehlo.reshape %25 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %27 = stablehlo.transpose %arg51, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
      %28 = stablehlo.dot_general %26, %27, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
      %29 = stablehlo.reshape %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
      %30 = sdy.reshard %arg50 <@mesh, [{"_axis_0"}]> : tensor<4096xbf16>
      %31 = stablehlo.broadcast_in_dim %30, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
      %32 = stablehlo.add %29, %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x4096xbf16>
      %33 = stablehlo.reshape %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
      %34 = stablehlo.transpose %33, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
      %35 = stablehlo.slice %34 [0:1, 0:64, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
      %36 = stablehlo.convert %35 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
      %37 = stablehlo.reshape %arg38 : (tensor<32xf32>) -> tensor<1x32x1xf32>
      %38 = stablehlo.dot_general %37, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
      %39 = stablehlo.transpose %38, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
      %40 = stablehlo.cosine %39 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
      %41 = stablehlo.broadcast_in_dim %arg37, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
      %42 = stablehlo.multiply %40, %41 : tensor<1x13x32xf32>
      %43 = stablehlo.convert %42 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
      %44 = stablehlo.convert %43 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
      %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
      %46 = stablehlo.multiply %36, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %48 = stablehlo.slice %34 [0:1, 0:64, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
      %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
      %50 = stablehlo.sine %39 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
      %51 = stablehlo.multiply %50, %41 : tensor<1x13x32xf32>
      %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
      %53 = stablehlo.convert %52 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
      %54 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
      %55 = stablehlo.multiply %49, %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %56 = stablehlo.convert %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %57 = stablehlo.subtract %47, %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
      %58 = stablehlo.multiply %49, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %59 = stablehlo.convert %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %60 = stablehlo.multiply %36, %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %61 = stablehlo.convert %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %62 = stablehlo.add %59, %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
      %63 = stablehlo.concatenate %57, %62, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
      %64 = stablehlo.reshape %63 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
      %65 = stablehlo.transpose %arg40, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
      %66 = stablehlo.dot_general %26, %65, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
      %67 = stablehlo.reshape %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
      %68 = sdy.reshard %arg39 <@mesh, [{"_axis_0"}]> : tensor<512xbf16>
      %69 = stablehlo.broadcast_in_dim %68, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
      %70 = stablehlo.add %67, %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
      %71 = stablehlo.reshape %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
      %72 = stablehlo.transpose %71, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:8, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
      %74 = stablehlo.convert %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
      %75 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
      %76 = stablehlo.multiply %74, %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %77 = stablehlo.convert %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %78 = stablehlo.slice %72 [0:1, 0:8, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
      %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
      %80 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
      %81 = stablehlo.multiply %79, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %83 = stablehlo.subtract %77, %82 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
      %84 = stablehlo.multiply %79, %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %85 = stablehlo.convert %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %86 = stablehlo.multiply %74, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %87 = stablehlo.convert %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %88 = stablehlo.add %85, %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
      %89 = stablehlo.concatenate %83, %88, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
      %90 = stablehlo.broadcast_in_dim %89, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
      %91 = stablehlo.reshape %90 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
      %92 = stablehlo.transpose %91, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
      %93 = stablehlo.reshape %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
      %94 = stablehlo.dot_general %64, %93, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
      %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
      %96 = stablehlo.reshape %95 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
      %97 = stablehlo.broadcast_in_dim %arg49, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x64x13x13xf32>
      %98 = stablehlo.multiply %96, %97 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xf32>
      %99 = stablehlo.convert %98 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
      %100 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
      %101 = stablehlo.broadcast_in_dim %arg48, dims = [] : (tensor<i64>) -> tensor<13xi64>
      %102 = stablehlo.subtract %c, %101 : tensor<13xi64>
      %103 = stablehlo.broadcast_in_dim %102, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
      %104 = stablehlo.compare  GT, %100, %103 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
      %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %106 = stablehlo.and %105, %4 : tensor<13x13xui8>
      %107 = stablehlo.compare  NE, %106, %6 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
      %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %109 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
      %110 = stablehlo.compare  LE, %100, %109 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
      %111 = stablehlo.convert %110 : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %112 = stablehlo.and %108, %111 : tensor<13x13xui8>
      %113 = stablehlo.compare  NE, %112, %6 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
      %114 = stablehlo.reshape %113 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
      %115 = stablehlo.reshape %arg47 : (tensor<bf16>) -> tensor<1x1xbf16>
      %116 = stablehlo.broadcast_in_dim %115, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
      %117 = stablehlo.select %114, %3, %116 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
      %119 = stablehlo.broadcast_in_dim %118, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
      %120 = stablehlo.add %99, %119 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xbf16>
      %121 = stablehlo.reshape %arg46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
      %122 = stablehlo.broadcast_in_dim %121, dims = [0, 1, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
      %123 = stablehlo.concatenate %120, %122, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
      %124 = stablehlo.transpose %arg32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
      %125 = stablehlo.dot_general %26, %124, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
      %126 = stablehlo.reshape %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
      %127 = sdy.reshard %arg31 <@mesh, [{"_axis_0"}]> : tensor<512xbf16>
      %128 = stablehlo.broadcast_in_dim %127, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
      %129 = stablehlo.add %126, %128 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
      %130 = stablehlo.reshape %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
      %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
      %132 = stablehlo.convert %arg61 : (tensor<2880xbf16>) -> tensor<2880xf32>
      %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %134 = stablehlo.reduce(%123 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
      %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
      %136 = stablehlo.subtract %123, %135 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
      %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
      %139 = stablehlo.subtract %136, %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %140 = stablehlo.exponential %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
      %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
      %143 = stablehlo.divide %140, %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
      %145 = stablehlo.reshape %144 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
      %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
      %147 = stablehlo.reshape %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
      %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
      %149 = stablehlo.reshape %148 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
      %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
      %151 = stablehlo.reshape %150 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
      %152 = stablehlo.transpose %arg45, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
      %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
      %154 = sdy.all_reduce {"_axis_0"} %153 out_sharding=<@mesh, [{}, {}]> : tensor<13x2880xbf16>
      %155 = stablehlo.reshape %154 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %156 = stablehlo.broadcast_in_dim %arg44, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
      %157 = stablehlo.add %155, %156 : tensor<1x13x2880xbf16>
      %158 = stablehlo.add %12, %157 : tensor<1x13x2880xbf16>
      %159 = stablehlo.broadcast_in_dim %arg60, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %160 = stablehlo.convert %arg52 : (tensor<2880xbf16>) -> tensor<2880xf32>
      %161 = stablehlo.broadcast_in_dim %160, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %162 = stablehlo.convert %158 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %163 = stablehlo.power %162, %5 : tensor<1x13x2880xf32>
      %164 = stablehlo.reduce(%163 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %165 = stablehlo.multiply %164, %cst_3 : tensor<1x13xf32>
      %166 = stablehlo.reshape %165 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %167 = stablehlo.add %166, %18 : tensor<1x13x1xf32>
      %168 = stablehlo.rsqrt %167 : tensor<1x13x1xf32>
      %169 = stablehlo.reshape %168 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %170 = stablehlo.broadcast_in_dim %169, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %171 = stablehlo.multiply %162, %170 : tensor<1x13x2880xf32>
      %172 = stablehlo.multiply %161, %171 : tensor<1x13x2880xf32>
      %173 = stablehlo.convert %172 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %174 = stablehlo.reshape %173 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %175 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %176 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %177 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %178 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %179 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %180 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %181 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %182 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %183 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %184 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %185 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %186 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %187 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %188 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %189 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %190 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %191 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %192 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %193 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %194 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %195 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %196 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %197 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %198 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %199 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %200 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %201 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %202 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %203 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %204 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %205 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %206 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %207 = stablehlo.concatenate %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196, %197, %198, %199, %200, %201, %202, %203, %204, %205, %206, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
      %208 = stablehlo.reshape %207 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
      %209 = stablehlo.dot_general %208, %arg59, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
      %210 = stablehlo.broadcast_in_dim %arg58, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
      %211 = stablehlo.add %209, %210 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x5760xbf16>
      %212 = stablehlo.slice %211 [0:32, 0:13, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
      %213 = stablehlo.broadcast_in_dim %arg56, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %214 = stablehlo.clamp %159, %212, %213 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %215 = stablehlo.add %214, %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %216 = stablehlo.convert %215 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %217 = stablehlo.broadcast_in_dim %arg57, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %218 = stablehlo.slice %211 [0:32, 0:13, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
      %219 = stablehlo.clamp %217, %218, %213 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %220 = stablehlo.convert %219 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %221 = stablehlo.broadcast_in_dim %arg55, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x13x2880xf32>
      %222 = stablehlo.multiply %220, %221 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
      %223 = stablehlo.convert %222 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
      %224 = stablehlo.logistic %223 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %225 = stablehlo.convert %224 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %226 = stablehlo.multiply %220, %225 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
      %227 = stablehlo.convert %226 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
      %228 = stablehlo.convert %227 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %229 = stablehlo.multiply %216, %228 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
      %230 = stablehlo.convert %229 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
      %231 = stablehlo.dot_general %230, %arg54, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
      %232 = stablehlo.broadcast_in_dim %arg53, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
      %233 = stablehlo.add %231, %232 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %234 = stablehlo.convert %233 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %235 = stablehlo.reshape %234 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
      %236 = stablehlo.iota dim = 0 : tensor<13xi64>
      %237 = stablehlo.broadcast_in_dim %236, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
      %238 = stablehlo.transpose %arg43, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %239 = stablehlo.dot_general %174, %238, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
      %240 = stablehlo.broadcast_in_dim %arg42, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
      %241 = stablehlo.add %239, %240 : tensor<13x32xbf16>
      %242 = stablehlo.iota dim = 0 : tensor<32xi32>
      %243 = stablehlo.broadcast_in_dim %242, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
      %244:2 = "stablehlo.sort"(%241, %243) <{dimension = 1 : i64}> ({
      ^bb0(%arg62: tensor<bf16>, %arg63: tensor<bf16>, %arg64: tensor<i32>, %arg65: tensor<i32>):
        %283 = stablehlo.compare  GT, %arg62, %arg63,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %283 : tensor<i1>
      }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
      %245 = stablehlo.slice %244#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
      %246 = stablehlo.convert %245 : (tensor<13x4xi32>) -> tensor<13x4xi64>
      %247 = stablehlo.reshape %246 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
      %248 = stablehlo.concatenate %237, %247, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
      %249 = stablehlo.slice %244#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
      %250 = stablehlo.reduce(%249 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
      %251 = stablehlo.broadcast_in_dim %250, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
      %252 = stablehlo.subtract %249, %251 : tensor<13x4xbf16>
      %253 = stablehlo.exponential %252 : tensor<13x4xbf16>
      %254 = stablehlo.reduce(%253 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
      %255 = stablehlo.broadcast_in_dim %254, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
      %256 = stablehlo.divide %253, %255 : tensor<13x4xbf16>
      %257 = "stablehlo.scatter"(%1, %248, %256) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg62: tensor<bf16>, %arg63: tensor<bf16>):
        stablehlo.return %arg63 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
      %258 = stablehlo.convert %257 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>) -> tensor<13x32xf32>
      %259 = stablehlo.transpose %258, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
      %260 = stablehlo.reshape %259 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
      %261 = stablehlo.broadcast_in_dim %260, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
      %262 = stablehlo.multiply %235, %261 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x13x2880xf32>
      %263 = stablehlo.convert %262 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
      %264 = stablehlo.reduce(%263 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
      %265 = sdy.all_reduce {"_axis_0"} %264 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x13x2880xbf16>
      %266 = stablehlo.add %158, %265 : tensor<1x13x2880xbf16>
      %267 = stablehlo.convert %266 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %268 = stablehlo.power %267, %5 : tensor<1x13x2880xf32>
      %269 = stablehlo.reduce(%268 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %270 = stablehlo.multiply %269, %cst_3 : tensor<1x13xf32>
      %271 = stablehlo.reshape %270 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %272 = stablehlo.add %271, %18 : tensor<1x13x1xf32>
      %273 = stablehlo.rsqrt %272 : tensor<1x13x1xf32>
      %274 = stablehlo.reshape %273 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %275 = stablehlo.broadcast_in_dim %274, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %276 = stablehlo.multiply %267, %275 : tensor<1x13x2880xf32>
      %277 = stablehlo.multiply %133, %276 : tensor<1x13x2880xf32>
      %278 = stablehlo.convert %277 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %279 = stablehlo.reshape %278 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %280 = stablehlo.transpose %arg41, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
      %281 = stablehlo.dot_general %279, %280, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
      %282 = stablehlo.reshape %281 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
      sdy.return %129, %130, %131, %89, %281, %282 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
    } : (tensor<512xbf16>, tensor<512x2880xbf16>, tensor<f32>, tensor<1x13xi64>, tensor<201088x2880xbf16>, tensor<2880xbf16>, tensor<f32>, tensor<32xf32>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<201088x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<bf16>, tensor<i64>, tensor<f32>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>, tensor<2880xbf16>) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4, %0#5 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:6 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg25, %arg26, %arg27, %arg28, %arg29, %arg30) in_shardings=[<@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, []>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, [{}]>] out_shardings=[<@mesh, [{?}, {?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {"_axis_0", ?}]>] manual_axes={} (%arg31: tensor<512xbf16>, %arg32: tensor<512x2880xbf16>, %arg33: tensor<f32>, %arg34: tensor<1x13xi64>, %arg35: tensor<201088x2880xbf16>, %arg36: tensor<2880xbf16>, %arg37: tensor<f32>, %arg38: tensor<32xf32>, %arg39: tensor<512xbf16>, %arg40: tensor<512x2880xbf16>, %arg41: tensor<201088x2880xbf16>, %arg42: tensor<32xbf16>, %arg43: tensor<32x2880xbf16>, %arg44: tensor<2880xbf16>, %arg45: tensor<2880x4096xbf16>, %arg46: tensor<64xbf16>, %arg47: tensor<bf16>, %arg48: tensor<i64>, %arg49: tensor<f32>, %arg50: tensor<4096xbf16>, %arg51: tensor<4096x2880xbf16>, %arg52: tensor<2880xbf16>, %arg53: tensor<32x2880xbf16>, %arg54: tensor<32x2880x2880xbf16>, %arg55: tensor<f32>, %arg56: tensor<bf16>, %arg57: tensor<bf16>, %arg58: tensor<32x5760xbf16>, %arg59: tensor<32x2880x5760xbf16>, %arg60: tensor<bf16>, %arg61: tensor<2880xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
      %c_0 = stablehlo.constant dense<0> : tensor<ui8>
      %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
      %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
      %c_5 = stablehlo.constant dense<1> : tensor<ui8>
      %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<13x32xbf16>
      %2 = stablehlo.broadcast_in_dim %cst_6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %3 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
      %4 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
      %5 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
      %6 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
      %7 = stablehlo.convert %arg36 : (tensor<2880xbf16>) -> tensor<2880xf32>
      %8 = stablehlo.broadcast_in_dim %7, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %9 = stablehlo.convert %arg34 : (tensor<1x13xi64>) -> tensor<1x13xui32>
      %10 = stablehlo.reshape %9 : (tensor<1x13xui32>) -> tensor<13xui32>
      %11 = "stablehlo.gather"(%arg35, %10) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
      %12 = stablehlo.reshape %11 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %13 = stablehlo.convert %12 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %14 = stablehlo.power %13, %5 : tensor<1x13x2880xf32>
      %15 = stablehlo.reduce(%14 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %16 = stablehlo.multiply %15, %cst_3 : tensor<1x13xf32>
      %17 = stablehlo.reshape %16 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %18 = stablehlo.broadcast_in_dim %arg33, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
      %19 = stablehlo.add %17, %18 : tensor<1x13x1xf32>
      %20 = stablehlo.rsqrt %19 : tensor<1x13x1xf32>
      %21 = stablehlo.reshape %20 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %23 = stablehlo.multiply %13, %22 : tensor<1x13x2880xf32>
      %24 = stablehlo.multiply %8, %23 : tensor<1x13x2880xf32>
      %25 = stablehlo.convert %24 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %26 = stablehlo.reshape %25 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %27 = stablehlo.transpose %arg51, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
      %28 = stablehlo.dot_general %26, %27, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
      %29 = stablehlo.reshape %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
      %30 = sdy.reshard %arg50 <@mesh, [{"_axis_0"}]> : tensor<4096xbf16>
      %31 = stablehlo.broadcast_in_dim %30, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
      %32 = stablehlo.add %29, %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x4096xbf16>
      %33 = stablehlo.reshape %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
      %34 = stablehlo.transpose %33, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
      %35 = stablehlo.slice %34 [0:1, 0:64, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
      %36 = stablehlo.convert %35 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
      %37 = stablehlo.reshape %arg38 : (tensor<32xf32>) -> tensor<1x32x1xf32>
      %38 = stablehlo.dot_general %37, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
      %39 = stablehlo.transpose %38, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
      %40 = stablehlo.cosine %39 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
      %41 = stablehlo.broadcast_in_dim %arg37, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
      %42 = stablehlo.multiply %40, %41 : tensor<1x13x32xf32>
      %43 = stablehlo.convert %42 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
      %44 = stablehlo.convert %43 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
      %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
      %46 = stablehlo.multiply %36, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %48 = stablehlo.slice %34 [0:1, 0:64, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
      %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
      %50 = stablehlo.sine %39 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
      %51 = stablehlo.multiply %50, %41 : tensor<1x13x32xf32>
      %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
      %53 = stablehlo.convert %52 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
      %54 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
      %55 = stablehlo.multiply %49, %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %56 = stablehlo.convert %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %57 = stablehlo.subtract %47, %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
      %58 = stablehlo.multiply %49, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %59 = stablehlo.convert %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %60 = stablehlo.multiply %36, %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %61 = stablehlo.convert %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %62 = stablehlo.add %59, %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
      %63 = stablehlo.concatenate %57, %62, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
      %64 = stablehlo.reshape %63 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
      %65 = stablehlo.transpose %arg40, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
      %66 = stablehlo.dot_general %26, %65, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
      %67 = stablehlo.reshape %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
      %68 = sdy.reshard %arg39 <@mesh, [{"_axis_0"}]> : tensor<512xbf16>
      %69 = stablehlo.broadcast_in_dim %68, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
      %70 = stablehlo.add %67, %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
      %71 = stablehlo.reshape %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
      %72 = stablehlo.transpose %71, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:8, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
      %74 = stablehlo.convert %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
      %75 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
      %76 = stablehlo.multiply %74, %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %77 = stablehlo.convert %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %78 = stablehlo.slice %72 [0:1, 0:8, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
      %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
      %80 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
      %81 = stablehlo.multiply %79, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %83 = stablehlo.subtract %77, %82 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
      %84 = stablehlo.multiply %79, %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %85 = stablehlo.convert %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %86 = stablehlo.multiply %74, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %87 = stablehlo.convert %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %88 = stablehlo.add %85, %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
      %89 = stablehlo.concatenate %83, %88, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
      %90 = stablehlo.broadcast_in_dim %89, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
      %91 = stablehlo.reshape %90 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
      %92 = stablehlo.transpose %91, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
      %93 = stablehlo.reshape %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
      %94 = stablehlo.dot_general %64, %93, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
      %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
      %96 = stablehlo.reshape %95 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
      %97 = stablehlo.broadcast_in_dim %arg49, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x64x13x13xf32>
      %98 = stablehlo.multiply %96, %97 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xf32>
      %99 = stablehlo.convert %98 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
      %100 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
      %101 = stablehlo.broadcast_in_dim %arg48, dims = [] : (tensor<i64>) -> tensor<13xi64>
      %102 = stablehlo.subtract %c, %101 : tensor<13xi64>
      %103 = stablehlo.broadcast_in_dim %102, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
      %104 = stablehlo.compare  GT, %100, %103 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
      %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %106 = stablehlo.and %105, %4 : tensor<13x13xui8>
      %107 = stablehlo.compare  NE, %106, %6 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
      %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %109 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
      %110 = stablehlo.compare  LE, %100, %109 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
      %111 = stablehlo.convert %110 : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %112 = stablehlo.and %108, %111 : tensor<13x13xui8>
      %113 = stablehlo.compare  NE, %112, %6 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
      %114 = stablehlo.reshape %113 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
      %115 = stablehlo.reshape %arg47 : (tensor<bf16>) -> tensor<1x1xbf16>
      %116 = stablehlo.broadcast_in_dim %115, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
      %117 = stablehlo.select %114, %3, %116 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
      %119 = stablehlo.broadcast_in_dim %118, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
      %120 = stablehlo.add %99, %119 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xbf16>
      %121 = stablehlo.reshape %arg46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
      %122 = stablehlo.broadcast_in_dim %121, dims = [0, 1, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
      %123 = stablehlo.concatenate %120, %122, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
      %124 = stablehlo.transpose %arg32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
      %125 = stablehlo.dot_general %26, %124, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
      %126 = stablehlo.reshape %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
      %127 = sdy.reshard %arg31 <@mesh, [{"_axis_0"}]> : tensor<512xbf16>
      %128 = stablehlo.broadcast_in_dim %127, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
      %129 = stablehlo.add %126, %128 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
      %130 = stablehlo.reshape %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
      %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
      %132 = stablehlo.convert %arg61 : (tensor<2880xbf16>) -> tensor<2880xf32>
      %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %134 = stablehlo.reduce(%123 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
      %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
      %136 = stablehlo.subtract %123, %135 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
      %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
      %139 = stablehlo.subtract %136, %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %140 = stablehlo.exponential %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
      %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
      %143 = stablehlo.divide %140, %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
      %145 = stablehlo.reshape %144 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
      %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
      %147 = stablehlo.reshape %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
      %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
      %149 = stablehlo.reshape %148 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
      %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
      %151 = stablehlo.reshape %150 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
      %152 = stablehlo.transpose %arg45, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
      %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
      %154 = sdy.all_reduce {"_axis_0"} %153 out_sharding=<@mesh, [{}, {}]> : tensor<13x2880xbf16>
      %155 = stablehlo.reshape %154 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %156 = stablehlo.broadcast_in_dim %arg44, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
      %157 = stablehlo.add %155, %156 : tensor<1x13x2880xbf16>
      %158 = stablehlo.add %12, %157 : tensor<1x13x2880xbf16>
      %159 = stablehlo.broadcast_in_dim %arg60, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %160 = stablehlo.convert %arg52 : (tensor<2880xbf16>) -> tensor<2880xf32>
      %161 = stablehlo.broadcast_in_dim %160, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %162 = stablehlo.convert %158 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %163 = stablehlo.power %162, %5 : tensor<1x13x2880xf32>
      %164 = stablehlo.reduce(%163 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %165 = stablehlo.multiply %164, %cst_3 : tensor<1x13xf32>
      %166 = stablehlo.reshape %165 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %167 = stablehlo.add %166, %18 : tensor<1x13x1xf32>
      %168 = stablehlo.rsqrt %167 : tensor<1x13x1xf32>
      %169 = stablehlo.reshape %168 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %170 = stablehlo.broadcast_in_dim %169, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %171 = stablehlo.multiply %162, %170 : tensor<1x13x2880xf32>
      %172 = stablehlo.multiply %161, %171 : tensor<1x13x2880xf32>
      %173 = stablehlo.convert %172 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %174 = stablehlo.reshape %173 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %175 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %176 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %177 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %178 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %179 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %180 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %181 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %182 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %183 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %184 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %185 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %186 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %187 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %188 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %189 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %190 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %191 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %192 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %193 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %194 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %195 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %196 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %197 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %198 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %199 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %200 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %201 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %202 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %203 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %204 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %205 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %206 = sdy.reshard %174 <@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %207 = stablehlo.concatenate %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196, %197, %198, %199, %200, %201, %202, %203, %204, %205, %206, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
      %208 = stablehlo.reshape %207 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
      %209 = stablehlo.dot_general %208, %arg59, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
      %210 = stablehlo.broadcast_in_dim %arg58, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
      %211 = stablehlo.add %209, %210 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x5760xbf16>
      %212 = stablehlo.slice %211 [0:32, 0:13, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
      %213 = stablehlo.broadcast_in_dim %arg56, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %214 = stablehlo.clamp %159, %212, %213 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %215 = stablehlo.add %214, %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %216 = stablehlo.convert %215 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %217 = stablehlo.broadcast_in_dim %arg57, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %218 = stablehlo.slice %211 [0:32, 0:13, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
      %219 = stablehlo.clamp %217, %218, %213 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %220 = stablehlo.convert %219 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %221 = stablehlo.broadcast_in_dim %arg55, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x13x2880xf32>
      %222 = stablehlo.multiply %220, %221 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
      %223 = stablehlo.convert %222 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
      %224 = stablehlo.logistic %223 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %225 = stablehlo.convert %224 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %226 = stablehlo.multiply %220, %225 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
      %227 = stablehlo.convert %226 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
      %228 = stablehlo.convert %227 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %229 = stablehlo.multiply %216, %228 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
      %230 = stablehlo.convert %229 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
      %231 = stablehlo.dot_general %230, %arg54, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
      %232 = stablehlo.broadcast_in_dim %arg53, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
      %233 = stablehlo.add %231, %232 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %234 = stablehlo.convert %233 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %235 = stablehlo.reshape %234 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
      %236 = stablehlo.iota dim = 0 : tensor<13xi64>
      %237 = stablehlo.broadcast_in_dim %236, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
      %238 = stablehlo.transpose %arg43, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %239 = stablehlo.dot_general %174, %238, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
      %240 = stablehlo.broadcast_in_dim %arg42, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
      %241 = stablehlo.add %239, %240 : tensor<13x32xbf16>
      %242 = stablehlo.iota dim = 0 : tensor<32xi32>
      %243 = stablehlo.broadcast_in_dim %242, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
      %244:2 = "stablehlo.sort"(%241, %243) <{dimension = 1 : i64}> ({
      ^bb0(%arg62: tensor<bf16>, %arg63: tensor<bf16>, %arg64: tensor<i32>, %arg65: tensor<i32>):
        %283 = stablehlo.compare  GT, %arg62, %arg63,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %283 : tensor<i1>
      }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
      %245 = stablehlo.slice %244#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
      %246 = stablehlo.convert %245 : (tensor<13x4xi32>) -> tensor<13x4xi64>
      %247 = stablehlo.reshape %246 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
      %248 = stablehlo.concatenate %237, %247, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
      %249 = stablehlo.slice %244#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
      %250 = stablehlo.reduce(%249 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
      %251 = stablehlo.broadcast_in_dim %250, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
      %252 = stablehlo.subtract %249, %251 : tensor<13x4xbf16>
      %253 = stablehlo.exponential %252 : tensor<13x4xbf16>
      %254 = stablehlo.reduce(%253 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
      %255 = stablehlo.broadcast_in_dim %254, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
      %256 = stablehlo.divide %253, %255 : tensor<13x4xbf16>
      %257 = "stablehlo.scatter"(%1, %248, %256) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg62: tensor<bf16>, %arg63: tensor<bf16>):
        stablehlo.return %arg63 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
      %258 = stablehlo.convert %257 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>) -> tensor<13x32xf32>
      %259 = stablehlo.transpose %258, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
      %260 = stablehlo.reshape %259 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
      %261 = stablehlo.broadcast_in_dim %260, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
      %262 = stablehlo.multiply %235, %261 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x13x2880xf32>
      %263 = stablehlo.convert %262 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
      %264 = stablehlo.reduce(%263 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
      %265 = sdy.all_reduce {"_axis_0"} %264 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x13x2880xbf16>
      %266 = stablehlo.add %158, %265 : tensor<1x13x2880xbf16>
      %267 = stablehlo.convert %266 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %268 = stablehlo.power %267, %5 : tensor<1x13x2880xf32>
      %269 = stablehlo.reduce(%268 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %270 = stablehlo.multiply %269, %cst_3 : tensor<1x13xf32>
      %271 = stablehlo.reshape %270 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %272 = stablehlo.add %271, %18 : tensor<1x13x1xf32>
      %273 = stablehlo.rsqrt %272 : tensor<1x13x1xf32>
      %274 = stablehlo.reshape %273 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %275 = stablehlo.broadcast_in_dim %274, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %276 = stablehlo.multiply %267, %275 : tensor<1x13x2880xf32>
      %277 = stablehlo.multiply %133, %276 : tensor<1x13x2880xf32>
      %278 = stablehlo.convert %277 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %279 = stablehlo.reshape %278 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %280 = stablehlo.transpose %arg41, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
      %281 = stablehlo.dot_general %279, %280, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
      %282 = stablehlo.reshape %281 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
      sdy.return %129, %130, %131, %89, %281, %282 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
    } : (tensor<512xbf16>, tensor<512x2880xbf16>, tensor<f32>, tensor<1x13xi64>, tensor<201088x2880xbf16>, tensor<2880xbf16>, tensor<f32>, tensor<32xf32>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<201088x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<bf16>, tensor<i64>, tensor<f32>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>, tensor<2880xbf16>) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4, %0#5 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump After ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:6 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg25, %arg26, %arg27, %arg28, %arg29, %arg30) in_shardings=[<@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, []>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, [{}]>] out_shardings=[<@mesh, [{?}, {?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {"_axis_0", ?}]>] manual_axes={} (%arg31: tensor<512xbf16>, %arg32: tensor<512x2880xbf16>, %arg33: tensor<f32>, %arg34: tensor<1x13xi64>, %arg35: tensor<201088x2880xbf16>, %arg36: tensor<2880xbf16>, %arg37: tensor<f32>, %arg38: tensor<32xf32>, %arg39: tensor<512xbf16>, %arg40: tensor<512x2880xbf16>, %arg41: tensor<201088x2880xbf16>, %arg42: tensor<32xbf16>, %arg43: tensor<32x2880xbf16>, %arg44: tensor<2880xbf16>, %arg45: tensor<2880x4096xbf16>, %arg46: tensor<64xbf16>, %arg47: tensor<bf16>, %arg48: tensor<i64>, %arg49: tensor<f32>, %arg50: tensor<4096xbf16>, %arg51: tensor<4096x2880xbf16>, %arg52: tensor<2880xbf16>, %arg53: tensor<32x2880xbf16>, %arg54: tensor<32x2880x2880xbf16>, %arg55: tensor<f32>, %arg56: tensor<bf16>, %arg57: tensor<bf16>, %arg58: tensor<32x5760xbf16>, %arg59: tensor<32x2880x5760xbf16>, %arg60: tensor<bf16>, %arg61: tensor<2880xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
      %c_0 = stablehlo.constant dense<0> : tensor<ui8>
      %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
      %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
      %c_5 = stablehlo.constant dense<1> : tensor<ui8>
      %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<13x32xbf16>
      %2 = stablehlo.broadcast_in_dim %cst_6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %3 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
      %4 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
      %5 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
      %6 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
      %7 = stablehlo.convert %arg36 : (tensor<2880xbf16>) -> tensor<2880xf32>
      %8 = stablehlo.broadcast_in_dim %7, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %9 = stablehlo.convert %arg34 : (tensor<1x13xi64>) -> tensor<1x13xui32>
      %10 = stablehlo.reshape %9 : (tensor<1x13xui32>) -> tensor<13xui32>
      %11 = "stablehlo.gather"(%arg35, %10) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
      %12 = stablehlo.reshape %11 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %13 = stablehlo.convert %12 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %14 = stablehlo.power %13, %5 : tensor<1x13x2880xf32>
      %15 = stablehlo.reduce(%14 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %16 = stablehlo.multiply %15, %cst_3 : tensor<1x13xf32>
      %17 = stablehlo.reshape %16 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %18 = stablehlo.broadcast_in_dim %arg33, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
      %19 = stablehlo.add %17, %18 : tensor<1x13x1xf32>
      %20 = stablehlo.rsqrt %19 : tensor<1x13x1xf32>
      %21 = stablehlo.reshape %20 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %23 = stablehlo.multiply %13, %22 : tensor<1x13x2880xf32>
      %24 = stablehlo.multiply %8, %23 : tensor<1x13x2880xf32>
      %25 = stablehlo.convert %24 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %26 = stablehlo.reshape %25 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %27 = stablehlo.transpose %arg51, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
      %28 = stablehlo.dot_general %26, %27, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
      %29 = stablehlo.reshape %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
      %30 = sdy.all_slice [{"_axis_0"}] %arg50 out_sharding=<@mesh, [{"_axis_0"}]> : tensor<4096xbf16>
      %31 = stablehlo.broadcast_in_dim %30, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
      %32 = stablehlo.add %29, %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x4096xbf16>
      %33 = stablehlo.reshape %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
      %34 = stablehlo.transpose %33, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
      %35 = stablehlo.slice %34 [0:1, 0:64, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
      %36 = stablehlo.convert %35 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
      %37 = stablehlo.reshape %arg38 : (tensor<32xf32>) -> tensor<1x32x1xf32>
      %38 = stablehlo.dot_general %37, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
      %39 = stablehlo.transpose %38, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
      %40 = stablehlo.cosine %39 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
      %41 = stablehlo.broadcast_in_dim %arg37, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
      %42 = stablehlo.multiply %40, %41 : tensor<1x13x32xf32>
      %43 = stablehlo.convert %42 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
      %44 = stablehlo.convert %43 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
      %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
      %46 = stablehlo.multiply %36, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %48 = stablehlo.slice %34 [0:1, 0:64, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
      %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
      %50 = stablehlo.sine %39 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
      %51 = stablehlo.multiply %50, %41 : tensor<1x13x32xf32>
      %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
      %53 = stablehlo.convert %52 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
      %54 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
      %55 = stablehlo.multiply %49, %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %56 = stablehlo.convert %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %57 = stablehlo.subtract %47, %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
      %58 = stablehlo.multiply %49, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %59 = stablehlo.convert %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %60 = stablehlo.multiply %36, %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %61 = stablehlo.convert %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %62 = stablehlo.add %59, %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
      %63 = stablehlo.concatenate %57, %62, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
      %64 = stablehlo.reshape %63 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
      %65 = stablehlo.transpose %arg40, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
      %66 = stablehlo.dot_general %26, %65, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
      %67 = stablehlo.reshape %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
      %68 = sdy.all_slice [{"_axis_0"}] %arg39 out_sharding=<@mesh, [{"_axis_0"}]> : tensor<512xbf16>
      %69 = stablehlo.broadcast_in_dim %68, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
      %70 = stablehlo.add %67, %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
      %71 = stablehlo.reshape %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
      %72 = stablehlo.transpose %71, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:8, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
      %74 = stablehlo.convert %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
      %75 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
      %76 = stablehlo.multiply %74, %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %77 = stablehlo.convert %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %78 = stablehlo.slice %72 [0:1, 0:8, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
      %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
      %80 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
      %81 = stablehlo.multiply %79, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %83 = stablehlo.subtract %77, %82 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
      %84 = stablehlo.multiply %79, %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %85 = stablehlo.convert %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %86 = stablehlo.multiply %74, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %87 = stablehlo.convert %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %88 = stablehlo.add %85, %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
      %89 = stablehlo.concatenate %83, %88, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
      %90 = stablehlo.broadcast_in_dim %89, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
      %91 = stablehlo.reshape %90 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
      %92 = stablehlo.transpose %91, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
      %93 = stablehlo.reshape %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
      %94 = stablehlo.dot_general %64, %93, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
      %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
      %96 = stablehlo.reshape %95 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
      %97 = stablehlo.broadcast_in_dim %arg49, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x64x13x13xf32>
      %98 = stablehlo.multiply %96, %97 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xf32>
      %99 = stablehlo.convert %98 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
      %100 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
      %101 = stablehlo.broadcast_in_dim %arg48, dims = [] : (tensor<i64>) -> tensor<13xi64>
      %102 = stablehlo.subtract %c, %101 : tensor<13xi64>
      %103 = stablehlo.broadcast_in_dim %102, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
      %104 = stablehlo.compare  GT, %100, %103 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
      %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %106 = stablehlo.and %105, %4 : tensor<13x13xui8>
      %107 = stablehlo.compare  NE, %106, %6 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
      %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %109 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
      %110 = stablehlo.compare  LE, %100, %109 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
      %111 = stablehlo.convert %110 : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %112 = stablehlo.and %108, %111 : tensor<13x13xui8>
      %113 = stablehlo.compare  NE, %112, %6 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
      %114 = stablehlo.reshape %113 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
      %115 = stablehlo.reshape %arg47 : (tensor<bf16>) -> tensor<1x1xbf16>
      %116 = stablehlo.broadcast_in_dim %115, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
      %117 = stablehlo.select %114, %3, %116 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
      %119 = stablehlo.broadcast_in_dim %118, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
      %120 = stablehlo.add %99, %119 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xbf16>
      %121 = stablehlo.reshape %arg46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
      %122 = stablehlo.broadcast_in_dim %121, dims = [0, 1, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
      %123 = stablehlo.concatenate %120, %122, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
      %124 = stablehlo.transpose %arg32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
      %125 = stablehlo.dot_general %26, %124, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
      %126 = stablehlo.reshape %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
      %127 = sdy.all_slice [{"_axis_0"}] %arg31 out_sharding=<@mesh, [{"_axis_0"}]> : tensor<512xbf16>
      %128 = stablehlo.broadcast_in_dim %127, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
      %129 = stablehlo.add %126, %128 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
      %130 = stablehlo.reshape %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
      %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
      %132 = stablehlo.convert %arg61 : (tensor<2880xbf16>) -> tensor<2880xf32>
      %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %134 = stablehlo.reduce(%123 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
      %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
      %136 = stablehlo.subtract %123, %135 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
      %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
      %139 = stablehlo.subtract %136, %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %140 = stablehlo.exponential %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
      %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
      %143 = stablehlo.divide %140, %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
      %145 = stablehlo.reshape %144 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
      %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
      %147 = stablehlo.reshape %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
      %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
      %149 = stablehlo.reshape %148 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
      %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
      %151 = stablehlo.reshape %150 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
      %152 = stablehlo.transpose %arg45, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
      %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
      %154 = sdy.all_reduce {"_axis_0"} %153 out_sharding=<@mesh, [{}, {}]> : tensor<13x2880xbf16>
      %155 = stablehlo.reshape %154 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %156 = stablehlo.broadcast_in_dim %arg44, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
      %157 = stablehlo.add %155, %156 : tensor<1x13x2880xbf16>
      %158 = stablehlo.add %12, %157 : tensor<1x13x2880xbf16>
      %159 = stablehlo.broadcast_in_dim %arg60, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %160 = stablehlo.convert %arg52 : (tensor<2880xbf16>) -> tensor<2880xf32>
      %161 = stablehlo.broadcast_in_dim %160, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %162 = stablehlo.convert %158 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %163 = stablehlo.power %162, %5 : tensor<1x13x2880xf32>
      %164 = stablehlo.reduce(%163 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %165 = stablehlo.multiply %164, %cst_3 : tensor<1x13xf32>
      %166 = stablehlo.reshape %165 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %167 = stablehlo.add %166, %18 : tensor<1x13x1xf32>
      %168 = stablehlo.rsqrt %167 : tensor<1x13x1xf32>
      %169 = stablehlo.reshape %168 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %170 = stablehlo.broadcast_in_dim %169, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %171 = stablehlo.multiply %162, %170 : tensor<1x13x2880xf32>
      %172 = stablehlo.multiply %161, %171 : tensor<1x13x2880xf32>
      %173 = stablehlo.convert %172 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %174 = stablehlo.reshape %173 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %175 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %176 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %177 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %178 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %179 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %180 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %181 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %182 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %183 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %184 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %185 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %186 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %187 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %188 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %189 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %190 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %191 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %192 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %193 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %194 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %195 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %196 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %197 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %198 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %199 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %200 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %201 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %202 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %203 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %204 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %205 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %206 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %207 = stablehlo.concatenate %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196, %197, %198, %199, %200, %201, %202, %203, %204, %205, %206, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
      %208 = stablehlo.reshape %207 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
      %209 = stablehlo.dot_general %208, %arg59, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
      %210 = stablehlo.broadcast_in_dim %arg58, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
      %211 = stablehlo.add %209, %210 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x5760xbf16>
      %212 = stablehlo.slice %211 [0:32, 0:13, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
      %213 = stablehlo.broadcast_in_dim %arg56, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %214 = stablehlo.clamp %159, %212, %213 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %215 = stablehlo.add %214, %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %216 = stablehlo.convert %215 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %217 = stablehlo.broadcast_in_dim %arg57, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %218 = stablehlo.slice %211 [0:32, 0:13, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
      %219 = stablehlo.clamp %217, %218, %213 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %220 = stablehlo.convert %219 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %221 = stablehlo.broadcast_in_dim %arg55, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x13x2880xf32>
      %222 = stablehlo.multiply %220, %221 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
      %223 = stablehlo.convert %222 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
      %224 = stablehlo.logistic %223 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %225 = stablehlo.convert %224 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %226 = stablehlo.multiply %220, %225 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
      %227 = stablehlo.convert %226 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
      %228 = stablehlo.convert %227 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %229 = stablehlo.multiply %216, %228 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
      %230 = stablehlo.convert %229 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
      %231 = stablehlo.dot_general %230, %arg54, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
      %232 = stablehlo.broadcast_in_dim %arg53, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
      %233 = stablehlo.add %231, %232 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %234 = stablehlo.convert %233 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %235 = stablehlo.reshape %234 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
      %236 = stablehlo.iota dim = 0 : tensor<13xi64>
      %237 = stablehlo.broadcast_in_dim %236, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
      %238 = stablehlo.transpose %arg43, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %239 = stablehlo.dot_general %174, %238, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
      %240 = stablehlo.broadcast_in_dim %arg42, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
      %241 = stablehlo.add %239, %240 : tensor<13x32xbf16>
      %242 = stablehlo.iota dim = 0 : tensor<32xi32>
      %243 = stablehlo.broadcast_in_dim %242, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
      %244:2 = "stablehlo.sort"(%241, %243) <{dimension = 1 : i64}> ({
      ^bb0(%arg62: tensor<bf16>, %arg63: tensor<bf16>, %arg64: tensor<i32>, %arg65: tensor<i32>):
        %283 = stablehlo.compare  GT, %arg62, %arg63,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %283 : tensor<i1>
      }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
      %245 = stablehlo.slice %244#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
      %246 = stablehlo.convert %245 : (tensor<13x4xi32>) -> tensor<13x4xi64>
      %247 = stablehlo.reshape %246 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
      %248 = stablehlo.concatenate %237, %247, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
      %249 = stablehlo.slice %244#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
      %250 = stablehlo.reduce(%249 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
      %251 = stablehlo.broadcast_in_dim %250, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
      %252 = stablehlo.subtract %249, %251 : tensor<13x4xbf16>
      %253 = stablehlo.exponential %252 : tensor<13x4xbf16>
      %254 = stablehlo.reduce(%253 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
      %255 = stablehlo.broadcast_in_dim %254, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
      %256 = stablehlo.divide %253, %255 : tensor<13x4xbf16>
      %257 = "stablehlo.scatter"(%1, %248, %256) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg62: tensor<bf16>, %arg63: tensor<bf16>):
        stablehlo.return %arg63 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
      %258 = stablehlo.convert %257 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>) -> tensor<13x32xf32>
      %259 = stablehlo.transpose %258, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
      %260 = stablehlo.reshape %259 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
      %261 = stablehlo.broadcast_in_dim %260, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
      %262 = stablehlo.multiply %235, %261 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x13x2880xf32>
      %263 = stablehlo.convert %262 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
      %264 = stablehlo.reduce(%263 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
      %265 = sdy.all_reduce {"_axis_0"} %264 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x13x2880xbf16>
      %266 = stablehlo.add %158, %265 : tensor<1x13x2880xbf16>
      %267 = stablehlo.convert %266 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %268 = stablehlo.power %267, %5 : tensor<1x13x2880xf32>
      %269 = stablehlo.reduce(%268 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %270 = stablehlo.multiply %269, %cst_3 : tensor<1x13xf32>
      %271 = stablehlo.reshape %270 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %272 = stablehlo.add %271, %18 : tensor<1x13x1xf32>
      %273 = stablehlo.rsqrt %272 : tensor<1x13x1xf32>
      %274 = stablehlo.reshape %273 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %275 = stablehlo.broadcast_in_dim %274, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %276 = stablehlo.multiply %267, %275 : tensor<1x13x2880xf32>
      %277 = stablehlo.multiply %133, %276 : tensor<1x13x2880xf32>
      %278 = stablehlo.convert %277 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %279 = stablehlo.reshape %278 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %280 = stablehlo.transpose %arg41, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
      %281 = stablehlo.dot_general %279, %280, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
      %282 = stablehlo.reshape %281 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
      sdy.return %129, %130, %131, %89, %281, %282 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
    } : (tensor<512xbf16>, tensor<512x2880xbf16>, tensor<f32>, tensor<1x13xi64>, tensor<201088x2880xbf16>, tensor<2880xbf16>, tensor<f32>, tensor<32xf32>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<201088x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<bf16>, tensor<i64>, tensor<f32>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>, tensor<2880xbf16>) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4, %0#5 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
module @SyncTensorsGraph.577 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<1x13xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<512x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<201088x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<32xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<2880x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<4096x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg22: tensor<32x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg23: tensor<32x2880x2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg24: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg25: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg26: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg27: tensor<32x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg28: tensor<32x2880x5760xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}, {}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg29: tensor<bf16> {sdy.sharding = #sdy.sharding<@mesh, []>, ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg30: tensor<2880xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x13x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x8x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x13x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x13x201088xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {?}, {"_axis_0", ?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:6 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg25, %arg26, %arg27, %arg28, %arg29, %arg30) in_shardings=[<@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, []>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, [{}]>] out_shardings=[<@mesh, [{?}, {?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {"_axis_0", ?}]>] manual_axes={} (%arg31: tensor<512xbf16>, %arg32: tensor<512x2880xbf16>, %arg33: tensor<f32>, %arg34: tensor<1x13xi64>, %arg35: tensor<201088x2880xbf16>, %arg36: tensor<2880xbf16>, %arg37: tensor<f32>, %arg38: tensor<32xf32>, %arg39: tensor<512xbf16>, %arg40: tensor<512x2880xbf16>, %arg41: tensor<201088x2880xbf16>, %arg42: tensor<32xbf16>, %arg43: tensor<32x2880xbf16>, %arg44: tensor<2880xbf16>, %arg45: tensor<2880x4096xbf16>, %arg46: tensor<64xbf16>, %arg47: tensor<bf16>, %arg48: tensor<i64>, %arg49: tensor<f32>, %arg50: tensor<4096xbf16>, %arg51: tensor<4096x2880xbf16>, %arg52: tensor<2880xbf16>, %arg53: tensor<32x2880xbf16>, %arg54: tensor<32x2880x2880xbf16>, %arg55: tensor<f32>, %arg56: tensor<bf16>, %arg57: tensor<bf16>, %arg58: tensor<32x5760xbf16>, %arg59: tensor<32x2880x5760xbf16>, %arg60: tensor<bf16>, %arg61: tensor<2880xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>
      %c_0 = stablehlo.constant dense<0> : tensor<ui8>
      %cst_1 = stablehlo.constant dense<0xFF80> : tensor<bf16>
      %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %cst_3 = stablehlo.constant dense<3.47222231E-4> : tensor<1x13xf32>
      %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>
      %c_5 = stablehlo.constant dense<1> : tensor<ui8>
      %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<bf16>
      %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<13x32xbf16>
      %2 = stablehlo.broadcast_in_dim %cst_6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %3 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
      %4 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
      %5 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x13x2880xf32>
      %6 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<ui8>) -> tensor<13x13xui8>
      %7 = stablehlo.convert %arg36 : (tensor<2880xbf16>) -> tensor<2880xf32>
      %8 = stablehlo.broadcast_in_dim %7, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %9 = stablehlo.convert %arg34 : (tensor<1x13xi64>) -> tensor<1x13xui32>
      %10 = stablehlo.reshape %9 : (tensor<1x13xui32>) -> tensor<13xui32>
      %11 = "stablehlo.gather"(%arg35, %10) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
      %12 = stablehlo.reshape %11 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %13 = stablehlo.convert %12 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %14 = stablehlo.power %13, %5 : tensor<1x13x2880xf32>
      %15 = stablehlo.reduce(%14 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %16 = stablehlo.multiply %15, %cst_3 : tensor<1x13xf32>
      %17 = stablehlo.reshape %16 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %18 = stablehlo.broadcast_in_dim %arg33, dims = [] : (tensor<f32>) -> tensor<1x13x1xf32>
      %19 = stablehlo.add %17, %18 : tensor<1x13x1xf32>
      %20 = stablehlo.rsqrt %19 : tensor<1x13x1xf32>
      %21 = stablehlo.reshape %20 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %23 = stablehlo.multiply %13, %22 : tensor<1x13x2880xf32>
      %24 = stablehlo.multiply %8, %23 : tensor<1x13x2880xf32>
      %25 = stablehlo.convert %24 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %26 = stablehlo.reshape %25 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %27 = stablehlo.transpose %arg51, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<4096x2880xbf16>) -> tensor<2880x4096xbf16>
      %28 = stablehlo.dot_general %26, %27, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x4096xbf16>) -> tensor<13x4096xbf16>
      %29 = stablehlo.reshape %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x4096xbf16>) -> tensor<1x13x4096xbf16>
      %30 = sdy.all_slice [{"_axis_0"}] %arg50 out_sharding=<@mesh, [{"_axis_0"}]> : tensor<4096xbf16>
      %31 = stablehlo.broadcast_in_dim %30, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<4096xbf16>) -> tensor<1x13x4096xbf16>
      %32 = stablehlo.add %29, %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x4096xbf16>
      %33 = stablehlo.reshape %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x4096xbf16>) -> tensor<1x13x64x64xbf16>
      %34 = stablehlo.transpose %33, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x64x64xbf16>) -> tensor<1x64x13x64xbf16>
      %35 = stablehlo.slice %34 [0:1, 0:64, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
      %36 = stablehlo.convert %35 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
      %37 = stablehlo.reshape %arg38 : (tensor<32xf32>) -> tensor<1x32x1xf32>
      %38 = stablehlo.dot_general %37, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
      %39 = stablehlo.transpose %38, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
      %40 = stablehlo.cosine %39 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
      %41 = stablehlo.broadcast_in_dim %arg37, dims = [] : (tensor<f32>) -> tensor<1x13x32xf32>
      %42 = stablehlo.multiply %40, %41 : tensor<1x13x32xf32>
      %43 = stablehlo.convert %42 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
      %44 = stablehlo.convert %43 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
      %45 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
      %46 = stablehlo.multiply %36, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %47 = stablehlo.convert %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %48 = stablehlo.slice %34 [0:1, 0:64, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x13x32xbf16>
      %49 = stablehlo.convert %48 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>) -> tensor<1x64x13x32xf32>
      %50 = stablehlo.sine %39 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : tensor<1x13x32xf32>
      %51 = stablehlo.multiply %50, %41 : tensor<1x13x32xf32>
      %52 = stablehlo.convert %51 : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
      %53 = stablehlo.convert %52 : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
      %54 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x64x13x32xf32>
      %55 = stablehlo.multiply %49, %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %56 = stablehlo.convert %55 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %57 = stablehlo.subtract %47, %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
      %58 = stablehlo.multiply %49, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %59 = stablehlo.convert %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %60 = stablehlo.multiply %36, %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xf32>
      %61 = stablehlo.convert %60 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xf32>) -> tensor<1x64x13x32xbf16>
      %62 = stablehlo.add %59, %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x32xbf16>
      %63 = stablehlo.concatenate %57, %62, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x32xbf16>, tensor<1x64x13x32xbf16>) -> tensor<1x64x13x64xbf16>
      %64 = stablehlo.reshape %63 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x64xbf16>) -> tensor<64x13x64xbf16>
      %65 = stablehlo.transpose %arg40, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
      %66 = stablehlo.dot_general %26, %65, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
      %67 = stablehlo.reshape %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
      %68 = sdy.all_slice [{"_axis_0"}] %arg39 out_sharding=<@mesh, [{"_axis_0"}]> : tensor<512xbf16>
      %69 = stablehlo.broadcast_in_dim %68, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
      %70 = stablehlo.add %67, %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
      %71 = stablehlo.reshape %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
      %72 = stablehlo.transpose %71, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
      %73 = stablehlo.slice %72 [0:1, 0:8, 0:13, 0:32] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
      %74 = stablehlo.convert %73 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
      %75 = stablehlo.broadcast_in_dim %44, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
      %76 = stablehlo.multiply %74, %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %77 = stablehlo.convert %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %78 = stablehlo.slice %72 [0:1, 0:8, 0:13, 32:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
      %79 = stablehlo.convert %78 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
      %80 = stablehlo.broadcast_in_dim %53, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
      %81 = stablehlo.multiply %79, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %82 = stablehlo.convert %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %83 = stablehlo.subtract %77, %82 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
      %84 = stablehlo.multiply %79, %75 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %85 = stablehlo.convert %84 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %86 = stablehlo.multiply %74, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xf32>
      %87 = stablehlo.convert %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %88 = stablehlo.add %85, %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x13x32xbf16>
      %89 = stablehlo.concatenate %83, %88, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
      %90 = stablehlo.broadcast_in_dim %89, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
      %91 = stablehlo.reshape %90 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<1x64x13x64xbf16>
      %92 = stablehlo.transpose %91, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x64x64x13xbf16>
      %93 = stablehlo.reshape %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x64x13xbf16>) -> tensor<64x64x13xbf16>
      %94 = stablehlo.dot_general %64, %93, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>, tensor<64x64x13xbf16>) -> tensor<64x13x13xbf16>
      %95 = stablehlo.convert %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>) -> tensor<64x13x13xf32>
      %96 = stablehlo.reshape %95 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xf32>) -> tensor<1x64x13x13xf32>
      %97 = stablehlo.broadcast_in_dim %arg49, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<1x64x13x13xf32>
      %98 = stablehlo.multiply %96, %97 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xf32>
      %99 = stablehlo.convert %98 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xf32>) -> tensor<1x64x13x13xbf16>
      %100 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<13xi64>) -> tensor<13x13xi64>
      %101 = stablehlo.broadcast_in_dim %arg48, dims = [] : (tensor<i64>) -> tensor<13xi64>
      %102 = stablehlo.subtract %c, %101 : tensor<13xi64>
      %103 = stablehlo.broadcast_in_dim %102, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
      %104 = stablehlo.compare  GT, %100, %103 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
      %105 = stablehlo.convert %104 : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %106 = stablehlo.and %105, %4 : tensor<13x13xui8>
      %107 = stablehlo.compare  NE, %106, %6 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
      %108 = stablehlo.convert %107 : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %109 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<13xi64>) -> tensor<13x13xi64>
      %110 = stablehlo.compare  LE, %100, %109 : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
      %111 = stablehlo.convert %110 : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %112 = stablehlo.and %108, %111 : tensor<13x13xui8>
      %113 = stablehlo.compare  NE, %112, %6 : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
      %114 = stablehlo.reshape %113 : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
      %115 = stablehlo.reshape %arg47 : (tensor<bf16>) -> tensor<1x1xbf16>
      %116 = stablehlo.broadcast_in_dim %115, dims = [0, 1] : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
      %117 = stablehlo.select %114, %3, %116 : tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>
      %118 = stablehlo.reshape %117 : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
      %119 = stablehlo.broadcast_in_dim %118, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x13x13xbf16>) -> tensor<1x64x13x13xbf16>
      %120 = stablehlo.add %99, %119 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x13xbf16>
      %121 = stablehlo.reshape %arg46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<64xbf16>) -> tensor<1x64x1xbf16>
      %122 = stablehlo.broadcast_in_dim %121, dims = [0, 1, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x1xbf16>) -> tensor<1x64x13x1xbf16>
      %123 = stablehlo.concatenate %120, %122, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>, tensor<1x64x13x1xbf16>) -> tensor<1x64x13x14xbf16>
      %124 = stablehlo.transpose %arg32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
      %125 = stablehlo.dot_general %26, %124, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
      %126 = stablehlo.reshape %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
      %127 = sdy.all_slice [{"_axis_0"}] %arg31 out_sharding=<@mesh, [{"_axis_0"}]> : tensor<512xbf16>
      %128 = stablehlo.broadcast_in_dim %127, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<512xbf16>) -> tensor<1x13x512xbf16>
      %129 = stablehlo.add %126, %128 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x13x512xbf16>
      %130 = stablehlo.reshape %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
      %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
      %132 = stablehlo.convert %arg61 : (tensor<2880xbf16>) -> tensor<2880xf32>
      %133 = stablehlo.broadcast_in_dim %132, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %134 = stablehlo.reduce(%123 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
      %135 = stablehlo.broadcast_in_dim %134, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
      %136 = stablehlo.subtract %123, %135 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %137 = stablehlo.reduce(%136 init: %cst_1) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
      %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
      %139 = stablehlo.subtract %136, %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %140 = stablehlo.exponential %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %141 = stablehlo.reduce(%140 init: %cst_7) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x13x14xbf16>, tensor<bf16>) -> tensor<1x64x13xbf16>
      %142 = stablehlo.broadcast_in_dim %141, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13xbf16>) -> tensor<1x64x13x14xbf16>
      %143 = stablehlo.divide %140, %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x13x14xbf16>
      %144 = stablehlo.slice %143 [0:1, 0:64, 0:13, 0:13] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x14xbf16>) -> tensor<1x64x13x13xbf16>
      %145 = stablehlo.reshape %144 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x13x13xbf16>) -> tensor<64x13x13xbf16>
      %146 = stablehlo.broadcast_in_dim %131, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x8x13x64xbf16>
      %147 = stablehlo.reshape %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x13x64xbf16>) -> tensor<64x13x64xbf16>
      %148 = stablehlo.dot_general %145, %147, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x13xbf16>, tensor<64x13x64xbf16>) -> tensor<64x13x64xbf16>
      %149 = stablehlo.reshape %148 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<64x13x64xbf16>) -> tensor<1x64x13x64xbf16>
      %150 = stablehlo.transpose %149, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x64x13x64xbf16>) -> tensor<1x13x64x64xbf16>
      %151 = stablehlo.reshape %150 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x13x64x64xbf16>) -> tensor<13x4096xbf16>
      %152 = stablehlo.transpose %arg45, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x4096xbf16>) -> tensor<4096x2880xbf16>
      %153 = stablehlo.dot_general %151, %152, contracting_dims = [1] x [0] : (tensor<13x4096xbf16>, tensor<4096x2880xbf16>) -> tensor<13x2880xbf16>
      %154 = sdy.all_reduce {"_axis_0"} %153 out_sharding=<@mesh, [{}, {}]> : tensor<13x2880xbf16>
      %155 = stablehlo.reshape %154 : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %156 = stablehlo.broadcast_in_dim %arg44, dims = [2] : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
      %157 = stablehlo.add %155, %156 : tensor<1x13x2880xbf16>
      %158 = stablehlo.add %12, %157 : tensor<1x13x2880xbf16>
      %159 = stablehlo.broadcast_in_dim %arg60, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %160 = stablehlo.convert %arg52 : (tensor<2880xbf16>) -> tensor<2880xf32>
      %161 = stablehlo.broadcast_in_dim %160, dims = [2] : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %162 = stablehlo.convert %158 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %163 = stablehlo.power %162, %5 : tensor<1x13x2880xf32>
      %164 = stablehlo.reduce(%163 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %165 = stablehlo.multiply %164, %cst_3 : tensor<1x13xf32>
      %166 = stablehlo.reshape %165 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %167 = stablehlo.add %166, %18 : tensor<1x13x1xf32>
      %168 = stablehlo.rsqrt %167 : tensor<1x13x1xf32>
      %169 = stablehlo.reshape %168 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %170 = stablehlo.broadcast_in_dim %169, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %171 = stablehlo.multiply %162, %170 : tensor<1x13x2880xf32>
      %172 = stablehlo.multiply %161, %171 : tensor<1x13x2880xf32>
      %173 = stablehlo.convert %172 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %174 = stablehlo.reshape %173 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %175 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %176 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %177 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %178 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %179 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %180 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %181 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %182 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %183 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %184 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %185 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %186 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %187 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %188 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %189 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %190 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %191 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %192 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %193 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %194 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %195 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %196 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %197 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %198 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %199 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %200 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %201 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %202 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %203 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %204 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %205 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %206 = sdy.all_slice [{"_axis_0"}, {}] %174 out_sharding=<@mesh, [{"_axis_0"}, {}]> : tensor<13x2880xbf16>
      %207 = stablehlo.concatenate %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196, %197, %198, %199, %200, %201, %202, %203, %204, %205, %206, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<416x2880xbf16>
      %208 = stablehlo.reshape %207 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<416x2880xbf16>) -> tensor<32x13x2880xbf16>
      %209 = stablehlo.dot_general %208, %arg59, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x5760xbf16>) -> tensor<32x13x5760xbf16>
      %210 = stablehlo.broadcast_in_dim %arg58, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x5760xbf16>) -> tensor<32x13x5760xbf16>
      %211 = stablehlo.add %209, %210 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x5760xbf16>
      %212 = stablehlo.slice %211 [0:32, 0:13, 1:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
      %213 = stablehlo.broadcast_in_dim %arg56, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %214 = stablehlo.clamp %159, %212, %213 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %215 = stablehlo.add %214, %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %216 = stablehlo.convert %215 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %217 = stablehlo.broadcast_in_dim %arg57, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<32x13x2880xbf16>
      %218 = stablehlo.slice %211 [0:32, 0:13, 0:5760:2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x5760xbf16>) -> tensor<32x13x2880xbf16>
      %219 = stablehlo.clamp %217, %218, %213 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %220 = stablehlo.convert %219 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %221 = stablehlo.broadcast_in_dim %arg55, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<f32>) -> tensor<32x13x2880xf32>
      %222 = stablehlo.multiply %220, %221 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
      %223 = stablehlo.convert %222 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
      %224 = stablehlo.logistic %223 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %225 = stablehlo.convert %224 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %226 = stablehlo.multiply %220, %225 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
      %227 = stablehlo.convert %226 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
      %228 = stablehlo.convert %227 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %229 = stablehlo.multiply %216, %228 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xf32>
      %230 = stablehlo.convert %229 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x13x2880xbf16>
      %231 = stablehlo.dot_general %230, %arg54, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>, tensor<32x2880x2880xbf16>) -> tensor<32x13x2880xbf16>
      %232 = stablehlo.broadcast_in_dim %arg53, dims = [0, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x2880xbf16>) -> tensor<32x13x2880xbf16>
      %233 = stablehlo.add %231, %232 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<32x13x2880xbf16>
      %234 = stablehlo.convert %233 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13x2880xbf16>) -> tensor<32x13x2880xf32>
      %235 = stablehlo.reshape %234 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x13x2880xf32>) -> tensor<32x1x13x2880xf32>
      %236 = stablehlo.iota dim = 0 : tensor<13xi64>
      %237 = stablehlo.broadcast_in_dim %236, dims = [0] : (tensor<13xi64>) -> tensor<13x4x1xi64>
      %238 = stablehlo.transpose %arg43, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %239 = stablehlo.dot_general %174, %238, contracting_dims = [1] x [0] : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
      %240 = stablehlo.broadcast_in_dim %arg42, dims = [1] : (tensor<32xbf16>) -> tensor<13x32xbf16>
      %241 = stablehlo.add %239, %240 : tensor<13x32xbf16>
      %242 = stablehlo.iota dim = 0 : tensor<32xi32>
      %243 = stablehlo.broadcast_in_dim %242, dims = [1] : (tensor<32xi32>) -> tensor<13x32xi32>
      %244:2 = "stablehlo.sort"(%241, %243) <{dimension = 1 : i64}> ({
      ^bb0(%arg62: tensor<bf16>, %arg63: tensor<bf16>, %arg64: tensor<i32>, %arg65: tensor<i32>):
        %283 = stablehlo.compare  GT, %arg62, %arg63,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        stablehlo.return %283 : tensor<i1>
      }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
      %245 = stablehlo.slice %244#1 [0:13, 0:4] : (tensor<13x32xi32>) -> tensor<13x4xi32>
      %246 = stablehlo.convert %245 : (tensor<13x4xi32>) -> tensor<13x4xi64>
      %247 = stablehlo.reshape %246 : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
      %248 = stablehlo.concatenate %237, %247, dim = 2 : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
      %249 = stablehlo.slice %244#0 [0:13, 0:4] : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
      %250 = stablehlo.reduce(%249 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
      %251 = stablehlo.broadcast_in_dim %250, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
      %252 = stablehlo.subtract %249, %251 : tensor<13x4xbf16>
      %253 = stablehlo.exponential %252 : tensor<13x4xbf16>
      %254 = stablehlo.reduce(%253 init: %cst_7) applies stablehlo.add across dimensions = [1] : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
      %255 = stablehlo.broadcast_in_dim %254, dims = [0] : (tensor<13xbf16>) -> tensor<13x4xbf16>
      %256 = stablehlo.divide %253, %255 : tensor<13x4xbf16>
      %257 = "stablehlo.scatter"(%1, %248, %256) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg62: tensor<bf16>, %arg63: tensor<bf16>):
        stablehlo.return %arg63 : tensor<bf16>
      }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
      %258 = stablehlo.convert %257 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x32xbf16>) -> tensor<13x32xf32>
      %259 = stablehlo.transpose %258, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
      %260 = stablehlo.reshape %259 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
      %261 = stablehlo.broadcast_in_dim %260, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
      %262 = stablehlo.multiply %235, %261 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<32x1x13x2880xf32>
      %263 = stablehlo.convert %262 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
      %264 = stablehlo.reduce(%263 init: %cst_7) applies stablehlo.add across dimensions = [0] : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
      %265 = sdy.all_reduce {"_axis_0"} %264 out_sharding=<@mesh, [{}, {}, {}]> : tensor<1x13x2880xbf16>
      %266 = stablehlo.add %158, %265 : tensor<1x13x2880xbf16>
      %267 = stablehlo.convert %266 : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %268 = stablehlo.power %267, %5 : tensor<1x13x2880xf32>
      %269 = stablehlo.reduce(%268 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %270 = stablehlo.multiply %269, %cst_3 : tensor<1x13xf32>
      %271 = stablehlo.reshape %270 : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %272 = stablehlo.add %271, %18 : tensor<1x13x1xf32>
      %273 = stablehlo.rsqrt %272 : tensor<1x13x1xf32>
      %274 = stablehlo.reshape %273 : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %275 = stablehlo.broadcast_in_dim %274, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %276 = stablehlo.multiply %267, %275 : tensor<1x13x2880xf32>
      %277 = stablehlo.multiply %133, %276 : tensor<1x13x2880xf32>
      %278 = stablehlo.convert %277 : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %279 = stablehlo.reshape %278 : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %280 = stablehlo.transpose %arg41, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<201088x2880xbf16>) -> tensor<2880x201088xbf16>
      %281 = stablehlo.dot_general %279, %280, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
      %282 = stablehlo.reshape %281 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
      sdy.return %129, %130, %131, %89, %281, %282 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
    } : (tensor<512xbf16>, tensor<512x2880xbf16>, tensor<f32>, tensor<1x13xi64>, tensor<201088x2880xbf16>, tensor<2880xbf16>, tensor<f32>, tensor<32xf32>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<201088x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<bf16>, tensor<i64>, tensor<f32>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>, tensor<2880xbf16>) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>)
    return %0#0, %0#1, %0#2, %0#3, %0#4, %0#5 : tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>
  }
}


loc("scatter.460"): error: Scatter operation is not supported in stablehlo-pipeline for meshes not 1x1: https://github.com/tenstorrent/tt-mlir/issues/3496.
loc("scatter.460"): error: Could not updated attribute dictionary for operation
loc("scatter.460"): error: Could not create a new operation with updated shapes
error: Could not update shapes based on their tensor sharding attributes
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("p0.2"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("p8.80"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("p19.236"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("reshape.478"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("p0.2"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("p8.80"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
loc("p19.236"): error: ShardyToStableHLO lowering for AllSliceOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3368.
// -----// IR Dump After UpdateGlobalToLocalShapesPass Failed (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.577) //----- //
"builtin.module"() <{sym_name = "SyncTensorsGraph.577"}> ({
  "sdy.mesh"() <{mesh = #sdy.mesh<["_axis_0_updated"=1, "_axis_0"=8]>, sym_name = "mesh"}> : () -> ()
  "func.func"() <{arg_attrs = [{ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}, {ttcore.shard_status = #ttcore.shard_status<presharded>}], function_type = (tensor<512xbf16>, tensor<512x2880xbf16>, tensor<f32>, tensor<1x13xi64>, tensor<201088x2880xbf16>, tensor<2880xbf16>, tensor<f32>, tensor<32xf32>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<201088x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<bf16>, tensor<i64>, tensor<f32>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>, tensor<2880xbf16>) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>), res_attrs = [{ttcore.shard_status = #ttcore.shard_status<unsharded>}, {ttcore.shard_status = #ttcore.shard_status<unsharded>}, {ttcore.shard_status = #ttcore.shard_status<unsharded>}, {ttcore.shard_status = #ttcore.shard_status<unsharded>}, {ttcore.shard_status = #ttcore.shard_status<unsharded>}, {ttcore.shard_status = #ttcore.shard_status<unsharded>}], sym_name = "main"}> ({
  ^bb0(%arg0: tensor<512xbf16>, %arg1: tensor<512x2880xbf16>, %arg2: tensor<f32>, %arg3: tensor<1x13xi64>, %arg4: tensor<201088x2880xbf16>, %arg5: tensor<2880xbf16>, %arg6: tensor<f32>, %arg7: tensor<32xf32>, %arg8: tensor<512xbf16>, %arg9: tensor<512x2880xbf16>, %arg10: tensor<201088x2880xbf16>, %arg11: tensor<32xbf16>, %arg12: tensor<32x2880xbf16>, %arg13: tensor<2880xbf16>, %arg14: tensor<2880x4096xbf16>, %arg15: tensor<64xbf16>, %arg16: tensor<bf16>, %arg17: tensor<i64>, %arg18: tensor<f32>, %arg19: tensor<4096xbf16>, %arg20: tensor<4096x2880xbf16>, %arg21: tensor<2880xbf16>, %arg22: tensor<32x2880xbf16>, %arg23: tensor<32x2880x2880xbf16>, %arg24: tensor<f32>, %arg25: tensor<bf16>, %arg26: tensor<bf16>, %arg27: tensor<32x5760xbf16>, %arg28: tensor<32x2880x5760xbf16>, %arg29: tensor<bf16>, %arg30: tensor<2880xbf16>):
    %0:6 = "sdy.manual_computation"(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg25, %arg26, %arg27, %arg28, %arg29, %arg30) <{in_shardings = #sdy.sharding_per_value<[<@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, []>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, []>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}, {}, {}]>, <@mesh, []>, <@mesh, [{}]>]>, manual_axes = #sdy<manual_axes{"_axis_0_updated", "_axis_0"}>, out_shardings = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}]>, <@mesh, [{?}, {?}, {"_axis_0", ?}]>]>}> ({
    ^bb0(%arg31: tensor<512xbf16>, %arg32: tensor<64x2880xbf16>, %arg33: tensor<f32>, %arg34: tensor<1x13xi64>, %arg35: tensor<201088x2880xbf16>, %arg36: tensor<2880xbf16>, %arg37: tensor<f32>, %arg38: tensor<32xf32>, %arg39: tensor<512xbf16>, %arg40: tensor<64x2880xbf16>, %arg41: tensor<25136x2880xbf16>, %arg42: tensor<32xbf16>, %arg43: tensor<32x2880xbf16>, %arg44: tensor<2880xbf16>, %arg45: tensor<2880x512xbf16>, %arg46: tensor<8xbf16>, %arg47: tensor<bf16>, %arg48: tensor<i64>, %arg49: tensor<f32>, %arg50: tensor<4096xbf16>, %arg51: tensor<512x2880xbf16>, %arg52: tensor<2880xbf16>, %arg53: tensor<4x2880xbf16>, %arg54: tensor<4x2880x2880xbf16>, %arg55: tensor<f32>, %arg56: tensor<bf16>, %arg57: tensor<bf16>, %arg58: tensor<4x5760xbf16>, %arg59: tensor<4x2880x5760xbf16>, %arg60: tensor<bf16>, %arg61: tensor<2880xbf16>):
      %1 = "stablehlo.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
      %2 = "stablehlo.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]> : tensor<13xi64>}> : () -> tensor<13xi64>
      %3 = "stablehlo.constant"() <{value = dense<0> : tensor<ui8>}> : () -> tensor<ui8>
      %4 = "stablehlo.constant"() <{value = dense<0xFF80> : tensor<bf16>}> : () -> tensor<bf16>
      %5 = "stablehlo.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
      %6 = "stablehlo.constant"() <{value = dense<3.47222231E-4> : tensor<1x13xf32>}> : () -> tensor<1x13xf32>
      %7 = "stablehlo.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01]]]> : tensor<1x1x13xf32>}> : () -> tensor<1x1x13xf32>
      %8 = "stablehlo.constant"() <{value = dense<1> : tensor<ui8>}> : () -> tensor<ui8>
      %9 = "stablehlo.constant"() <{value = dense<1.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
      %10 = "stablehlo.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
      %11 = "stablehlo.broadcast_in_dim"(%10) <{broadcast_dimensions = array<i64>}> : (tensor<bf16>) -> tensor<13x4xbf16>
      %12 = "stablehlo.broadcast_in_dim"(%9) <{broadcast_dimensions = array<i64>}> : (tensor<bf16>) -> tensor<4x13x2880xbf16>
      %13 = "stablehlo.broadcast_in_dim"(%10) <{broadcast_dimensions = array<i64>}> : (tensor<bf16>) -> tensor<1x1x13x13xbf16>
      %14 = "stablehlo.broadcast_in_dim"(%8) <{broadcast_dimensions = array<i64>}> : (tensor<ui8>) -> tensor<13x13xui8>
      %15 = "stablehlo.broadcast_in_dim"(%5) <{broadcast_dimensions = array<i64>}> : (tensor<f32>) -> tensor<1x13x2880xf32>
      %16 = "stablehlo.broadcast_in_dim"(%3) <{broadcast_dimensions = array<i64>}> : (tensor<ui8>) -> tensor<13x13xui8>
      %17 = "stablehlo.convert"(%arg36) : (tensor<2880xbf16>) -> tensor<2880xf32>
      %18 = "stablehlo.broadcast_in_dim"(%17) <{broadcast_dimensions = array<i64: 2>}> : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %19 = "stablehlo.convert"(%arg34) : (tensor<1x13xi64>) -> tensor<1x13xui32>
      %20 = "stablehlo.reshape"(%19) : (tensor<1x13xui32>) -> tensor<13xui32>
      %21 = "stablehlo.gather"(%arg35, %20) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2880>}> : (tensor<201088x2880xbf16>, tensor<13xui32>) -> tensor<13x2880xbf16>
      %22 = "stablehlo.reshape"(%21) : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %23 = "stablehlo.convert"(%22) : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %24 = "stablehlo.power"(%23, %15) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
      %25 = "stablehlo.reduce"(%24, %1) <{dimensions = array<i64: 2>}> ({
      ^bb0(%arg88: tensor<f32>, %arg89: tensor<f32>):
        %304 = "stablehlo.add"(%arg88, %arg89) : (tensor<f32>, tensor<f32>) -> tensor<f32>
        "stablehlo.return"(%304) : (tensor<f32>) -> ()
      }) : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %26 = "stablehlo.multiply"(%25, %6) : (tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
      %27 = "stablehlo.reshape"(%26) : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %28 = "stablehlo.broadcast_in_dim"(%arg33) <{broadcast_dimensions = array<i64>}> : (tensor<f32>) -> tensor<1x13x1xf32>
      %29 = "stablehlo.add"(%27, %28) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
      %30 = "stablehlo.rsqrt"(%29) : (tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
      %31 = "stablehlo.reshape"(%30) : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %32 = "stablehlo.broadcast_in_dim"(%31) <{broadcast_dimensions = array<i64: 0, 1>}> : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %33 = "stablehlo.multiply"(%23, %32) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
      %34 = "stablehlo.multiply"(%18, %33) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
      %35 = "stablehlo.convert"(%34) : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %36 = "stablehlo.reshape"(%35) : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %37 = "stablehlo.transpose"(%arg51) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<512x2880xbf16>) -> tensor<2880x512xbf16>
      %38 = "stablehlo.dot_general"(%36, %37) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<13x2880xbf16>, tensor<2880x512xbf16>) -> tensor<13x512xbf16>
      %39 = "stablehlo.reshape"(%38) : (tensor<13x512xbf16>) -> tensor<1x13x512xbf16>
      %40 = "sdy.all_slice"(%arg50) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}]>}> : (tensor<4096xbf16>) -> tensor<4096xbf16>
      %41 = "stablehlo.broadcast_in_dim"(%40) <{broadcast_dimensions = array<i64: 2>}> : (tensor<4096xbf16>) -> tensor<1x13x512xbf16>
      %42 = "stablehlo.add"(%39, %41) : (tensor<1x13x512xbf16>, tensor<1x13x512xbf16>) -> tensor<1x13x512xbf16>
      %43 = "stablehlo.reshape"(%42) : (tensor<1x13x512xbf16>) -> tensor<1x13x8x64xbf16>
      %44 = "stablehlo.transpose"(%43) <{permutation = array<i64: 0, 2, 1, 3>}> {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,13,64]{3,1,2,0}"} : (tensor<1x13x8x64xbf16>) -> tensor<1x8x13x64xbf16>
      %45 = "stablehlo.slice"(%44) <{limit_indices = array<i64: 1, 8, 13, 32>, start_indices = array<i64: 0, 0, 0, 0>, strides = array<i64: 1, 1, 1, 1>}> : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
      %46 = "stablehlo.convert"(%45) : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
      %47 = "stablehlo.reshape"(%arg38) : (tensor<32xf32>) -> tensor<1x32x1xf32>
      %48 = "stablehlo.dot_general"(%47, %7) <{dot_dimension_numbers = #stablehlo.dot<lhs_batching_dimensions = [0], rhs_batching_dimensions = [0], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1]>}> : (tensor<1x32x1xf32>, tensor<1x1x13xf32>) -> tensor<1x32x13xf32>
      %49 = "stablehlo.transpose"(%48) <{permutation = array<i64: 0, 2, 1>}> {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x32x13xf32>) -> tensor<1x13x32xf32>
      %50 = "stablehlo.cosine"(%49) {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
      %51 = "stablehlo.broadcast_in_dim"(%arg37) <{broadcast_dimensions = array<i64>}> : (tensor<f32>) -> tensor<1x13x32xf32>
      %52 = "stablehlo.multiply"(%50, %51) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
      %53 = "stablehlo.convert"(%52) : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
      %54 = "stablehlo.convert"(%53) : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
      %55 = "stablehlo.broadcast_in_dim"(%54) <{broadcast_dimensions = array<i64: 0, 2, 3>}> : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
      %56 = "stablehlo.multiply"(%46, %55) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
      %57 = "stablehlo.convert"(%56) : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %58 = "stablehlo.slice"(%44) <{limit_indices = array<i64: 1, 8, 13, 64>, start_indices = array<i64: 0, 0, 0, 32>, strides = array<i64: 1, 1, 1, 1>}> : (tensor<1x8x13x64xbf16>) -> tensor<1x8x13x32xbf16>
      %59 = "stablehlo.convert"(%58) : (tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xf32>
      %60 = "stablehlo.sine"(%49) {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,13,32]{1,2,0}"} : (tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
      %61 = "stablehlo.multiply"(%60, %51) : (tensor<1x13x32xf32>, tensor<1x13x32xf32>) -> tensor<1x13x32xf32>
      %62 = "stablehlo.convert"(%61) : (tensor<1x13x32xf32>) -> tensor<1x13x32xbf16>
      %63 = "stablehlo.convert"(%62) : (tensor<1x13x32xbf16>) -> tensor<1x13x32xf32>
      %64 = "stablehlo.broadcast_in_dim"(%63) <{broadcast_dimensions = array<i64: 0, 2, 3>}> : (tensor<1x13x32xf32>) -> tensor<1x8x13x32xf32>
      %65 = "stablehlo.multiply"(%59, %64) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
      %66 = "stablehlo.convert"(%65) : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %67 = "stablehlo.subtract"(%57, %66) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
      %68 = "stablehlo.multiply"(%59, %55) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
      %69 = "stablehlo.convert"(%68) : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %70 = "stablehlo.multiply"(%46, %64) : (tensor<1x8x13x32xf32>, tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xf32>
      %71 = "stablehlo.convert"(%70) : (tensor<1x8x13x32xf32>) -> tensor<1x8x13x32xbf16>
      %72 = "stablehlo.add"(%69, %71) : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x32xbf16>
      %73 = "stablehlo.concatenate"(%67, %72) <{dimension = 3 : i64}> : (tensor<1x8x13x32xbf16>, tensor<1x8x13x32xbf16>) -> tensor<1x8x13x64xbf16>
      %74 = "stablehlo.reshape"(%73) : (tensor<1x8x13x64xbf16>) -> tensor<8x13x64xbf16>
      %75 = "stablehlo.transpose"(%arg40) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<64x2880xbf16>) -> tensor<2880x64xbf16>
      %76 = "stablehlo.dot_general"(%36, %75) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<13x2880xbf16>, tensor<2880x64xbf16>) -> tensor<13x64xbf16>
      %77 = "stablehlo.reshape"(%76) : (tensor<13x64xbf16>) -> tensor<1x13x64xbf16>
      %78 = "sdy.all_slice"(%arg39) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}]>}> : (tensor<512xbf16>) -> tensor<512xbf16>
      %79 = "stablehlo.broadcast_in_dim"(%78) <{broadcast_dimensions = array<i64: 2>}> : (tensor<512xbf16>) -> tensor<1x13x64xbf16>
      %80 = "stablehlo.add"(%77, %79) : (tensor<1x13x64xbf16>, tensor<1x13x64xbf16>) -> tensor<1x13x64xbf16>
      %81 = "stablehlo.reshape"(%80) : (tensor<1x13x64xbf16>) -> tensor<1x13x1x64xbf16>
      %82 = "stablehlo.transpose"(%81) <{permutation = array<i64: 0, 2, 1, 3>}> {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x1x64xbf16>) -> tensor<1x1x13x64xbf16>
      %83 = "stablehlo.slice"(%82) <{limit_indices = array<i64: 1, 1, 13, 32>, start_indices = array<i64: 0, 0, 0, 0>, strides = array<i64: 1, 1, 1, 1>}> : (tensor<1x1x13x64xbf16>) -> tensor<1x1x13x32xbf16>
      %84 = "stablehlo.convert"(%83) : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
      %85 = "stablehlo.broadcast_in_dim"(%54) <{broadcast_dimensions = array<i64: 0, 2, 3>}> : (tensor<1x13x32xf32>) -> tensor<1x1x13x32xf32>
      %86 = "stablehlo.multiply"(%84, %85) : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
      %87 = "stablehlo.convert"(%86) : (tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xbf16>
      %88 = "stablehlo.slice"(%82) <{limit_indices = array<i64: 1, 1, 13, 64>, start_indices = array<i64: 0, 0, 0, 32>, strides = array<i64: 1, 1, 1, 1>}> : (tensor<1x1x13x64xbf16>) -> tensor<1x1x13x32xbf16>
      %89 = "stablehlo.convert"(%88) : (tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xf32>
      %90 = "stablehlo.broadcast_in_dim"(%63) <{broadcast_dimensions = array<i64: 0, 2, 3>}> : (tensor<1x13x32xf32>) -> tensor<1x1x13x32xf32>
      %91 = "stablehlo.multiply"(%89, %90) : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
      %92 = "stablehlo.convert"(%91) : (tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xbf16>
      %93 = "stablehlo.subtract"(%87, %92) : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
      %94 = "stablehlo.multiply"(%89, %85) : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
      %95 = "stablehlo.convert"(%94) : (tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xbf16>
      %96 = "stablehlo.multiply"(%84, %90) : (tensor<1x1x13x32xf32>, tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xf32>
      %97 = "stablehlo.convert"(%96) : (tensor<1x1x13x32xf32>) -> tensor<1x1x13x32xbf16>
      %98 = "stablehlo.add"(%95, %97) : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x32xbf16>
      %99 = "stablehlo.concatenate"(%93, %98) <{dimension = 3 : i64}> : (tensor<1x1x13x32xbf16>, tensor<1x1x13x32xbf16>) -> tensor<1x1x13x64xbf16>
      %100 = "stablehlo.broadcast_in_dim"(%99) <{broadcast_dimensions = array<i64: 0, 1, 3, 4>}> : (tensor<1x1x13x64xbf16>) -> tensor<1x1x8x13x64xbf16>
      %101 = "stablehlo.reshape"(%100) : (tensor<1x1x8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
      %102 = "stablehlo.transpose"(%101) <{permutation = array<i64: 0, 1, 3, 2>}> {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,13]{2,3,1,0}"} : (tensor<1x8x13x64xbf16>) -> tensor<1x8x64x13xbf16>
      %103 = "stablehlo.reshape"(%102) : (tensor<1x8x64x13xbf16>) -> tensor<8x64x13xbf16>
      %104 = "stablehlo.dot_general"(%74, %103) <{dot_dimension_numbers = #stablehlo.dot<lhs_batching_dimensions = [0], rhs_batching_dimensions = [0], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1]>}> : (tensor<8x13x64xbf16>, tensor<8x64x13xbf16>) -> tensor<8x13x13xbf16>
      %105 = "stablehlo.convert"(%104) : (tensor<8x13x13xbf16>) -> tensor<8x13x13xf32>
      %106 = "stablehlo.reshape"(%105) : (tensor<8x13x13xf32>) -> tensor<1x8x13x13xf32>
      %107 = "stablehlo.broadcast_in_dim"(%arg49) <{broadcast_dimensions = array<i64>}> : (tensor<f32>) -> tensor<1x8x13x13xf32>
      %108 = "stablehlo.multiply"(%106, %107) : (tensor<1x8x13x13xf32>, tensor<1x8x13x13xf32>) -> tensor<1x8x13x13xf32>
      %109 = "stablehlo.convert"(%108) : (tensor<1x8x13x13xf32>) -> tensor<1x8x13x13xbf16>
      %110 = "stablehlo.broadcast_in_dim"(%2) <{broadcast_dimensions = array<i64: 1>}> : (tensor<13xi64>) -> tensor<13x13xi64>
      %111 = "stablehlo.broadcast_in_dim"(%arg48) <{broadcast_dimensions = array<i64>}> : (tensor<i64>) -> tensor<13xi64>
      %112 = "stablehlo.subtract"(%2, %111) : (tensor<13xi64>, tensor<13xi64>) -> tensor<13xi64>
      %113 = "stablehlo.broadcast_in_dim"(%112) <{broadcast_dimensions = array<i64: 0>}> : (tensor<13xi64>) -> tensor<13x13xi64>
      %114 = "stablehlo.compare"(%110, %113) <{comparison_direction = #stablehlo<comparison_direction GT>}> : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
      %115 = "stablehlo.convert"(%114) : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %116 = "stablehlo.and"(%115, %14) : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
      %117 = "stablehlo.compare"(%116, %16) <{comparison_direction = #stablehlo<comparison_direction NE>}> : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
      %118 = "stablehlo.convert"(%117) : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %119 = "stablehlo.broadcast_in_dim"(%2) <{broadcast_dimensions = array<i64: 0>}> : (tensor<13xi64>) -> tensor<13x13xi64>
      %120 = "stablehlo.compare"(%110, %119) <{comparison_direction = #stablehlo<comparison_direction LE>}> : (tensor<13x13xi64>, tensor<13x13xi64>) -> tensor<13x13xi1>
      %121 = "stablehlo.convert"(%120) : (tensor<13x13xi1>) -> tensor<13x13xui8>
      %122 = "stablehlo.and"(%118, %121) : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xui8>
      %123 = "stablehlo.compare"(%122, %16) <{comparison_direction = #stablehlo<comparison_direction NE>}> : (tensor<13x13xui8>, tensor<13x13xui8>) -> tensor<13x13xi1>
      %124 = "stablehlo.reshape"(%123) : (tensor<13x13xi1>) -> tensor<1x1x13x13xi1>
      %125 = "stablehlo.reshape"(%arg47) : (tensor<bf16>) -> tensor<1x1xbf16>
      %126 = "stablehlo.broadcast_in_dim"(%125) <{broadcast_dimensions = array<i64: 0, 1>}> : (tensor<1x1xbf16>) -> tensor<1x1x13x13xbf16>
      %127 = "stablehlo.select"(%124, %13, %126) : (tensor<1x1x13x13xi1>, tensor<1x1x13x13xbf16>, tensor<1x1x13x13xbf16>) -> tensor<1x1x13x13xbf16>
      %128 = "stablehlo.reshape"(%127) : (tensor<1x1x13x13xbf16>) -> tensor<1x13x13xbf16>
      %129 = "stablehlo.broadcast_in_dim"(%128) <{broadcast_dimensions = array<i64: 0, 2, 3>}> : (tensor<1x13x13xbf16>) -> tensor<1x8x13x13xbf16>
      %130 = "stablehlo.add"(%109, %129) : (tensor<1x8x13x13xbf16>, tensor<1x8x13x13xbf16>) -> tensor<1x8x13x13xbf16>
      %131 = "stablehlo.reshape"(%arg46) : (tensor<8xbf16>) -> tensor<1x8x1xbf16>
      %132 = "stablehlo.broadcast_in_dim"(%131) <{broadcast_dimensions = array<i64: 0, 1, 3>}> : (tensor<1x8x1xbf16>) -> tensor<1x8x13x1xbf16>
      %133 = "stablehlo.concatenate"(%130, %132) <{dimension = 3 : i64}> : (tensor<1x8x13x13xbf16>, tensor<1x8x13x1xbf16>) -> tensor<1x8x13x14xbf16>
      %134 = "stablehlo.transpose"(%arg32) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<64x2880xbf16>) -> tensor<2880x64xbf16>
      %135 = "stablehlo.dot_general"(%36, %134) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<13x2880xbf16>, tensor<2880x64xbf16>) -> tensor<13x64xbf16>
      %136 = "stablehlo.reshape"(%135) : (tensor<13x64xbf16>) -> tensor<1x13x64xbf16>
      %137 = "sdy.all_slice"(%arg31) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}]>}> : (tensor<512xbf16>) -> tensor<512xbf16>
      %138 = "stablehlo.broadcast_in_dim"(%137) <{broadcast_dimensions = array<i64: 2>}> : (tensor<512xbf16>) -> tensor<1x13x64xbf16>
      %139 = "stablehlo.add"(%136, %138) : (tensor<1x13x64xbf16>, tensor<1x13x64xbf16>) -> tensor<1x13x64xbf16>
      %140 = "stablehlo.reshape"(%139) : (tensor<1x13x64xbf16>) -> tensor<1x13x1x64xbf16>
      %141 = "stablehlo.transpose"(%140) <{permutation = array<i64: 0, 2, 1, 3>}> {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,13,64]{3,1,2,0}"} : (tensor<1x13x1x64xbf16>) -> tensor<1x1x13x64xbf16>
      %142 = "stablehlo.convert"(%arg61) : (tensor<2880xbf16>) -> tensor<2880xf32>
      %143 = "stablehlo.broadcast_in_dim"(%142) <{broadcast_dimensions = array<i64: 2>}> : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %144 = "stablehlo.reduce"(%133, %4) <{dimensions = array<i64: 3>}> ({
      ^bb0(%arg86: tensor<bf16>, %arg87: tensor<bf16>):
        %303 = "stablehlo.maximum"(%arg86, %arg87) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%303) : (tensor<bf16>) -> ()
      }) : (tensor<1x8x13x14xbf16>, tensor<bf16>) -> tensor<1x8x13xbf16>
      %145 = "stablehlo.broadcast_in_dim"(%144) <{broadcast_dimensions = array<i64: 0, 1, 2>}> : (tensor<1x8x13xbf16>) -> tensor<1x8x13x14xbf16>
      %146 = "stablehlo.subtract"(%133, %145) : (tensor<1x8x13x14xbf16>, tensor<1x8x13x14xbf16>) -> tensor<1x8x13x14xbf16>
      %147 = "stablehlo.reduce"(%146, %4) <{dimensions = array<i64: 3>}> ({
      ^bb0(%arg84: tensor<bf16>, %arg85: tensor<bf16>):
        %302 = "stablehlo.maximum"(%arg84, %arg85) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%302) : (tensor<bf16>) -> ()
      }) : (tensor<1x8x13x14xbf16>, tensor<bf16>) -> tensor<1x8x13xbf16>
      %148 = "stablehlo.broadcast_in_dim"(%147) <{broadcast_dimensions = array<i64: 0, 1, 2>}> : (tensor<1x8x13xbf16>) -> tensor<1x8x13x14xbf16>
      %149 = "stablehlo.subtract"(%146, %148) : (tensor<1x8x13x14xbf16>, tensor<1x8x13x14xbf16>) -> tensor<1x8x13x14xbf16>
      %150 = "stablehlo.exponential"(%149) : (tensor<1x8x13x14xbf16>) -> tensor<1x8x13x14xbf16>
      %151 = "stablehlo.reduce"(%150, %10) <{dimensions = array<i64: 3>}> ({
      ^bb0(%arg82: tensor<bf16>, %arg83: tensor<bf16>):
        %301 = "stablehlo.add"(%arg82, %arg83) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%301) : (tensor<bf16>) -> ()
      }) : (tensor<1x8x13x14xbf16>, tensor<bf16>) -> tensor<1x8x13xbf16>
      %152 = "stablehlo.broadcast_in_dim"(%151) <{broadcast_dimensions = array<i64: 0, 1, 2>}> : (tensor<1x8x13xbf16>) -> tensor<1x8x13x14xbf16>
      %153 = "stablehlo.divide"(%150, %152) : (tensor<1x8x13x14xbf16>, tensor<1x8x13x14xbf16>) -> tensor<1x8x13x14xbf16>
      %154 = "stablehlo.slice"(%153) <{limit_indices = array<i64: 1, 8, 13, 13>, start_indices = array<i64: 0, 0, 0, 0>, strides = array<i64: 1, 1, 1, 1>}> : (tensor<1x8x13x14xbf16>) -> tensor<1x8x13x13xbf16>
      %155 = "stablehlo.reshape"(%154) : (tensor<1x8x13x13xbf16>) -> tensor<8x13x13xbf16>
      %156 = "stablehlo.broadcast_in_dim"(%141) <{broadcast_dimensions = array<i64: 0, 1, 3, 4>}> : (tensor<1x1x13x64xbf16>) -> tensor<1x1x8x13x64xbf16>
      %157 = "stablehlo.reshape"(%156) : (tensor<1x1x8x13x64xbf16>) -> tensor<8x13x64xbf16>
      %158 = "stablehlo.dot_general"(%155, %157) <{dot_dimension_numbers = #stablehlo.dot<lhs_batching_dimensions = [0], rhs_batching_dimensions = [0], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1]>}> : (tensor<8x13x13xbf16>, tensor<8x13x64xbf16>) -> tensor<8x13x64xbf16>
      %159 = "stablehlo.reshape"(%158) : (tensor<8x13x64xbf16>) -> tensor<1x8x13x64xbf16>
      %160 = "stablehlo.transpose"(%159) <{permutation = array<i64: 0, 2, 1, 3>}> {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,13,64,64]{3,1,2,0}"} : (tensor<1x8x13x64xbf16>) -> tensor<1x13x8x64xbf16>
      %161 = "stablehlo.reshape"(%160) : (tensor<1x13x8x64xbf16>) -> tensor<13x512xbf16>
      %162 = "stablehlo.transpose"(%arg45) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<2880x512xbf16>) -> tensor<512x2880xbf16>
      %163 = "stablehlo.dot_general"(%161, %162) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<13x512xbf16>, tensor<512x2880xbf16>) -> tensor<13x2880xbf16>
      %164 = "stablehlo.all_reduce"(%163) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg80: tensor<bf16>, %arg81: tensor<bf16>):
        %300 = "stablehlo.add"(%arg80, %arg81) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%300) : (tensor<bf16>) -> ()
      }) : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %165 = "stablehlo.reshape"(%164) : (tensor<13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %166 = "stablehlo.broadcast_in_dim"(%arg44) <{broadcast_dimensions = array<i64: 2>}> : (tensor<2880xbf16>) -> tensor<1x13x2880xbf16>
      %167 = "stablehlo.add"(%165, %166) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %168 = "stablehlo.add"(%22, %167) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %169 = "stablehlo.broadcast_in_dim"(%arg60) <{broadcast_dimensions = array<i64>}> : (tensor<bf16>) -> tensor<4x13x2880xbf16>
      %170 = "stablehlo.convert"(%arg52) : (tensor<2880xbf16>) -> tensor<2880xf32>
      %171 = "stablehlo.broadcast_in_dim"(%170) <{broadcast_dimensions = array<i64: 2>}> : (tensor<2880xf32>) -> tensor<1x13x2880xf32>
      %172 = "stablehlo.convert"(%168) : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %173 = "stablehlo.power"(%172, %15) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
      %174 = "stablehlo.reduce"(%173, %1) <{dimensions = array<i64: 2>}> ({
      ^bb0(%arg78: tensor<f32>, %arg79: tensor<f32>):
        %299 = "stablehlo.add"(%arg78, %arg79) : (tensor<f32>, tensor<f32>) -> tensor<f32>
        "stablehlo.return"(%299) : (tensor<f32>) -> ()
      }) : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %175 = "stablehlo.multiply"(%174, %6) : (tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
      %176 = "stablehlo.reshape"(%175) : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %177 = "stablehlo.add"(%176, %28) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
      %178 = "stablehlo.rsqrt"(%177) : (tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
      %179 = "stablehlo.reshape"(%178) : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %180 = "stablehlo.broadcast_in_dim"(%179) <{broadcast_dimensions = array<i64: 0, 1>}> : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %181 = "stablehlo.multiply"(%172, %180) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
      %182 = "stablehlo.multiply"(%171, %181) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
      %183 = "stablehlo.convert"(%182) : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %184 = "stablehlo.reshape"(%183) : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %185 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %186 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %187 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %188 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %189 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %190 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %191 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %192 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %193 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %194 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %195 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %196 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %197 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %198 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %199 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %200 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %201 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %202 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %203 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %204 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %205 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %206 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %207 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %208 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %209 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %210 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %211 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %212 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %213 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %214 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %215 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %216 = "sdy.all_slice"(%184) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, slicing_axes = #sdy<list_of_axis_ref_lists[{"_axis_0"}, {}]>}> : (tensor<13x2880xbf16>) -> tensor<13x2880xbf16>
      %217 = "stablehlo.concatenate"(%185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196, %197, %198, %199, %200, %201, %202, %203, %204, %205, %206, %207, %208, %209, %210, %211, %212, %213, %214, %215, %216) <{dimension = 0 : i64}> : (tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>, tensor<13x2880xbf16>) -> tensor<52x2880xbf16>
      %218 = "stablehlo.reshape"(%217) : (tensor<52x2880xbf16>) -> tensor<4x13x2880xbf16>
      %219 = "stablehlo.dot_general"(%218, %arg59) <{dot_dimension_numbers = #stablehlo.dot<lhs_batching_dimensions = [0], rhs_batching_dimensions = [0], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1]>}> : (tensor<4x13x2880xbf16>, tensor<4x2880x5760xbf16>) -> tensor<4x13x5760xbf16>
      %220 = "stablehlo.broadcast_in_dim"(%arg58) <{broadcast_dimensions = array<i64: 0, 2>}> : (tensor<4x5760xbf16>) -> tensor<4x13x5760xbf16>
      %221 = "stablehlo.add"(%219, %220) : (tensor<4x13x5760xbf16>, tensor<4x13x5760xbf16>) -> tensor<4x13x5760xbf16>
      %222 = "stablehlo.slice"(%221) <{limit_indices = array<i64: 4, 13, 5760>, start_indices = array<i64: 0, 0, 1>, strides = array<i64: 1, 1, 2>}> : (tensor<4x13x5760xbf16>) -> tensor<4x13x2880xbf16>
      %223 = "stablehlo.broadcast_in_dim"(%arg56) <{broadcast_dimensions = array<i64>}> : (tensor<bf16>) -> tensor<4x13x2880xbf16>
      %224 = "stablehlo.clamp"(%169, %222, %223) : (tensor<4x13x2880xbf16>, tensor<4x13x2880xbf16>, tensor<4x13x2880xbf16>) -> tensor<4x13x2880xbf16>
      %225 = "stablehlo.add"(%224, %12) : (tensor<4x13x2880xbf16>, tensor<4x13x2880xbf16>) -> tensor<4x13x2880xbf16>
      %226 = "stablehlo.convert"(%225) : (tensor<4x13x2880xbf16>) -> tensor<4x13x2880xf32>
      %227 = "stablehlo.broadcast_in_dim"(%arg57) <{broadcast_dimensions = array<i64>}> : (tensor<bf16>) -> tensor<4x13x2880xbf16>
      %228 = "stablehlo.slice"(%221) <{limit_indices = array<i64: 4, 13, 5760>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 2>}> : (tensor<4x13x5760xbf16>) -> tensor<4x13x2880xbf16>
      %229 = "stablehlo.clamp"(%227, %228, %223) : (tensor<4x13x2880xbf16>, tensor<4x13x2880xbf16>, tensor<4x13x2880xbf16>) -> tensor<4x13x2880xbf16>
      %230 = "stablehlo.convert"(%229) : (tensor<4x13x2880xbf16>) -> tensor<4x13x2880xf32>
      %231 = "stablehlo.broadcast_in_dim"(%arg55) <{broadcast_dimensions = array<i64>}> : (tensor<f32>) -> tensor<4x13x2880xf32>
      %232 = "stablehlo.multiply"(%230, %231) : (tensor<4x13x2880xf32>, tensor<4x13x2880xf32>) -> tensor<4x13x2880xf32>
      %233 = "stablehlo.convert"(%232) : (tensor<4x13x2880xf32>) -> tensor<4x13x2880xbf16>
      %234 = "stablehlo.logistic"(%233) : (tensor<4x13x2880xbf16>) -> tensor<4x13x2880xbf16>
      %235 = "stablehlo.convert"(%234) : (tensor<4x13x2880xbf16>) -> tensor<4x13x2880xf32>
      %236 = "stablehlo.multiply"(%230, %235) : (tensor<4x13x2880xf32>, tensor<4x13x2880xf32>) -> tensor<4x13x2880xf32>
      %237 = "stablehlo.convert"(%236) : (tensor<4x13x2880xf32>) -> tensor<4x13x2880xbf16>
      %238 = "stablehlo.convert"(%237) : (tensor<4x13x2880xbf16>) -> tensor<4x13x2880xf32>
      %239 = "stablehlo.multiply"(%226, %238) : (tensor<4x13x2880xf32>, tensor<4x13x2880xf32>) -> tensor<4x13x2880xf32>
      %240 = "stablehlo.convert"(%239) : (tensor<4x13x2880xf32>) -> tensor<4x13x2880xbf16>
      %241 = "stablehlo.dot_general"(%240, %arg54) <{dot_dimension_numbers = #stablehlo.dot<lhs_batching_dimensions = [0], rhs_batching_dimensions = [0], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1]>}> : (tensor<4x13x2880xbf16>, tensor<4x2880x2880xbf16>) -> tensor<4x13x2880xbf16>
      %242 = "stablehlo.broadcast_in_dim"(%arg53) <{broadcast_dimensions = array<i64: 0, 2>}> : (tensor<4x2880xbf16>) -> tensor<4x13x2880xbf16>
      %243 = "stablehlo.add"(%241, %242) : (tensor<4x13x2880xbf16>, tensor<4x13x2880xbf16>) -> tensor<4x13x2880xbf16>
      %244 = "stablehlo.convert"(%243) : (tensor<4x13x2880xbf16>) -> tensor<4x13x2880xf32>
      %245 = "stablehlo.reshape"(%244) : (tensor<4x13x2880xf32>) -> tensor<4x1x13x2880xf32>
      %246 = "stablehlo.iota"() <{iota_dimension = 0 : i64}> : () -> tensor<13xi64>
      %247 = "stablehlo.broadcast_in_dim"(%246) <{broadcast_dimensions = array<i64: 0>}> : (tensor<13xi64>) -> tensor<13x4x1xi64>
      %248 = "stablehlo.transpose"(%arg43) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x2880xbf16>) -> tensor<2880x32xbf16>
      %249 = "stablehlo.dot_general"(%184, %248) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<13x2880xbf16>, tensor<2880x32xbf16>) -> tensor<13x32xbf16>
      %250 = "stablehlo.broadcast_in_dim"(%arg42) <{broadcast_dimensions = array<i64: 1>}> : (tensor<32xbf16>) -> tensor<13x32xbf16>
      %251 = "stablehlo.add"(%249, %250) : (tensor<13x32xbf16>, tensor<13x32xbf16>) -> tensor<13x32xbf16>
      %252 = "stablehlo.iota"() <{iota_dimension = 0 : i64}> : () -> tensor<32xi32>
      %253 = "stablehlo.broadcast_in_dim"(%252) <{broadcast_dimensions = array<i64: 1>}> : (tensor<32xi32>) -> tensor<13x32xi32>
      %254:2 = "stablehlo.sort"(%251, %253) <{dimension = 1 : i64}> ({
      ^bb0(%arg74: tensor<bf16>, %arg75: tensor<bf16>, %arg76: tensor<i32>, %arg77: tensor<i32>):
        %298 = "stablehlo.compare"(%arg74, %arg75) <{compare_type = #stablehlo<comparison_type TOTALORDER>, comparison_direction = #stablehlo<comparison_direction GT>}> : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
        "stablehlo.return"(%298) : (tensor<i1>) -> ()
      }) : (tensor<13x32xbf16>, tensor<13x32xi32>) -> (tensor<13x32xbf16>, tensor<13x32xi32>)
      %255 = "stablehlo.slice"(%254#1) <{limit_indices = array<i64: 13, 4>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>}> : (tensor<13x32xi32>) -> tensor<13x4xi32>
      %256 = "stablehlo.convert"(%255) : (tensor<13x4xi32>) -> tensor<13x4xi64>
      %257 = "stablehlo.reshape"(%256) : (tensor<13x4xi64>) -> tensor<13x4x1xi64>
      %258 = "stablehlo.concatenate"(%247, %257) <{dimension = 2 : i64}> : (tensor<13x4x1xi64>, tensor<13x4x1xi64>) -> tensor<13x4x2xi64>
      %259 = "stablehlo.slice"(%254#0) <{limit_indices = array<i64: 13, 4>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>}> : (tensor<13x32xbf16>) -> tensor<13x4xbf16>
      %260 = "stablehlo.reduce"(%259, %4) <{dimensions = array<i64: 1>}> ({
      ^bb0(%arg72: tensor<bf16>, %arg73: tensor<bf16>):
        %297 = "stablehlo.maximum"(%arg72, %arg73) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%297) : (tensor<bf16>) -> ()
      }) : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
      %261 = "stablehlo.broadcast_in_dim"(%260) <{broadcast_dimensions = array<i64: 0>}> : (tensor<13xbf16>) -> tensor<13x4xbf16>
      %262 = "stablehlo.subtract"(%259, %261) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
      %263 = "stablehlo.exponential"(%262) : (tensor<13x4xbf16>) -> tensor<13x4xbf16>
      %264 = "stablehlo.reduce"(%263, %10) <{dimensions = array<i64: 1>}> ({
      ^bb0(%arg70: tensor<bf16>, %arg71: tensor<bf16>):
        %296 = "stablehlo.add"(%arg70, %arg71) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%296) : (tensor<bf16>) -> ()
      }) : (tensor<13x4xbf16>, tensor<bf16>) -> tensor<13xbf16>
      %265 = "stablehlo.broadcast_in_dim"(%264) <{broadcast_dimensions = array<i64: 0>}> : (tensor<13xbf16>) -> tensor<13x4xbf16>
      %266 = "stablehlo.divide"(%263, %265) : (tensor<13x4xbf16>, tensor<13x4xbf16>) -> tensor<13x4xbf16>
      %267 = "stablehlo.scatter"(%11, %258, %266) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg68: tensor<bf16>, %arg69: tensor<bf16>):
        "stablehlo.return"(%arg69) : (tensor<bf16>) -> ()
      }) : (tensor<13x4xbf16>, tensor<13x4x2xi64>, tensor<13x4xbf16>) -> tensor<13x32xbf16>
      %268 = "stablehlo.convert"(%267) : (tensor<13x32xbf16>) -> tensor<13x32xf32>
      %269 = "stablehlo.transpose"(%268) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[32,13]{0,1}"} : (tensor<13x32xf32>) -> tensor<32x13xf32>
      %270 = "stablehlo.reshape"(%269) : (tensor<32x13xf32>) -> tensor<32x1x13xf32>
      %271 = "stablehlo.broadcast_in_dim"(%270) <{broadcast_dimensions = array<i64: 0, 1, 2>}> : (tensor<32x1x13xf32>) -> tensor<32x1x13x2880xf32>
      %272 = "stablehlo.multiply"(%245, %271) : (tensor<4x1x13x2880xf32>, tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xf32>
      %273 = "stablehlo.convert"(%272) : (tensor<32x1x13x2880xf32>) -> tensor<32x1x13x2880xbf16>
      %274 = "stablehlo.reduce"(%273, %10) <{dimensions = array<i64: 0>}> ({
      ^bb0(%arg66: tensor<bf16>, %arg67: tensor<bf16>):
        %295 = "stablehlo.add"(%arg66, %arg67) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%295) : (tensor<bf16>) -> ()
      }) : (tensor<32x1x13x2880xbf16>, tensor<bf16>) -> tensor<1x13x2880xbf16>
      %275 = "stablehlo.all_reduce"(%274) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg64: tensor<bf16>, %arg65: tensor<bf16>):
        %294 = "stablehlo.add"(%arg64, %arg65) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%294) : (tensor<bf16>) -> ()
      }) : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %276 = "stablehlo.add"(%168, %275) : (tensor<1x13x2880xbf16>, tensor<1x13x2880xbf16>) -> tensor<1x13x2880xbf16>
      %277 = "stablehlo.convert"(%276) : (tensor<1x13x2880xbf16>) -> tensor<1x13x2880xf32>
      %278 = "stablehlo.power"(%277, %15) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
      %279 = "stablehlo.reduce"(%278, %1) <{dimensions = array<i64: 2>}> ({
      ^bb0(%arg62: tensor<f32>, %arg63: tensor<f32>):
        %293 = "stablehlo.add"(%arg62, %arg63) : (tensor<f32>, tensor<f32>) -> tensor<f32>
        "stablehlo.return"(%293) : (tensor<f32>) -> ()
      }) : (tensor<1x13x2880xf32>, tensor<f32>) -> tensor<1x13xf32>
      %280 = "stablehlo.multiply"(%279, %6) : (tensor<1x13xf32>, tensor<1x13xf32>) -> tensor<1x13xf32>
      %281 = "stablehlo.reshape"(%280) : (tensor<1x13xf32>) -> tensor<1x13x1xf32>
      %282 = "stablehlo.add"(%281, %28) : (tensor<1x13x1xf32>, tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
      %283 = "stablehlo.rsqrt"(%282) : (tensor<1x13x1xf32>) -> tensor<1x13x1xf32>
      %284 = "stablehlo.reshape"(%283) : (tensor<1x13x1xf32>) -> tensor<1x13xf32>
      %285 = "stablehlo.broadcast_in_dim"(%284) <{broadcast_dimensions = array<i64: 0, 1>}> : (tensor<1x13xf32>) -> tensor<1x13x2880xf32>
      %286 = "stablehlo.multiply"(%277, %285) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
      %287 = "stablehlo.multiply"(%143, %286) : (tensor<1x13x2880xf32>, tensor<1x13x2880xf32>) -> tensor<1x13x2880xf32>
      %288 = "stablehlo.convert"(%287) : (tensor<1x13x2880xf32>) -> tensor<1x13x2880xbf16>
      %289 = "stablehlo.reshape"(%288) : (tensor<1x13x2880xbf16>) -> tensor<13x2880xbf16>
      %290 = "stablehlo.transpose"(%arg41) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<25136x2880xbf16>) -> tensor<2880x201088xbf16>
      %291 = "stablehlo.dot_general"(%289, %290) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<13x2880xbf16>, tensor<2880x201088xbf16>) -> tensor<13x201088xbf16>
      %292 = "stablehlo.reshape"(%291) : (tensor<13x201088xbf16>) -> tensor<1x13x201088xbf16>
      "sdy.return"(%139, %140, %141, %99, %291, %292) : (tensor<1x13x64xbf16>, tensor<1x13x1x64xbf16>, tensor<1x1x13x64xbf16>, tensor<1x1x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> ()
    }) : (tensor<512xbf16>, tensor<512x2880xbf16>, tensor<f32>, tensor<1x13xi64>, tensor<201088x2880xbf16>, tensor<2880xbf16>, tensor<f32>, tensor<32xf32>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<201088x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<bf16>, tensor<i64>, tensor<f32>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<f32>, tensor<bf16>, tensor<bf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<bf16>, tensor<2880xbf16>) -> (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>)
    "func.return"(%0#0, %0#1, %0#2, %0#3, %0#4, %0#5) : (tensor<1x13x512xbf16>, tensor<1x13x8x64xbf16>, tensor<1x8x13x64xbf16>, tensor<1x8x13x64xbf16>, tensor<13x201088xbf16>, tensor<1x13x201088xbf16>) -> ()
  }) : () -> ()
}) {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} : () -> ()


Running Tensor Parallel Inference
Tensor parallel sharding applied successfully!
Preparing inputs for TP: batch_size=1, seq_length=13
Error during execution: Error code: 13
