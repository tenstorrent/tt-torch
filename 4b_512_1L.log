WARNING:root:Defaulting to PJRT_DEVICE=CPU
============================= test session starts ==============================
platform linux -- Python 3.11.13, pytest-8.4.2, pluggy-1.6.0 -- /localdev/ssalice/tt-torch/env/venv/bin/python3.11
cachedir: .pytest_cache
rootdir: /localdev/ssalice/tt-torch
configfile: pytest.ini
plugins: cov-7.0.0, forked-1.6.0, split-0.10.0, xdist-3.8.0
collecting ... Cannot import path: /localdev/ssalice/tt-torch/third_party/tt_forge_models/yoloworld/pytorch/loader.py: libGL.so.1: cannot open shared object file: No such file or directory
Cannot import path: /localdev/ssalice/tt-torch/third_party/tt_forge_models/yolov10/pytorch/loader.py: libGL.so.1: cannot open shared object file: No such file or directory
Cannot import path: /localdev/ssalice/tt-torch/third_party/tt_forge_models/yolov9/pytorch/loader.py: libGL.so.1: cannot open shared object file: No such file or directory
Cannot import path: /localdev/ssalice/tt-torch/third_party/tt_forge_models/yolov8/pytorch/loader.py: libGL.so.1: cannot open shared object file: No such file or directory
Cannot import path: /localdev/ssalice/tt-torch/third_party/tt_forge_models/yolov6/pytorch/loader.py: libGL.so.1: cannot open shared object file: No such file or directory
Cannot import path: /localdev/ssalice/tt-torch/third_party/tt_forge_models/rcnn/pytorch/loader.py: libGL.so.1: cannot open shared object file: No such file or directory
Cannot import path: /localdev/ssalice/tt-torch/third_party/tt_forge_models/yolov4/pytorch/loader.py: libGL.so.1: cannot open shared object file: No such file or directory
collected 1 item

tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval] 
Running tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval]
model_name: pytorch_qwen_3_embedding_embedding_4b_nlp_embed_gen_huggingface status: ModelStatus.EXPECTED_PASSING
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:27<00:27, 27.81s/it]Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:27<00:00, 13.90s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 130.45it/s]
2025-09-23 12:45:23.356628: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
#loc1 = loc("p0.1")
#loc2 = loc("p1.3")
module @SyncTensorsGraph.7 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x512x!vhlo.i32_v1> loc("p0.1"), %arg1: !vhlo.tensor_v1<151665x2560x!vhlo.bf16_v1> loc("p1.3")) -> (!vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1>) {
    %0 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<1x512x!vhlo.i32_v1>) -> !vhlo.tensor_v1<512x!vhlo.i32_v1> loc(#loc3)
    %1 = "vhlo.gather_v2"(%arg1, %0) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 2560]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<151665x2560x!vhlo.bf16_v1>, !vhlo.tensor_v1<512x!vhlo.i32_v1>) -> !vhlo.tensor_v1<512x2560x!vhlo.bf16_v1> loc(#loc4)
    %2 = "vhlo.reshape_v1"(%1) : (!vhlo.tensor_v1<512x2560x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1> loc(#loc5)
    "vhlo.return_v1"(%2) : (!vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("reshape.2")
#loc4 = loc("gather.4")
#loc5 = loc("reshape.5")
#loc1 = loc("p0.1")
#loc2 = loc("p1.3")
module @SyncTensorsGraph.7 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1x512xi32> loc("p0.1"), %arg1: tensor<151665x2560xbf16> loc("p1.3")) -> tensor<1x512x2560xbf16> {
    %0 = stablehlo.reshape %arg0 : (tensor<1x512xi32>) -> tensor<512xi32> loc(#loc3)
    %1 = "stablehlo.gather"(%arg1, %0) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2560>}> : (tensor<151665x2560xbf16>, tensor<512xi32>) -> tensor<512x2560xbf16> loc(#loc4)
    %2 = stablehlo.reshape %1 : (tensor<512x2560xbf16>) -> tensor<1x512x2560xbf16> loc(#loc5)
    return %2 : tensor<1x512x2560xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("reshape.2")
#loc4 = loc("gather.4")
#loc5 = loc("reshape.5")
#loc1 = loc("p0.1")
#loc2 = loc("p1.3")
module @SyncTensorsGraph.7 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1x512xi32> loc("p0.1"), %arg1: tensor<151665x2560xbf16> loc("p1.3")) -> tensor<1x512x2560xbf16> {
    %0 = stablehlo.reshape %arg0 : (tensor<1x512xi32>) -> tensor<512xi32> loc(#loc3)
    %1 = "stablehlo.gather"(%arg1, %0) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2560>}> : (tensor<151665x2560xbf16>, tensor<512xi32>) -> tensor<512x2560xbf16> loc(#loc4)
    %2 = stablehlo.reshape %1 : (tensor<512x2560xbf16>) -> tensor<1x512x2560xbf16> loc(#loc5)
    return %2 : tensor<1x512x2560xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("reshape.2")
#loc4 = loc("gather.4")
#loc5 = loc("reshape.5")
#loc1 = loc("p0.1")
#loc2 = loc("p1.3")
module @SyncTensorsGraph.7 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]> loc(#loc)
  func.func @main(%arg0: tensor<1x512xi32> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p0.1"), %arg1: tensor<151665x2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p1.3")) -> (tensor<1x512x2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = stablehlo.reshape %arg0 : (tensor<1x512xi32>) -> tensor<512xi32> loc(#loc3)
    %1 = "stablehlo.gather"(%arg1, %0) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2560>}> : (tensor<151665x2560xbf16>, tensor<512xi32>) -> tensor<512x2560xbf16> loc(#loc4)
    %2 = stablehlo.reshape %1 : (tensor<512x2560xbf16>) -> tensor<1x512x2560xbf16> loc(#loc5)
    return %2 : tensor<1x512x2560xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("reshape.2")
#loc4 = loc("gather.4")
#loc5 = loc("reshape.5")
#loc1 = loc("p0.1")
#loc2 = loc("p1.3")
module @SyncTensorsGraph.7 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  func.func @main(%arg0: tensor<1x512xi32> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p0.1"), %arg1: tensor<151665x2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p1.3")) -> (tensor<1x512x2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = ttir.empty() : tensor<512xi32> loc(#loc3)
    %1 = "ttir.reshape"(%arg0, %0) <{shape = [512 : i32]}> : (tensor<1x512xi32>, tensor<512xi32>) -> tensor<512xi32> loc(#loc3)
    %2 = ttir.empty() : tensor<512x2560xbf16> loc(#loc4)
    %3 = "ttir.gather"(%arg1, %1, %2) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 2560>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<151665x2560xbf16>, tensor<512xi32>, tensor<512x2560xbf16>) -> tensor<512x2560xbf16> loc(#loc4)
    %4 = ttir.empty() : tensor<1x512x2560xbf16> loc(#loc5)
    %5 = "ttir.reshape"(%3, %4) <{shape = [1 : i32, 512 : i32, 2560 : i32]}> : (tensor<512x2560xbf16>, tensor<1x512x2560xbf16>) -> tensor<1x512x2560xbf16> loc(#loc5)
    return %5 : tensor<1x512x2560xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("reshape.2")
#loc4 = loc("gather.4")
#loc5 = loc("reshape.5")
#dram = #ttnn.buffer_type<dram>
#loc1 = loc("p0.1")
#loc2 = loc("p1.3")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 100352, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073139712, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [1 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4740x80x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4740x80x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, u32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, u32>, #system_memory>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xui32, #system_memory>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xui32, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
module @SyncTensorsGraph.7 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.7 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func @main(%arg0: tensor<1x512xsi32, #ttnn_layout> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p0.1"), %arg1: tensor<151665x2560xbf16, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p1.3")) -> (tensor<1x512x2560xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc6)
        %1 = "ttnn.typecast"(%arg1) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<151665x2560xbf16, #ttnn_layout1>) -> tensor<151665x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout3> loc(#loc3)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<151665x2560xbf16, #ttnn_layout1>) -> () loc(#loc3)
        %2 = "ttnn.reshape"(%arg0) <{shape = [512 : i32]}> : (tensor<1x512xsi32, #ttnn_layout>) -> tensor<512xsi32, #ttnn_layout4> loc(#loc4)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x512xsi32, #ttnn_layout>) -> () loc(#loc4)
        %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<512xsi32, #ttnn_layout4>) -> tensor<512xui32, #ttnn_layout5> loc(#loc6)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<512xsi32, #ttnn_layout4>) -> () loc(#loc6)
        %4 = "ttnn.from_device"(%3) : (tensor<512xui32, #ttnn_layout5>) -> tensor<512xui32, #ttnn_layout6> loc(#loc6)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<512xui32, #ttnn_layout5>) -> () loc(#loc6)
        %5 = "ttnn.to_layout"(%4) <{layout = #ttnn.layout<row_major>}> : (tensor<512xui32, #ttnn_layout6>) -> tensor<512xui32, #ttnn_layout7> loc(#loc6)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<512xui32, #ttnn_layout6>) -> () loc(#loc6)
        %6 = "ttnn.to_device"(%5, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512xui32, #ttnn_layout7>, !ttnn.device) -> tensor<512xui32, #ttnn_layout8> loc(#loc6)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<512xui32, #ttnn_layout7>) -> () loc(#loc6)
        %7 = "ttnn.typecast"(%1) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<151665x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout3>) -> tensor<151665x2560xbf16, #ttnn_layout1> loc(#loc6)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<151665x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout3>) -> () loc(#loc6)
        %8 = "ttnn.embedding"(%6, %7) : (tensor<512xui32, #ttnn_layout8>, tensor<151665x2560xbf16, #ttnn_layout1>) -> tensor<512x2560xbf16, #ttnn_layout9> loc(#loc3)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<151665x2560xbf16, #ttnn_layout1>) -> () loc(#loc3)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<512xui32, #ttnn_layout8>) -> () loc(#loc3)
        %9 = "ttnn.typecast"(%8) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<512x2560xbf16, #ttnn_layout9>) -> tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout10> loc(#loc6)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<512x2560xbf16, #ttnn_layout9>) -> () loc(#loc6)
        %10 = "ttnn.typecast"(%9) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout10>) -> tensor<512x2560xbf16, #ttnn_layout9> loc(#loc5)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout10>) -> () loc(#loc5)
        %11 = "ttnn.reshape"(%10) <{shape = [1 : i32, 512 : i32, 2560 : i32]}> : (tensor<512x2560xbf16, #ttnn_layout9>) -> tensor<1x512x2560xbf16, #ttnn_layout2> loc(#loc5)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<512x2560xbf16, #ttnn_layout9>) -> () loc(#loc5)
        return %11 : tensor<1x512x2560xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("gather.4")
#loc4 = loc("reshape.2")
#loc5 = loc("reshape.5")
#loc6 = loc("gather.4_workaround"(#loc3))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc("gather.4_workaround"("gather.4"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 61 MB FreePerBank: 962 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %1 = "ttnn.typecast"(%arg1) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<151665x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4740x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<151665x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4740x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.4")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 61 MB FreePerBank: 962 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<151665x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4740x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.4")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 94 MB FreePerBank: 929 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %2 = "ttnn.reshape"(%arg0) <{shape = [512 : i32]}> : (tensor<1x512xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.2")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 94 MB FreePerBank: 929 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x512xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.2")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 94 MB FreePerBank: 929 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %3 = "ttnn.typecast"(%2) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<512xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.4_workaround"("gather.4"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 94 MB FreePerBank: 929 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<512xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.4_workaround"("gather.4"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 94 MB FreePerBank: 929 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %4 = "ttnn.from_device"(%3) : (tensor<512xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>> loc("gather.4_workaround"("gather.4"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 94 MB FreePerBank: 929 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<512xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.4_workaround"("gather.4"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 94 MB FreePerBank: 929 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %5 = "ttnn.to_layout"(%4) <{layout = #ttnn.layout<row_major>}> : (tensor<512xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<512xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xui32, #ttnn.buffer_type<system_memory>>>> loc("gather.4_workaround"("gather.4"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 94 MB FreePerBank: 958 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<512xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> () loc("gather.4_workaround"("gather.4"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 94 MB FreePerBank: 958 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %6 = "ttnn.to_device"(%5, %0) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<512xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<512xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xui32, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.4_workaround"("gather.4"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 94 MB FreePerBank: 958 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%5) <{force = false}> : (tensor<512xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xui32, #ttnn.buffer_type<system_memory>>>>) -> () loc("gather.4_workaround"("gather.4"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 94 MB FreePerBank: 929 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %7 = "ttnn.typecast"(%1) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<151665x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4740x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<151665x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4740x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.4_workaround"("gather.4"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 94 MB FreePerBank: 929 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<151665x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4740x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.4_workaround"("gather.4"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 156 MB FreePerBank: 867 MB ContiguousFreePerBank: 838 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %8 = "ttnn.embedding"(%6, %7) : (tensor<512xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<151665x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4740x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.4")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 123 MB FreePerBank: 900 MB ContiguousFreePerBank: 838 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%7) <{force = false}> : (tensor<151665x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4740x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.4")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 123 MB FreePerBank: 1677 MB ContiguousFreePerBank: 838 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%6) <{force = false}> : (tensor<512xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.4")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 61 MB FreePerBank: 2578 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %9 = "ttnn.typecast"(%8) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<512x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("gather.4_workaround"("gather.4"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 61 MB FreePerBank: 2606 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%8) <{force = false}> : (tensor<512x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("gather.4_workaround"("gather.4"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 62 MB FreePerBank: 2577 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %10 = "ttnn.typecast"(%9) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.5")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 61 MB FreePerBank: 2578 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%9) <{force = false}> : (tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.5")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 62 MB FreePerBank: 2577 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %11 = "ttnn.reshape"(%10) <{shape = [1 : i32, 512 : i32, 2560 : i32]}> : (tensor<512x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x2560xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.5")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 61 MB FreePerBank: 2639 MB ContiguousFreePerBank: 900 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%10) <{force = false}> : (tensor<512x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.5")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 61 MB FreePerBank: 2639 MB ContiguousFreePerBank: 900 MB
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: Int32 into buffer with expected data type: Int64, the values will be casted, this may impact the throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
#loc1 = loc("p0.1")
#loc2 = loc("p1.3")
module @SyncTensorsGraph.9 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<512x!vhlo.i64_v1> loc("p0.1"), %arg1: !vhlo.tensor_v1<513x!vhlo.i64_v1> loc("p1.3")) -> (!vhlo.tensor_v1<512x513x!vhlo.bool_v1>) {
    %0 = "vhlo.broadcast_in_dim_v1"(%arg1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<513x!vhlo.i64_v1>) -> !vhlo.tensor_v1<512x513x!vhlo.i64_v1> loc(#loc3)
    %1 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<512x!vhlo.i64_v1>) -> !vhlo.tensor_v1<512x513x!vhlo.i64_v1> loc(#loc4)
    %2 = "vhlo.compare_v1"(%0, %1) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<512x513x!vhlo.i64_v1>, !vhlo.tensor_v1<512x513x!vhlo.i64_v1>) -> !vhlo.tensor_v1<512x513x!vhlo.bool_v1> loc(#loc5)
    "vhlo.return_v1"(%2) : (!vhlo.tensor_v1<512x513x!vhlo.bool_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("broadcast.4")
#loc4 = loc("broadcast.6")
#loc5 = loc("compare.7")
#loc1 = loc("p0.1")
#loc2 = loc("p1.3")
module @SyncTensorsGraph.9 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xi64> loc("p0.1"), %arg1: tensor<513xi64> loc("p1.3")) -> tensor<512x513xi1> {
    %0 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<513xi64>) -> tensor<512x513xi64> loc(#loc3)
    %1 = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<512xi64>) -> tensor<512x513xi64> loc(#loc4)
    %2 = stablehlo.compare  GT, %0, %1 : (tensor<512x513xi64>, tensor<512x513xi64>) -> tensor<512x513xi1> loc(#loc5)
    return %2 : tensor<512x513xi1> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("broadcast.4")
#loc4 = loc("broadcast.6")
#loc5 = loc("compare.7")
#loc1 = loc("p0.1")
#loc2 = loc("p1.3")
module @SyncTensorsGraph.9 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<512xi64> loc("p0.1"), %arg1: tensor<513xi64> loc("p1.3")) -> tensor<512x513xi1> {
    %0 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<513xi64>) -> tensor<512x513xi64> loc(#loc3)
    %1 = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<512xi64>) -> tensor<512x513xi64> loc(#loc4)
    %2 = stablehlo.compare  GT, %0, %1 : (tensor<512x513xi64>, tensor<512x513xi64>) -> tensor<512x513xi1> loc(#loc5)
    return %2 : tensor<512x513xi1> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("broadcast.4")
#loc4 = loc("broadcast.6")
#loc5 = loc("compare.7")
#loc1 = loc("p0.1")
#loc2 = loc("p1.3")
module @SyncTensorsGraph.9 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]> loc(#loc)
  func.func @main(%arg0: tensor<512xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p0.1"), %arg1: tensor<513xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p1.3")) -> (tensor<512x513xi1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<513xi64>) -> tensor<512x513xi64> loc(#loc3)
    %1 = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<512xi64>) -> tensor<512x513xi64> loc(#loc4)
    %2 = stablehlo.compare  GT, %0, %1 : (tensor<512x513xi64>, tensor<512x513xi64>) -> tensor<512x513xi1> loc(#loc5)
    return %2 : tensor<512x513xi1> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("broadcast.4")
#loc4 = loc("broadcast.6")
#loc5 = loc("compare.7")
#loc1 = loc("p0.1")
#loc2 = loc("p1.3")
module @SyncTensorsGraph.9 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  func.func @main(%arg0: tensor<512xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p0.1"), %arg1: tensor<513xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p1.3")) -> (tensor<512x513xi1> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = ttir.empty() : tensor<1x513xi64> loc(#loc3)
    %1 = "ttir.reshape"(%arg1, %0) <{shape = [1 : i32, 513 : i32]}> : (tensor<513xi64>, tensor<1x513xi64>) -> tensor<1x513xi64> loc(#loc3)
    %2 = ttir.empty() : tensor<512x513xi64> loc(#loc3)
    %3 = "ttir.broadcast"(%1, %2) <{broadcast_dimensions = array<i64: 512, 1>}> : (tensor<1x513xi64>, tensor<512x513xi64>) -> tensor<512x513xi64> loc(#loc3)
    %4 = ttir.empty() : tensor<512x1xi64> loc(#loc4)
    %5 = "ttir.reshape"(%arg0, %4) <{shape = [512 : i32, 1 : i32]}> : (tensor<512xi64>, tensor<512x1xi64>) -> tensor<512x1xi64> loc(#loc4)
    %6 = ttir.empty() : tensor<512x513xi64> loc(#loc4)
    %7 = "ttir.broadcast"(%5, %6) <{broadcast_dimensions = array<i64: 1, 513>}> : (tensor<512x1xi64>, tensor<512x513xi64>) -> tensor<512x513xi64> loc(#loc4)
    %8 = ttir.empty() : tensor<512x513xi1> loc(#loc5)
    %9 = "ttir.gt"(%3, %7, %8) : (tensor<512x513xi64>, tensor<512x513xi64>, tensor<512x513xi1>) -> tensor<512x513xi1> loc(#loc5)
    return %9 : tensor<512x513xi1> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("broadcast.4")
#loc4 = loc("broadcast.6")
#loc5 = loc("compare.7")
#dram = #ttnn.buffer_type<dram>
#loc1 = loc("p0.1")
#loc2 = loc("p1.3")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 100352, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073139712, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [1 : i32], [ 0x0x0x0]>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x17x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x17x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x17x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x17x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x17x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x17x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
module @SyncTensorsGraph.9 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.9 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func @main(%arg0: tensor<512xsi32, #ttnn_layout> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p0.1"), %arg1: tensor<513xsi32, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p1.3")) -> (tensor<512x513xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 513 : i32]}> : (tensor<513xsi32, #ttnn_layout1>) -> tensor<1x513xsi32, #ttnn_layout3> loc(#loc3)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<513xsi32, #ttnn_layout1>) -> () loc(#loc3)
        %1 = "ttnn.reshape"(%arg0) <{shape = [512 : i32, 1 : i32]}> : (tensor<512xsi32, #ttnn_layout>) -> tensor<512x1xsi32, #ttnn_layout4> loc(#loc4)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<512xsi32, #ttnn_layout>) -> () loc(#loc4)
        %2 = "ttnn.typecast"(%0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x513xsi32, #ttnn_layout3>) -> tensor<1x513xf32, #ttnn_layout5> loc(#loc6)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x513xsi32, #ttnn_layout3>) -> () loc(#loc6)
        %3 = "ttnn.typecast"(%1) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<512x1xsi32, #ttnn_layout4>) -> tensor<512x1xf32, #ttnn_layout6> loc(#loc6)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<512x1xsi32, #ttnn_layout4>) -> () loc(#loc6)
        %4 = "ttnn.gt"(%2, %3) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x513xf32, #ttnn_layout5>, tensor<512x1xf32, #ttnn_layout6>) -> tensor<512x513xf32, #ttnn_layout7> loc(#loc5)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<512x1xf32, #ttnn_layout6>) -> () loc(#loc5)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x513xf32, #ttnn_layout5>) -> () loc(#loc5)
        %5 = "ttnn.typecast"(%4) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<512x513xf32, #ttnn_layout7>) -> tensor<512x513x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout8> loc(#loc6)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<512x513xf32, #ttnn_layout7>) -> () loc(#loc6)
        %6 = "ttnn.typecast"(%5) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x513x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout8>) -> tensor<512x513xbf16, #ttnn_layout2> loc(#loc5)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<512x513x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout8>) -> () loc(#loc5)
        return %6 : tensor<512x513xbf16, #ttnn_layout2> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("broadcast.4")
#loc4 = loc("broadcast.6")
#loc5 = loc("compare.7")
#loc6 = loc("compare.7_workaround"(#loc5))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %0 = "ttnn.reshape"(%arg1) <{shape = [1 : i32, 513 : i32]}> : (tensor<513xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x17x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x513xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x17x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.4")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 1023 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<513xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x17x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.4")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 1023 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %1 = "ttnn.reshape"(%arg0) <{shape = [512 : i32, 1 : i32]}> : (tensor<512xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.6")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 1023 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<512xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.6")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 2047 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %2 = "ttnn.typecast"(%0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x513xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x17x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x513xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x17x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.7_workaround"("compare.7"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 2047 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x513xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x17x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.7_workaround"("compare.7"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 1023 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %3 = "ttnn.typecast"(%1) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<512x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.7_workaround"("compare.7"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 1023 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<512x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.7_workaround"("compare.7"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 1023 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %4 = "ttnn.gt"(%2, %3) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x513xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x17x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x513xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x17x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.7")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 1023 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<512x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.7")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 1023 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x513xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x17x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.7")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 1023 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %5 = "ttnn.typecast"(%4) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<512x513xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x17x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x513x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x17x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.7_workaround"("compare.7"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 1023 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<512x513xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x17x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.7_workaround"("compare.7"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 1023 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %6 = "ttnn.typecast"(%5) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<512x513x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x17x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x513xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x17x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("compare.7")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 1024 MB ContiguousFreePerBank: 1023 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%5) <{force = false}> : (tensor<512x513x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x17x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("compare.7")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 0 MB FreePerBank: 1023 MB ContiguousFreePerBank: 1023 MB
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User is requesting to copy the data from a runtime tensor with data type: BFloat16 into buffer with expected data type: Bool, the values will be casted, this may impact the throughput and the integrity of the data.
inputs: tensor([[148895,  10626,  34325,   6344, 151310,  75122, 100651,  93075,  19140,
          18660,  88702, 121168, 111057,   5838,  88627,  41718, 107657,  62622,
          25814, 129529, 142137, 145833, 125894,  51312, 100240, 121109,  86024,
          24729, 143476,   4972, 114200,  14818,  97567, 100513,  29769,  59998,
          97745,  21074,  38548,  57839,  44747, 143484,  72234,  15677,  64761,
          64294, 127125, 125434, 132375,  60041,  62108,  72757,  27093, 145624,
         113646,  27782,  37963,  92349,  64084, 110768, 148343,  71215,  80659,
          98421,  74858,  50181, 104561, 116585,  27492,  92272,  33278,  33968,
          48062, 118752,  53209,  73243, 101372,  29264,  17908, 133741,  45656,
         110110,    890,  17482, 140775, 137390, 115094, 119952, 114807, 122864,
          60593,  98963,  97228, 136909,  85642,  90593, 127527,  78590,  51334,
          41754,  93698,  48362,  69810,  71463,  90785,  31870,  30838, 137287,
         150502, 106110, 113851, 114557, 138313, 103151, 143905,  17286,  40418,
         121768, 127061,  91777,  72389,  53224,  21595, 141427,  22406,  10408,
         112190, 150097, 118212,  89769,  54679, 101979, 136041,  58846,  28534,
         121081,   2651, 149287, 118887,  43480,  22661, 147531,  84031,   4948,
         104989,    130, 147129,  19815,  11771,  16461, 138185,  80032, 137855,
         109579,  47542,  76499,  82630,  44351, 107884, 132201,  30472,  93675,
          18668,  11825, 127731,   4308, 122590, 118193, 119470,  30434,  90397,
          71197, 135658,  36266, 101021, 127591,  70745, 149433,  53018, 119955,
         120097,   1333, 149753,  84631, 135831, 127213,  68704,  73188, 123668,
         139827,  77031,  65437, 111993,  75744,  76405,  89431, 147601,   4895,
         150066,  99063, 109198,  35899,  61374, 131973, 123728,  73533, 144940,
         146811,  61665,  18667,  66441,  97977,  50888,  59524, 125101,  13416,
          47834,  59730,  91124,  20622, 133569,  87322, 111544, 144370,  41148,
          26290,  41348,  49186,  15514, 141732,  28291,  58157,  58367, 144250,
          49283,  37440,  43894,  56537,  74735,   4407,  44557, 108096, 144516,
         142470,   7168,  54648, 100156, 127027, 120980,  67314, 122153,  93768,
          92020,  75829,  71110,  45020,   2943, 111635, 126603, 114740, 148618,
          98771, 146589,  45592,   2503,  58708,  32779,  44734,   7706,  94484,
          33588,   4322,  83153,  54542,  80640, 120247,  67670,  86151,   2250,
         120948, 119263,    200,  62588, 103382, 144382, 138809, 127831,  91935,
         112210,   4119,  23607,  85169, 108366,  12703,  60746,  73206,  56771,
          74376,  34741,  78160, 122405,  35530, 126598, 119819,  40740, 145607,
          64149,  91458, 142417, 147493,   6029,  61635,  91337, 142884,  87211,
          64815,  91871, 144643,   1324,  54258,  29590, 101779,  96644, 113195,
         105860, 121514,  67822, 137102,  88544, 144375, 124336,  43806, 117764,
          76775,  93606,  45825,  66564,  21635,   7229, 135774,  39419,  66501,
           1296,   5546,  72584, 103462,  89568, 126418, 122678,  40768,  29283,
          65160,  53982,  29634, 142545,  72213,   7315,    428, 116794,  10032,
          93064,  28413,   9346,  94837, 118132,  71980, 122969,  80263,  96448,
          97976,  43560, 122177, 101037, 143176, 144461,  11447,  24628,  20170,
          27649, 112966, 116707,  77615,  57712,  35470,  79330,  92996,  53203,
         146459,  32207,   4918,  31601, 136834,   5832, 150023,  16603, 123826,
          83906, 141031, 141385, 147305, 145404,  89631,  69252,  89402,   3330,
         129564, 135623,  19810, 147519,  18575, 106693,  81952,  55147,  34705,
            538, 137059,  71482,  40585,  48239,  42347,  38878,  78095,  22491,
          98549,  43659,  56952,   7508,  72522, 135878,  23140,  71511,   6412,
         122662, 140254, 121892,  73266, 127113, 145364, 104656, 109257,  88069,
          86085, 149290, 118472,  75766,   2289, 149951,  48089, 113054,  22416,
         122657, 138181,  98795,  79890, 121199,  57043,  40745,  68938, 143313,
          86146,  54606, 102176,  92593, 116883, 142440,  55467, 127697,  16220,
          22935,  10028,  48180,  34893,  86950,  11141, 120333,   6438,  37314,
           5268,  64556,    488, 133918,  67658, 127580,  94618, 101483,  65204,
          83392, 140107,  81052, 112779,  83944, 113716,  55023,  54745,   2104,
          59060,  76817,  16862,  24592,  95909, 148414,  61830, 113171,  78226,
          33262,   4870,  47611,  89527, 144141, 125168, 115863,   7113]],
       dtype=torch.int32)
inputs.shape: torch.Size([1, 512])
self.tokenizer.vocab_size: 151643
Neither required_atol, or relative_atol is provided. Setting required_atol=0.01.
[MODEL NAME] pytorch_qwen_3_embedding_embedding_4b_nlp_embed_gen_huggingface
Note: Using experimental XLA backend.
After all passes
opcode         name                           target                         args                                                                kwargs
-------------  -----------------------------  -----------------------------  ------------------------------------------------------------------  --------
get_attr       l__self___embed_tokens_weight  L__self___embed_tokens.weight  ()                                                                  {}
get_attr       _fx_const_folded_attrs_0       _FX_CONST_FOLDED_ATTRS.0       ()                                                                  {}
get_attr       _fx_const_folded_attrs_1       _FX_CONST_FOLDED_ATTRS.1       ()                                                                  {}
placeholder    args_0                         args_0                         ()                                                                  {}
call_function  embedding                      aten.embedding.default         (l__self___embed_tokens_weight, args_0)                             {}
output         output                         output                         ((embedding, _fx_const_folded_attrs_0, _fx_const_folded_attrs_1),)  {}
Note: Using experimental XLA backend.
After all passes
opcode         name                      target                    args                               kwargs
-------------  ------------------------  ------------------------  ---------------------------------  --------
get_attr       _fx_const_folded_attrs_0  _FX_CONST_FOLDED_ATTRS.0  ()                                 {}
get_attr       _fx_const_folded_attrs_1  _FX_CONST_FOLDED_ATTRS.1  ()                                 {}
placeholder    args_0                    args_0                    ()                                 {}
call_function  view                      aten.view.default         (args_0, [-1, 1])                  {}
call_function  gt                        aten.gt.Tensor            (_fx_const_folded_attrs_0, view)   {}
output         output                    output                    ((_fx_const_folded_attrs_1, gt),)  {}
Note: Using experimental XLA backend.
After all passes
opcode         name                                                                          target                                                                        args                                                                      kwargs
-------------  ----------------------------------------------------------------------------  ----------------------------------------------------------------------------  ------------------------------------------------------------------------  ------------------------------------------
get_attr       l__self___layers__modules__0___input_layernorm_weight                         L__self___layers__modules__0___input_layernorm_weight                         ()                                                                        {}
get_attr       l__self___layers__modules__0___self_attn_q_norm_weight                        L__self___layers__modules__0___self_attn_q_norm_weight                        ()                                                                        {}
get_attr       l__self___layers__modules__0___self_attn_k_norm_weight                        L__self___layers__modules__0___self_attn_k_norm_weight                        ()                                                                        {}
get_attr       l__self___layers__modules__0___post_attention_layernorm_weight                L__self___layers__modules__0___post_attention_layernorm_weight                ()                                                                        {}
get_attr       l__self___norm_weight                                                         L__self___norm_weight                                                         ()                                                                        {}
get_attr       _fx_const_folded_attrs_0                                                      _FX_CONST_FOLDED_ATTRS.0                                                      ()                                                                        {}
get_attr       _fx_const_folded_attrs_1                                                      _FX_CONST_FOLDED_ATTRS.1                                                      ()                                                                        {}
get_attr       _fx_const_folded_attrs_2                                                      _FX_CONST_FOLDED_ATTRS.2                                                      ()                                                                        {}
get_attr       _fx_const_folded_attrs_3                                                      _FX_CONST_FOLDED_ATTRS.3                                                      ()                                                                        {}
get_attr       _fx_const_folded_attrs_4                                                      _FX_CONST_FOLDED_ATTRS.4                                                      ()                                                                        {}
get_attr       _fx_const_folded_attrs_5                                                      _FX_CONST_FOLDED_ATTRS.5                                                      ()                                                                        {}
get_attr       _fx_const_folded_attrs_6                                                      _FX_CONST_FOLDED_ATTRS.6                                                      ()                                                                        {}
get_attr       _fx_const_folded_attrs_7                                                      _FX_CONST_FOLDED_ATTRS.7                                                      ()                                                                        {}
get_attr       const_subgraph_module_l__self___layers__modules__0___self_attn_q_proj_weight  const_subgraph_module.L__self___layers__modules__0___self_attn_q_proj.weight  ()                                                                        {}
get_attr       const_subgraph_module_l__self___layers__modules__0___self_attn_k_proj_weight  const_subgraph_module.L__self___layers__modules__0___self_attn_k_proj.weight  ()                                                                        {}
get_attr       const_subgraph_module_l__self___layers__modules__0___self_attn_v_proj_weight  const_subgraph_module.L__self___layers__modules__0___self_attn_v_proj.weight  ()                                                                        {}
get_attr       const_subgraph_module_l__self___layers__modules__0___self_attn_o_proj_weight  const_subgraph_module.L__self___layers__modules__0___self_attn_o_proj.weight  ()                                                                        {}
get_attr       const_subgraph_module_l__self___layers__modules__0___mlp_gate_proj_weight     const_subgraph_module.L__self___layers__modules__0___mlp_gate_proj.weight     ()                                                                        {}
get_attr       const_subgraph_module_l__self___layers__modules__0___mlp_up_proj_weight       const_subgraph_module.L__self___layers__modules__0___mlp_up_proj.weight       ()                                                                        {}
get_attr       const_subgraph_module_l__self___layers__modules__0___mlp_down_proj_weight     const_subgraph_module.L__self___layers__modules__0___mlp_down_proj.weight     ()                                                                        {}
get_attr       const_subgraph_module_l__self___rotary_emb_inv_freq                           const_subgraph_module.L__self___rotary_emb_inv_freq                           ()                                                                        {}
placeholder    args_0                                                                        args_0                                                                        ()                                                                        {}
placeholder    args_1                                                                        args_1                                                                        ()                                                                        {}
placeholder    args_2                                                                        args_2                                                                        ()                                                                        {}
call_function  slice_1                                                                       aten.slice.Tensor                                                             (args_2, 0, 0, 9223372036854775807)                                       {}
call_function  unsqueeze                                                                     aten.unsqueeze.default                                                        (slice_1, 1)                                                              {}
call_function  slice_2                                                                       aten.slice.Tensor                                                             (unsqueeze, 2, 0, 9223372036854775807)                                    {}
call_function  expand                                                                        aten.expand.default                                                           (slice_2, [1, 1, 512])                                                    {}
call_function  view                                                                          aten.view.default                                                             (expand, [1, 1, 512])                                                     {}
call_function  bmm                                                                           aten.bmm.default                                                              (_fx_const_folded_attrs_0, view)                                          {}
call_function  view_1                                                                        aten.view.default                                                             (bmm, [1, 64, 512])                                                       {}
call_function  permute                                                                       aten.permute.default                                                          (view_1, [0, 2, 1])                                                       {}
call_function  cat                                                                           aten.cat.default                                                              ([permute, permute], -1)                                                  {}
call_function  cos                                                                           aten.cos.default                                                              (cat,)                                                                    {}
call_function  mul                                                                           aten.mul.Tensor                                                               (cos, 1.0)                                                                {}
call_function  sin                                                                           aten.sin.default                                                              (cat,)                                                                    {}
call_function  mul_1                                                                         aten.mul.Tensor                                                               (sin, 1.0)                                                                {}
call_function  pow_1                                                                         aten.pow.Tensor_Scalar                                                        (args_1, 2)                                                               {}
call_function  mean                                                                          aten.mean.dim                                                                 (pow_1, [-1], True)                                                       {}
call_function  add                                                                           aten.add.Tensor                                                               (mean, 1e-06)                                                             {}
call_function  rsqrt                                                                         aten.rsqrt.default                                                            (add,)                                                                    {}
call_function  mul_2                                                                         aten.mul.Tensor                                                               (args_1, rsqrt)                                                           {}
call_function  mul_3                                                                         aten.mul.Tensor                                                               (l__self___layers__modules__0___input_layernorm_weight, mul_2)            {}
call_function  view_2                                                                        aten.view.default                                                             (mul_3, [512, 2560])                                                      {}
call_function  mm                                                                            aten.mm.default                                                               (view_2, _fx_const_folded_attrs_1)                                        {}
call_function  view_3                                                                        aten.view.default                                                             (mm, [1, 512, 4096])                                                      {}
call_function  view_4                                                                        aten.view.default                                                             (view_3, [1, 512, -1, 128])                                               {}
call_function  pow_2                                                                         aten.pow.Tensor_Scalar                                                        (view_4, 2)                                                               {}
call_function  mean_1                                                                        aten.mean.dim                                                                 (pow_2, [-1], True)                                                       {}
call_function  add_1                                                                         aten.add.Tensor                                                               (mean_1, 1e-06)                                                           {}
call_function  rsqrt_1                                                                       aten.rsqrt.default                                                            (add_1,)                                                                  {}
call_function  mul_4                                                                         aten.mul.Tensor                                                               (view_4, rsqrt_1)                                                         {}
call_function  mul_5                                                                         aten.mul.Tensor                                                               (l__self___layers__modules__0___self_attn_q_norm_weight, mul_4)           {}
call_function  permute_1                                                                     aten.permute.default                                                          (mul_5, [0, 2, 1, 3])                                                     {}
call_function  view_5                                                                        aten.view.default                                                             (mul_3, [512, 2560])                                                      {}
call_function  mm_1                                                                          aten.mm.default                                                               (view_5, _fx_const_folded_attrs_2)                                        {}
call_function  view_6                                                                        aten.view.default                                                             (mm_1, [1, 512, 1024])                                                    {}
call_function  view_7                                                                        aten.view.default                                                             (view_6, [1, 512, -1, 128])                                               {}
call_function  pow_3                                                                         aten.pow.Tensor_Scalar                                                        (view_7, 2)                                                               {}
call_function  mean_2                                                                        aten.mean.dim                                                                 (pow_3, [-1], True)                                                       {}
call_function  add_2                                                                         aten.add.Tensor                                                               (mean_2, 1e-06)                                                           {}
call_function  rsqrt_2                                                                       aten.rsqrt.default                                                            (add_2,)                                                                  {}
call_function  mul_6                                                                         aten.mul.Tensor                                                               (view_7, rsqrt_2)                                                         {}
call_function  mul_7                                                                         aten.mul.Tensor                                                               (l__self___layers__modules__0___self_attn_k_norm_weight, mul_6)           {}
call_function  permute_2                                                                     aten.permute.default                                                          (mul_7, [0, 2, 1, 3])                                                     {}
call_function  view_8                                                                        aten.view.default                                                             (mul_3, [512, 2560])                                                      {}
call_function  mm_2                                                                          aten.mm.default                                                               (view_8, _fx_const_folded_attrs_3)                                        {}
call_function  view_9                                                                        aten.view.default                                                             (mm_2, [1, 512, 1024])                                                    {}
call_function  view_10                                                                       aten.view.default                                                             (view_9, [1, 512, -1, 128])                                               {}
call_function  permute_3                                                                     aten.permute.default                                                          (view_10, [0, 2, 1, 3])                                                   {}
call_function  unsqueeze_1                                                                   aten.unsqueeze.default                                                        (mul, 1)                                                                  {}
call_function  unsqueeze_2                                                                   aten.unsqueeze.default                                                        (mul_1, 1)                                                                {}
call_function  mul_8                                                                         aten.mul.Tensor                                                               (permute_1, unsqueeze_1)                                                  {}
call_function  slice_3                                                                       aten.slice.Tensor                                                             (permute_1, 3, 0, 64)                                                     {}
call_function  slice_4                                                                       aten.slice.Tensor                                                             (permute_1, 3, 64, 9223372036854775807)                                   {}
call_function  neg                                                                           aten.neg.default                                                              (slice_4,)                                                                {}
call_function  cat_1                                                                         aten.cat.default                                                              ([neg, slice_3], -1)                                                      {}
call_function  mul_9                                                                         aten.mul.Tensor                                                               (cat_1, unsqueeze_2)                                                      {}
call_function  add_3                                                                         aten.add.Tensor                                                               (mul_8, mul_9)                                                            {}
call_function  mul_10                                                                        aten.mul.Tensor                                                               (permute_2, unsqueeze_1)                                                  {}
call_function  slice_5                                                                       aten.slice.Tensor                                                             (permute_2, 3, 0, 64)                                                     {}
call_function  slice_6                                                                       aten.slice.Tensor                                                             (permute_2, 3, 64, 9223372036854775807)                                   {}
call_function  neg_1                                                                         aten.neg.default                                                              (slice_6,)                                                                {}
call_function  cat_2                                                                         aten.cat.default                                                              ([neg_1, slice_5], -1)                                                    {}
call_function  mul_11                                                                        aten.mul.Tensor                                                               (cat_2, unsqueeze_2)                                                      {}
call_function  add_4                                                                         aten.add.Tensor                                                               (mul_10, mul_11)                                                          {}
call_function  slice_7                                                                       aten.slice.Tensor                                                             (add_4, 0, 0, 9223372036854775807)                                        {}
call_function  slice_8                                                                       aten.slice.Tensor                                                             (slice_7, 1, 0, 9223372036854775807)                                      {}
call_function  unsqueeze_3                                                                   aten.unsqueeze.default                                                        (slice_8, 2)                                                              {}
call_function  slice_9                                                                       aten.slice.Tensor                                                             (unsqueeze_3, 3, 0, 9223372036854775807)                                  {}
call_function  slice_10                                                                      aten.slice.Tensor                                                             (slice_9, 4, 0, 9223372036854775807)                                      {}
call_function  expand_1                                                                      aten.expand.default                                                           (slice_10, [1, 8, 4, 512, 128])                                           {}
call_function  clone                                                                         aten.clone.default                                                            (expand_1,)                                                               {'memory_format': torch.contiguous_format}
call_function  view_11                                                                       aten.view.default                                                             (clone, [1, 32, 512, 128])                                                {}
call_function  slice_11                                                                      aten.slice.Tensor                                                             (permute_3, 0, 0, 9223372036854775807)                                    {}
call_function  slice_12                                                                      aten.slice.Tensor                                                             (slice_11, 1, 0, 9223372036854775807)                                     {}
call_function  unsqueeze_4                                                                   aten.unsqueeze.default                                                        (slice_12, 2)                                                             {}
call_function  slice_13                                                                      aten.slice.Tensor                                                             (unsqueeze_4, 3, 0, 9223372036854775807)                                  {}
call_function  slice_14                                                                      aten.slice.Tensor                                                             (slice_13, 4, 0, 9223372036854775807)                                     {}
call_function  expand_2                                                                      aten.expand.default                                                           (slice_14, [1, 8, 4, 512, 128])                                           {}
call_function  clone_1                                                                       aten.clone.default                                                            (expand_2,)                                                               {'memory_format': torch.contiguous_format}
call_function  view_12                                                                       aten.view.default                                                             (clone_1, [1, 32, 512, 128])                                              {}
call_function  permute_4                                                                     aten.permute.default                                                          (view_11, [0, 1, 3, 2])                                                   {}
call_function  expand_3                                                                      aten.expand.default                                                           (add_3, [1, 32, 512, 128])                                                {}
call_function  view_13                                                                       aten.view.default                                                             (expand_3, [32, 512, 128])                                                {}
call_function  expand_4                                                                      aten.expand.default                                                           (permute_4, [1, 32, 128, 512])                                            {}
call_function  view_14                                                                       aten.view.default                                                             (expand_4, [32, 128, 512])                                                {}
call_function  bmm_1                                                                         aten.bmm.default                                                              (view_13, view_14)                                                        {}
call_function  view_15                                                                       aten.view.default                                                             (bmm_1, [1, 32, 512, 512])                                                {}
call_function  mul_12                                                                        aten.mul.Tensor                                                               (view_15, 0.08838834764831845)                                            {}
call_function  slice_15                                                                      aten.slice.Tensor                                                             (args_0, 0, 0, 9223372036854775807)                                       {}
call_function  slice_16                                                                      aten.slice.Tensor                                                             (slice_15, 1, 0, 9223372036854775807)                                     {}
call_function  slice_17                                                                      aten.slice.Tensor                                                             (slice_16, 2, 0, 9223372036854775807)                                     {}
call_function  slice_18                                                                      aten.slice.Tensor                                                             (slice_17, 3, 0, 512)                                                     {}
call_function  add_5                                                                         aten.add.Tensor                                                               (mul_12, slice_18)                                                        {}
call_function  _softmax                                                                      aten._softmax.default                                                         (add_5, -1, False)                                                        {}
call_function  clone_2                                                                       aten.clone.default                                                            (_softmax,)                                                               {}
call_function  expand_5                                                                      aten.expand.default                                                           (clone_2, [1, 32, 512, 512])                                              {}
call_function  view_16                                                                       aten.view.default                                                             (expand_5, [32, 512, 512])                                                {}
call_function  expand_6                                                                      aten.expand.default                                                           (view_12, [1, 32, 512, 128])                                              {}
call_function  view_17                                                                       aten.view.default                                                             (expand_6, [32, 512, 128])                                                {}
call_function  bmm_2                                                                         aten.bmm.default                                                              (view_16, view_17)                                                        {}
call_function  view_18                                                                       aten.view.default                                                             (bmm_2, [1, 32, 512, 128])                                                {}
call_function  permute_5                                                                     aten.permute.default                                                          (view_18, [0, 2, 1, 3])                                                   {}
call_function  clone_3                                                                       aten.clone.default                                                            (permute_5,)                                                              {'memory_format': torch.contiguous_format}
call_function  view_19                                                                       aten.view.default                                                             (clone_3, [1, 512, -1])                                                   {}
call_function  view_20                                                                       aten.view.default                                                             (view_19, [512, 4096])                                                    {}
call_function  mm_3                                                                          aten.mm.default                                                               (view_20, _fx_const_folded_attrs_4)                                       {}
call_function  view_21                                                                       aten.view.default                                                             (mm_3, [1, 512, 2560])                                                    {}
call_function  add_6                                                                         aten.add.Tensor                                                               (args_1, view_21)                                                         {}
call_function  pow_4                                                                         aten.pow.Tensor_Scalar                                                        (add_6, 2)                                                                {}
call_function  mean_3                                                                        aten.mean.dim                                                                 (pow_4, [-1], True)                                                       {}
call_function  add_7                                                                         aten.add.Tensor                                                               (mean_3, 1e-06)                                                           {}
call_function  rsqrt_3                                                                       aten.rsqrt.default                                                            (add_7,)                                                                  {}
call_function  mul_13                                                                        aten.mul.Tensor                                                               (add_6, rsqrt_3)                                                          {}
call_function  mul_14                                                                        aten.mul.Tensor                                                               (l__self___layers__modules__0___post_attention_layernorm_weight, mul_13)  {}
call_function  view_22                                                                       aten.view.default                                                             (mul_14, [512, 2560])                                                     {}
call_function  mm_4                                                                          aten.mm.default                                                               (view_22, _fx_const_folded_attrs_5)                                       {}
call_function  view_23                                                                       aten.view.default                                                             (mm_4, [1, 512, 9728])                                                    {}
call_function  sigmoid                                                                       aten.sigmoid.default                                                          (view_23,)                                                                {}
call_function  mul_15                                                                        aten.mul.Tensor                                                               (view_23, sigmoid)                                                        {}
call_function  view_24                                                                       aten.view.default                                                             (mul_14, [512, 2560])                                                     {}
call_function  mm_5                                                                          aten.mm.default                                                               (view_24, _fx_const_folded_attrs_6)                                       {}
call_function  view_25                                                                       aten.view.default                                                             (mm_5, [1, 512, 9728])                                                    {}
call_function  mul_16                                                                        aten.mul.Tensor                                                               (mul_15, view_25)                                                         {}
call_function  view_26                                                                       aten.view.default                                                             (mul_16, [512, 9728])                                                     {}
call_function  mm_6                                                                          aten.mm.default                                                               (view_26, _fx_const_folded_attrs_7)                                       {}
call_function  view_27                                                                       aten.view.default                                                             (mm_6, [1, 512, 2560])                                                    {}
call_function  add_8                                                                         aten.add.Tensor                                                               (add_6, view_27)                                                          {}
call_function  pow_5                                                                         aten.pow.Tensor_Scalar                                                        (add_8, 2)                                                                {}
call_function  mean_4                                                                        aten.mean.dim                                                                 (pow_5, [-1], True)                                                       {}
call_function  add_9                                                                         aten.add.Tensor                                                               (mean_4, 1e-06)                                                           {}
call_function  rsqrt_4                                                                       aten.rsqrt.default                                                            (add_9,)                                                                  {}
call_function  mul_17                                                                        aten.mul.Tensor                                                               (add_8, rsqrt_4)                                                          {}
call_function  mul_18                                                                        aten.mul.Tensor                                                               (l__self___norm_weight, mul_17)                                           {}
output         output                                                                        output                                                                        ((mul_18,),)                                                              {}[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
#loc1 = loc("p0.2")
#loc2 = loc("p1.5")
#loc3 = loc("p2.6")
#loc4 = loc("p3.10")
#loc5 = loc("p4.11")
#loc6 = loc("p5.13")
#loc7 = loc("p6.15")
#loc8 = loc("p7.47")
#loc9 = loc("p8.69")
#loc10 = loc("p9.74")
#loc11 = loc("p10.77")
#loc12 = loc("p11.82")
#loc13 = loc("p12.93")
#loc14 = loc("p13.129")
#loc15 = loc("p14.170")
#loc16 = loc("p15.206")
#loc17 = loc("p16.297")
#loc18 = loc("p17.304")
#loc19 = loc("p18.344")
#loc28 = loc("reduce.24")
#loc48 = loc("reduce.183")
#loc89 = loc("reduce.106")
#loc129 = loc("reduce.244")
#loc134 = loc("reduce.253")
#loc155 = loc("reduce.278")
#loc179 = loc("reduce.325")
module @SyncTensorsGraph.349 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<!vhlo.f32_v1> loc("p0.2"), %arg1: !vhlo.tensor_v1<9728x2560x!vhlo.bf16_v1> loc("p1.5"), %arg2: !vhlo.tensor_v1<2560x9728x!vhlo.bf16_v1> loc("p2.6"), %arg3: !vhlo.tensor_v1<4096x2560x!vhlo.bf16_v1> loc("p3.10"), %arg4: !vhlo.tensor_v1<2560x1024x!vhlo.bf16_v1> loc("p4.11"), %arg5: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("p5.13"), %arg6: !vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1> loc("p6.15"), %arg7: !vhlo.tensor_v1<2560x!vhlo.bf16_v1> loc("p7.47"), %arg8: !vhlo.tensor_v1<1x1x512x513x!vhlo.bf16_v1> loc("p8.69"), %arg9: !vhlo.tensor_v1<!vhlo.f32_v1> loc("p9.74"), %arg10: !vhlo.tensor_v1<1x512x!vhlo.i64_v1> loc("p10.77"), %arg11: !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1> loc("p11.82"), %arg12: !vhlo.tensor_v1<2560x1024x!vhlo.bf16_v1> loc("p12.93"), %arg13: !vhlo.tensor_v1<128x!vhlo.bf16_v1> loc("p13.129"), %arg14: !vhlo.tensor_v1<2560x4096x!vhlo.bf16_v1> loc("p14.170"), %arg15: !vhlo.tensor_v1<128x!vhlo.bf16_v1> loc("p15.206"), %arg16: !vhlo.tensor_v1<2560x!vhlo.bf16_v1> loc("p16.297"), %arg17: !vhlo.tensor_v1<2560x9728x!vhlo.bf16_v1> loc("p17.304"), %arg18: !vhlo.tensor_v1<2560x!vhlo.bf16_v1> loc("p18.344")) -> (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.906250e-04> : tensor<1x512xf32>>}> : () -> !vhlo.tensor_v1<1x512x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<1x512x2560xf32>>}> : () -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-03> : tensor<1x512x8xbf16>>}> : () -> !vhlo.tensor_v1<1x512x8x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<1x512x8x128xbf16>>}> : () -> !vhlo.tensor_v1<1x512x8x128x!vhlo.bf16_v1> loc(#loc)
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-03> : tensor<1x512x32xbf16>>}> : () -> !vhlo.tensor_v1<1x512x32x!vhlo.bf16_v1> loc(#loc)
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<1x512x32x128xbf16>>}> : () -> !vhlo.tensor_v1<1x512x32x128x!vhlo.bf16_v1> loc(#loc)
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.910060e-04> : tensor<1x512xbf16>>}> : () -> !vhlo.tensor_v1<1x512x!vhlo.bf16_v1> loc(#loc)
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<1x512x2560xbf16>>}> : () -> !vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1> loc(#loc)
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %9 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %10 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %11 = "vhlo.convert_v1"(%arg18) : (!vhlo.tensor_v1<2560x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2560x!vhlo.f32_v1> loc(#loc20)
    %12 = "vhlo.broadcast_in_dim_v1"(%11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc21)
    %13 = "vhlo.convert_v1"(%arg6) : (!vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc22)
    %14 = "vhlo.convert_v1"(%arg15) : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.f32_v1> loc(#loc23)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1> loc(#loc24)
    %16 = "vhlo.convert_v1"(%arg7) : (!vhlo.tensor_v1<2560x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2560x!vhlo.f32_v1> loc(#loc25)
    %17 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc26)
    %18 = "vhlo.power_v1"(%arg6, %7) : (!vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1> loc(#loc27)
    %19 = "vhlo.reduce_v1"(%18, %10) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.24"), %arg20: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.24")):
      %173 = "vhlo.add_v1"(%arg19, %arg20) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc29)
      "vhlo.return_v1"(%173) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x!vhlo.bf16_v1> loc(#loc28)
    %20 = "vhlo.multiply_v1"(%19, %6) : (!vhlo.tensor_v1<1x512x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x!vhlo.bf16_v1> loc(#loc30)
    %21 = "vhlo.reshape_v1"(%20) : (!vhlo.tensor_v1<1x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x1x!vhlo.bf16_v1> loc(#loc31)
    %22 = "vhlo.broadcast_in_dim_v1"(%arg5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x1x!vhlo.bf16_v1> loc(#loc32)
    %23 = "vhlo.add_v1"(%21, %22) : (!vhlo.tensor_v1<1x512x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x512x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x1x!vhlo.bf16_v1> loc(#loc33)
    %24 = "vhlo.rsqrt_v2"(%23) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x512x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x1x!vhlo.bf16_v1> loc(#loc34)
    %25 = "vhlo.convert_v1"(%24) : (!vhlo.tensor_v1<1x512x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x1x!vhlo.f32_v1> loc(#loc35)
    %26 = "vhlo.reshape_v1"(%25) : (!vhlo.tensor_v1<1x512x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x!vhlo.f32_v1> loc(#loc36)
    %27 = "vhlo.broadcast_in_dim_v1"(%26) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc37)
    %28 = "vhlo.multiply_v1"(%13, %27) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc38)
    %29 = "vhlo.convert_v1"(%28) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1> loc(#loc39)
    %30 = "vhlo.convert_v1"(%29) : (!vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc40)
    %31 = "vhlo.multiply_v1"(%17, %30) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc41)
    %32 = "vhlo.convert_v1"(%31) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1> loc(#loc42)
    %33 = "vhlo.reshape_v1"(%32) : (!vhlo.tensor_v1<1x512x2560x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<512x2560x!vhlo.bf16_v1> loc(#loc43)
    %34 = "vhlo.dot_general_v2"(%33, %arg14) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<512x2560x!vhlo.bf16_v1>, !vhlo.tensor_v1<2560x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<512x4096x!vhlo.bf16_v1> loc(#loc44)
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<512x4096x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x32x128x!vhlo.bf16_v1> loc(#loc45)
    %36 = "vhlo.convert_v1"(%35) : (!vhlo.tensor_v1<1x512x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1> loc(#loc46)
    %37 = "vhlo.power_v1"(%35, %5) : (!vhlo.tensor_v1<1x512x32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x512x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x32x128x!vhlo.bf16_v1> loc(#loc47)
    %38 = "vhlo.reduce_v1"(%37, %10) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.183"), %arg20: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.183")):
      %173 = "vhlo.add_v1"(%arg19, %arg20) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc49)
      "vhlo.return_v1"(%173) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x512x32x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x32x!vhlo.bf16_v1> loc(#loc48)
    %39 = "vhlo.multiply_v1"(%38, %4) : (!vhlo.tensor_v1<1x512x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x512x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x32x!vhlo.bf16_v1> loc(#loc50)
    %40 = "vhlo.reshape_v1"(%39) : (!vhlo.tensor_v1<1x512x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x32x1x!vhlo.bf16_v1> loc(#loc51)
    %41 = "vhlo.broadcast_in_dim_v1"(%arg5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x32x1x!vhlo.bf16_v1> loc(#loc52)
    %42 = "vhlo.add_v1"(%40, %41) : (!vhlo.tensor_v1<1x512x32x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x512x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x32x1x!vhlo.bf16_v1> loc(#loc53)
    %43 = "vhlo.rsqrt_v2"(%42) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x512x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x32x1x!vhlo.bf16_v1> loc(#loc54)
    %44 = "vhlo.convert_v1"(%43) : (!vhlo.tensor_v1<1x512x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x32x1x!vhlo.f32_v1> loc(#loc55)
    %45 = "vhlo.reshape_v1"(%44) : (!vhlo.tensor_v1<1x512x32x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x32x!vhlo.f32_v1> loc(#loc56)
    %46 = "vhlo.broadcast_in_dim_v1"(%45) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x512x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1> loc(#loc57)
    %47 = "vhlo.multiply_v1"(%36, %46) : (!vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1> loc(#loc58)
    %48 = "vhlo.convert_v1"(%47) : (!vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x32x128x!vhlo.bf16_v1> loc(#loc59)
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<1x512x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1> loc(#loc60)
    %50 = "vhlo.multiply_v1"(%15, %49) : (!vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1> loc(#loc61)
    %51 = "vhlo.convert_v1"(%50) : (!vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x32x128x!vhlo.bf16_v1> loc(#loc62)
    %52 = "vhlo.transpose_v1"(%51) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,32,512,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x512x32x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x32x512x128x!vhlo.bf16_v1> loc(#loc63)
    %53 = "vhlo.convert_v1"(%52) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,32,512,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x32x512x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1> loc(#loc64)
    %54 = "vhlo.reshape_v1"(%arg10) : (!vhlo.tensor_v1<1x512x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x512x!vhlo.i64_v1> loc(#loc65)
    %55 = "vhlo.convert_v1"(%54) : (!vhlo.tensor_v1<1x1x512x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x512x!vhlo.f32_v1> loc(#loc66)
    %56 = "vhlo.dot_general_v2"(%arg11, %55) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x512x!vhlo.f32_v1> loc(#loc67)
    %57 = "vhlo.transpose_v1"(%56) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,512,64]{1,2,0}">} : (!vhlo.tensor_v1<1x64x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x64x!vhlo.f32_v1> loc(#loc68)
    %58 = "vhlo.concatenate_v1"(%57, %57) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x512x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x128x!vhlo.f32_v1> loc(#loc69)
    %59 = "vhlo.cosine_v2"(%58) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x128x!vhlo.f32_v1> loc(#loc70)
    %60 = "vhlo.broadcast_in_dim_v1"(%59) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1> loc(#loc71)
    %61 = "vhlo.multiply_v1"(%53, %60) : (!vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1> loc(#loc72)
    %62 = "vhlo.slice_v1"(%52) <{limit_indices = #vhlo.tensor_v1<dense<[1, 32, 512, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x32x512x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x32x512x64x!vhlo.bf16_v1> loc(#loc73)
    %63 = "vhlo.negate_v1"(%62) : (!vhlo.tensor_v1<1x32x512x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x32x512x64x!vhlo.bf16_v1> loc(#loc74)
    %64 = "vhlo.slice_v1"(%52) <{limit_indices = #vhlo.tensor_v1<dense<[1, 32, 512, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x32x512x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x32x512x64x!vhlo.bf16_v1> loc(#loc75)
    %65 = "vhlo.concatenate_v1"(%63, %64) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x32x512x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x32x512x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x32x512x128x!vhlo.bf16_v1> loc(#loc76)
    %66 = "vhlo.convert_v1"(%65) : (!vhlo.tensor_v1<1x32x512x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1> loc(#loc77)
    %67 = "vhlo.sine_v2"(%58) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x128x!vhlo.f32_v1> loc(#loc78)
    %68 = "vhlo.broadcast_in_dim_v1"(%67) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1> loc(#loc79)
    %69 = "vhlo.multiply_v1"(%66, %68) : (!vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1> loc(#loc80)
    %70 = "vhlo.add_v1"(%61, %69) : (!vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1> loc(#loc81)
    %71 = "vhlo.reshape_v1"(%70) : (!vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x512x128x!vhlo.f32_v1> loc(#loc82)
    %72 = "vhlo.convert_v1"(%arg13) : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.f32_v1> loc(#loc83)
    %73 = "vhlo.broadcast_in_dim_v1"(%72) <{broadcast_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x8x128x!vhlo.f32_v1> loc(#loc84)
    %74 = "vhlo.dot_general_v2"(%33, %arg12) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<512x2560x!vhlo.bf16_v1>, !vhlo.tensor_v1<2560x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<512x1024x!vhlo.bf16_v1> loc(#loc85)
    %75 = "vhlo.reshape_v1"(%74) : (!vhlo.tensor_v1<512x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x8x128x!vhlo.bf16_v1> loc(#loc86)
    %76 = "vhlo.convert_v1"(%75) : (!vhlo.tensor_v1<1x512x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x8x128x!vhlo.f32_v1> loc(#loc87)
    %77 = "vhlo.power_v1"(%75, %3) : (!vhlo.tensor_v1<1x512x8x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x512x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x8x128x!vhlo.bf16_v1> loc(#loc88)
    %78 = "vhlo.reduce_v1"(%77, %10) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.106"), %arg20: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.106")):
      %173 = "vhlo.add_v1"(%arg19, %arg20) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc90)
      "vhlo.return_v1"(%173) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x512x8x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x8x!vhlo.bf16_v1> loc(#loc89)
    %79 = "vhlo.multiply_v1"(%78, %2) : (!vhlo.tensor_v1<1x512x8x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x512x8x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x8x!vhlo.bf16_v1> loc(#loc91)
    %80 = "vhlo.reshape_v1"(%79) : (!vhlo.tensor_v1<1x512x8x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x8x1x!vhlo.bf16_v1> loc(#loc92)
    %81 = "vhlo.broadcast_in_dim_v1"(%arg5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x8x1x!vhlo.bf16_v1> loc(#loc93)
    %82 = "vhlo.add_v1"(%80, %81) : (!vhlo.tensor_v1<1x512x8x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x512x8x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x8x1x!vhlo.bf16_v1> loc(#loc94)
    %83 = "vhlo.rsqrt_v2"(%82) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x512x8x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x8x1x!vhlo.bf16_v1> loc(#loc95)
    %84 = "vhlo.convert_v1"(%83) : (!vhlo.tensor_v1<1x512x8x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x8x1x!vhlo.f32_v1> loc(#loc96)
    %85 = "vhlo.reshape_v1"(%84) : (!vhlo.tensor_v1<1x512x8x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x8x!vhlo.f32_v1> loc(#loc97)
    %86 = "vhlo.broadcast_in_dim_v1"(%85) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x512x8x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x8x128x!vhlo.f32_v1> loc(#loc98)
    %87 = "vhlo.multiply_v1"(%76, %86) : (!vhlo.tensor_v1<1x512x8x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x8x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x8x128x!vhlo.f32_v1> loc(#loc99)
    %88 = "vhlo.convert_v1"(%87) : (!vhlo.tensor_v1<1x512x8x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x8x128x!vhlo.bf16_v1> loc(#loc100)
    %89 = "vhlo.convert_v1"(%88) : (!vhlo.tensor_v1<1x512x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x8x128x!vhlo.f32_v1> loc(#loc101)
    %90 = "vhlo.multiply_v1"(%73, %89) : (!vhlo.tensor_v1<1x512x8x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x8x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x8x128x!vhlo.f32_v1> loc(#loc102)
    %91 = "vhlo.convert_v1"(%90) : (!vhlo.tensor_v1<1x512x8x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x8x128x!vhlo.bf16_v1> loc(#loc103)
    %92 = "vhlo.transpose_v1"(%91) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,512,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x512x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x512x128x!vhlo.bf16_v1> loc(#loc104)
    %93 = "vhlo.convert_v1"(%92) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,512,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x512x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1> loc(#loc105)
    %94 = "vhlo.broadcast_in_dim_v1"(%59) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1> loc(#loc106)
    %95 = "vhlo.multiply_v1"(%93, %94) : (!vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1> loc(#loc107)
    %96 = "vhlo.slice_v1"(%92) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 512, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x512x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x512x64x!vhlo.bf16_v1> loc(#loc108)
    %97 = "vhlo.negate_v1"(%96) : (!vhlo.tensor_v1<1x8x512x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x512x64x!vhlo.bf16_v1> loc(#loc109)
    %98 = "vhlo.slice_v1"(%92) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 512, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x512x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x512x64x!vhlo.bf16_v1> loc(#loc110)
    %99 = "vhlo.concatenate_v1"(%97, %98) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x512x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x512x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x512x128x!vhlo.bf16_v1> loc(#loc111)
    %100 = "vhlo.convert_v1"(%99) : (!vhlo.tensor_v1<1x8x512x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1> loc(#loc112)
    %101 = "vhlo.broadcast_in_dim_v1"(%67) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1> loc(#loc113)
    %102 = "vhlo.multiply_v1"(%100, %101) : (!vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1> loc(#loc114)
    %103 = "vhlo.add_v1"(%95, %102) : (!vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1> loc(#loc115)
    %104 = "vhlo.broadcast_in_dim_v1"(%103) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x4x512x128x!vhlo.f32_v1> loc(#loc116)
    %105 = "vhlo.reshape_v1"(%104) : (!vhlo.tensor_v1<1x8x4x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1> loc(#loc117)
    %106 = "vhlo.transpose_v1"(%105) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,32,128,512]{2,3,1,0}">} : (!vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x128x512x!vhlo.f32_v1> loc(#loc118)
    %107 = "vhlo.reshape_v1"(%106) : (!vhlo.tensor_v1<1x32x128x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x128x512x!vhlo.f32_v1> loc(#loc119)
    %108 = "vhlo.dot_general_v2"(%71, %107) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x512x128x!vhlo.f32_v1>, !vhlo.tensor_v1<32x128x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x512x512x!vhlo.f32_v1> loc(#loc120)
    %109 = "vhlo.reshape_v1"(%108) : (!vhlo.tensor_v1<32x512x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1> loc(#loc121)
    %110 = "vhlo.broadcast_in_dim_v1"(%arg9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1> loc(#loc122)
    %111 = "vhlo.multiply_v1"(%109, %110) : (!vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1>, !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1> loc(#loc123)
    %112 = "vhlo.slice_v1"(%arg8) <{limit_indices = #vhlo.tensor_v1<dense<[1, 1, 512, 512]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x1x512x513x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x512x512x!vhlo.bf16_v1> loc(#loc124)
    %113 = "vhlo.convert_v1"(%112) : (!vhlo.tensor_v1<1x1x512x512x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x512x512x!vhlo.f32_v1> loc(#loc125)
    %114 = "vhlo.reshape_v1"(%113) : (!vhlo.tensor_v1<1x1x512x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x512x!vhlo.f32_v1> loc(#loc126)
    %115 = "vhlo.broadcast_in_dim_v1"(%114) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x512x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1> loc(#loc127)
    %116 = "vhlo.add_v1"(%111, %115) : (!vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1>, !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1> loc(#loc128)
    %117 = "vhlo.reduce_v1"(%116, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.244"), %arg20: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.244")):
      %173 = "vhlo.maximum_v1"(%arg19, %arg20) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc130)
      "vhlo.return_v1"(%173) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x!vhlo.f32_v1> loc(#loc129)
    %118 = "vhlo.broadcast_in_dim_v1"(%117) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x32x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1> loc(#loc131)
    %119 = "vhlo.subtract_v1"(%116, %118) : (!vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1>, !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1> loc(#loc132)
    %120 = "vhlo.exponential_v2"(%119) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1> loc(#loc133)
    %121 = "vhlo.reduce_v1"(%120, %8) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.253"), %arg20: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.253")):
      %173 = "vhlo.add_v1"(%arg19, %arg20) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc135)
      "vhlo.return_v1"(%173) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x!vhlo.f32_v1> loc(#loc134)
    %122 = "vhlo.broadcast_in_dim_v1"(%121) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x32x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1> loc(#loc136)
    %123 = "vhlo.divide_v1"(%120, %122) : (!vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1>, !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1> loc(#loc137)
    %124 = "vhlo.reshape_v1"(%123) : (!vhlo.tensor_v1<1x32x512x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x512x512x!vhlo.f32_v1> loc(#loc138)
    %125 = "vhlo.dot_general_v2"(%33, %arg4) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<512x2560x!vhlo.bf16_v1>, !vhlo.tensor_v1<2560x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<512x1024x!vhlo.bf16_v1> loc(#loc139)
    %126 = "vhlo.reshape_v1"(%125) : (!vhlo.tensor_v1<512x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x512x8x128x!vhlo.bf16_v1> loc(#loc140)
    %127 = "vhlo.transpose_v1"(%126) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,512,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x512x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x512x128x!vhlo.bf16_v1> loc(#loc141)
    %128 = "vhlo.broadcast_in_dim_v1"(%127) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x512x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x4x512x128x!vhlo.bf16_v1> loc(#loc142)
    %129 = "vhlo.reshape_v1"(%128) : (!vhlo.tensor_v1<1x8x4x512x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x128x!vhlo.bf16_v1> loc(#loc143)
    %130 = "vhlo.convert_v1"(%129) : (!vhlo.tensor_v1<32x512x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x512x128x!vhlo.f32_v1> loc(#loc144)
    %131 = "vhlo.dot_general_v2"(%124, %130) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<32x512x512x!vhlo.f32_v1>, !vhlo.tensor_v1<32x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<32x512x128x!vhlo.f32_v1> loc(#loc145)
    %132 = "vhlo.reshape_v1"(%131) : (!vhlo.tensor_v1<32x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1> loc(#loc146)
    %133 = "vhlo.transpose_v1"(%132) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,512,32,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x32x512x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1> loc(#loc147)
    %134 = "vhlo.reshape_v1"(%133) : (!vhlo.tensor_v1<1x512x32x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<512x4096x!vhlo.f32_v1> loc(#loc148)
    %135 = "vhlo.dot_general_v2"(%134, %arg3) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<512x4096x!vhlo.f32_v1>, !vhlo.tensor_v1<4096x2560x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<512x2560x!vhlo.f32_v1> loc(#loc149)
    %136 = "vhlo.reshape_v1"(%135) : (!vhlo.tensor_v1<512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc150)
    %137 = "vhlo.add_v1"(%13, %136) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc151)
    %138 = "vhlo.convert_v1"(%arg16) : (!vhlo.tensor_v1<2560x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2560x!vhlo.f32_v1> loc(#loc152)
    %139 = "vhlo.broadcast_in_dim_v1"(%138) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc153)
    %140 = "vhlo.power_v1"(%137, %1) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc154)
    %141 = "vhlo.reduce_v1"(%140, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.278"), %arg20: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.278")):
      %173 = "vhlo.add_v1"(%arg19, %arg20) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc156)
      "vhlo.return_v1"(%173) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x!vhlo.f32_v1> loc(#loc155)
    %142 = "vhlo.multiply_v1"(%141, %0) : (!vhlo.tensor_v1<1x512x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x!vhlo.f32_v1> loc(#loc157)
    %143 = "vhlo.reshape_v1"(%142) : (!vhlo.tensor_v1<1x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x1x!vhlo.f32_v1> loc(#loc158)
    %144 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x1x!vhlo.f32_v1> loc(#loc159)
    %145 = "vhlo.add_v1"(%143, %144) : (!vhlo.tensor_v1<1x512x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x1x!vhlo.f32_v1> loc(#loc160)
    %146 = "vhlo.rsqrt_v2"(%145) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x512x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x1x!vhlo.f32_v1> loc(#loc161)
    %147 = "vhlo.reshape_v1"(%146) : (!vhlo.tensor_v1<1x512x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x!vhlo.f32_v1> loc(#loc162)
    %148 = "vhlo.broadcast_in_dim_v1"(%147) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc163)
    %149 = "vhlo.multiply_v1"(%137, %148) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc164)
    %150 = "vhlo.multiply_v1"(%139, %149) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc165)
    %151 = "vhlo.reshape_v1"(%150) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<512x2560x!vhlo.f32_v1> loc(#loc166)
    %152 = "vhlo.dot_general_v2"(%151, %arg17) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<2560x9728x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<512x9728x!vhlo.f32_v1> loc(#loc167)
    %153 = "vhlo.reshape_v1"(%152) : (!vhlo.tensor_v1<512x9728x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x9728x!vhlo.f32_v1> loc(#loc168)
    %154 = "vhlo.logistic_v2"(%153) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x512x9728x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x9728x!vhlo.f32_v1> loc(#loc169)
    %155 = "vhlo.multiply_v1"(%153, %154) : (!vhlo.tensor_v1<1x512x9728x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x9728x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x9728x!vhlo.f32_v1> loc(#loc170)
    %156 = "vhlo.dot_general_v2"(%151, %arg2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<2560x9728x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<512x9728x!vhlo.f32_v1> loc(#loc171)
    %157 = "vhlo.reshape_v1"(%156) : (!vhlo.tensor_v1<512x9728x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x9728x!vhlo.f32_v1> loc(#loc172)
    %158 = "vhlo.multiply_v1"(%155, %157) : (!vhlo.tensor_v1<1x512x9728x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x9728x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x9728x!vhlo.f32_v1> loc(#loc173)
    %159 = "vhlo.reshape_v1"(%158) : (!vhlo.tensor_v1<1x512x9728x!vhlo.f32_v1>) -> !vhlo.tensor_v1<512x9728x!vhlo.f32_v1> loc(#loc174)
    %160 = "vhlo.dot_general_v2"(%159, %arg1) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<512x9728x!vhlo.f32_v1>, !vhlo.tensor_v1<9728x2560x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<512x2560x!vhlo.f32_v1> loc(#loc175)
    %161 = "vhlo.reshape_v1"(%160) : (!vhlo.tensor_v1<512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc176)
    %162 = "vhlo.add_v1"(%137, %161) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc177)
    %163 = "vhlo.power_v1"(%162, %1) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc178)
    %164 = "vhlo.reduce_v1"(%163, %8) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg19: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.325"), %arg20: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.325")):
      %173 = "vhlo.add_v1"(%arg19, %arg20) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc180)
      "vhlo.return_v1"(%173) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x!vhlo.f32_v1> loc(#loc179)
    %165 = "vhlo.multiply_v1"(%164, %0) : (!vhlo.tensor_v1<1x512x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x!vhlo.f32_v1> loc(#loc181)
    %166 = "vhlo.reshape_v1"(%165) : (!vhlo.tensor_v1<1x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x1x!vhlo.f32_v1> loc(#loc182)
    %167 = "vhlo.add_v1"(%166, %144) : (!vhlo.tensor_v1<1x512x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x1x!vhlo.f32_v1> loc(#loc183)
    %168 = "vhlo.rsqrt_v2"(%167) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x512x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x1x!vhlo.f32_v1> loc(#loc184)
    %169 = "vhlo.reshape_v1"(%168) : (!vhlo.tensor_v1<1x512x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x!vhlo.f32_v1> loc(#loc185)
    %170 = "vhlo.broadcast_in_dim_v1"(%169) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x512x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc186)
    %171 = "vhlo.multiply_v1"(%162, %170) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc187)
    %172 = "vhlo.multiply_v1"(%12, %171) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>, !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1> loc(#loc188)
    "vhlo.return_v1"(%172) : (!vhlo.tensor_v1<1x512x2560x!vhlo.f32_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc20 = loc("convert.345")
#loc21 = loc("broadcast.346")
#loc22 = loc("convert.266")
#loc23 = loc("convert.207")
#loc24 = loc("broadcast.208")
#loc25 = loc("convert.48")
#loc26 = loc("broadcast.49")
#loc27 = loc("power.17")
#loc29 = loc("add.23")
#loc30 = loc("multiply.33")
#loc31 = loc("reshape.34")
#loc32 = loc("broadcast.37")
#loc33 = loc("add.38")
#loc34 = loc("rsqrt.39")
#loc35 = loc("convert.40")
#loc36 = loc("reshape.42")
#loc37 = loc("broadcast.43")
#loc38 = loc("multiply.44")
#loc39 = loc("convert.45")
#loc40 = loc("convert.46")
#loc41 = loc("multiply.50")
#loc42 = loc("convert.51")
#loc43 = loc("reshape.171")
#loc44 = loc("dot.172")
#loc45 = loc("reshape.174")
#loc46 = loc("convert.200")
#loc47 = loc("power.176")
#loc49 = loc("add.182")
#loc50 = loc("multiply.192")
#loc51 = loc("reshape.193")
#loc52 = loc("broadcast.196")
#loc53 = loc("add.197")
#loc54 = loc("rsqrt.198")
#loc55 = loc("convert.199")
#loc56 = loc("reshape.201")
#loc57 = loc("broadcast.202")
#loc58 = loc("multiply.203")
#loc59 = loc("convert.204")
#loc60 = loc("convert.205")
#loc61 = loc("multiply.209")
#loc62 = loc("convert.210")
#loc63 = loc("transpose.211")
#loc64 = loc("convert.220")
#loc65 = loc("reshape.79")
#loc66 = loc("convert.83")
#loc67 = loc("dot.84")
#loc68 = loc("transpose.85")
#loc69 = loc("concatenate.86")
#loc70 = loc("cosine.144")
#loc71 = loc("broadcast.222")
#loc72 = loc("multiply.223")
#loc73 = loc("slice.213")
#loc74 = loc("negate.214")
#loc75 = loc("slice.212")
#loc76 = loc("concatenate.215")
#loc77 = loc("convert.216")
#loc78 = loc("sine.87")
#loc79 = loc("broadcast.218")
#loc80 = loc("multiply.219")
#loc81 = loc("add.226")
#loc82 = loc("reshape.228")
#loc83 = loc("convert.130")
#loc84 = loc("broadcast.131")
#loc85 = loc("dot.95")
#loc86 = loc("reshape.97")
#loc87 = loc("convert.123")
#loc88 = loc("power.99")
#loc90 = loc("add.105")
#loc91 = loc("multiply.115")
#loc92 = loc("reshape.116")
#loc93 = loc("broadcast.119")
#loc94 = loc("add.120")
#loc95 = loc("rsqrt.121")
#loc96 = loc("convert.122")
#loc97 = loc("reshape.124")
#loc98 = loc("broadcast.125")
#loc99 = loc("multiply.126")
#loc100 = loc("convert.127")
#loc101 = loc("convert.128")
#loc102 = loc("multiply.132")
#loc103 = loc("convert.133")
#loc104 = loc("transpose.134")
#loc105 = loc("convert.148")
#loc106 = loc("broadcast.150")
#loc107 = loc("multiply.151")
#loc108 = loc("slice.136")
#loc109 = loc("negate.137")
#loc110 = loc("slice.135")
#loc111 = loc("concatenate.138")
#loc112 = loc("convert.139")
#loc113 = loc("broadcast.141")
#loc114 = loc("multiply.142")
#loc115 = loc("add.154")
#loc116 = loc("broadcast.162")
#loc117 = loc("reshape.163")
#loc118 = loc("transpose.164")
#loc119 = loc("reshape.166")
#loc120 = loc("dot.229")
#loc121 = loc("reshape.230")
#loc122 = loc("broadcast.231")
#loc123 = loc("multiply.232")
#loc124 = loc("slice.73")
#loc125 = loc("convert.233")
#loc126 = loc("reshape.236")
#loc127 = loc("broadcast.237")
#loc128 = loc("add.238")
#loc130 = loc("maximum.243")
#loc131 = loc("broadcast.245")
#loc132 = loc("subtract.246")
#loc133 = loc("exponential.247")
#loc135 = loc("add.252")
#loc136 = loc("broadcast.254")
#loc137 = loc("divide.255")
#loc138 = loc("reshape.257")
#loc139 = loc("dot.53")
#loc140 = loc("reshape.55")
#loc141 = loc("transpose.56")
#loc142 = loc("broadcast.64")
#loc143 = loc("reshape.67")
#loc144 = loc("convert.258")
#loc145 = loc("dot.259")
#loc146 = loc("reshape.260")
#loc147 = loc("transpose.261")
#loc148 = loc("reshape.263")
#loc149 = loc("dot.264")
#loc150 = loc("reshape.265")
#loc151 = loc("add.269")
#loc152 = loc("convert.298")
#loc153 = loc("broadcast.299")
#loc154 = loc("power.271")
#loc156 = loc("add.277")
#loc157 = loc("multiply.287")
#loc158 = loc("reshape.288")
#loc159 = loc("broadcast.291")
#loc160 = loc("add.292")
#loc161 = loc("rsqrt.293")
#loc162 = loc("reshape.294")
#loc163 = loc("broadcast.295")
#loc164 = loc("multiply.296")
#loc165 = loc("multiply.300")
#loc166 = loc("reshape.305")
#loc167 = loc("dot.306")
#loc168 = loc("reshape.307")
#loc169 = loc("logistic.308")
#loc170 = loc("multiply.309")
#loc171 = loc("dot.302")
#loc172 = loc("reshape.303")
#loc173 = loc("multiply.310")
#loc174 = loc("reshape.311")
#loc175 = loc("dot.312")
#loc176 = loc("reshape.313")
#loc177 = loc("add.316")
#loc178 = loc("power.318")
#loc180 = loc("add.324")
#loc181 = loc("multiply.334")
#loc182 = loc("reshape.335")
#loc183 = loc("add.339")
#loc184 = loc("rsqrt.340")
#loc185 = loc("reshape.341")
#loc186 = loc("broadcast.342")
#loc187 = loc("multiply.343")
#loc188 = loc("multiply.347")
#loc1 = loc("p0.2")
#loc2 = loc("p1.5")
#loc3 = loc("p2.6")
#loc4 = loc("p3.10")
#loc5 = loc("p4.11")
#loc6 = loc("p5.13")
#loc7 = loc("p6.15")
#loc8 = loc("p7.47")
#loc9 = loc("p8.69")
#loc10 = loc("p9.74")
#loc11 = loc("p10.77")
#loc12 = loc("p11.82")
#loc13 = loc("p12.93")
#loc14 = loc("p13.129")
#loc15 = loc("p14.170")
#loc16 = loc("p15.206")
#loc17 = loc("p16.297")
#loc18 = loc("p17.304")
#loc19 = loc("p18.344")
module @SyncTensorsGraph.349 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<f32> loc("p0.2"), %arg1: tensor<9728x2560xbf16> loc("p1.5"), %arg2: tensor<2560x9728xbf16> loc("p2.6"), %arg3: tensor<4096x2560xbf16> loc("p3.10"), %arg4: tensor<2560x1024xbf16> loc("p4.11"), %arg5: tensor<bf16> loc("p5.13"), %arg6: tensor<1x512x2560xbf16> loc("p6.15"), %arg7: tensor<2560xbf16> loc("p7.47"), %arg8: tensor<1x1x512x513xbf16> loc("p8.69"), %arg9: tensor<f32> loc("p9.74"), %arg10: tensor<1x512xi64> loc("p10.77"), %arg11: tensor<1x64x1xf32> loc("p11.82"), %arg12: tensor<2560x1024xbf16> loc("p12.93"), %arg13: tensor<128xbf16> loc("p13.129"), %arg14: tensor<2560x4096xbf16> loc("p14.170"), %arg15: tensor<128xbf16> loc("p15.206"), %arg16: tensor<2560xbf16> loc("p16.297"), %arg17: tensor<2560x9728xbf16> loc("p17.304"), %arg18: tensor<2560xbf16> loc("p18.344")) -> tensor<1x512x2560xf32> {
    %cst = stablehlo.constant dense<3.906250e-04> : tensor<1x512xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<1x512x2560xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<7.812500e-03> : tensor<1x512x8xbf16> loc(#loc)
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<1x512x8x128xbf16> loc(#loc)
    %cst_3 = stablehlo.constant dense<7.812500e-03> : tensor<1x512x32xbf16> loc(#loc)
    %cst_4 = stablehlo.constant dense<2.000000e+00> : tensor<1x512x32x128xbf16> loc(#loc)
    %cst_5 = stablehlo.constant dense<3.910060e-04> : tensor<1x512xbf16> loc(#loc)
    %cst_6 = stablehlo.constant dense<2.000000e+00> : tensor<1x512x2560xbf16> loc(#loc)
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %cst_8 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %cst_9 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.convert %arg18 : (tensor<2560xbf16>) -> tensor<2560xf32> loc(#loc20)
    %1 = stablehlo.broadcast_in_dim %0, dims = [2] : (tensor<2560xf32>) -> tensor<1x512x2560xf32> loc(#loc21)
    %2 = stablehlo.convert %arg6 : (tensor<1x512x2560xbf16>) -> tensor<1x512x2560xf32> loc(#loc22)
    %3 = stablehlo.convert %arg15 : (tensor<128xbf16>) -> tensor<128xf32> loc(#loc23)
    %4 = stablehlo.broadcast_in_dim %3, dims = [3] : (tensor<128xf32>) -> tensor<1x512x32x128xf32> loc(#loc24)
    %5 = stablehlo.convert %arg7 : (tensor<2560xbf16>) -> tensor<2560xf32> loc(#loc25)
    %6 = stablehlo.broadcast_in_dim %5, dims = [2] : (tensor<2560xf32>) -> tensor<1x512x2560xf32> loc(#loc26)
    %7 = stablehlo.power %arg6, %cst_6 : tensor<1x512x2560xbf16> loc(#loc27)
    %8 = stablehlo.reduce(%7 init: %cst_9) applies stablehlo.add across dimensions = [2] : (tensor<1x512x2560xbf16>, tensor<bf16>) -> tensor<1x512xbf16> loc(#loc28)
    %9 = stablehlo.multiply %8, %cst_5 : tensor<1x512xbf16> loc(#loc29)
    %10 = stablehlo.reshape %9 : (tensor<1x512xbf16>) -> tensor<1x512x1xbf16> loc(#loc30)
    %11 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<1x512x1xbf16> loc(#loc31)
    %12 = stablehlo.add %10, %11 : tensor<1x512x1xbf16> loc(#loc32)
    %13 = stablehlo.rsqrt %12 : tensor<1x512x1xbf16> loc(#loc33)
    %14 = stablehlo.convert %13 : (tensor<1x512x1xbf16>) -> tensor<1x512x1xf32> loc(#loc34)
    %15 = stablehlo.reshape %14 : (tensor<1x512x1xf32>) -> tensor<1x512xf32> loc(#loc35)
    %16 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x512xf32>) -> tensor<1x512x2560xf32> loc(#loc36)
    %17 = stablehlo.multiply %2, %16 : tensor<1x512x2560xf32> loc(#loc37)
    %18 = stablehlo.convert %17 : (tensor<1x512x2560xf32>) -> tensor<1x512x2560xbf16> loc(#loc38)
    %19 = stablehlo.convert %18 : (tensor<1x512x2560xbf16>) -> tensor<1x512x2560xf32> loc(#loc39)
    %20 = stablehlo.multiply %6, %19 : tensor<1x512x2560xf32> loc(#loc40)
    %21 = stablehlo.convert %20 : (tensor<1x512x2560xf32>) -> tensor<1x512x2560xbf16> loc(#loc41)
    %22 = stablehlo.reshape %21 : (tensor<1x512x2560xbf16>) -> tensor<512x2560xbf16> loc(#loc42)
    %23 = stablehlo.dot_general %22, %arg14, contracting_dims = [1] x [0] : (tensor<512x2560xbf16>, tensor<2560x4096xbf16>) -> tensor<512x4096xbf16> loc(#loc43)
    %24 = stablehlo.reshape %23 : (tensor<512x4096xbf16>) -> tensor<1x512x32x128xbf16> loc(#loc44)
    %25 = stablehlo.convert %24 : (tensor<1x512x32x128xbf16>) -> tensor<1x512x32x128xf32> loc(#loc45)
    %26 = stablehlo.power %24, %cst_4 : tensor<1x512x32x128xbf16> loc(#loc46)
    %27 = stablehlo.reduce(%26 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x512x32x128xbf16>, tensor<bf16>) -> tensor<1x512x32xbf16> loc(#loc47)
    %28 = stablehlo.multiply %27, %cst_3 : tensor<1x512x32xbf16> loc(#loc48)
    %29 = stablehlo.reshape %28 : (tensor<1x512x32xbf16>) -> tensor<1x512x32x1xbf16> loc(#loc49)
    %30 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<1x512x32x1xbf16> loc(#loc50)
    %31 = stablehlo.add %29, %30 : tensor<1x512x32x1xbf16> loc(#loc51)
    %32 = stablehlo.rsqrt %31 : tensor<1x512x32x1xbf16> loc(#loc52)
    %33 = stablehlo.convert %32 : (tensor<1x512x32x1xbf16>) -> tensor<1x512x32x1xf32> loc(#loc53)
    %34 = stablehlo.reshape %33 : (tensor<1x512x32x1xf32>) -> tensor<1x512x32xf32> loc(#loc54)
    %35 = stablehlo.broadcast_in_dim %34, dims = [0, 1, 2] : (tensor<1x512x32xf32>) -> tensor<1x512x32x128xf32> loc(#loc55)
    %36 = stablehlo.multiply %25, %35 : tensor<1x512x32x128xf32> loc(#loc56)
    %37 = stablehlo.convert %36 : (tensor<1x512x32x128xf32>) -> tensor<1x512x32x128xbf16> loc(#loc57)
    %38 = stablehlo.convert %37 : (tensor<1x512x32x128xbf16>) -> tensor<1x512x32x128xf32> loc(#loc58)
    %39 = stablehlo.multiply %4, %38 : tensor<1x512x32x128xf32> loc(#loc59)
    %40 = stablehlo.convert %39 : (tensor<1x512x32x128xf32>) -> tensor<1x512x32x128xbf16> loc(#loc60)
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,32,512,128]{3,1,2,0}"} : (tensor<1x512x32x128xbf16>) -> tensor<1x32x512x128xbf16> loc(#loc61)
    %42 = stablehlo.convert %41 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,32,512,128]{3,1,2,0}"} : (tensor<1x32x512x128xbf16>) -> tensor<1x32x512x128xf32> loc(#loc62)
    %43 = stablehlo.reshape %arg10 : (tensor<1x512xi64>) -> tensor<1x1x512xi64> loc(#loc63)
    %44 = stablehlo.convert %43 : (tensor<1x1x512xi64>) -> tensor<1x1x512xf32> loc(#loc64)
    %45 = stablehlo.dot_general %arg11, %44, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x512xf32>) -> tensor<1x64x512xf32> loc(#loc65)
    %46 = stablehlo.transpose %45, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,512,64]{1,2,0}"} : (tensor<1x64x512xf32>) -> tensor<1x512x64xf32> loc(#loc66)
    %47 = stablehlo.concatenate %46, %46, dim = 2 : (tensor<1x512x64xf32>, tensor<1x512x64xf32>) -> tensor<1x512x128xf32> loc(#loc67)
    %48 = stablehlo.cosine %47 : tensor<1x512x128xf32> loc(#loc68)
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc69)
    %50 = stablehlo.multiply %42, %49 : tensor<1x32x512x128xf32> loc(#loc70)
    %51 = stablehlo.slice %41 [0:1, 0:32, 0:512, 64:128] : (tensor<1x32x512x128xbf16>) -> tensor<1x32x512x64xbf16> loc(#loc71)
    %52 = stablehlo.negate %51 : tensor<1x32x512x64xbf16> loc(#loc72)
    %53 = stablehlo.slice %41 [0:1, 0:32, 0:512, 0:64] : (tensor<1x32x512x128xbf16>) -> tensor<1x32x512x64xbf16> loc(#loc73)
    %54 = stablehlo.concatenate %52, %53, dim = 3 : (tensor<1x32x512x64xbf16>, tensor<1x32x512x64xbf16>) -> tensor<1x32x512x128xbf16> loc(#loc74)
    %55 = stablehlo.convert %54 : (tensor<1x32x512x128xbf16>) -> tensor<1x32x512x128xf32> loc(#loc75)
    %56 = stablehlo.sine %47 : tensor<1x512x128xf32> loc(#loc76)
    %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc77)
    %58 = stablehlo.multiply %55, %57 : tensor<1x32x512x128xf32> loc(#loc78)
    %59 = stablehlo.add %50, %58 : tensor<1x32x512x128xf32> loc(#loc79)
    %60 = stablehlo.reshape %59 : (tensor<1x32x512x128xf32>) -> tensor<32x512x128xf32> loc(#loc80)
    %61 = stablehlo.convert %arg13 : (tensor<128xbf16>) -> tensor<128xf32> loc(#loc81)
    %62 = stablehlo.broadcast_in_dim %61, dims = [3] : (tensor<128xf32>) -> tensor<1x512x8x128xf32> loc(#loc82)
    %63 = stablehlo.dot_general %22, %arg12, contracting_dims = [1] x [0] : (tensor<512x2560xbf16>, tensor<2560x1024xbf16>) -> tensor<512x1024xbf16> loc(#loc83)
    %64 = stablehlo.reshape %63 : (tensor<512x1024xbf16>) -> tensor<1x512x8x128xbf16> loc(#loc84)
    %65 = stablehlo.convert %64 : (tensor<1x512x8x128xbf16>) -> tensor<1x512x8x128xf32> loc(#loc85)
    %66 = stablehlo.power %64, %cst_2 : tensor<1x512x8x128xbf16> loc(#loc86)
    %67 = stablehlo.reduce(%66 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x512x8x128xbf16>, tensor<bf16>) -> tensor<1x512x8xbf16> loc(#loc87)
    %68 = stablehlo.multiply %67, %cst_1 : tensor<1x512x8xbf16> loc(#loc88)
    %69 = stablehlo.reshape %68 : (tensor<1x512x8xbf16>) -> tensor<1x512x8x1xbf16> loc(#loc89)
    %70 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<1x512x8x1xbf16> loc(#loc90)
    %71 = stablehlo.add %69, %70 : tensor<1x512x8x1xbf16> loc(#loc91)
    %72 = stablehlo.rsqrt %71 : tensor<1x512x8x1xbf16> loc(#loc92)
    %73 = stablehlo.convert %72 : (tensor<1x512x8x1xbf16>) -> tensor<1x512x8x1xf32> loc(#loc93)
    %74 = stablehlo.reshape %73 : (tensor<1x512x8x1xf32>) -> tensor<1x512x8xf32> loc(#loc94)
    %75 = stablehlo.broadcast_in_dim %74, dims = [0, 1, 2] : (tensor<1x512x8xf32>) -> tensor<1x512x8x128xf32> loc(#loc95)
    %76 = stablehlo.multiply %65, %75 : tensor<1x512x8x128xf32> loc(#loc96)
    %77 = stablehlo.convert %76 : (tensor<1x512x8x128xf32>) -> tensor<1x512x8x128xbf16> loc(#loc97)
    %78 = stablehlo.convert %77 : (tensor<1x512x8x128xbf16>) -> tensor<1x512x8x128xf32> loc(#loc98)
    %79 = stablehlo.multiply %62, %78 : tensor<1x512x8x128xf32> loc(#loc99)
    %80 = stablehlo.convert %79 : (tensor<1x512x8x128xf32>) -> tensor<1x512x8x128xbf16> loc(#loc100)
    %81 = stablehlo.transpose %80, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,512,128]{3,1,2,0}"} : (tensor<1x512x8x128xbf16>) -> tensor<1x8x512x128xbf16> loc(#loc101)
    %82 = stablehlo.convert %81 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,512,128]{3,1,2,0}"} : (tensor<1x8x512x128xbf16>) -> tensor<1x8x512x128xf32> loc(#loc102)
    %83 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x512x128xf32>) -> tensor<1x8x512x128xf32> loc(#loc103)
    %84 = stablehlo.multiply %82, %83 : tensor<1x8x512x128xf32> loc(#loc104)
    %85 = stablehlo.slice %81 [0:1, 0:8, 0:512, 64:128] : (tensor<1x8x512x128xbf16>) -> tensor<1x8x512x64xbf16> loc(#loc105)
    %86 = stablehlo.negate %85 : tensor<1x8x512x64xbf16> loc(#loc106)
    %87 = stablehlo.slice %81 [0:1, 0:8, 0:512, 0:64] : (tensor<1x8x512x128xbf16>) -> tensor<1x8x512x64xbf16> loc(#loc107)
    %88 = stablehlo.concatenate %86, %87, dim = 3 : (tensor<1x8x512x64xbf16>, tensor<1x8x512x64xbf16>) -> tensor<1x8x512x128xbf16> loc(#loc108)
    %89 = stablehlo.convert %88 : (tensor<1x8x512x128xbf16>) -> tensor<1x8x512x128xf32> loc(#loc109)
    %90 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x512x128xf32>) -> tensor<1x8x512x128xf32> loc(#loc110)
    %91 = stablehlo.multiply %89, %90 : tensor<1x8x512x128xf32> loc(#loc111)
    %92 = stablehlo.add %84, %91 : tensor<1x8x512x128xf32> loc(#loc112)
    %93 = stablehlo.broadcast_in_dim %92, dims = [0, 1, 3, 4] : (tensor<1x8x512x128xf32>) -> tensor<1x8x4x512x128xf32> loc(#loc113)
    %94 = stablehlo.reshape %93 : (tensor<1x8x4x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc114)
    %95 = stablehlo.transpose %94, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[1,32,128,512]{2,3,1,0}"} : (tensor<1x32x512x128xf32>) -> tensor<1x32x128x512xf32> loc(#loc115)
    %96 = stablehlo.reshape %95 : (tensor<1x32x128x512xf32>) -> tensor<32x128x512xf32> loc(#loc116)
    %97 = stablehlo.dot_general %60, %96, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x512x128xf32>, tensor<32x128x512xf32>) -> tensor<32x512x512xf32> loc(#loc117)
    %98 = stablehlo.reshape %97 : (tensor<32x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc118)
    %99 = stablehlo.broadcast_in_dim %arg9, dims = [] : (tensor<f32>) -> tensor<1x32x512x512xf32> loc(#loc119)
    %100 = stablehlo.multiply %98, %99 : tensor<1x32x512x512xf32> loc(#loc120)
    %101 = stablehlo.slice %arg8 [0:1, 0:1, 0:512, 0:512] : (tensor<1x1x512x513xbf16>) -> tensor<1x1x512x512xbf16> loc(#loc121)
    %102 = stablehlo.convert %101 : (tensor<1x1x512x512xbf16>) -> tensor<1x1x512x512xf32> loc(#loc122)
    %103 = stablehlo.reshape %102 : (tensor<1x1x512x512xf32>) -> tensor<1x512x512xf32> loc(#loc123)
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 2, 3] : (tensor<1x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc124)
    %105 = stablehlo.add %100, %104 : tensor<1x32x512x512xf32> loc(#loc125)
    %106 = stablehlo.reduce(%105 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x32x512x512xf32>, tensor<f32>) -> tensor<1x32x512xf32> loc(#loc126)
    %107 = stablehlo.broadcast_in_dim %106, dims = [0, 1, 2] : (tensor<1x32x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc127)
    %108 = stablehlo.subtract %105, %107 : tensor<1x32x512x512xf32> loc(#loc128)
    %109 = stablehlo.exponential %108 : tensor<1x32x512x512xf32> loc(#loc129)
    %110 = stablehlo.reduce(%109 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x32x512x512xf32>, tensor<f32>) -> tensor<1x32x512xf32> loc(#loc130)
    %111 = stablehlo.broadcast_in_dim %110, dims = [0, 1, 2] : (tensor<1x32x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc131)
    %112 = stablehlo.divide %109, %111 : tensor<1x32x512x512xf32> loc(#loc132)
    %113 = stablehlo.reshape %112 : (tensor<1x32x512x512xf32>) -> tensor<32x512x512xf32> loc(#loc133)
    %114 = stablehlo.dot_general %22, %arg4, contracting_dims = [1] x [0] : (tensor<512x2560xbf16>, tensor<2560x1024xbf16>) -> tensor<512x1024xbf16> loc(#loc134)
    %115 = stablehlo.reshape %114 : (tensor<512x1024xbf16>) -> tensor<1x512x8x128xbf16> loc(#loc135)
    %116 = stablehlo.transpose %115, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,512,128]{3,1,2,0}"} : (tensor<1x512x8x128xbf16>) -> tensor<1x8x512x128xbf16> loc(#loc136)
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1, 3, 4] : (tensor<1x8x512x128xbf16>) -> tensor<1x8x4x512x128xbf16> loc(#loc137)
    %118 = stablehlo.reshape %117 : (tensor<1x8x4x512x128xbf16>) -> tensor<32x512x128xbf16> loc(#loc138)
    %119 = stablehlo.convert %118 : (tensor<32x512x128xbf16>) -> tensor<32x512x128xf32> loc(#loc139)
    %120 = stablehlo.dot_general %113, %119, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x512x512xf32>, tensor<32x512x128xf32>) -> tensor<32x512x128xf32> loc(#loc140)
    %121 = stablehlo.reshape %120 : (tensor<32x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc141)
    %122 = stablehlo.transpose %121, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,512,32,128]{3,1,2,0}"} : (tensor<1x32x512x128xf32>) -> tensor<1x512x32x128xf32> loc(#loc142)
    %123 = stablehlo.reshape %122 : (tensor<1x512x32x128xf32>) -> tensor<512x4096xf32> loc(#loc143)
    %124 = stablehlo.dot_general %123, %arg3, contracting_dims = [1] x [0] : (tensor<512x4096xf32>, tensor<4096x2560xbf16>) -> tensor<512x2560xf32> loc(#loc144)
    %125 = stablehlo.reshape %124 : (tensor<512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc145)
    %126 = stablehlo.add %2, %125 : tensor<1x512x2560xf32> loc(#loc146)
    %127 = stablehlo.convert %arg16 : (tensor<2560xbf16>) -> tensor<2560xf32> loc(#loc147)
    %128 = stablehlo.broadcast_in_dim %127, dims = [2] : (tensor<2560xf32>) -> tensor<1x512x2560xf32> loc(#loc148)
    %129 = stablehlo.power %126, %cst_0 : tensor<1x512x2560xf32> loc(#loc149)
    %130 = stablehlo.reduce(%129 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x512x2560xf32>, tensor<f32>) -> tensor<1x512xf32> loc(#loc150)
    %131 = stablehlo.multiply %130, %cst : tensor<1x512xf32> loc(#loc151)
    %132 = stablehlo.reshape %131 : (tensor<1x512xf32>) -> tensor<1x512x1xf32> loc(#loc152)
    %133 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<1x512x1xf32> loc(#loc153)
    %134 = stablehlo.add %132, %133 : tensor<1x512x1xf32> loc(#loc154)
    %135 = stablehlo.rsqrt %134 : tensor<1x512x1xf32> loc(#loc155)
    %136 = stablehlo.reshape %135 : (tensor<1x512x1xf32>) -> tensor<1x512xf32> loc(#loc156)
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1] : (tensor<1x512xf32>) -> tensor<1x512x2560xf32> loc(#loc157)
    %138 = stablehlo.multiply %126, %137 : tensor<1x512x2560xf32> loc(#loc158)
    %139 = stablehlo.multiply %128, %138 : tensor<1x512x2560xf32> loc(#loc159)
    %140 = stablehlo.reshape %139 : (tensor<1x512x2560xf32>) -> tensor<512x2560xf32> loc(#loc160)
    %141 = stablehlo.dot_general %140, %arg17, contracting_dims = [1] x [0] : (tensor<512x2560xf32>, tensor<2560x9728xbf16>) -> tensor<512x9728xf32> loc(#loc161)
    %142 = stablehlo.reshape %141 : (tensor<512x9728xf32>) -> tensor<1x512x9728xf32> loc(#loc162)
    %143 = stablehlo.logistic %142 : tensor<1x512x9728xf32> loc(#loc163)
    %144 = stablehlo.multiply %142, %143 : tensor<1x512x9728xf32> loc(#loc164)
    %145 = stablehlo.dot_general %140, %arg2, contracting_dims = [1] x [0] : (tensor<512x2560xf32>, tensor<2560x9728xbf16>) -> tensor<512x9728xf32> loc(#loc165)
    %146 = stablehlo.reshape %145 : (tensor<512x9728xf32>) -> tensor<1x512x9728xf32> loc(#loc166)
    %147 = stablehlo.multiply %144, %146 : tensor<1x512x9728xf32> loc(#loc167)
    %148 = stablehlo.reshape %147 : (tensor<1x512x9728xf32>) -> tensor<512x9728xf32> loc(#loc168)
    %149 = stablehlo.dot_general %148, %arg1, contracting_dims = [1] x [0] : (tensor<512x9728xf32>, tensor<9728x2560xbf16>) -> tensor<512x2560xf32> loc(#loc169)
    %150 = stablehlo.reshape %149 : (tensor<512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc170)
    %151 = stablehlo.add %126, %150 : tensor<1x512x2560xf32> loc(#loc171)
    %152 = stablehlo.power %151, %cst_0 : tensor<1x512x2560xf32> loc(#loc172)
    %153 = stablehlo.reduce(%152 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x512x2560xf32>, tensor<f32>) -> tensor<1x512xf32> loc(#loc173)
    %154 = stablehlo.multiply %153, %cst : tensor<1x512xf32> loc(#loc174)
    %155 = stablehlo.reshape %154 : (tensor<1x512xf32>) -> tensor<1x512x1xf32> loc(#loc175)
    %156 = stablehlo.add %155, %133 : tensor<1x512x1xf32> loc(#loc176)
    %157 = stablehlo.rsqrt %156 : tensor<1x512x1xf32> loc(#loc177)
    %158 = stablehlo.reshape %157 : (tensor<1x512x1xf32>) -> tensor<1x512xf32> loc(#loc178)
    %159 = stablehlo.broadcast_in_dim %158, dims = [0, 1] : (tensor<1x512xf32>) -> tensor<1x512x2560xf32> loc(#loc179)
    %160 = stablehlo.multiply %151, %159 : tensor<1x512x2560xf32> loc(#loc180)
    %161 = stablehlo.multiply %1, %160 : tensor<1x512x2560xf32> loc(#loc181)
    return %161 : tensor<1x512x2560xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc20 = loc("convert.345")
#loc21 = loc("broadcast.346")
#loc22 = loc("convert.266")
#loc23 = loc("convert.207")
#loc24 = loc("broadcast.208")
#loc25 = loc("convert.48")
#loc26 = loc("broadcast.49")
#loc27 = loc("power.17")
#loc28 = loc("reduce.24")
#loc29 = loc("multiply.33")
#loc30 = loc("reshape.34")
#loc31 = loc("broadcast.37")
#loc32 = loc("add.38")
#loc33 = loc("rsqrt.39")
#loc34 = loc("convert.40")
#loc35 = loc("reshape.42")
#loc36 = loc("broadcast.43")
#loc37 = loc("multiply.44")
#loc38 = loc("convert.45")
#loc39 = loc("convert.46")
#loc40 = loc("multiply.50")
#loc41 = loc("convert.51")
#loc42 = loc("reshape.171")
#loc43 = loc("dot.172")
#loc44 = loc("reshape.174")
#loc45 = loc("convert.200")
#loc46 = loc("power.176")
#loc47 = loc("reduce.183")
#loc48 = loc("multiply.192")
#loc49 = loc("reshape.193")
#loc50 = loc("broadcast.196")
#loc51 = loc("add.197")
#loc52 = loc("rsqrt.198")
#loc53 = loc("convert.199")
#loc54 = loc("reshape.201")
#loc55 = loc("broadcast.202")
#loc56 = loc("multiply.203")
#loc57 = loc("convert.204")
#loc58 = loc("convert.205")
#loc59 = loc("multiply.209")
#loc60 = loc("convert.210")
#loc61 = loc("transpose.211")
#loc62 = loc("convert.220")
#loc63 = loc("reshape.79")
#loc64 = loc("convert.83")
#loc65 = loc("dot.84")
#loc66 = loc("transpose.85")
#loc67 = loc("concatenate.86")
#loc68 = loc("cosine.144")
#loc69 = loc("broadcast.222")
#loc70 = loc("multiply.223")
#loc71 = loc("slice.213")
#loc72 = loc("negate.214")
#loc73 = loc("slice.212")
#loc74 = loc("concatenate.215")
#loc75 = loc("convert.216")
#loc76 = loc("sine.87")
#loc77 = loc("broadcast.218")
#loc78 = loc("multiply.219")
#loc79 = loc("add.226")
#loc80 = loc("reshape.228")
#loc81 = loc("convert.130")
#loc82 = loc("broadcast.131")
#loc83 = loc("dot.95")
#loc84 = loc("reshape.97")
#loc85 = loc("convert.123")
#loc86 = loc("power.99")
#loc87 = loc("reduce.106")
#loc88 = loc("multiply.115")
#loc89 = loc("reshape.116")
#loc90 = loc("broadcast.119")
#loc91 = loc("add.120")
#loc92 = loc("rsqrt.121")
#loc93 = loc("convert.122")
#loc94 = loc("reshape.124")
#loc95 = loc("broadcast.125")
#loc96 = loc("multiply.126")
#loc97 = loc("convert.127")
#loc98 = loc("convert.128")
#loc99 = loc("multiply.132")
#loc100 = loc("convert.133")
#loc101 = loc("transpose.134")
#loc102 = loc("convert.148")
#loc103 = loc("broadcast.150")
#loc104 = loc("multiply.151")
#loc105 = loc("slice.136")
#loc106 = loc("negate.137")
#loc107 = loc("slice.135")
#loc108 = loc("concatenate.138")
#loc109 = loc("convert.139")
#loc110 = loc("broadcast.141")
#loc111 = loc("multiply.142")
#loc112 = loc("add.154")
#loc113 = loc("broadcast.162")
#loc114 = loc("reshape.163")
#loc115 = loc("transpose.164")
#loc116 = loc("reshape.166")
#loc117 = loc("dot.229")
#loc118 = loc("reshape.230")
#loc119 = loc("broadcast.231")
#loc120 = loc("multiply.232")
#loc121 = loc("slice.73")
#loc122 = loc("convert.233")
#loc123 = loc("reshape.236")
#loc124 = loc("broadcast.237")
#loc125 = loc("add.238")
#loc126 = loc("reduce.244")
#loc127 = loc("broadcast.245")
#loc128 = loc("subtract.246")
#loc129 = loc("exponential.247")
#loc130 = loc("reduce.253")
#loc131 = loc("broadcast.254")
#loc132 = loc("divide.255")
#loc133 = loc("reshape.257")
#loc134 = loc("dot.53")
#loc135 = loc("reshape.55")
#loc136 = loc("transpose.56")
#loc137 = loc("broadcast.64")
#loc138 = loc("reshape.67")
#loc139 = loc("convert.258")
#loc140 = loc("dot.259")
#loc141 = loc("reshape.260")
#loc142 = loc("transpose.261")
#loc143 = loc("reshape.263")
#loc144 = loc("dot.264")
#loc145 = loc("reshape.265")
#loc146 = loc("add.269")
#loc147 = loc("convert.298")
#loc148 = loc("broadcast.299")
#loc149 = loc("power.271")
#loc150 = loc("reduce.278")
#loc151 = loc("multiply.287")
#loc152 = loc("reshape.288")
#loc153 = loc("broadcast.291")
#loc154 = loc("add.292")
#loc155 = loc("rsqrt.293")
#loc156 = loc("reshape.294")
#loc157 = loc("broadcast.295")
#loc158 = loc("multiply.296")
#loc159 = loc("multiply.300")
#loc160 = loc("reshape.305")
#loc161 = loc("dot.306")
#loc162 = loc("reshape.307")
#loc163 = loc("logistic.308")
#loc164 = loc("multiply.309")
#loc165 = loc("dot.302")
#loc166 = loc("reshape.303")
#loc167 = loc("multiply.310")
#loc168 = loc("reshape.311")
#loc169 = loc("dot.312")
#loc170 = loc("reshape.313")
#loc171 = loc("add.316")
#loc172 = loc("power.318")
#loc173 = loc("reduce.325")
#loc174 = loc("multiply.334")
#loc175 = loc("reshape.335")
#loc176 = loc("add.339")
#loc177 = loc("rsqrt.340")
#loc178 = loc("reshape.341")
#loc179 = loc("broadcast.342")
#loc180 = loc("multiply.343")
#loc181 = loc("multiply.347")
#loc1 = loc("p0.2")
#loc2 = loc("p1.5")
#loc3 = loc("p2.6")
#loc4 = loc("p3.10")
#loc5 = loc("p4.11")
#loc6 = loc("p5.13")
#loc7 = loc("p6.15")
#loc8 = loc("p7.47")
#loc9 = loc("p8.69")
#loc10 = loc("p9.74")
#loc11 = loc("p10.77")
#loc12 = loc("p11.82")
#loc13 = loc("p12.93")
#loc14 = loc("p13.129")
#loc15 = loc("p14.170")
#loc16 = loc("p15.206")
#loc17 = loc("p16.297")
#loc18 = loc("p17.304")
#loc19 = loc("p18.344")
module @SyncTensorsGraph.349 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<f32> loc("p0.2"), %arg1: tensor<9728x2560xbf16> loc("p1.5"), %arg2: tensor<2560x9728xbf16> loc("p2.6"), %arg3: tensor<4096x2560xbf16> loc("p3.10"), %arg4: tensor<2560x1024xbf16> loc("p4.11"), %arg5: tensor<bf16> loc("p5.13"), %arg6: tensor<1x512x2560xbf16> loc("p6.15"), %arg7: tensor<2560xbf16> loc("p7.47"), %arg8: tensor<1x1x512x513xbf16> loc("p8.69"), %arg9: tensor<f32> loc("p9.74"), %arg10: tensor<1x512xi64> loc("p10.77"), %arg11: tensor<1x64x1xf32> loc("p11.82"), %arg12: tensor<2560x1024xbf16> loc("p12.93"), %arg13: tensor<128xbf16> loc("p13.129"), %arg14: tensor<2560x4096xbf16> loc("p14.170"), %arg15: tensor<128xbf16> loc("p15.206"), %arg16: tensor<2560xbf16> loc("p16.297"), %arg17: tensor<2560x9728xbf16> loc("p17.304"), %arg18: tensor<2560xbf16> loc("p18.344")) -> tensor<1x512x2560xf32> {
    %cst = stablehlo.constant dense<3.906250e-04> : tensor<1x512xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<1x512x2560xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<7.812500e-03> : tensor<1x512x8xbf16> loc(#loc)
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<1x512x8x128xbf16> loc(#loc)
    %cst_3 = stablehlo.constant dense<7.812500e-03> : tensor<1x512x32xbf16> loc(#loc)
    %cst_4 = stablehlo.constant dense<2.000000e+00> : tensor<1x512x32x128xbf16> loc(#loc)
    %cst_5 = stablehlo.constant dense<3.910060e-04> : tensor<1x512xbf16> loc(#loc)
    %cst_6 = stablehlo.constant dense<2.000000e+00> : tensor<1x512x2560xbf16> loc(#loc)
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %cst_8 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %cst_9 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.convert %arg18 : (tensor<2560xbf16>) -> tensor<2560xf32> loc(#loc20)
    %1 = stablehlo.broadcast_in_dim %0, dims = [2] : (tensor<2560xf32>) -> tensor<1x512x2560xf32> loc(#loc21)
    %2 = stablehlo.convert %arg6 : (tensor<1x512x2560xbf16>) -> tensor<1x512x2560xf32> loc(#loc22)
    %3 = stablehlo.convert %arg15 : (tensor<128xbf16>) -> tensor<128xf32> loc(#loc23)
    %4 = stablehlo.broadcast_in_dim %3, dims = [3] : (tensor<128xf32>) -> tensor<1x512x32x128xf32> loc(#loc24)
    %5 = stablehlo.convert %arg7 : (tensor<2560xbf16>) -> tensor<2560xf32> loc(#loc25)
    %6 = stablehlo.broadcast_in_dim %5, dims = [2] : (tensor<2560xf32>) -> tensor<1x512x2560xf32> loc(#loc26)
    %7 = stablehlo.power %arg6, %cst_6 : tensor<1x512x2560xbf16> loc(#loc27)
    %8 = stablehlo.reduce(%7 init: %cst_9) applies stablehlo.add across dimensions = [2] : (tensor<1x512x2560xbf16>, tensor<bf16>) -> tensor<1x512xbf16> loc(#loc28)
    %9 = stablehlo.multiply %8, %cst_5 : tensor<1x512xbf16> loc(#loc29)
    %10 = stablehlo.reshape %9 : (tensor<1x512xbf16>) -> tensor<1x512x1xbf16> loc(#loc30)
    %11 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<1x512x1xbf16> loc(#loc31)
    %12 = stablehlo.add %10, %11 : tensor<1x512x1xbf16> loc(#loc32)
    %13 = stablehlo.rsqrt %12 : tensor<1x512x1xbf16> loc(#loc33)
    %14 = stablehlo.convert %13 : (tensor<1x512x1xbf16>) -> tensor<1x512x1xf32> loc(#loc34)
    %15 = stablehlo.reshape %14 : (tensor<1x512x1xf32>) -> tensor<1x512xf32> loc(#loc35)
    %16 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x512xf32>) -> tensor<1x512x2560xf32> loc(#loc36)
    %17 = stablehlo.multiply %2, %16 : tensor<1x512x2560xf32> loc(#loc37)
    %18 = stablehlo.convert %17 : (tensor<1x512x2560xf32>) -> tensor<1x512x2560xbf16> loc(#loc38)
    %19 = stablehlo.convert %18 : (tensor<1x512x2560xbf16>) -> tensor<1x512x2560xf32> loc(#loc39)
    %20 = stablehlo.multiply %6, %19 : tensor<1x512x2560xf32> loc(#loc40)
    %21 = stablehlo.convert %20 : (tensor<1x512x2560xf32>) -> tensor<1x512x2560xbf16> loc(#loc41)
    %22 = stablehlo.reshape %21 : (tensor<1x512x2560xbf16>) -> tensor<512x2560xbf16> loc(#loc42)
    %23 = stablehlo.dot_general %22, %arg14, contracting_dims = [1] x [0] : (tensor<512x2560xbf16>, tensor<2560x4096xbf16>) -> tensor<512x4096xbf16> loc(#loc43)
    %24 = stablehlo.reshape %23 : (tensor<512x4096xbf16>) -> tensor<1x512x32x128xbf16> loc(#loc44)
    %25 = stablehlo.convert %24 : (tensor<1x512x32x128xbf16>) -> tensor<1x512x32x128xf32> loc(#loc45)
    %26 = stablehlo.power %24, %cst_4 : tensor<1x512x32x128xbf16> loc(#loc46)
    %27 = stablehlo.reduce(%26 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x512x32x128xbf16>, tensor<bf16>) -> tensor<1x512x32xbf16> loc(#loc47)
    %28 = stablehlo.multiply %27, %cst_3 : tensor<1x512x32xbf16> loc(#loc48)
    %29 = stablehlo.reshape %28 : (tensor<1x512x32xbf16>) -> tensor<1x512x32x1xbf16> loc(#loc49)
    %30 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<1x512x32x1xbf16> loc(#loc50)
    %31 = stablehlo.add %29, %30 : tensor<1x512x32x1xbf16> loc(#loc51)
    %32 = stablehlo.rsqrt %31 : tensor<1x512x32x1xbf16> loc(#loc52)
    %33 = stablehlo.convert %32 : (tensor<1x512x32x1xbf16>) -> tensor<1x512x32x1xf32> loc(#loc53)
    %34 = stablehlo.reshape %33 : (tensor<1x512x32x1xf32>) -> tensor<1x512x32xf32> loc(#loc54)
    %35 = stablehlo.broadcast_in_dim %34, dims = [0, 1, 2] : (tensor<1x512x32xf32>) -> tensor<1x512x32x128xf32> loc(#loc55)
    %36 = stablehlo.multiply %25, %35 : tensor<1x512x32x128xf32> loc(#loc56)
    %37 = stablehlo.convert %36 : (tensor<1x512x32x128xf32>) -> tensor<1x512x32x128xbf16> loc(#loc57)
    %38 = stablehlo.convert %37 : (tensor<1x512x32x128xbf16>) -> tensor<1x512x32x128xf32> loc(#loc58)
    %39 = stablehlo.multiply %4, %38 : tensor<1x512x32x128xf32> loc(#loc59)
    %40 = stablehlo.convert %39 : (tensor<1x512x32x128xf32>) -> tensor<1x512x32x128xbf16> loc(#loc60)
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,32,512,128]{3,1,2,0}"} : (tensor<1x512x32x128xbf16>) -> tensor<1x32x512x128xbf16> loc(#loc61)
    %42 = stablehlo.convert %41 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,32,512,128]{3,1,2,0}"} : (tensor<1x32x512x128xbf16>) -> tensor<1x32x512x128xf32> loc(#loc62)
    %43 = stablehlo.reshape %arg10 : (tensor<1x512xi64>) -> tensor<1x1x512xi64> loc(#loc63)
    %44 = stablehlo.convert %43 : (tensor<1x1x512xi64>) -> tensor<1x1x512xf32> loc(#loc64)
    %45 = stablehlo.dot_general %arg11, %44, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x512xf32>) -> tensor<1x64x512xf32> loc(#loc65)
    %46 = stablehlo.transpose %45, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,512,64]{1,2,0}"} : (tensor<1x64x512xf32>) -> tensor<1x512x64xf32> loc(#loc66)
    %47 = stablehlo.concatenate %46, %46, dim = 2 : (tensor<1x512x64xf32>, tensor<1x512x64xf32>) -> tensor<1x512x128xf32> loc(#loc67)
    %48 = stablehlo.cosine %47 : tensor<1x512x128xf32> loc(#loc68)
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc69)
    %50 = stablehlo.multiply %42, %49 : tensor<1x32x512x128xf32> loc(#loc70)
    %51 = stablehlo.slice %41 [0:1, 0:32, 0:512, 64:128] : (tensor<1x32x512x128xbf16>) -> tensor<1x32x512x64xbf16> loc(#loc71)
    %52 = stablehlo.negate %51 : tensor<1x32x512x64xbf16> loc(#loc72)
    %53 = stablehlo.slice %41 [0:1, 0:32, 0:512, 0:64] : (tensor<1x32x512x128xbf16>) -> tensor<1x32x512x64xbf16> loc(#loc73)
    %54 = stablehlo.concatenate %52, %53, dim = 3 : (tensor<1x32x512x64xbf16>, tensor<1x32x512x64xbf16>) -> tensor<1x32x512x128xbf16> loc(#loc74)
    %55 = stablehlo.convert %54 : (tensor<1x32x512x128xbf16>) -> tensor<1x32x512x128xf32> loc(#loc75)
    %56 = stablehlo.sine %47 : tensor<1x512x128xf32> loc(#loc76)
    %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc77)
    %58 = stablehlo.multiply %55, %57 : tensor<1x32x512x128xf32> loc(#loc78)
    %59 = stablehlo.add %50, %58 : tensor<1x32x512x128xf32> loc(#loc79)
    %60 = stablehlo.reshape %59 : (tensor<1x32x512x128xf32>) -> tensor<32x512x128xf32> loc(#loc80)
    %61 = stablehlo.convert %arg13 : (tensor<128xbf16>) -> tensor<128xf32> loc(#loc81)
    %62 = stablehlo.broadcast_in_dim %61, dims = [3] : (tensor<128xf32>) -> tensor<1x512x8x128xf32> loc(#loc82)
    %63 = stablehlo.dot_general %22, %arg12, contracting_dims = [1] x [0] : (tensor<512x2560xbf16>, tensor<2560x1024xbf16>) -> tensor<512x1024xbf16> loc(#loc83)
    %64 = stablehlo.reshape %63 : (tensor<512x1024xbf16>) -> tensor<1x512x8x128xbf16> loc(#loc84)
    %65 = stablehlo.convert %64 : (tensor<1x512x8x128xbf16>) -> tensor<1x512x8x128xf32> loc(#loc85)
    %66 = stablehlo.power %64, %cst_2 : tensor<1x512x8x128xbf16> loc(#loc86)
    %67 = stablehlo.reduce(%66 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x512x8x128xbf16>, tensor<bf16>) -> tensor<1x512x8xbf16> loc(#loc87)
    %68 = stablehlo.multiply %67, %cst_1 : tensor<1x512x8xbf16> loc(#loc88)
    %69 = stablehlo.reshape %68 : (tensor<1x512x8xbf16>) -> tensor<1x512x8x1xbf16> loc(#loc89)
    %70 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<1x512x8x1xbf16> loc(#loc90)
    %71 = stablehlo.add %69, %70 : tensor<1x512x8x1xbf16> loc(#loc91)
    %72 = stablehlo.rsqrt %71 : tensor<1x512x8x1xbf16> loc(#loc92)
    %73 = stablehlo.convert %72 : (tensor<1x512x8x1xbf16>) -> tensor<1x512x8x1xf32> loc(#loc93)
    %74 = stablehlo.reshape %73 : (tensor<1x512x8x1xf32>) -> tensor<1x512x8xf32> loc(#loc94)
    %75 = stablehlo.broadcast_in_dim %74, dims = [0, 1, 2] : (tensor<1x512x8xf32>) -> tensor<1x512x8x128xf32> loc(#loc95)
    %76 = stablehlo.multiply %65, %75 : tensor<1x512x8x128xf32> loc(#loc96)
    %77 = stablehlo.convert %76 : (tensor<1x512x8x128xf32>) -> tensor<1x512x8x128xbf16> loc(#loc97)
    %78 = stablehlo.convert %77 : (tensor<1x512x8x128xbf16>) -> tensor<1x512x8x128xf32> loc(#loc98)
    %79 = stablehlo.multiply %62, %78 : tensor<1x512x8x128xf32> loc(#loc99)
    %80 = stablehlo.convert %79 : (tensor<1x512x8x128xf32>) -> tensor<1x512x8x128xbf16> loc(#loc100)
    %81 = stablehlo.transpose %80, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,512,128]{3,1,2,0}"} : (tensor<1x512x8x128xbf16>) -> tensor<1x8x512x128xbf16> loc(#loc101)
    %82 = stablehlo.convert %81 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,512,128]{3,1,2,0}"} : (tensor<1x8x512x128xbf16>) -> tensor<1x8x512x128xf32> loc(#loc102)
    %83 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x512x128xf32>) -> tensor<1x8x512x128xf32> loc(#loc103)
    %84 = stablehlo.multiply %82, %83 : tensor<1x8x512x128xf32> loc(#loc104)
    %85 = stablehlo.slice %81 [0:1, 0:8, 0:512, 64:128] : (tensor<1x8x512x128xbf16>) -> tensor<1x8x512x64xbf16> loc(#loc105)
    %86 = stablehlo.negate %85 : tensor<1x8x512x64xbf16> loc(#loc106)
    %87 = stablehlo.slice %81 [0:1, 0:8, 0:512, 0:64] : (tensor<1x8x512x128xbf16>) -> tensor<1x8x512x64xbf16> loc(#loc107)
    %88 = stablehlo.concatenate %86, %87, dim = 3 : (tensor<1x8x512x64xbf16>, tensor<1x8x512x64xbf16>) -> tensor<1x8x512x128xbf16> loc(#loc108)
    %89 = stablehlo.convert %88 : (tensor<1x8x512x128xbf16>) -> tensor<1x8x512x128xf32> loc(#loc109)
    %90 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x512x128xf32>) -> tensor<1x8x512x128xf32> loc(#loc110)
    %91 = stablehlo.multiply %89, %90 : tensor<1x8x512x128xf32> loc(#loc111)
    %92 = stablehlo.add %84, %91 : tensor<1x8x512x128xf32> loc(#loc112)
    %93 = stablehlo.broadcast_in_dim %92, dims = [0, 1, 3, 4] : (tensor<1x8x512x128xf32>) -> tensor<1x8x4x512x128xf32> loc(#loc113)
    %94 = stablehlo.reshape %93 : (tensor<1x8x4x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc114)
    %95 = stablehlo.transpose %94, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[1,32,128,512]{2,3,1,0}"} : (tensor<1x32x512x128xf32>) -> tensor<1x32x128x512xf32> loc(#loc115)
    %96 = stablehlo.reshape %95 : (tensor<1x32x128x512xf32>) -> tensor<32x128x512xf32> loc(#loc116)
    %97 = stablehlo.dot_general %60, %96, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x512x128xf32>, tensor<32x128x512xf32>) -> tensor<32x512x512xf32> loc(#loc117)
    %98 = stablehlo.reshape %97 : (tensor<32x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc118)
    %99 = stablehlo.broadcast_in_dim %arg9, dims = [] : (tensor<f32>) -> tensor<1x32x512x512xf32> loc(#loc119)
    %100 = stablehlo.multiply %98, %99 : tensor<1x32x512x512xf32> loc(#loc120)
    %101 = stablehlo.slice %arg8 [0:1, 0:1, 0:512, 0:512] : (tensor<1x1x512x513xbf16>) -> tensor<1x1x512x512xbf16> loc(#loc121)
    %102 = stablehlo.convert %101 : (tensor<1x1x512x512xbf16>) -> tensor<1x1x512x512xf32> loc(#loc122)
    %103 = stablehlo.reshape %102 : (tensor<1x1x512x512xf32>) -> tensor<1x512x512xf32> loc(#loc123)
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 2, 3] : (tensor<1x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc124)
    %105 = stablehlo.add %100, %104 : tensor<1x32x512x512xf32> loc(#loc125)
    %106 = stablehlo.reduce(%105 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x32x512x512xf32>, tensor<f32>) -> tensor<1x32x512xf32> loc(#loc126)
    %107 = stablehlo.broadcast_in_dim %106, dims = [0, 1, 2] : (tensor<1x32x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc127)
    %108 = stablehlo.subtract %105, %107 : tensor<1x32x512x512xf32> loc(#loc128)
    %109 = stablehlo.exponential %108 : tensor<1x32x512x512xf32> loc(#loc129)
    %110 = stablehlo.reduce(%109 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x32x512x512xf32>, tensor<f32>) -> tensor<1x32x512xf32> loc(#loc130)
    %111 = stablehlo.broadcast_in_dim %110, dims = [0, 1, 2] : (tensor<1x32x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc131)
    %112 = stablehlo.divide %109, %111 : tensor<1x32x512x512xf32> loc(#loc132)
    %113 = stablehlo.reshape %112 : (tensor<1x32x512x512xf32>) -> tensor<32x512x512xf32> loc(#loc133)
    %114 = stablehlo.dot_general %22, %arg4, contracting_dims = [1] x [0] : (tensor<512x2560xbf16>, tensor<2560x1024xbf16>) -> tensor<512x1024xbf16> loc(#loc134)
    %115 = stablehlo.reshape %114 : (tensor<512x1024xbf16>) -> tensor<1x512x8x128xbf16> loc(#loc135)
    %116 = stablehlo.transpose %115, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,512,128]{3,1,2,0}"} : (tensor<1x512x8x128xbf16>) -> tensor<1x8x512x128xbf16> loc(#loc136)
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1, 3, 4] : (tensor<1x8x512x128xbf16>) -> tensor<1x8x4x512x128xbf16> loc(#loc137)
    %118 = stablehlo.reshape %117 : (tensor<1x8x4x512x128xbf16>) -> tensor<32x512x128xbf16> loc(#loc138)
    %119 = stablehlo.convert %118 : (tensor<32x512x128xbf16>) -> tensor<32x512x128xf32> loc(#loc139)
    %120 = stablehlo.dot_general %113, %119, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x512x512xf32>, tensor<32x512x128xf32>) -> tensor<32x512x128xf32> loc(#loc140)
    %121 = stablehlo.reshape %120 : (tensor<32x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc141)
    %122 = stablehlo.transpose %121, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,512,32,128]{3,1,2,0}"} : (tensor<1x32x512x128xf32>) -> tensor<1x512x32x128xf32> loc(#loc142)
    %123 = stablehlo.reshape %122 : (tensor<1x512x32x128xf32>) -> tensor<512x4096xf32> loc(#loc143)
    %124 = stablehlo.dot_general %123, %arg3, contracting_dims = [1] x [0] : (tensor<512x4096xf32>, tensor<4096x2560xbf16>) -> tensor<512x2560xf32> loc(#loc144)
    %125 = stablehlo.reshape %124 : (tensor<512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc145)
    %126 = stablehlo.add %2, %125 : tensor<1x512x2560xf32> loc(#loc146)
    %127 = stablehlo.convert %arg16 : (tensor<2560xbf16>) -> tensor<2560xf32> loc(#loc147)
    %128 = stablehlo.broadcast_in_dim %127, dims = [2] : (tensor<2560xf32>) -> tensor<1x512x2560xf32> loc(#loc148)
    %129 = stablehlo.power %126, %cst_0 : tensor<1x512x2560xf32> loc(#loc149)
    %130 = stablehlo.reduce(%129 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x512x2560xf32>, tensor<f32>) -> tensor<1x512xf32> loc(#loc150)
    %131 = stablehlo.multiply %130, %cst : tensor<1x512xf32> loc(#loc151)
    %132 = stablehlo.reshape %131 : (tensor<1x512xf32>) -> tensor<1x512x1xf32> loc(#loc152)
    %133 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<1x512x1xf32> loc(#loc153)
    %134 = stablehlo.add %132, %133 : tensor<1x512x1xf32> loc(#loc154)
    %135 = stablehlo.rsqrt %134 : tensor<1x512x1xf32> loc(#loc155)
    %136 = stablehlo.reshape %135 : (tensor<1x512x1xf32>) -> tensor<1x512xf32> loc(#loc156)
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1] : (tensor<1x512xf32>) -> tensor<1x512x2560xf32> loc(#loc157)
    %138 = stablehlo.multiply %126, %137 : tensor<1x512x2560xf32> loc(#loc158)
    %139 = stablehlo.multiply %128, %138 : tensor<1x512x2560xf32> loc(#loc159)
    %140 = stablehlo.reshape %139 : (tensor<1x512x2560xf32>) -> tensor<512x2560xf32> loc(#loc160)
    %141 = stablehlo.dot_general %140, %arg17, contracting_dims = [1] x [0] : (tensor<512x2560xf32>, tensor<2560x9728xbf16>) -> tensor<512x9728xf32> loc(#loc161)
    %142 = stablehlo.reshape %141 : (tensor<512x9728xf32>) -> tensor<1x512x9728xf32> loc(#loc162)
    %143 = stablehlo.logistic %142 : tensor<1x512x9728xf32> loc(#loc163)
    %144 = stablehlo.multiply %142, %143 : tensor<1x512x9728xf32> loc(#loc164)
    %145 = stablehlo.dot_general %140, %arg2, contracting_dims = [1] x [0] : (tensor<512x2560xf32>, tensor<2560x9728xbf16>) -> tensor<512x9728xf32> loc(#loc165)
    %146 = stablehlo.reshape %145 : (tensor<512x9728xf32>) -> tensor<1x512x9728xf32> loc(#loc166)
    %147 = stablehlo.multiply %144, %146 : tensor<1x512x9728xf32> loc(#loc167)
    %148 = stablehlo.reshape %147 : (tensor<1x512x9728xf32>) -> tensor<512x9728xf32> loc(#loc168)
    %149 = stablehlo.dot_general %148, %arg1, contracting_dims = [1] x [0] : (tensor<512x9728xf32>, tensor<9728x2560xbf16>) -> tensor<512x2560xf32> loc(#loc169)
    %150 = stablehlo.reshape %149 : (tensor<512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc170)
    %151 = stablehlo.add %126, %150 : tensor<1x512x2560xf32> loc(#loc171)
    %152 = stablehlo.power %151, %cst_0 : tensor<1x512x2560xf32> loc(#loc172)
    %153 = stablehlo.reduce(%152 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x512x2560xf32>, tensor<f32>) -> tensor<1x512xf32> loc(#loc173)
    %154 = stablehlo.multiply %153, %cst : tensor<1x512xf32> loc(#loc174)
    %155 = stablehlo.reshape %154 : (tensor<1x512xf32>) -> tensor<1x512x1xf32> loc(#loc175)
    %156 = stablehlo.add %155, %133 : tensor<1x512x1xf32> loc(#loc176)
    %157 = stablehlo.rsqrt %156 : tensor<1x512x1xf32> loc(#loc177)
    %158 = stablehlo.reshape %157 : (tensor<1x512x1xf32>) -> tensor<1x512xf32> loc(#loc178)
    %159 = stablehlo.broadcast_in_dim %158, dims = [0, 1] : (tensor<1x512xf32>) -> tensor<1x512x2560xf32> loc(#loc179)
    %160 = stablehlo.multiply %151, %159 : tensor<1x512x2560xf32> loc(#loc180)
    %161 = stablehlo.multiply %1, %160 : tensor<1x512x2560xf32> loc(#loc181)
    return %161 : tensor<1x512x2560xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc20 = loc("convert.345")
#loc21 = loc("broadcast.346")
#loc22 = loc("convert.266")
#loc23 = loc("convert.207")
#loc24 = loc("broadcast.208")
#loc25 = loc("convert.48")
#loc26 = loc("broadcast.49")
#loc27 = loc("power.17")
#loc28 = loc("reduce.24")
#loc29 = loc("multiply.33")
#loc30 = loc("reshape.34")
#loc31 = loc("broadcast.37")
#loc32 = loc("add.38")
#loc33 = loc("rsqrt.39")
#loc34 = loc("convert.40")
#loc35 = loc("reshape.42")
#loc36 = loc("broadcast.43")
#loc37 = loc("multiply.44")
#loc38 = loc("convert.45")
#loc39 = loc("convert.46")
#loc40 = loc("multiply.50")
#loc41 = loc("convert.51")
#loc42 = loc("reshape.171")
#loc43 = loc("dot.172")
#loc44 = loc("reshape.174")
#loc45 = loc("convert.200")
#loc46 = loc("power.176")
#loc47 = loc("reduce.183")
#loc48 = loc("multiply.192")
#loc49 = loc("reshape.193")
#loc50 = loc("broadcast.196")
#loc51 = loc("add.197")
#loc52 = loc("rsqrt.198")
#loc53 = loc("convert.199")
#loc54 = loc("reshape.201")
#loc55 = loc("broadcast.202")
#loc56 = loc("multiply.203")
#loc57 = loc("convert.204")
#loc58 = loc("convert.205")
#loc59 = loc("multiply.209")
#loc60 = loc("convert.210")
#loc61 = loc("transpose.211")
#loc62 = loc("convert.220")
#loc63 = loc("reshape.79")
#loc64 = loc("convert.83")
#loc65 = loc("dot.84")
#loc66 = loc("transpose.85")
#loc67 = loc("concatenate.86")
#loc68 = loc("cosine.144")
#loc69 = loc("broadcast.222")
#loc70 = loc("multiply.223")
#loc71 = loc("slice.213")
#loc72 = loc("negate.214")
#loc73 = loc("slice.212")
#loc74 = loc("concatenate.215")
#loc75 = loc("convert.216")
#loc76 = loc("sine.87")
#loc77 = loc("broadcast.218")
#loc78 = loc("multiply.219")
#loc79 = loc("add.226")
#loc80 = loc("reshape.228")
#loc81 = loc("convert.130")
#loc82 = loc("broadcast.131")
#loc83 = loc("dot.95")
#loc84 = loc("reshape.97")
#loc85 = loc("convert.123")
#loc86 = loc("power.99")
#loc87 = loc("reduce.106")
#loc88 = loc("multiply.115")
#loc89 = loc("reshape.116")
#loc90 = loc("broadcast.119")
#loc91 = loc("add.120")
#loc92 = loc("rsqrt.121")
#loc93 = loc("convert.122")
#loc94 = loc("reshape.124")
#loc95 = loc("broadcast.125")
#loc96 = loc("multiply.126")
#loc97 = loc("convert.127")
#loc98 = loc("convert.128")
#loc99 = loc("multiply.132")
#loc100 = loc("convert.133")
#loc101 = loc("transpose.134")
#loc102 = loc("convert.148")
#loc103 = loc("broadcast.150")
#loc104 = loc("multiply.151")
#loc105 = loc("slice.136")
#loc106 = loc("negate.137")
#loc107 = loc("slice.135")
#loc108 = loc("concatenate.138")
#loc109 = loc("convert.139")
#loc110 = loc("broadcast.141")
#loc111 = loc("multiply.142")
#loc112 = loc("add.154")
#loc113 = loc("broadcast.162")
#loc114 = loc("reshape.163")
#loc115 = loc("transpose.164")
#loc116 = loc("reshape.166")
#loc117 = loc("dot.229")
#loc118 = loc("reshape.230")
#loc119 = loc("broadcast.231")
#loc120 = loc("multiply.232")
#loc121 = loc("slice.73")
#loc122 = loc("convert.233")
#loc123 = loc("reshape.236")
#loc124 = loc("broadcast.237")
#loc125 = loc("add.238")
#loc126 = loc("reduce.244")
#loc127 = loc("broadcast.245")
#loc128 = loc("subtract.246")
#loc129 = loc("exponential.247")
#loc130 = loc("reduce.253")
#loc131 = loc("broadcast.254")
#loc132 = loc("divide.255")
#loc133 = loc("reshape.257")
#loc134 = loc("dot.53")
#loc135 = loc("reshape.55")
#loc136 = loc("transpose.56")
#loc137 = loc("broadcast.64")
#loc138 = loc("reshape.67")
#loc139 = loc("convert.258")
#loc140 = loc("dot.259")
#loc141 = loc("reshape.260")
#loc142 = loc("transpose.261")
#loc143 = loc("reshape.263")
#loc144 = loc("dot.264")
#loc145 = loc("reshape.265")
#loc146 = loc("add.269")
#loc147 = loc("convert.298")
#loc148 = loc("broadcast.299")
#loc149 = loc("power.271")
#loc150 = loc("reduce.278")
#loc151 = loc("multiply.287")
#loc152 = loc("reshape.288")
#loc153 = loc("broadcast.291")
#loc154 = loc("add.292")
#loc155 = loc("rsqrt.293")
#loc156 = loc("reshape.294")
#loc157 = loc("broadcast.295")
#loc158 = loc("multiply.296")
#loc159 = loc("multiply.300")
#loc160 = loc("reshape.305")
#loc161 = loc("dot.306")
#loc162 = loc("reshape.307")
#loc163 = loc("logistic.308")
#loc164 = loc("multiply.309")
#loc165 = loc("dot.302")
#loc166 = loc("reshape.303")
#loc167 = loc("multiply.310")
#loc168 = loc("reshape.311")
#loc169 = loc("dot.312")
#loc170 = loc("reshape.313")
#loc171 = loc("add.316")
#loc172 = loc("power.318")
#loc173 = loc("reduce.325")
#loc174 = loc("multiply.334")
#loc175 = loc("reshape.335")
#loc176 = loc("add.339")
#loc177 = loc("rsqrt.340")
#loc178 = loc("reshape.341")
#loc179 = loc("broadcast.342")
#loc180 = loc("multiply.343")
#loc181 = loc("multiply.347")
#loc1 = loc("p0.2")
#loc2 = loc("p1.5")
#loc3 = loc("p2.6")
#loc4 = loc("p3.10")
#loc5 = loc("p4.11")
#loc6 = loc("p5.13")
#loc7 = loc("p6.15")
#loc8 = loc("p7.47")
#loc9 = loc("p8.69")
#loc10 = loc("p9.74")
#loc11 = loc("p10.77")
#loc12 = loc("p11.82")
#loc13 = loc("p12.93")
#loc14 = loc("p13.129")
#loc15 = loc("p14.170")
#loc16 = loc("p15.206")
#loc17 = loc("p16.297")
#loc18 = loc("p17.304")
#loc19 = loc("p18.344")
module @SyncTensorsGraph.349 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]> loc(#loc)
  func.func @main(%arg0: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p0.2"), %arg1: tensor<9728x2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p1.5"), %arg2: tensor<2560x9728xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p2.6"), %arg3: tensor<4096x2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p3.10"), %arg4: tensor<2560x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p4.11"), %arg5: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p5.13"), %arg6: tensor<1x512x2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p6.15"), %arg7: tensor<2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p7.47"), %arg8: tensor<1x1x512x513xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p8.69"), %arg9: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p9.74"), %arg10: tensor<1x512xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p10.77"), %arg11: tensor<1x64x1xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p11.82"), %arg12: tensor<2560x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p12.93"), %arg13: tensor<128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p13.129"), %arg14: tensor<2560x4096xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p14.170"), %arg15: tensor<128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p15.206"), %arg16: tensor<2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p16.297"), %arg17: tensor<2560x9728xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p17.304"), %arg18: tensor<2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p18.344")) -> (tensor<1x512x2560xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<3.906250e-04> : tensor<1x512xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<1x512x2560xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<7.812500e-03> : tensor<1x512x8xbf16> loc(#loc)
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<1x512x8x128xbf16> loc(#loc)
    %cst_3 = stablehlo.constant dense<7.812500e-03> : tensor<1x512x32xbf16> loc(#loc)
    %cst_4 = stablehlo.constant dense<2.000000e+00> : tensor<1x512x32x128xbf16> loc(#loc)
    %cst_5 = stablehlo.constant dense<3.910060e-04> : tensor<1x512xbf16> loc(#loc)
    %cst_6 = stablehlo.constant dense<2.000000e+00> : tensor<1x512x2560xbf16> loc(#loc)
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %cst_8 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %cst_9 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.convert %arg18 : (tensor<2560xbf16>) -> tensor<2560xf32> loc(#loc20)
    %1 = stablehlo.broadcast_in_dim %0, dims = [2] : (tensor<2560xf32>) -> tensor<1x512x2560xf32> loc(#loc21)
    %2 = stablehlo.convert %arg6 : (tensor<1x512x2560xbf16>) -> tensor<1x512x2560xf32> loc(#loc22)
    %3 = stablehlo.convert %arg15 : (tensor<128xbf16>) -> tensor<128xf32> loc(#loc23)
    %4 = stablehlo.broadcast_in_dim %3, dims = [3] : (tensor<128xf32>) -> tensor<1x512x32x128xf32> loc(#loc24)
    %5 = stablehlo.convert %arg7 : (tensor<2560xbf16>) -> tensor<2560xf32> loc(#loc25)
    %6 = stablehlo.broadcast_in_dim %5, dims = [2] : (tensor<2560xf32>) -> tensor<1x512x2560xf32> loc(#loc26)
    %7 = stablehlo.power %arg6, %cst_6 : tensor<1x512x2560xbf16> loc(#loc27)
    %8 = stablehlo.reduce(%7 init: %cst_9) applies stablehlo.add across dimensions = [2] : (tensor<1x512x2560xbf16>, tensor<bf16>) -> tensor<1x512xbf16> loc(#loc28)
    %9 = stablehlo.multiply %8, %cst_5 : tensor<1x512xbf16> loc(#loc29)
    %10 = stablehlo.reshape %9 : (tensor<1x512xbf16>) -> tensor<1x512x1xbf16> loc(#loc30)
    %11 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<1x512x1xbf16> loc(#loc31)
    %12 = stablehlo.add %10, %11 : tensor<1x512x1xbf16> loc(#loc32)
    %13 = stablehlo.rsqrt %12 : tensor<1x512x1xbf16> loc(#loc33)
    %14 = stablehlo.convert %13 : (tensor<1x512x1xbf16>) -> tensor<1x512x1xf32> loc(#loc34)
    %15 = stablehlo.reshape %14 : (tensor<1x512x1xf32>) -> tensor<1x512xf32> loc(#loc35)
    %16 = stablehlo.broadcast_in_dim %15, dims = [0, 1] : (tensor<1x512xf32>) -> tensor<1x512x2560xf32> loc(#loc36)
    %17 = stablehlo.multiply %2, %16 : tensor<1x512x2560xf32> loc(#loc37)
    %18 = stablehlo.convert %17 : (tensor<1x512x2560xf32>) -> tensor<1x512x2560xbf16> loc(#loc38)
    %19 = stablehlo.convert %18 : (tensor<1x512x2560xbf16>) -> tensor<1x512x2560xf32> loc(#loc39)
    %20 = stablehlo.multiply %6, %19 : tensor<1x512x2560xf32> loc(#loc40)
    %21 = stablehlo.convert %20 : (tensor<1x512x2560xf32>) -> tensor<1x512x2560xbf16> loc(#loc41)
    %22 = stablehlo.reshape %21 : (tensor<1x512x2560xbf16>) -> tensor<512x2560xbf16> loc(#loc42)
    %23 = stablehlo.dot_general %22, %arg14, contracting_dims = [1] x [0] : (tensor<512x2560xbf16>, tensor<2560x4096xbf16>) -> tensor<512x4096xbf16> loc(#loc43)
    %24 = stablehlo.reshape %23 : (tensor<512x4096xbf16>) -> tensor<1x512x32x128xbf16> loc(#loc44)
    %25 = stablehlo.convert %24 : (tensor<1x512x32x128xbf16>) -> tensor<1x512x32x128xf32> loc(#loc45)
    %26 = stablehlo.power %24, %cst_4 : tensor<1x512x32x128xbf16> loc(#loc46)
    %27 = stablehlo.reduce(%26 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x512x32x128xbf16>, tensor<bf16>) -> tensor<1x512x32xbf16> loc(#loc47)
    %28 = stablehlo.multiply %27, %cst_3 : tensor<1x512x32xbf16> loc(#loc48)
    %29 = stablehlo.reshape %28 : (tensor<1x512x32xbf16>) -> tensor<1x512x32x1xbf16> loc(#loc49)
    %30 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<1x512x32x1xbf16> loc(#loc50)
    %31 = stablehlo.add %29, %30 : tensor<1x512x32x1xbf16> loc(#loc51)
    %32 = stablehlo.rsqrt %31 : tensor<1x512x32x1xbf16> loc(#loc52)
    %33 = stablehlo.convert %32 : (tensor<1x512x32x1xbf16>) -> tensor<1x512x32x1xf32> loc(#loc53)
    %34 = stablehlo.reshape %33 : (tensor<1x512x32x1xf32>) -> tensor<1x512x32xf32> loc(#loc54)
    %35 = stablehlo.broadcast_in_dim %34, dims = [0, 1, 2] : (tensor<1x512x32xf32>) -> tensor<1x512x32x128xf32> loc(#loc55)
    %36 = stablehlo.multiply %25, %35 : tensor<1x512x32x128xf32> loc(#loc56)
    %37 = stablehlo.convert %36 : (tensor<1x512x32x128xf32>) -> tensor<1x512x32x128xbf16> loc(#loc57)
    %38 = stablehlo.convert %37 : (tensor<1x512x32x128xbf16>) -> tensor<1x512x32x128xf32> loc(#loc58)
    %39 = stablehlo.multiply %4, %38 : tensor<1x512x32x128xf32> loc(#loc59)
    %40 = stablehlo.convert %39 : (tensor<1x512x32x128xf32>) -> tensor<1x512x32x128xbf16> loc(#loc60)
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,32,512,128]{3,1,2,0}"} : (tensor<1x512x32x128xbf16>) -> tensor<1x32x512x128xbf16> loc(#loc61)
    %42 = stablehlo.convert %41 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,32,512,128]{3,1,2,0}"} : (tensor<1x32x512x128xbf16>) -> tensor<1x32x512x128xf32> loc(#loc62)
    %43 = stablehlo.reshape %arg10 : (tensor<1x512xi64>) -> tensor<1x1x512xi64> loc(#loc63)
    %44 = stablehlo.convert %43 : (tensor<1x1x512xi64>) -> tensor<1x1x512xf32> loc(#loc64)
    %45 = stablehlo.dot_general %arg11, %44, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x512xf32>) -> tensor<1x64x512xf32> loc(#loc65)
    %46 = stablehlo.transpose %45, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,512,64]{1,2,0}"} : (tensor<1x64x512xf32>) -> tensor<1x512x64xf32> loc(#loc66)
    %47 = stablehlo.concatenate %46, %46, dim = 2 : (tensor<1x512x64xf32>, tensor<1x512x64xf32>) -> tensor<1x512x128xf32> loc(#loc67)
    %48 = stablehlo.cosine %47 : tensor<1x512x128xf32> loc(#loc68)
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc69)
    %50 = stablehlo.multiply %42, %49 : tensor<1x32x512x128xf32> loc(#loc70)
    %51 = stablehlo.slice %41 [0:1, 0:32, 0:512, 64:128] : (tensor<1x32x512x128xbf16>) -> tensor<1x32x512x64xbf16> loc(#loc71)
    %52 = stablehlo.negate %51 : tensor<1x32x512x64xbf16> loc(#loc72)
    %53 = stablehlo.slice %41 [0:1, 0:32, 0:512, 0:64] : (tensor<1x32x512x128xbf16>) -> tensor<1x32x512x64xbf16> loc(#loc73)
    %54 = stablehlo.concatenate %52, %53, dim = 3 : (tensor<1x32x512x64xbf16>, tensor<1x32x512x64xbf16>) -> tensor<1x32x512x128xbf16> loc(#loc74)
    %55 = stablehlo.convert %54 : (tensor<1x32x512x128xbf16>) -> tensor<1x32x512x128xf32> loc(#loc75)
    %56 = stablehlo.sine %47 : tensor<1x512x128xf32> loc(#loc76)
    %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc77)
    %58 = stablehlo.multiply %55, %57 : tensor<1x32x512x128xf32> loc(#loc78)
    %59 = stablehlo.add %50, %58 : tensor<1x32x512x128xf32> loc(#loc79)
    %60 = stablehlo.reshape %59 : (tensor<1x32x512x128xf32>) -> tensor<32x512x128xf32> loc(#loc80)
    %61 = stablehlo.convert %arg13 : (tensor<128xbf16>) -> tensor<128xf32> loc(#loc81)
    %62 = stablehlo.broadcast_in_dim %61, dims = [3] : (tensor<128xf32>) -> tensor<1x512x8x128xf32> loc(#loc82)
    %63 = stablehlo.dot_general %22, %arg12, contracting_dims = [1] x [0] : (tensor<512x2560xbf16>, tensor<2560x1024xbf16>) -> tensor<512x1024xbf16> loc(#loc83)
    %64 = stablehlo.reshape %63 : (tensor<512x1024xbf16>) -> tensor<1x512x8x128xbf16> loc(#loc84)
    %65 = stablehlo.convert %64 : (tensor<1x512x8x128xbf16>) -> tensor<1x512x8x128xf32> loc(#loc85)
    %66 = stablehlo.power %64, %cst_2 : tensor<1x512x8x128xbf16> loc(#loc86)
    %67 = stablehlo.reduce(%66 init: %cst_9) applies stablehlo.add across dimensions = [3] : (tensor<1x512x8x128xbf16>, tensor<bf16>) -> tensor<1x512x8xbf16> loc(#loc87)
    %68 = stablehlo.multiply %67, %cst_1 : tensor<1x512x8xbf16> loc(#loc88)
    %69 = stablehlo.reshape %68 : (tensor<1x512x8xbf16>) -> tensor<1x512x8x1xbf16> loc(#loc89)
    %70 = stablehlo.broadcast_in_dim %arg5, dims = [] : (tensor<bf16>) -> tensor<1x512x8x1xbf16> loc(#loc90)
    %71 = stablehlo.add %69, %70 : tensor<1x512x8x1xbf16> loc(#loc91)
    %72 = stablehlo.rsqrt %71 : tensor<1x512x8x1xbf16> loc(#loc92)
    %73 = stablehlo.convert %72 : (tensor<1x512x8x1xbf16>) -> tensor<1x512x8x1xf32> loc(#loc93)
    %74 = stablehlo.reshape %73 : (tensor<1x512x8x1xf32>) -> tensor<1x512x8xf32> loc(#loc94)
    %75 = stablehlo.broadcast_in_dim %74, dims = [0, 1, 2] : (tensor<1x512x8xf32>) -> tensor<1x512x8x128xf32> loc(#loc95)
    %76 = stablehlo.multiply %65, %75 : tensor<1x512x8x128xf32> loc(#loc96)
    %77 = stablehlo.convert %76 : (tensor<1x512x8x128xf32>) -> tensor<1x512x8x128xbf16> loc(#loc97)
    %78 = stablehlo.convert %77 : (tensor<1x512x8x128xbf16>) -> tensor<1x512x8x128xf32> loc(#loc98)
    %79 = stablehlo.multiply %62, %78 : tensor<1x512x8x128xf32> loc(#loc99)
    %80 = stablehlo.convert %79 : (tensor<1x512x8x128xf32>) -> tensor<1x512x8x128xbf16> loc(#loc100)
    %81 = stablehlo.transpose %80, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,512,128]{3,1,2,0}"} : (tensor<1x512x8x128xbf16>) -> tensor<1x8x512x128xbf16> loc(#loc101)
    %82 = stablehlo.convert %81 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,512,128]{3,1,2,0}"} : (tensor<1x8x512x128xbf16>) -> tensor<1x8x512x128xf32> loc(#loc102)
    %83 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x512x128xf32>) -> tensor<1x8x512x128xf32> loc(#loc103)
    %84 = stablehlo.multiply %82, %83 : tensor<1x8x512x128xf32> loc(#loc104)
    %85 = stablehlo.slice %81 [0:1, 0:8, 0:512, 64:128] : (tensor<1x8x512x128xbf16>) -> tensor<1x8x512x64xbf16> loc(#loc105)
    %86 = stablehlo.negate %85 : tensor<1x8x512x64xbf16> loc(#loc106)
    %87 = stablehlo.slice %81 [0:1, 0:8, 0:512, 0:64] : (tensor<1x8x512x128xbf16>) -> tensor<1x8x512x64xbf16> loc(#loc107)
    %88 = stablehlo.concatenate %86, %87, dim = 3 : (tensor<1x8x512x64xbf16>, tensor<1x8x512x64xbf16>) -> tensor<1x8x512x128xbf16> loc(#loc108)
    %89 = stablehlo.convert %88 : (tensor<1x8x512x128xbf16>) -> tensor<1x8x512x128xf32> loc(#loc109)
    %90 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x512x128xf32>) -> tensor<1x8x512x128xf32> loc(#loc110)
    %91 = stablehlo.multiply %89, %90 : tensor<1x8x512x128xf32> loc(#loc111)
    %92 = stablehlo.add %84, %91 : tensor<1x8x512x128xf32> loc(#loc112)
    %93 = stablehlo.broadcast_in_dim %92, dims = [0, 1, 3, 4] : (tensor<1x8x512x128xf32>) -> tensor<1x8x4x512x128xf32> loc(#loc113)
    %94 = stablehlo.reshape %93 : (tensor<1x8x4x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc114)
    %95 = stablehlo.transpose %94, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[1,32,128,512]{2,3,1,0}"} : (tensor<1x32x512x128xf32>) -> tensor<1x32x128x512xf32> loc(#loc115)
    %96 = stablehlo.reshape %95 : (tensor<1x32x128x512xf32>) -> tensor<32x128x512xf32> loc(#loc116)
    %97 = stablehlo.dot_general %60, %96, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x512x128xf32>, tensor<32x128x512xf32>) -> tensor<32x512x512xf32> loc(#loc117)
    %98 = stablehlo.reshape %97 : (tensor<32x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc118)
    %99 = stablehlo.broadcast_in_dim %arg9, dims = [] : (tensor<f32>) -> tensor<1x32x512x512xf32> loc(#loc119)
    %100 = stablehlo.multiply %98, %99 : tensor<1x32x512x512xf32> loc(#loc120)
    %101 = stablehlo.slice %arg8 [0:1, 0:1, 0:512, 0:512] : (tensor<1x1x512x513xbf16>) -> tensor<1x1x512x512xbf16> loc(#loc121)
    %102 = stablehlo.convert %101 : (tensor<1x1x512x512xbf16>) -> tensor<1x1x512x512xf32> loc(#loc122)
    %103 = stablehlo.reshape %102 : (tensor<1x1x512x512xf32>) -> tensor<1x512x512xf32> loc(#loc123)
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 2, 3] : (tensor<1x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc124)
    %105 = stablehlo.add %100, %104 : tensor<1x32x512x512xf32> loc(#loc125)
    %106 = stablehlo.reduce(%105 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x32x512x512xf32>, tensor<f32>) -> tensor<1x32x512xf32> loc(#loc126)
    %107 = stablehlo.broadcast_in_dim %106, dims = [0, 1, 2] : (tensor<1x32x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc127)
    %108 = stablehlo.subtract %105, %107 : tensor<1x32x512x512xf32> loc(#loc128)
    %109 = stablehlo.exponential %108 : tensor<1x32x512x512xf32> loc(#loc129)
    %110 = stablehlo.reduce(%109 init: %cst_7) applies stablehlo.add across dimensions = [3] : (tensor<1x32x512x512xf32>, tensor<f32>) -> tensor<1x32x512xf32> loc(#loc130)
    %111 = stablehlo.broadcast_in_dim %110, dims = [0, 1, 2] : (tensor<1x32x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc131)
    %112 = stablehlo.divide %109, %111 : tensor<1x32x512x512xf32> loc(#loc132)
    %113 = stablehlo.reshape %112 : (tensor<1x32x512x512xf32>) -> tensor<32x512x512xf32> loc(#loc133)
    %114 = stablehlo.dot_general %22, %arg4, contracting_dims = [1] x [0] : (tensor<512x2560xbf16>, tensor<2560x1024xbf16>) -> tensor<512x1024xbf16> loc(#loc134)
    %115 = stablehlo.reshape %114 : (tensor<512x1024xbf16>) -> tensor<1x512x8x128xbf16> loc(#loc135)
    %116 = stablehlo.transpose %115, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,512,128]{3,1,2,0}"} : (tensor<1x512x8x128xbf16>) -> tensor<1x8x512x128xbf16> loc(#loc136)
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1, 3, 4] : (tensor<1x8x512x128xbf16>) -> tensor<1x8x4x512x128xbf16> loc(#loc137)
    %118 = stablehlo.reshape %117 : (tensor<1x8x4x512x128xbf16>) -> tensor<32x512x128xbf16> loc(#loc138)
    %119 = stablehlo.convert %118 : (tensor<32x512x128xbf16>) -> tensor<32x512x128xf32> loc(#loc139)
    %120 = stablehlo.dot_general %113, %119, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x512x512xf32>, tensor<32x512x128xf32>) -> tensor<32x512x128xf32> loc(#loc140)
    %121 = stablehlo.reshape %120 : (tensor<32x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc141)
    %122 = stablehlo.transpose %121, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,512,32,128]{3,1,2,0}"} : (tensor<1x32x512x128xf32>) -> tensor<1x512x32x128xf32> loc(#loc142)
    %123 = stablehlo.reshape %122 : (tensor<1x512x32x128xf32>) -> tensor<512x4096xf32> loc(#loc143)
    %124 = stablehlo.dot_general %123, %arg3, contracting_dims = [1] x [0] : (tensor<512x4096xf32>, tensor<4096x2560xbf16>) -> tensor<512x2560xf32> loc(#loc144)
    %125 = stablehlo.reshape %124 : (tensor<512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc145)
    %126 = stablehlo.add %2, %125 : tensor<1x512x2560xf32> loc(#loc146)
    %127 = stablehlo.convert %arg16 : (tensor<2560xbf16>) -> tensor<2560xf32> loc(#loc147)
    %128 = stablehlo.broadcast_in_dim %127, dims = [2] : (tensor<2560xf32>) -> tensor<1x512x2560xf32> loc(#loc148)
    %129 = stablehlo.power %126, %cst_0 : tensor<1x512x2560xf32> loc(#loc149)
    %130 = stablehlo.reduce(%129 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x512x2560xf32>, tensor<f32>) -> tensor<1x512xf32> loc(#loc150)
    %131 = stablehlo.multiply %130, %cst : tensor<1x512xf32> loc(#loc151)
    %132 = stablehlo.reshape %131 : (tensor<1x512xf32>) -> tensor<1x512x1xf32> loc(#loc152)
    %133 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<1x512x1xf32> loc(#loc153)
    %134 = stablehlo.add %132, %133 : tensor<1x512x1xf32> loc(#loc154)
    %135 = stablehlo.rsqrt %134 : tensor<1x512x1xf32> loc(#loc155)
    %136 = stablehlo.reshape %135 : (tensor<1x512x1xf32>) -> tensor<1x512xf32> loc(#loc156)
    %137 = stablehlo.broadcast_in_dim %136, dims = [0, 1] : (tensor<1x512xf32>) -> tensor<1x512x2560xf32> loc(#loc157)
    %138 = stablehlo.multiply %126, %137 : tensor<1x512x2560xf32> loc(#loc158)
    %139 = stablehlo.multiply %128, %138 : tensor<1x512x2560xf32> loc(#loc159)
    %140 = stablehlo.reshape %139 : (tensor<1x512x2560xf32>) -> tensor<512x2560xf32> loc(#loc160)
    %141 = stablehlo.dot_general %140, %arg17, contracting_dims = [1] x [0] : (tensor<512x2560xf32>, tensor<2560x9728xbf16>) -> tensor<512x9728xf32> loc(#loc161)
    %142 = stablehlo.reshape %141 : (tensor<512x9728xf32>) -> tensor<1x512x9728xf32> loc(#loc162)
    %143 = stablehlo.logistic %142 : tensor<1x512x9728xf32> loc(#loc163)
    %144 = stablehlo.multiply %142, %143 : tensor<1x512x9728xf32> loc(#loc164)
    %145 = stablehlo.dot_general %140, %arg2, contracting_dims = [1] x [0] : (tensor<512x2560xf32>, tensor<2560x9728xbf16>) -> tensor<512x9728xf32> loc(#loc165)
    %146 = stablehlo.reshape %145 : (tensor<512x9728xf32>) -> tensor<1x512x9728xf32> loc(#loc166)
    %147 = stablehlo.multiply %144, %146 : tensor<1x512x9728xf32> loc(#loc167)
    %148 = stablehlo.reshape %147 : (tensor<1x512x9728xf32>) -> tensor<512x9728xf32> loc(#loc168)
    %149 = stablehlo.dot_general %148, %arg1, contracting_dims = [1] x [0] : (tensor<512x9728xf32>, tensor<9728x2560xbf16>) -> tensor<512x2560xf32> loc(#loc169)
    %150 = stablehlo.reshape %149 : (tensor<512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc170)
    %151 = stablehlo.add %126, %150 : tensor<1x512x2560xf32> loc(#loc171)
    %152 = stablehlo.power %151, %cst_0 : tensor<1x512x2560xf32> loc(#loc172)
    %153 = stablehlo.reduce(%152 init: %cst_7) applies stablehlo.add across dimensions = [2] : (tensor<1x512x2560xf32>, tensor<f32>) -> tensor<1x512xf32> loc(#loc173)
    %154 = stablehlo.multiply %153, %cst : tensor<1x512xf32> loc(#loc174)
    %155 = stablehlo.reshape %154 : (tensor<1x512xf32>) -> tensor<1x512x1xf32> loc(#loc175)
    %156 = stablehlo.add %155, %133 : tensor<1x512x1xf32> loc(#loc176)
    %157 = stablehlo.rsqrt %156 : tensor<1x512x1xf32> loc(#loc177)
    %158 = stablehlo.reshape %157 : (tensor<1x512x1xf32>) -> tensor<1x512xf32> loc(#loc178)
    %159 = stablehlo.broadcast_in_dim %158, dims = [0, 1] : (tensor<1x512xf32>) -> tensor<1x512x2560xf32> loc(#loc179)
    %160 = stablehlo.multiply %151, %159 : tensor<1x512x2560xf32> loc(#loc180)
    %161 = stablehlo.multiply %1, %160 : tensor<1x512x2560xf32> loc(#loc181)
    return %161 : tensor<1x512x2560xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc20 = loc("convert.345")
#loc21 = loc("broadcast.346")
#loc22 = loc("convert.266")
#loc23 = loc("convert.207")
#loc24 = loc("broadcast.208")
#loc25 = loc("convert.48")
#loc26 = loc("broadcast.49")
#loc27 = loc("power.17")
#loc28 = loc("reduce.24")
#loc29 = loc("multiply.33")
#loc30 = loc("reshape.34")
#loc31 = loc("broadcast.37")
#loc32 = loc("add.38")
#loc33 = loc("rsqrt.39")
#loc34 = loc("convert.40")
#loc35 = loc("reshape.42")
#loc36 = loc("broadcast.43")
#loc37 = loc("multiply.44")
#loc38 = loc("convert.45")
#loc39 = loc("convert.46")
#loc40 = loc("multiply.50")
#loc41 = loc("convert.51")
#loc42 = loc("reshape.171")
#loc43 = loc("dot.172")
#loc44 = loc("reshape.174")
#loc45 = loc("convert.200")
#loc46 = loc("power.176")
#loc47 = loc("reduce.183")
#loc48 = loc("multiply.192")
#loc49 = loc("reshape.193")
#loc50 = loc("broadcast.196")
#loc51 = loc("add.197")
#loc52 = loc("rsqrt.198")
#loc53 = loc("convert.199")
#loc54 = loc("reshape.201")
#loc55 = loc("broadcast.202")
#loc56 = loc("multiply.203")
#loc57 = loc("convert.204")
#loc58 = loc("convert.205")
#loc59 = loc("multiply.209")
#loc60 = loc("convert.210")
#loc61 = loc("transpose.211")
#loc62 = loc("convert.220")
#loc63 = loc("reshape.79")
#loc64 = loc("convert.83")
#loc65 = loc("dot.84")
#loc66 = loc("transpose.85")
#loc67 = loc("concatenate.86")
#loc68 = loc("cosine.144")
#loc69 = loc("broadcast.222")
#loc70 = loc("multiply.223")
#loc71 = loc("slice.213")
#loc72 = loc("negate.214")
#loc73 = loc("slice.212")
#loc74 = loc("concatenate.215")
#loc75 = loc("convert.216")
#loc76 = loc("sine.87")
#loc77 = loc("broadcast.218")
#loc78 = loc("multiply.219")
#loc79 = loc("add.226")
#loc80 = loc("reshape.228")
#loc81 = loc("convert.130")
#loc82 = loc("broadcast.131")
#loc83 = loc("dot.95")
#loc84 = loc("reshape.97")
#loc85 = loc("convert.123")
#loc86 = loc("power.99")
#loc87 = loc("reduce.106")
#loc88 = loc("multiply.115")
#loc89 = loc("reshape.116")
#loc90 = loc("broadcast.119")
#loc91 = loc("add.120")
#loc92 = loc("rsqrt.121")
#loc93 = loc("convert.122")
#loc94 = loc("reshape.124")
#loc95 = loc("broadcast.125")
#loc96 = loc("multiply.126")
#loc97 = loc("convert.127")
#loc98 = loc("convert.128")
#loc99 = loc("multiply.132")
#loc100 = loc("convert.133")
#loc101 = loc("transpose.134")
#loc102 = loc("convert.148")
#loc103 = loc("broadcast.150")
#loc104 = loc("multiply.151")
#loc105 = loc("slice.136")
#loc106 = loc("negate.137")
#loc107 = loc("slice.135")
#loc108 = loc("concatenate.138")
#loc109 = loc("convert.139")
#loc110 = loc("broadcast.141")
#loc111 = loc("multiply.142")
#loc112 = loc("add.154")
#loc113 = loc("broadcast.162")
#loc114 = loc("reshape.163")
#loc115 = loc("transpose.164")
#loc116 = loc("reshape.166")
#loc117 = loc("dot.229")
#loc118 = loc("reshape.230")
#loc119 = loc("broadcast.231")
#loc120 = loc("multiply.232")
#loc121 = loc("slice.73")
#loc122 = loc("convert.233")
#loc123 = loc("reshape.236")
#loc124 = loc("broadcast.237")
#loc125 = loc("add.238")
#loc126 = loc("reduce.244")
#loc127 = loc("broadcast.245")
#loc128 = loc("subtract.246")
#loc129 = loc("exponential.247")
#loc130 = loc("reduce.253")
#loc131 = loc("broadcast.254")
#loc132 = loc("divide.255")
#loc133 = loc("reshape.257")
#loc134 = loc("dot.53")
#loc135 = loc("reshape.55")
#loc136 = loc("transpose.56")
#loc137 = loc("broadcast.64")
#loc138 = loc("reshape.67")
#loc139 = loc("convert.258")
#loc140 = loc("dot.259")
#loc141 = loc("reshape.260")
#loc142 = loc("transpose.261")
#loc143 = loc("reshape.263")
#loc144 = loc("dot.264")
#loc145 = loc("reshape.265")
#loc146 = loc("add.269")
#loc147 = loc("convert.298")
#loc148 = loc("broadcast.299")
#loc149 = loc("power.271")
#loc150 = loc("reduce.278")
#loc151 = loc("multiply.287")
#loc152 = loc("reshape.288")
#loc153 = loc("broadcast.291")
#loc154 = loc("add.292")
#loc155 = loc("rsqrt.293")
#loc156 = loc("reshape.294")
#loc157 = loc("broadcast.295")
#loc158 = loc("multiply.296")
#loc159 = loc("multiply.300")
#loc160 = loc("reshape.305")
#loc161 = loc("dot.306")
#loc162 = loc("reshape.307")
#loc163 = loc("logistic.308")
#loc164 = loc("multiply.309")
#loc165 = loc("dot.302")
#loc166 = loc("reshape.303")
#loc167 = loc("multiply.310")
#loc168 = loc("reshape.311")
#loc169 = loc("dot.312")
#loc170 = loc("reshape.313")
#loc171 = loc("add.316")
#loc172 = loc("power.318")
#loc173 = loc("reduce.325")
#loc174 = loc("multiply.334")
#loc175 = loc("reshape.335")
#loc176 = loc("add.339")
#loc177 = loc("rsqrt.340")
#loc178 = loc("reshape.341")
#loc179 = loc("broadcast.342")
#loc180 = loc("multiply.343")
#loc181 = loc("multiply.347")
#loc1 = loc("p0.2")
#loc2 = loc("p1.5")
#loc3 = loc("p2.6")
#loc4 = loc("p3.10")
#loc5 = loc("p4.11")
#loc6 = loc("p5.13")
#loc7 = loc("p6.15")
#loc8 = loc("p7.47")
#loc9 = loc("p8.69")
#loc10 = loc("p9.74")
#loc11 = loc("p10.77")
#loc12 = loc("p11.82")
#loc13 = loc("p12.93")
#loc14 = loc("p13.129")
#loc15 = loc("p14.170")
#loc16 = loc("p15.206")
#loc17 = loc("p16.297")
#loc18 = loc("p17.304")
#loc19 = loc("p18.344")
module @SyncTensorsGraph.349 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  func.func @main(%arg0: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p0.2"), %arg1: tensor<9728x2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p1.5"), %arg2: tensor<2560x9728xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p2.6"), %arg3: tensor<4096x2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p3.10"), %arg4: tensor<2560x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p4.11"), %arg5: tensor<bf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p5.13"), %arg6: tensor<1x512x2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p6.15"), %arg7: tensor<2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p7.47"), %arg8: tensor<1x1x512x513xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p8.69"), %arg9: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p9.74"), %arg10: tensor<1x512xi64> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p10.77"), %arg11: tensor<1x64x1xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p11.82"), %arg12: tensor<2560x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p12.93"), %arg13: tensor<128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p13.129"), %arg14: tensor<2560x4096xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p14.170"), %arg15: tensor<128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p15.206"), %arg16: tensor<2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p16.297"), %arg17: tensor<2560x9728xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p17.304"), %arg18: tensor<2560xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p18.344")) -> (tensor<1x512x2560xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<3.906250e-04> : tensor<1x512xf32>}> : () -> tensor<1x512xf32> loc(#loc)
    %1 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<1x512x2560xf32>}> : () -> tensor<1x512x2560xf32> loc(#loc)
    %2 = "ttir.constant"() <{value = dense<7.812500e-03> : tensor<1x512x8xbf16>}> : () -> tensor<1x512x8xbf16> loc(#loc)
    %3 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<1x512x8x128xbf16>}> : () -> tensor<1x512x8x128xbf16> loc(#loc)
    %4 = "ttir.constant"() <{value = dense<7.812500e-03> : tensor<1x512x32xbf16>}> : () -> tensor<1x512x32xbf16> loc(#loc)
    %5 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<1x512x32x128xbf16>}> : () -> tensor<1x512x32x128xbf16> loc(#loc)
    %6 = "ttir.constant"() <{value = dense<3.910060e-04> : tensor<1x512xbf16>}> : () -> tensor<1x512xbf16> loc(#loc)
    %7 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<1x512x2560xbf16>}> : () -> tensor<1x512x2560xbf16> loc(#loc)
    %8 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32> loc(#loc)
    %9 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32> loc(#loc)
    %10 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16> loc(#loc)
    %11 = ttir.empty() : tensor<2560xf32> loc(#loc20)
    %12 = "ttir.typecast"(%arg18, %11) <{conservative_folding = false}> : (tensor<2560xbf16>, tensor<2560xf32>) -> tensor<2560xf32> loc(#loc20)
    %13 = ttir.empty() : tensor<1x1x2560xf32> loc(#loc21)
    %14 = "ttir.reshape"(%12, %13) <{shape = [1 : i32, 1 : i32, 2560 : i32]}> : (tensor<2560xf32>, tensor<1x1x2560xf32>) -> tensor<1x1x2560xf32> loc(#loc21)
    %15 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc21)
    %16 = "ttir.broadcast"(%14, %15) <{broadcast_dimensions = array<i64: 1, 512, 1>}> : (tensor<1x1x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc21)
    %17 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc22)
    %18 = "ttir.typecast"(%arg6, %17) <{conservative_folding = false}> : (tensor<1x512x2560xbf16>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc22)
    %19 = ttir.empty() : tensor<128xf32> loc(#loc23)
    %20 = "ttir.typecast"(%arg15, %19) <{conservative_folding = false}> : (tensor<128xbf16>, tensor<128xf32>) -> tensor<128xf32> loc(#loc23)
    %21 = ttir.empty() : tensor<1x1x1x128xf32> loc(#loc24)
    %22 = "ttir.reshape"(%20, %21) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32> loc(#loc24)
    %23 = ttir.empty() : tensor<1x512x32x128xf32> loc(#loc24)
    %24 = "ttir.broadcast"(%22, %23) <{broadcast_dimensions = array<i64: 1, 512, 32, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x512x32x128xf32>) -> tensor<1x512x32x128xf32> loc(#loc24)
    %25 = ttir.empty() : tensor<2560xf32> loc(#loc25)
    %26 = "ttir.typecast"(%arg7, %25) <{conservative_folding = false}> : (tensor<2560xbf16>, tensor<2560xf32>) -> tensor<2560xf32> loc(#loc25)
    %27 = ttir.empty() : tensor<1x1x2560xf32> loc(#loc26)
    %28 = "ttir.reshape"(%26, %27) <{shape = [1 : i32, 1 : i32, 2560 : i32]}> : (tensor<2560xf32>, tensor<1x1x2560xf32>) -> tensor<1x1x2560xf32> loc(#loc26)
    %29 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc26)
    %30 = "ttir.broadcast"(%28, %29) <{broadcast_dimensions = array<i64: 1, 512, 1>}> : (tensor<1x1x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc26)
    %31 = ttir.empty() : tensor<1x512x2560xbf16> loc(#loc27)
    %32 = "ttir.pow"(%arg6, %7, %31) : (tensor<1x512x2560xbf16>, tensor<1x512x2560xbf16>, tensor<1x512x2560xbf16>) -> tensor<1x512x2560xbf16> loc(#loc27)
    %33 = ttir.empty() : tensor<1x512xbf16> loc(#loc28)
    %34 = "ttir.sum"(%32, %33) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x512x2560xbf16>, tensor<1x512xbf16>) -> tensor<1x512xbf16> loc(#loc28)
    %35 = ttir.empty() : tensor<1x512xbf16> loc(#loc29)
    %36 = "ttir.multiply"(%34, %6, %35) : (tensor<1x512xbf16>, tensor<1x512xbf16>, tensor<1x512xbf16>) -> tensor<1x512xbf16> loc(#loc29)
    %37 = ttir.empty() : tensor<1x512x1xbf16> loc(#loc30)
    %38 = "ttir.reshape"(%36, %37) <{shape = [1 : i32, 512 : i32, 1 : i32]}> : (tensor<1x512xbf16>, tensor<1x512x1xbf16>) -> tensor<1x512x1xbf16> loc(#loc30)
    %39 = ttir.empty() : tensor<1x1x1xbf16> loc(#loc31)
    %40 = "ttir.reshape"(%arg5, %39) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1xbf16>) -> tensor<1x1x1xbf16> loc(#loc31)
    %41 = ttir.empty() : tensor<1x512x1xbf16> loc(#loc31)
    %42 = "ttir.broadcast"(%40, %41) <{broadcast_dimensions = array<i64: 1, 512, 1>}> : (tensor<1x1x1xbf16>, tensor<1x512x1xbf16>) -> tensor<1x512x1xbf16> loc(#loc31)
    %43 = ttir.empty() : tensor<1x512x1xbf16> loc(#loc32)
    %44 = "ttir.add"(%38, %42, %43) : (tensor<1x512x1xbf16>, tensor<1x512x1xbf16>, tensor<1x512x1xbf16>) -> tensor<1x512x1xbf16> loc(#loc32)
    %45 = ttir.empty() : tensor<1x512x1xbf16> loc(#loc33)
    %46 = "ttir.rsqrt"(%44, %45) : (tensor<1x512x1xbf16>, tensor<1x512x1xbf16>) -> tensor<1x512x1xbf16> loc(#loc33)
    %47 = ttir.empty() : tensor<1x512x1xf32> loc(#loc34)
    %48 = "ttir.typecast"(%46, %47) <{conservative_folding = false}> : (tensor<1x512x1xbf16>, tensor<1x512x1xf32>) -> tensor<1x512x1xf32> loc(#loc34)
    %49 = ttir.empty() : tensor<1x512xf32> loc(#loc35)
    %50 = "ttir.reshape"(%48, %49) <{shape = [1 : i32, 512 : i32]}> : (tensor<1x512x1xf32>, tensor<1x512xf32>) -> tensor<1x512xf32> loc(#loc35)
    %51 = ttir.empty() : tensor<1x512x1xf32> loc(#loc36)
    %52 = "ttir.reshape"(%50, %51) <{shape = [1 : i32, 512 : i32, 1 : i32]}> : (tensor<1x512xf32>, tensor<1x512x1xf32>) -> tensor<1x512x1xf32> loc(#loc36)
    %53 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc36)
    %54 = "ttir.broadcast"(%52, %53) <{broadcast_dimensions = array<i64: 1, 1, 2560>}> : (tensor<1x512x1xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc36)
    %55 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc37)
    %56 = "ttir.multiply"(%18, %54, %55) : (tensor<1x512x2560xf32>, tensor<1x512x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc37)
    %57 = ttir.empty() : tensor<1x512x2560xbf16> loc(#loc38)
    %58 = "ttir.typecast"(%56, %57) <{conservative_folding = false}> : (tensor<1x512x2560xf32>, tensor<1x512x2560xbf16>) -> tensor<1x512x2560xbf16> loc(#loc38)
    %59 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc39)
    %60 = "ttir.typecast"(%58, %59) <{conservative_folding = false}> : (tensor<1x512x2560xbf16>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc39)
    %61 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc40)
    %62 = "ttir.multiply"(%30, %60, %61) : (tensor<1x512x2560xf32>, tensor<1x512x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc40)
    %63 = ttir.empty() : tensor<1x512x2560xbf16> loc(#loc41)
    %64 = "ttir.typecast"(%62, %63) <{conservative_folding = false}> : (tensor<1x512x2560xf32>, tensor<1x512x2560xbf16>) -> tensor<1x512x2560xbf16> loc(#loc41)
    %65 = ttir.empty() : tensor<512x2560xbf16> loc(#loc42)
    %66 = "ttir.reshape"(%64, %65) <{shape = [512 : i32, 2560 : i32]}> : (tensor<1x512x2560xbf16>, tensor<512x2560xbf16>) -> tensor<512x2560xbf16> loc(#loc42)
    %67 = "ttir.dot_general"(%66, %arg14) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<512x2560xbf16>, tensor<2560x4096xbf16>) -> tensor<512x4096xbf16> loc(#loc43)
    %68 = ttir.empty() : tensor<1x512x32x128xbf16> loc(#loc44)
    %69 = "ttir.reshape"(%67, %68) <{shape = [1 : i32, 512 : i32, 32 : i32, 128 : i32]}> : (tensor<512x4096xbf16>, tensor<1x512x32x128xbf16>) -> tensor<1x512x32x128xbf16> loc(#loc44)
    %70 = ttir.empty() : tensor<1x512x32x128xf32> loc(#loc45)
    %71 = "ttir.typecast"(%69, %70) <{conservative_folding = false}> : (tensor<1x512x32x128xbf16>, tensor<1x512x32x128xf32>) -> tensor<1x512x32x128xf32> loc(#loc45)
    %72 = ttir.empty() : tensor<1x512x32x128xbf16> loc(#loc46)
    %73 = "ttir.pow"(%69, %5, %72) : (tensor<1x512x32x128xbf16>, tensor<1x512x32x128xbf16>, tensor<1x512x32x128xbf16>) -> tensor<1x512x32x128xbf16> loc(#loc46)
    %74 = ttir.empty() : tensor<1x512x32xbf16> loc(#loc47)
    %75 = "ttir.sum"(%73, %74) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x512x32x128xbf16>, tensor<1x512x32xbf16>) -> tensor<1x512x32xbf16> loc(#loc47)
    %76 = ttir.empty() : tensor<1x512x32xbf16> loc(#loc48)
    %77 = "ttir.multiply"(%75, %4, %76) : (tensor<1x512x32xbf16>, tensor<1x512x32xbf16>, tensor<1x512x32xbf16>) -> tensor<1x512x32xbf16> loc(#loc48)
    %78 = ttir.empty() : tensor<1x512x32x1xbf16> loc(#loc49)
    %79 = "ttir.reshape"(%77, %78) <{shape = [1 : i32, 512 : i32, 32 : i32, 1 : i32]}> : (tensor<1x512x32xbf16>, tensor<1x512x32x1xbf16>) -> tensor<1x512x32x1xbf16> loc(#loc49)
    %80 = ttir.empty() : tensor<1x1x1x1xbf16> loc(#loc50)
    %81 = "ttir.reshape"(%arg5, %80) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16> loc(#loc50)
    %82 = ttir.empty() : tensor<1x512x32x1xbf16> loc(#loc50)
    %83 = "ttir.broadcast"(%81, %82) <{broadcast_dimensions = array<i64: 1, 512, 32, 1>}> : (tensor<1x1x1x1xbf16>, tensor<1x512x32x1xbf16>) -> tensor<1x512x32x1xbf16> loc(#loc50)
    %84 = ttir.empty() : tensor<1x512x32x1xbf16> loc(#loc51)
    %85 = "ttir.add"(%79, %83, %84) : (tensor<1x512x32x1xbf16>, tensor<1x512x32x1xbf16>, tensor<1x512x32x1xbf16>) -> tensor<1x512x32x1xbf16> loc(#loc51)
    %86 = ttir.empty() : tensor<1x512x32x1xbf16> loc(#loc52)
    %87 = "ttir.rsqrt"(%85, %86) : (tensor<1x512x32x1xbf16>, tensor<1x512x32x1xbf16>) -> tensor<1x512x32x1xbf16> loc(#loc52)
    %88 = ttir.empty() : tensor<1x512x32x1xf32> loc(#loc53)
    %89 = "ttir.typecast"(%87, %88) <{conservative_folding = false}> : (tensor<1x512x32x1xbf16>, tensor<1x512x32x1xf32>) -> tensor<1x512x32x1xf32> loc(#loc53)
    %90 = ttir.empty() : tensor<1x512x32xf32> loc(#loc54)
    %91 = "ttir.reshape"(%89, %90) <{shape = [1 : i32, 512 : i32, 32 : i32]}> : (tensor<1x512x32x1xf32>, tensor<1x512x32xf32>) -> tensor<1x512x32xf32> loc(#loc54)
    %92 = ttir.empty() : tensor<1x512x32x1xf32> loc(#loc55)
    %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 512 : i32, 32 : i32, 1 : i32]}> : (tensor<1x512x32xf32>, tensor<1x512x32x1xf32>) -> tensor<1x512x32x1xf32> loc(#loc55)
    %94 = ttir.empty() : tensor<1x512x32x128xf32> loc(#loc55)
    %95 = "ttir.broadcast"(%93, %94) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x512x32x1xf32>, tensor<1x512x32x128xf32>) -> tensor<1x512x32x128xf32> loc(#loc55)
    %96 = ttir.empty() : tensor<1x512x32x128xf32> loc(#loc56)
    %97 = "ttir.multiply"(%71, %95, %96) : (tensor<1x512x32x128xf32>, tensor<1x512x32x128xf32>, tensor<1x512x32x128xf32>) -> tensor<1x512x32x128xf32> loc(#loc56)
    %98 = ttir.empty() : tensor<1x512x32x128xbf16> loc(#loc57)
    %99 = "ttir.typecast"(%97, %98) <{conservative_folding = false}> : (tensor<1x512x32x128xf32>, tensor<1x512x32x128xbf16>) -> tensor<1x512x32x128xbf16> loc(#loc57)
    %100 = ttir.empty() : tensor<1x512x32x128xf32> loc(#loc58)
    %101 = "ttir.typecast"(%99, %100) <{conservative_folding = false}> : (tensor<1x512x32x128xbf16>, tensor<1x512x32x128xf32>) -> tensor<1x512x32x128xf32> loc(#loc58)
    %102 = ttir.empty() : tensor<1x512x32x128xf32> loc(#loc59)
    %103 = "ttir.multiply"(%24, %101, %102) : (tensor<1x512x32x128xf32>, tensor<1x512x32x128xf32>, tensor<1x512x32x128xf32>) -> tensor<1x512x32x128xf32> loc(#loc59)
    %104 = ttir.empty() : tensor<1x512x32x128xbf16> loc(#loc60)
    %105 = "ttir.typecast"(%103, %104) <{conservative_folding = false}> : (tensor<1x512x32x128xf32>, tensor<1x512x32x128xbf16>) -> tensor<1x512x32x128xbf16> loc(#loc60)
    %106 = ttir.empty() : tensor<1x32x512x128xbf16> loc(#loc61)
    %107 = "ttir.permute"(%105, %106) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x512x32x128xbf16>, tensor<1x32x512x128xbf16>) -> tensor<1x32x512x128xbf16> loc(#loc61)
    %108 = ttir.empty() : tensor<1x32x512x128xf32> loc(#loc62)
    %109 = "ttir.typecast"(%107, %108) <{conservative_folding = false}> : (tensor<1x32x512x128xbf16>, tensor<1x32x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc62)
    %110 = ttir.empty() : tensor<1x1x512xi64> loc(#loc63)
    %111 = "ttir.reshape"(%arg10, %110) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512xi64>, tensor<1x1x512xi64>) -> tensor<1x1x512xi64> loc(#loc63)
    %112 = ttir.empty() : tensor<1x1x512xf32> loc(#loc64)
    %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x1x512xi64>, tensor<1x1x512xf32>) -> tensor<1x1x512xf32> loc(#loc64)
    %114 = "ttir.dot_general"(%arg11, %113) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x512xf32>) -> tensor<1x64x512xf32> loc(#loc65)
    %115 = ttir.empty() : tensor<1x512x64xf32> loc(#loc66)
    %116 = "ttir.permute"(%114, %115) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x512xf32>, tensor<1x512x64xf32>) -> tensor<1x512x64xf32> loc(#loc66)
    %117 = ttir.empty() : tensor<1x512x128xf32> loc(#loc67)
    %118 = "ttir.concat"(%116, %116, %117) <{dim = 2 : si32}> : (tensor<1x512x64xf32>, tensor<1x512x64xf32>, tensor<1x512x128xf32>) -> tensor<1x512x128xf32> loc(#loc67)
    %119 = ttir.empty() : tensor<1x512x128xf32> loc(#loc68)
    %120 = "ttir.cos"(%118, %119) : (tensor<1x512x128xf32>, tensor<1x512x128xf32>) -> tensor<1x512x128xf32> loc(#loc68)
    %121 = ttir.empty() : tensor<1x1x512x128xf32> loc(#loc69)
    %122 = "ttir.reshape"(%120, %121) <{shape = [1 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x512x128xf32>, tensor<1x1x512x128xf32>) -> tensor<1x1x512x128xf32> loc(#loc69)
    %123 = ttir.empty() : tensor<1x32x512x128xf32> loc(#loc69)
    %124 = "ttir.broadcast"(%122, %123) <{broadcast_dimensions = array<i64: 1, 32, 1, 1>}> : (tensor<1x1x512x128xf32>, tensor<1x32x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc69)
    %125 = ttir.empty() : tensor<1x32x512x128xf32> loc(#loc70)
    %126 = "ttir.multiply"(%109, %124, %125) : (tensor<1x32x512x128xf32>, tensor<1x32x512x128xf32>, tensor<1x32x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc70)
    %127 = ttir.empty() : tensor<1x32x512x64xbf16> loc(#loc71)
    %128 = "ttir.slice_static"(%107, %127) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 32 : i32, 512 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x32x512x128xbf16>, tensor<1x32x512x64xbf16>) -> tensor<1x32x512x64xbf16> loc(#loc71)
    %129 = ttir.empty() : tensor<1x32x512x64xbf16> loc(#loc72)
    %130 = "ttir.neg"(%128, %129) : (tensor<1x32x512x64xbf16>, tensor<1x32x512x64xbf16>) -> tensor<1x32x512x64xbf16> loc(#loc72)
    %131 = ttir.empty() : tensor<1x32x512x64xbf16> loc(#loc73)
    %132 = "ttir.slice_static"(%107, %131) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 32 : i32, 512 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x32x512x128xbf16>, tensor<1x32x512x64xbf16>) -> tensor<1x32x512x64xbf16> loc(#loc73)
    %133 = ttir.empty() : tensor<1x32x512x128xbf16> loc(#loc74)
    %134 = "ttir.concat"(%130, %132, %133) <{dim = 3 : si32}> : (tensor<1x32x512x64xbf16>, tensor<1x32x512x64xbf16>, tensor<1x32x512x128xbf16>) -> tensor<1x32x512x128xbf16> loc(#loc74)
    %135 = ttir.empty() : tensor<1x32x512x128xf32> loc(#loc75)
    %136 = "ttir.typecast"(%134, %135) <{conservative_folding = false}> : (tensor<1x32x512x128xbf16>, tensor<1x32x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc75)
    %137 = ttir.empty() : tensor<1x512x128xf32> loc(#loc76)
    %138 = "ttir.sin"(%118, %137) : (tensor<1x512x128xf32>, tensor<1x512x128xf32>) -> tensor<1x512x128xf32> loc(#loc76)
    %139 = ttir.empty() : tensor<1x1x512x128xf32> loc(#loc77)
    %140 = "ttir.reshape"(%138, %139) <{shape = [1 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x512x128xf32>, tensor<1x1x512x128xf32>) -> tensor<1x1x512x128xf32> loc(#loc77)
    %141 = ttir.empty() : tensor<1x32x512x128xf32> loc(#loc77)
    %142 = "ttir.broadcast"(%140, %141) <{broadcast_dimensions = array<i64: 1, 32, 1, 1>}> : (tensor<1x1x512x128xf32>, tensor<1x32x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc77)
    %143 = ttir.empty() : tensor<1x32x512x128xf32> loc(#loc78)
    %144 = "ttir.multiply"(%136, %142, %143) : (tensor<1x32x512x128xf32>, tensor<1x32x512x128xf32>, tensor<1x32x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc78)
    %145 = ttir.empty() : tensor<1x32x512x128xf32> loc(#loc79)
    %146 = "ttir.add"(%126, %144, %145) : (tensor<1x32x512x128xf32>, tensor<1x32x512x128xf32>, tensor<1x32x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc79)
    %147 = ttir.empty() : tensor<32x512x128xf32> loc(#loc80)
    %148 = "ttir.reshape"(%146, %147) <{shape = [32 : i32, 512 : i32, 128 : i32]}> : (tensor<1x32x512x128xf32>, tensor<32x512x128xf32>) -> tensor<32x512x128xf32> loc(#loc80)
    %149 = ttir.empty() : tensor<128xf32> loc(#loc81)
    %150 = "ttir.typecast"(%arg13, %149) <{conservative_folding = false}> : (tensor<128xbf16>, tensor<128xf32>) -> tensor<128xf32> loc(#loc81)
    %151 = ttir.empty() : tensor<1x1x1x128xf32> loc(#loc82)
    %152 = "ttir.reshape"(%150, %151) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32> loc(#loc82)
    %153 = ttir.empty() : tensor<1x512x8x128xf32> loc(#loc82)
    %154 = "ttir.broadcast"(%152, %153) <{broadcast_dimensions = array<i64: 1, 512, 8, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x512x8x128xf32>) -> tensor<1x512x8x128xf32> loc(#loc82)
    %155 = "ttir.dot_general"(%66, %arg12) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<512x2560xbf16>, tensor<2560x1024xbf16>) -> tensor<512x1024xbf16> loc(#loc83)
    %156 = ttir.empty() : tensor<1x512x8x128xbf16> loc(#loc84)
    %157 = "ttir.reshape"(%155, %156) <{shape = [1 : i32, 512 : i32, 8 : i32, 128 : i32]}> : (tensor<512x1024xbf16>, tensor<1x512x8x128xbf16>) -> tensor<1x512x8x128xbf16> loc(#loc84)
    %158 = ttir.empty() : tensor<1x512x8x128xf32> loc(#loc85)
    %159 = "ttir.typecast"(%157, %158) <{conservative_folding = false}> : (tensor<1x512x8x128xbf16>, tensor<1x512x8x128xf32>) -> tensor<1x512x8x128xf32> loc(#loc85)
    %160 = ttir.empty() : tensor<1x512x8x128xbf16> loc(#loc86)
    %161 = "ttir.pow"(%157, %3, %160) : (tensor<1x512x8x128xbf16>, tensor<1x512x8x128xbf16>, tensor<1x512x8x128xbf16>) -> tensor<1x512x8x128xbf16> loc(#loc86)
    %162 = ttir.empty() : tensor<1x512x8xbf16> loc(#loc87)
    %163 = "ttir.sum"(%161, %162) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x512x8x128xbf16>, tensor<1x512x8xbf16>) -> tensor<1x512x8xbf16> loc(#loc87)
    %164 = ttir.empty() : tensor<1x512x8xbf16> loc(#loc88)
    %165 = "ttir.multiply"(%163, %2, %164) : (tensor<1x512x8xbf16>, tensor<1x512x8xbf16>, tensor<1x512x8xbf16>) -> tensor<1x512x8xbf16> loc(#loc88)
    %166 = ttir.empty() : tensor<1x512x8x1xbf16> loc(#loc89)
    %167 = "ttir.reshape"(%165, %166) <{shape = [1 : i32, 512 : i32, 8 : i32, 1 : i32]}> : (tensor<1x512x8xbf16>, tensor<1x512x8x1xbf16>) -> tensor<1x512x8x1xbf16> loc(#loc89)
    %168 = ttir.empty() : tensor<1x1x1x1xbf16> loc(#loc90)
    %169 = "ttir.reshape"(%arg5, %168) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16> loc(#loc90)
    %170 = ttir.empty() : tensor<1x512x8x1xbf16> loc(#loc90)
    %171 = "ttir.broadcast"(%169, %170) <{broadcast_dimensions = array<i64: 1, 512, 8, 1>}> : (tensor<1x1x1x1xbf16>, tensor<1x512x8x1xbf16>) -> tensor<1x512x8x1xbf16> loc(#loc90)
    %172 = ttir.empty() : tensor<1x512x8x1xbf16> loc(#loc91)
    %173 = "ttir.add"(%167, %171, %172) : (tensor<1x512x8x1xbf16>, tensor<1x512x8x1xbf16>, tensor<1x512x8x1xbf16>) -> tensor<1x512x8x1xbf16> loc(#loc91)
    %174 = ttir.empty() : tensor<1x512x8x1xbf16> loc(#loc92)
    %175 = "ttir.rsqrt"(%173, %174) : (tensor<1x512x8x1xbf16>, tensor<1x512x8x1xbf16>) -> tensor<1x512x8x1xbf16> loc(#loc92)
    %176 = ttir.empty() : tensor<1x512x8x1xf32> loc(#loc93)
    %177 = "ttir.typecast"(%175, %176) <{conservative_folding = false}> : (tensor<1x512x8x1xbf16>, tensor<1x512x8x1xf32>) -> tensor<1x512x8x1xf32> loc(#loc93)
    %178 = ttir.empty() : tensor<1x512x8xf32> loc(#loc94)
    %179 = "ttir.reshape"(%177, %178) <{shape = [1 : i32, 512 : i32, 8 : i32]}> : (tensor<1x512x8x1xf32>, tensor<1x512x8xf32>) -> tensor<1x512x8xf32> loc(#loc94)
    %180 = ttir.empty() : tensor<1x512x8x1xf32> loc(#loc95)
    %181 = "ttir.reshape"(%179, %180) <{shape = [1 : i32, 512 : i32, 8 : i32, 1 : i32]}> : (tensor<1x512x8xf32>, tensor<1x512x8x1xf32>) -> tensor<1x512x8x1xf32> loc(#loc95)
    %182 = ttir.empty() : tensor<1x512x8x128xf32> loc(#loc95)
    %183 = "ttir.broadcast"(%181, %182) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x512x8x1xf32>, tensor<1x512x8x128xf32>) -> tensor<1x512x8x128xf32> loc(#loc95)
    %184 = ttir.empty() : tensor<1x512x8x128xf32> loc(#loc96)
    %185 = "ttir.multiply"(%159, %183, %184) : (tensor<1x512x8x128xf32>, tensor<1x512x8x128xf32>, tensor<1x512x8x128xf32>) -> tensor<1x512x8x128xf32> loc(#loc96)
    %186 = ttir.empty() : tensor<1x512x8x128xbf16> loc(#loc97)
    %187 = "ttir.typecast"(%185, %186) <{conservative_folding = false}> : (tensor<1x512x8x128xf32>, tensor<1x512x8x128xbf16>) -> tensor<1x512x8x128xbf16> loc(#loc97)
    %188 = ttir.empty() : tensor<1x512x8x128xf32> loc(#loc98)
    %189 = "ttir.typecast"(%187, %188) <{conservative_folding = false}> : (tensor<1x512x8x128xbf16>, tensor<1x512x8x128xf32>) -> tensor<1x512x8x128xf32> loc(#loc98)
    %190 = ttir.empty() : tensor<1x512x8x128xf32> loc(#loc99)
    %191 = "ttir.multiply"(%154, %189, %190) : (tensor<1x512x8x128xf32>, tensor<1x512x8x128xf32>, tensor<1x512x8x128xf32>) -> tensor<1x512x8x128xf32> loc(#loc99)
    %192 = ttir.empty() : tensor<1x512x8x128xbf16> loc(#loc100)
    %193 = "ttir.typecast"(%191, %192) <{conservative_folding = false}> : (tensor<1x512x8x128xf32>, tensor<1x512x8x128xbf16>) -> tensor<1x512x8x128xbf16> loc(#loc100)
    %194 = ttir.empty() : tensor<1x8x512x128xbf16> loc(#loc101)
    %195 = "ttir.permute"(%193, %194) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x512x8x128xbf16>, tensor<1x8x512x128xbf16>) -> tensor<1x8x512x128xbf16> loc(#loc101)
    %196 = ttir.empty() : tensor<1x8x512x128xf32> loc(#loc102)
    %197 = "ttir.typecast"(%195, %196) <{conservative_folding = false}> : (tensor<1x8x512x128xbf16>, tensor<1x8x512x128xf32>) -> tensor<1x8x512x128xf32> loc(#loc102)
    %198 = ttir.empty() : tensor<1x1x512x128xf32> loc(#loc103)
    %199 = "ttir.reshape"(%120, %198) <{shape = [1 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x512x128xf32>, tensor<1x1x512x128xf32>) -> tensor<1x1x512x128xf32> loc(#loc103)
    %200 = ttir.empty() : tensor<1x8x512x128xf32> loc(#loc103)
    %201 = "ttir.broadcast"(%199, %200) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x512x128xf32>, tensor<1x8x512x128xf32>) -> tensor<1x8x512x128xf32> loc(#loc103)
    %202 = ttir.empty() : tensor<1x8x512x128xf32> loc(#loc104)
    %203 = "ttir.multiply"(%197, %201, %202) : (tensor<1x8x512x128xf32>, tensor<1x8x512x128xf32>, tensor<1x8x512x128xf32>) -> tensor<1x8x512x128xf32> loc(#loc104)
    %204 = ttir.empty() : tensor<1x8x512x64xbf16> loc(#loc105)
    %205 = "ttir.slice_static"(%195, %204) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 512 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x512x128xbf16>, tensor<1x8x512x64xbf16>) -> tensor<1x8x512x64xbf16> loc(#loc105)
    %206 = ttir.empty() : tensor<1x8x512x64xbf16> loc(#loc106)
    %207 = "ttir.neg"(%205, %206) : (tensor<1x8x512x64xbf16>, tensor<1x8x512x64xbf16>) -> tensor<1x8x512x64xbf16> loc(#loc106)
    %208 = ttir.empty() : tensor<1x8x512x64xbf16> loc(#loc107)
    %209 = "ttir.slice_static"(%195, %208) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 512 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x512x128xbf16>, tensor<1x8x512x64xbf16>) -> tensor<1x8x512x64xbf16> loc(#loc107)
    %210 = ttir.empty() : tensor<1x8x512x128xbf16> loc(#loc108)
    %211 = "ttir.concat"(%207, %209, %210) <{dim = 3 : si32}> : (tensor<1x8x512x64xbf16>, tensor<1x8x512x64xbf16>, tensor<1x8x512x128xbf16>) -> tensor<1x8x512x128xbf16> loc(#loc108)
    %212 = ttir.empty() : tensor<1x8x512x128xf32> loc(#loc109)
    %213 = "ttir.typecast"(%211, %212) <{conservative_folding = false}> : (tensor<1x8x512x128xbf16>, tensor<1x8x512x128xf32>) -> tensor<1x8x512x128xf32> loc(#loc109)
    %214 = ttir.empty() : tensor<1x1x512x128xf32> loc(#loc110)
    %215 = "ttir.reshape"(%138, %214) <{shape = [1 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x512x128xf32>, tensor<1x1x512x128xf32>) -> tensor<1x1x512x128xf32> loc(#loc110)
    %216 = ttir.empty() : tensor<1x8x512x128xf32> loc(#loc110)
    %217 = "ttir.broadcast"(%215, %216) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x512x128xf32>, tensor<1x8x512x128xf32>) -> tensor<1x8x512x128xf32> loc(#loc110)
    %218 = ttir.empty() : tensor<1x8x512x128xf32> loc(#loc111)
    %219 = "ttir.multiply"(%213, %217, %218) : (tensor<1x8x512x128xf32>, tensor<1x8x512x128xf32>, tensor<1x8x512x128xf32>) -> tensor<1x8x512x128xf32> loc(#loc111)
    %220 = ttir.empty() : tensor<1x8x512x128xf32> loc(#loc112)
    %221 = "ttir.add"(%203, %219, %220) : (tensor<1x8x512x128xf32>, tensor<1x8x512x128xf32>, tensor<1x8x512x128xf32>) -> tensor<1x8x512x128xf32> loc(#loc112)
    %222 = ttir.empty() : tensor<1x8x1x512x128xf32> loc(#loc113)
    %223 = "ttir.reshape"(%221, %222) <{shape = [1 : i32, 8 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x512x128xf32>, tensor<1x8x1x512x128xf32>) -> tensor<1x8x1x512x128xf32> loc(#loc113)
    %224 = ttir.empty() : tensor<1x8x4x512x128xf32> loc(#loc113)
    %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x8x1x512x128xf32>, tensor<1x8x4x512x128xf32>) -> tensor<1x8x4x512x128xf32> loc(#loc113)
    %226 = ttir.empty() : tensor<1x32x512x128xf32> loc(#loc114)
    %227 = "ttir.reshape"(%225, %226) <{shape = [1 : i32, 32 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x4x512x128xf32>, tensor<1x32x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc114)
    %228 = ttir.empty() : tensor<1x32x128x512xf32> loc(#loc115)
    %229 = "ttir.permute"(%227, %228) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x32x512x128xf32>, tensor<1x32x128x512xf32>) -> tensor<1x32x128x512xf32> loc(#loc115)
    %230 = ttir.empty() : tensor<32x128x512xf32> loc(#loc116)
    %231 = "ttir.reshape"(%229, %230) <{shape = [32 : i32, 128 : i32, 512 : i32]}> : (tensor<1x32x128x512xf32>, tensor<32x128x512xf32>) -> tensor<32x128x512xf32> loc(#loc116)
    %232 = "ttir.dot_general"(%148, %231) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x512x128xf32>, tensor<32x128x512xf32>) -> tensor<32x512x512xf32> loc(#loc117)
    %233 = ttir.empty() : tensor<1x32x512x512xf32> loc(#loc118)
    %234 = "ttir.reshape"(%232, %233) <{shape = [1 : i32, 32 : i32, 512 : i32, 512 : i32]}> : (tensor<32x512x512xf32>, tensor<1x32x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc118)
    %235 = ttir.empty() : tensor<1x1x1x1xf32> loc(#loc119)
    %236 = "ttir.reshape"(%arg9, %235) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32> loc(#loc119)
    %237 = ttir.empty() : tensor<1x32x512x512xf32> loc(#loc119)
    %238 = "ttir.broadcast"(%236, %237) <{broadcast_dimensions = array<i64: 1, 32, 512, 512>}> : (tensor<1x1x1x1xf32>, tensor<1x32x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc119)
    %239 = ttir.empty() : tensor<1x32x512x512xf32> loc(#loc120)
    %240 = "ttir.multiply"(%234, %238, %239) : (tensor<1x32x512x512xf32>, tensor<1x32x512x512xf32>, tensor<1x32x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc120)
    %241 = ttir.empty() : tensor<1x1x512x512xbf16> loc(#loc121)
    %242 = "ttir.slice_static"(%arg8, %241) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 512 : i32, 512 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x512x513xbf16>, tensor<1x1x512x512xbf16>) -> tensor<1x1x512x512xbf16> loc(#loc121)
    %243 = ttir.empty() : tensor<1x1x512x512xf32> loc(#loc122)
    %244 = "ttir.typecast"(%242, %243) <{conservative_folding = false}> : (tensor<1x1x512x512xbf16>, tensor<1x1x512x512xf32>) -> tensor<1x1x512x512xf32> loc(#loc122)
    %245 = ttir.empty() : tensor<1x512x512xf32> loc(#loc123)
    %246 = "ttir.reshape"(%244, %245) <{shape = [1 : i32, 512 : i32, 512 : i32]}> : (tensor<1x1x512x512xf32>, tensor<1x512x512xf32>) -> tensor<1x512x512xf32> loc(#loc123)
    %247 = ttir.empty() : tensor<1x1x512x512xf32> loc(#loc124)
    %248 = "ttir.reshape"(%246, %247) <{shape = [1 : i32, 1 : i32, 512 : i32, 512 : i32]}> : (tensor<1x512x512xf32>, tensor<1x1x512x512xf32>) -> tensor<1x1x512x512xf32> loc(#loc124)
    %249 = ttir.empty() : tensor<1x32x512x512xf32> loc(#loc124)
    %250 = "ttir.broadcast"(%248, %249) <{broadcast_dimensions = array<i64: 1, 32, 1, 1>}> : (tensor<1x1x512x512xf32>, tensor<1x32x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc124)
    %251 = ttir.empty() : tensor<1x32x512x512xf32> loc(#loc125)
    %252 = "ttir.add"(%240, %250, %251) : (tensor<1x32x512x512xf32>, tensor<1x32x512x512xf32>, tensor<1x32x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc125)
    %253 = ttir.empty() : tensor<1x32x512xf32> loc(#loc126)
    %254 = "ttir.max"(%252, %253) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x32x512x512xf32>, tensor<1x32x512xf32>) -> tensor<1x32x512xf32> loc(#loc126)
    %255 = ttir.empty() : tensor<1x32x512x1xf32> loc(#loc127)
    %256 = "ttir.reshape"(%254, %255) <{shape = [1 : i32, 32 : i32, 512 : i32, 1 : i32]}> : (tensor<1x32x512xf32>, tensor<1x32x512x1xf32>) -> tensor<1x32x512x1xf32> loc(#loc127)
    %257 = ttir.empty() : tensor<1x32x512x512xf32> loc(#loc127)
    %258 = "ttir.broadcast"(%256, %257) <{broadcast_dimensions = array<i64: 1, 1, 1, 512>}> : (tensor<1x32x512x1xf32>, tensor<1x32x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc127)
    %259 = ttir.empty() : tensor<1x32x512x512xf32> loc(#loc128)
    %260 = "ttir.subtract"(%252, %258, %259) : (tensor<1x32x512x512xf32>, tensor<1x32x512x512xf32>, tensor<1x32x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc128)
    %261 = ttir.empty() : tensor<1x32x512x512xf32> loc(#loc129)
    %262 = "ttir.exp"(%260, %261) : (tensor<1x32x512x512xf32>, tensor<1x32x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc129)
    %263 = ttir.empty() : tensor<1x32x512xf32> loc(#loc130)
    %264 = "ttir.sum"(%262, %263) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x32x512x512xf32>, tensor<1x32x512xf32>) -> tensor<1x32x512xf32> loc(#loc130)
    %265 = ttir.empty() : tensor<1x32x512x1xf32> loc(#loc131)
    %266 = "ttir.reshape"(%264, %265) <{shape = [1 : i32, 32 : i32, 512 : i32, 1 : i32]}> : (tensor<1x32x512xf32>, tensor<1x32x512x1xf32>) -> tensor<1x32x512x1xf32> loc(#loc131)
    %267 = ttir.empty() : tensor<1x32x512x512xf32> loc(#loc131)
    %268 = "ttir.broadcast"(%266, %267) <{broadcast_dimensions = array<i64: 1, 1, 1, 512>}> : (tensor<1x32x512x1xf32>, tensor<1x32x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc131)
    %269 = ttir.empty() : tensor<1x32x512x512xf32> loc(#loc132)
    %270 = "ttir.div"(%262, %268, %269) : (tensor<1x32x512x512xf32>, tensor<1x32x512x512xf32>, tensor<1x32x512x512xf32>) -> tensor<1x32x512x512xf32> loc(#loc132)
    %271 = ttir.empty() : tensor<32x512x512xf32> loc(#loc133)
    %272 = "ttir.reshape"(%270, %271) <{shape = [32 : i32, 512 : i32, 512 : i32]}> : (tensor<1x32x512x512xf32>, tensor<32x512x512xf32>) -> tensor<32x512x512xf32> loc(#loc133)
    %273 = "ttir.dot_general"(%66, %arg4) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<512x2560xbf16>, tensor<2560x1024xbf16>) -> tensor<512x1024xbf16> loc(#loc134)
    %274 = ttir.empty() : tensor<1x512x8x128xbf16> loc(#loc135)
    %275 = "ttir.reshape"(%273, %274) <{shape = [1 : i32, 512 : i32, 8 : i32, 128 : i32]}> : (tensor<512x1024xbf16>, tensor<1x512x8x128xbf16>) -> tensor<1x512x8x128xbf16> loc(#loc135)
    %276 = ttir.empty() : tensor<1x8x512x128xbf16> loc(#loc136)
    %277 = "ttir.permute"(%275, %276) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x512x8x128xbf16>, tensor<1x8x512x128xbf16>) -> tensor<1x8x512x128xbf16> loc(#loc136)
    %278 = ttir.empty() : tensor<1x8x1x512x128xbf16> loc(#loc137)
    %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 8 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x512x128xbf16>, tensor<1x8x1x512x128xbf16>) -> tensor<1x8x1x512x128xbf16> loc(#loc137)
    %280 = ttir.empty() : tensor<1x8x4x512x128xbf16> loc(#loc137)
    %281 = "ttir.broadcast"(%279, %280) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x8x1x512x128xbf16>, tensor<1x8x4x512x128xbf16>) -> tensor<1x8x4x512x128xbf16> loc(#loc137)
    %282 = ttir.empty() : tensor<32x512x128xbf16> loc(#loc138)
    %283 = "ttir.reshape"(%281, %282) <{shape = [32 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x4x512x128xbf16>, tensor<32x512x128xbf16>) -> tensor<32x512x128xbf16> loc(#loc138)
    %284 = ttir.empty() : tensor<32x512x128xf32> loc(#loc139)
    %285 = "ttir.typecast"(%283, %284) <{conservative_folding = false}> : (tensor<32x512x128xbf16>, tensor<32x512x128xf32>) -> tensor<32x512x128xf32> loc(#loc139)
    %286 = "ttir.dot_general"(%272, %285) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<32x512x512xf32>, tensor<32x512x128xf32>) -> tensor<32x512x128xf32> loc(#loc140)
    %287 = ttir.empty() : tensor<1x32x512x128xf32> loc(#loc141)
    %288 = "ttir.reshape"(%286, %287) <{shape = [1 : i32, 32 : i32, 512 : i32, 128 : i32]}> : (tensor<32x512x128xf32>, tensor<1x32x512x128xf32>) -> tensor<1x32x512x128xf32> loc(#loc141)
    %289 = ttir.empty() : tensor<1x512x32x128xf32> loc(#loc142)
    %290 = "ttir.permute"(%288, %289) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x32x512x128xf32>, tensor<1x512x32x128xf32>) -> tensor<1x512x32x128xf32> loc(#loc142)
    %291 = ttir.empty() : tensor<512x4096xf32> loc(#loc143)
    %292 = "ttir.reshape"(%290, %291) <{shape = [512 : i32, 4096 : i32]}> : (tensor<1x512x32x128xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32> loc(#loc143)
    %293 = "ttir.dot_general"(%292, %arg3) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<512x4096xf32>, tensor<4096x2560xbf16>) -> tensor<512x2560xf32> loc(#loc144)
    %294 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc145)
    %295 = "ttir.reshape"(%293, %294) <{shape = [1 : i32, 512 : i32, 2560 : i32]}> : (tensor<512x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc145)
    %296 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc146)
    %297 = "ttir.add"(%18, %295, %296) : (tensor<1x512x2560xf32>, tensor<1x512x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc146)
    %298 = ttir.empty() : tensor<2560xf32> loc(#loc147)
    %299 = "ttir.typecast"(%arg16, %298) <{conservative_folding = false}> : (tensor<2560xbf16>, tensor<2560xf32>) -> tensor<2560xf32> loc(#loc147)
    %300 = ttir.empty() : tensor<1x1x2560xf32> loc(#loc148)
    %301 = "ttir.reshape"(%299, %300) <{shape = [1 : i32, 1 : i32, 2560 : i32]}> : (tensor<2560xf32>, tensor<1x1x2560xf32>) -> tensor<1x1x2560xf32> loc(#loc148)
    %302 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc148)
    %303 = "ttir.broadcast"(%301, %302) <{broadcast_dimensions = array<i64: 1, 512, 1>}> : (tensor<1x1x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc148)
    %304 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc149)
    %305 = "ttir.pow"(%297, %1, %304) : (tensor<1x512x2560xf32>, tensor<1x512x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc149)
    %306 = ttir.empty() : tensor<1x512xf32> loc(#loc150)
    %307 = "ttir.sum"(%305, %306) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x512x2560xf32>, tensor<1x512xf32>) -> tensor<1x512xf32> loc(#loc150)
    %308 = ttir.empty() : tensor<1x512xf32> loc(#loc151)
    %309 = "ttir.multiply"(%307, %0, %308) : (tensor<1x512xf32>, tensor<1x512xf32>, tensor<1x512xf32>) -> tensor<1x512xf32> loc(#loc151)
    %310 = ttir.empty() : tensor<1x512x1xf32> loc(#loc152)
    %311 = "ttir.reshape"(%309, %310) <{shape = [1 : i32, 512 : i32, 1 : i32]}> : (tensor<1x512xf32>, tensor<1x512x1xf32>) -> tensor<1x512x1xf32> loc(#loc152)
    %312 = ttir.empty() : tensor<1x1x1xf32> loc(#loc153)
    %313 = "ttir.reshape"(%arg0, %312) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32> loc(#loc153)
    %314 = ttir.empty() : tensor<1x512x1xf32> loc(#loc153)
    %315 = "ttir.broadcast"(%313, %314) <{broadcast_dimensions = array<i64: 1, 512, 1>}> : (tensor<1x1x1xf32>, tensor<1x512x1xf32>) -> tensor<1x512x1xf32> loc(#loc153)
    %316 = ttir.empty() : tensor<1x512x1xf32> loc(#loc154)
    %317 = "ttir.add"(%311, %315, %316) : (tensor<1x512x1xf32>, tensor<1x512x1xf32>, tensor<1x512x1xf32>) -> tensor<1x512x1xf32> loc(#loc154)
    %318 = ttir.empty() : tensor<1x512x1xf32> loc(#loc155)
    %319 = "ttir.rsqrt"(%317, %318) : (tensor<1x512x1xf32>, tensor<1x512x1xf32>) -> tensor<1x512x1xf32> loc(#loc155)
    %320 = ttir.empty() : tensor<1x512xf32> loc(#loc156)
    %321 = "ttir.reshape"(%319, %320) <{shape = [1 : i32, 512 : i32]}> : (tensor<1x512x1xf32>, tensor<1x512xf32>) -> tensor<1x512xf32> loc(#loc156)
    %322 = ttir.empty() : tensor<1x512x1xf32> loc(#loc157)
    %323 = "ttir.reshape"(%321, %322) <{shape = [1 : i32, 512 : i32, 1 : i32]}> : (tensor<1x512xf32>, tensor<1x512x1xf32>) -> tensor<1x512x1xf32> loc(#loc157)
    %324 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc157)
    %325 = "ttir.broadcast"(%323, %324) <{broadcast_dimensions = array<i64: 1, 1, 2560>}> : (tensor<1x512x1xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc157)
    %326 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc158)
    %327 = "ttir.multiply"(%297, %325, %326) : (tensor<1x512x2560xf32>, tensor<1x512x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc158)
    %328 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc159)
    %329 = "ttir.multiply"(%303, %327, %328) : (tensor<1x512x2560xf32>, tensor<1x512x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc159)
    %330 = ttir.empty() : tensor<512x2560xf32> loc(#loc160)
    %331 = "ttir.reshape"(%329, %330) <{shape = [512 : i32, 2560 : i32]}> : (tensor<1x512x2560xf32>, tensor<512x2560xf32>) -> tensor<512x2560xf32> loc(#loc160)
    %332 = "ttir.dot_general"(%331, %arg17) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<512x2560xf32>, tensor<2560x9728xbf16>) -> tensor<512x9728xf32> loc(#loc161)
    %333 = ttir.empty() : tensor<1x512x9728xf32> loc(#loc162)
    %334 = "ttir.reshape"(%332, %333) <{shape = [1 : i32, 512 : i32, 9728 : i32]}> : (tensor<512x9728xf32>, tensor<1x512x9728xf32>) -> tensor<1x512x9728xf32> loc(#loc162)
    %335 = ttir.empty() : tensor<1x512x9728xf32> loc(#loc163)
    %336 = "ttir.sigmoid"(%334, %335) : (tensor<1x512x9728xf32>, tensor<1x512x9728xf32>) -> tensor<1x512x9728xf32> loc(#loc163)
    %337 = ttir.empty() : tensor<1x512x9728xf32> loc(#loc164)
    %338 = "ttir.multiply"(%334, %336, %337) : (tensor<1x512x9728xf32>, tensor<1x512x9728xf32>, tensor<1x512x9728xf32>) -> tensor<1x512x9728xf32> loc(#loc164)
    %339 = "ttir.dot_general"(%331, %arg2) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<512x2560xf32>, tensor<2560x9728xbf16>) -> tensor<512x9728xf32> loc(#loc165)
    %340 = ttir.empty() : tensor<1x512x9728xf32> loc(#loc166)
    %341 = "ttir.reshape"(%339, %340) <{shape = [1 : i32, 512 : i32, 9728 : i32]}> : (tensor<512x9728xf32>, tensor<1x512x9728xf32>) -> tensor<1x512x9728xf32> loc(#loc166)
    %342 = ttir.empty() : tensor<1x512x9728xf32> loc(#loc167)
    %343 = "ttir.multiply"(%338, %341, %342) : (tensor<1x512x9728xf32>, tensor<1x512x9728xf32>, tensor<1x512x9728xf32>) -> tensor<1x512x9728xf32> loc(#loc167)
    %344 = ttir.empty() : tensor<512x9728xf32> loc(#loc168)
    %345 = "ttir.reshape"(%343, %344) <{shape = [512 : i32, 9728 : i32]}> : (tensor<1x512x9728xf32>, tensor<512x9728xf32>) -> tensor<512x9728xf32> loc(#loc168)
    %346 = "ttir.dot_general"(%345, %arg1) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<512x9728xf32>, tensor<9728x2560xbf16>) -> tensor<512x2560xf32> loc(#loc169)
    %347 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc170)
    %348 = "ttir.reshape"(%346, %347) <{shape = [1 : i32, 512 : i32, 2560 : i32]}> : (tensor<512x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc170)
    %349 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc171)
    %350 = "ttir.add"(%297, %348, %349) : (tensor<1x512x2560xf32>, tensor<1x512x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc171)
    %351 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc172)
    %352 = "ttir.pow"(%350, %1, %351) : (tensor<1x512x2560xf32>, tensor<1x512x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc172)
    %353 = ttir.empty() : tensor<1x512xf32> loc(#loc173)
    %354 = "ttir.sum"(%352, %353) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x512x2560xf32>, tensor<1x512xf32>) -> tensor<1x512xf32> loc(#loc173)
    %355 = ttir.empty() : tensor<1x512xf32> loc(#loc174)
    %356 = "ttir.multiply"(%354, %0, %355) : (tensor<1x512xf32>, tensor<1x512xf32>, tensor<1x512xf32>) -> tensor<1x512xf32> loc(#loc174)
    %357 = ttir.empty() : tensor<1x512x1xf32> loc(#loc175)
    %358 = "ttir.reshape"(%356, %357) <{shape = [1 : i32, 512 : i32, 1 : i32]}> : (tensor<1x512xf32>, tensor<1x512x1xf32>) -> tensor<1x512x1xf32> loc(#loc175)
    %359 = ttir.empty() : tensor<1x512x1xf32> loc(#loc176)
    %360 = "ttir.add"(%358, %315, %359) : (tensor<1x512x1xf32>, tensor<1x512x1xf32>, tensor<1x512x1xf32>) -> tensor<1x512x1xf32> loc(#loc176)
    %361 = ttir.empty() : tensor<1x512x1xf32> loc(#loc177)
    %362 = "ttir.rsqrt"(%360, %361) : (tensor<1x512x1xf32>, tensor<1x512x1xf32>) -> tensor<1x512x1xf32> loc(#loc177)
    %363 = ttir.empty() : tensor<1x512xf32> loc(#loc178)
    %364 = "ttir.reshape"(%362, %363) <{shape = [1 : i32, 512 : i32]}> : (tensor<1x512x1xf32>, tensor<1x512xf32>) -> tensor<1x512xf32> loc(#loc178)
    %365 = ttir.empty() : tensor<1x512x1xf32> loc(#loc179)
    %366 = "ttir.reshape"(%364, %365) <{shape = [1 : i32, 512 : i32, 1 : i32]}> : (tensor<1x512xf32>, tensor<1x512x1xf32>) -> tensor<1x512x1xf32> loc(#loc179)
    %367 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc179)
    %368 = "ttir.broadcast"(%366, %367) <{broadcast_dimensions = array<i64: 1, 1, 2560>}> : (tensor<1x512x1xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc179)
    %369 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc180)
    %370 = "ttir.multiply"(%350, %368, %369) : (tensor<1x512x2560xf32>, tensor<1x512x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc180)
    %371 = ttir.empty() : tensor<1x512x2560xf32> loc(#loc181)
    %372 = "ttir.multiply"(%16, %370, %371) : (tensor<1x512x2560xf32>, tensor<1x512x2560xf32>, tensor<1x512x2560xf32>) -> tensor<1x512x2560xf32> loc(#loc181)
    return %372 : tensor<1x512x2560xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc20 = loc("convert.345")
#loc21 = loc("broadcast.346")
#loc22 = loc("convert.266")
#loc23 = loc("convert.207")
#loc24 = loc("broadcast.208")
#loc25 = loc("convert.48")
#loc26 = loc("broadcast.49")
#loc27 = loc("power.17")
#loc28 = loc("reduce.24")
#loc29 = loc("multiply.33")
#loc30 = loc("reshape.34")
#loc31 = loc("broadcast.37")
#loc32 = loc("add.38")
#loc33 = loc("rsqrt.39")
#loc34 = loc("convert.40")
#loc35 = loc("reshape.42")
#loc36 = loc("broadcast.43")
#loc37 = loc("multiply.44")
#loc38 = loc("convert.45")
#loc39 = loc("convert.46")
#loc40 = loc("multiply.50")
#loc41 = loc("convert.51")
#loc42 = loc("reshape.171")
#loc43 = loc("dot.172")
#loc44 = loc("reshape.174")
#loc45 = loc("convert.200")
#loc46 = loc("power.176")
#loc47 = loc("reduce.183")
#loc48 = loc("multiply.192")
#loc49 = loc("reshape.193")
#loc50 = loc("broadcast.196")
#loc51 = loc("add.197")
#loc52 = loc("rsqrt.198")
#loc53 = loc("convert.199")
#loc54 = loc("reshape.201")
#loc55 = loc("broadcast.202")
#loc56 = loc("multiply.203")
#loc57 = loc("convert.204")
#loc58 = loc("convert.205")
#loc59 = loc("multiply.209")
#loc60 = loc("convert.210")
#loc61 = loc("transpose.211")
#loc62 = loc("convert.220")
#loc63 = loc("reshape.79")
#loc64 = loc("convert.83")
#loc65 = loc("dot.84")
#loc66 = loc("transpose.85")
#loc67 = loc("concatenate.86")
#loc68 = loc("cosine.144")
#loc69 = loc("broadcast.222")
#loc70 = loc("multiply.223")
#loc71 = loc("slice.213")
#loc72 = loc("negate.214")
#loc73 = loc("slice.212")
#loc74 = loc("concatenate.215")
#loc75 = loc("convert.216")
#loc76 = loc("sine.87")
#loc77 = loc("broadcast.218")
#loc78 = loc("multiply.219")
#loc79 = loc("add.226")
#loc80 = loc("reshape.228")
#loc81 = loc("convert.130")
#loc82 = loc("broadcast.131")
#loc83 = loc("dot.95")
#loc84 = loc("reshape.97")
#loc85 = loc("convert.123")
#loc86 = loc("power.99")
#loc87 = loc("reduce.106")
#loc88 = loc("multiply.115")
#loc89 = loc("reshape.116")
#loc90 = loc("broadcast.119")
#loc91 = loc("add.120")
#loc92 = loc("rsqrt.121")
#loc93 = loc("convert.122")
#loc94 = loc("reshape.124")
#loc95 = loc("broadcast.125")
#loc96 = loc("multiply.126")
#loc97 = loc("convert.127")
#loc98 = loc("convert.128")
#loc99 = loc("multiply.132")
#loc100 = loc("convert.133")
#loc101 = loc("transpose.134")
#loc102 = loc("convert.148")
#loc103 = loc("broadcast.150")
#loc104 = loc("multiply.151")
#loc105 = loc("slice.136")
#loc106 = loc("negate.137")
#loc107 = loc("slice.135")
#loc108 = loc("concatenate.138")
#loc109 = loc("convert.139")
#loc110 = loc("broadcast.141")
#loc111 = loc("multiply.142")
#loc112 = loc("add.154")
#loc113 = loc("broadcast.162")
#loc114 = loc("reshape.163")
#loc115 = loc("transpose.164")
#loc116 = loc("reshape.166")
#loc117 = loc("dot.229")
#loc118 = loc("reshape.230")
#loc119 = loc("broadcast.231")
#loc120 = loc("multiply.232")
#loc121 = loc("slice.73")
#loc122 = loc("convert.233")
#loc123 = loc("reshape.236")
#loc124 = loc("broadcast.237")
#loc125 = loc("add.238")
#loc126 = loc("reduce.244")
#loc127 = loc("broadcast.245")
#loc128 = loc("subtract.246")
#loc129 = loc("exponential.247")
#loc130 = loc("reduce.253")
#loc131 = loc("broadcast.254")
#loc132 = loc("divide.255")
#loc133 = loc("reshape.257")
#loc134 = loc("dot.53")
#loc135 = loc("reshape.55")
#loc136 = loc("transpose.56")
#loc137 = loc("broadcast.64")
#loc138 = loc("reshape.67")
#loc139 = loc("convert.258")
#loc140 = loc("dot.259")
#loc141 = loc("reshape.260")
#loc142 = loc("transpose.261")
#loc143 = loc("reshape.263")
#loc144 = loc("dot.264")
#loc145 = loc("reshape.265")
#loc146 = loc("add.269")
#loc147 = loc("convert.298")
#loc148 = loc("broadcast.299")
#loc149 = loc("power.271")
#loc150 = loc("reduce.278")
#loc151 = loc("multiply.287")
#loc152 = loc("reshape.288")
#loc153 = loc("broadcast.291")
#loc154 = loc("add.292")
#loc155 = loc("rsqrt.293")
#loc156 = loc("reshape.294")
#loc157 = loc("broadcast.295")
#loc158 = loc("multiply.296")
#loc159 = loc("multiply.300")
#loc160 = loc("reshape.305")
#loc161 = loc("dot.306")
#loc162 = loc("reshape.307")
#loc163 = loc("logistic.308")
#loc164 = loc("multiply.309")
#loc165 = loc("dot.302")
#loc166 = loc("reshape.303")
#loc167 = loc("multiply.310")
#loc168 = loc("reshape.311")
#loc169 = loc("dot.312")
#loc170 = loc("reshape.313")
#loc171 = loc("add.316")
#loc172 = loc("power.318")
#loc173 = loc("reduce.325")
#loc174 = loc("multiply.334")
#loc175 = loc("reshape.335")
#loc176 = loc("add.339")
#loc177 = loc("rsqrt.340")
#loc178 = loc("reshape.341")
#loc179 = loc("broadcast.342")
#loc180 = loc("multiply.343")
#loc181 = loc("multiply.347")
#dram = #ttnn.buffer_type<dram>
#loc1 = loc("p0.2")
#loc2 = loc("p1.5")
#loc3 = loc("p2.6")
#loc4 = loc("p3.10")
#loc5 = loc("p4.11")
#loc6 = loc("p5.13")
#loc7 = loc("p6.15")
#loc8 = loc("p7.47")
#loc9 = loc("p8.69")
#loc10 = loc("p9.74")
#loc11 = loc("p10.77")
#loc12 = loc("p11.82")
#loc13 = loc("p12.93")
#loc14 = loc("p13.129")
#loc15 = loc("p14.170")
#loc16 = loc("p15.206")
#loc17 = loc("p16.297")
#loc18 = loc("p17.304")
#loc19 = loc("p18.344")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 100352, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073139712, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [1 : i32], [ 0x0x0x0]>
#ttnn_layout = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<304x80x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x304x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x80x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x17x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x128x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x32x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x17x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x128x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout44 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout45 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout46 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x16x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout47 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout48 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout49 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout50 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout51 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout52 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x32x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout53 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout54 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout55 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout56 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout57 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout58 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout59 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout60 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout61 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout62 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 16384 + d1 * 2048 + d2 * 512 + d3, d4), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout63 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 128 + d2, d3), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout64 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout65 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout66 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout67 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout68 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x16x!ttcore.tile<32x32, bfp_bf8>, #dram>, <interleaved>>
#ttnn_layout69 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x16x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout70 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout71 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout72 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout73 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout74 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
module @SyncTensorsGraph.349 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.349 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func @main(%arg0: tensor<f32, #ttnn_layout> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p0.2"), %arg1: tensor<9728x2560xbf16, #ttnn_layout1> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p1.5"), %arg2: tensor<2560x9728xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p2.6"), %arg3: tensor<4096x2560xbf16, #ttnn_layout3> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p3.10"), %arg4: tensor<2560x1024xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p4.11"), %arg5: tensor<bf16, #ttnn_layout5> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p5.13"), %arg6: tensor<1x512x2560xbf16, #ttnn_layout6> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p6.15"), %arg7: tensor<2560xbf16, #ttnn_layout7> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p7.47"), %arg8: tensor<1x1x512x513xbf16, #ttnn_layout8> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p8.69"), %arg9: tensor<f32, #ttnn_layout> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p9.74"), %arg10: tensor<1x512xsi32, #ttnn_layout9> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p10.77"), %arg11: tensor<1x64x1xf32, #ttnn_layout10> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p11.82"), %arg12: tensor<2560x1024xbf16, #ttnn_layout4> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p12.93"), %arg13: tensor<128xbf16, #ttnn_layout11> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p13.129"), %arg14: tensor<2560x4096xbf16, #ttnn_layout12> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p14.170"), %arg15: tensor<128xbf16, #ttnn_layout11> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p15.206"), %arg16: tensor<2560xbf16, #ttnn_layout7> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p16.297"), %arg17: tensor<2560x9728xbf16, #ttnn_layout2> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p17.304"), %arg18: tensor<2560xbf16, #ttnn_layout7> {ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p18.344")) -> (tensor<1x512x2560xf32, #ttnn_layout13> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc20)
        %1 = "ttnn.typecast"(%arg4) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<2560x1024xbf16, #ttnn_layout4>) -> tensor<2560x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout14> loc(#loc20)
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<2560x1024xbf16, #ttnn_layout4>) -> () loc(#loc20)
        %2 = "ttnn.typecast"(%arg8) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x1x512x513xbf16, #ttnn_layout8>) -> tensor<1x1x512x513x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout15> loc(#loc21)
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<1x1x512x513xbf16, #ttnn_layout8>) -> () loc(#loc21)
        %3 = "ttnn.typecast"(%arg12) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<2560x1024xbf16, #ttnn_layout4>) -> tensor<2560x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout14> loc(#loc22)
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<2560x1024xbf16, #ttnn_layout4>) -> () loc(#loc22)
        %4 = "ttnn.typecast"(%arg14) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<2560x4096xbf16, #ttnn_layout12>) -> tensor<2560x4096x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout16> loc(#loc23)
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<2560x4096xbf16, #ttnn_layout12>) -> () loc(#loc23)
        %5 = "ttnn.typecast"(%arg5) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<bf16, #ttnn_layout5>) -> tensor<!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout17> loc(#loc24)
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<bf16, #ttnn_layout5>) -> () loc(#loc24)
        %6 = "ttnn.typecast"(%arg6) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512x2560xbf16, #ttnn_layout6>) -> tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout18> loc(#loc25)
        %7 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.906250e-04 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512>}> : (!ttnn.device) -> tensor<1x512xf32, #ttnn_layout19> loc(#loc)
        %8 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512x2560>}> : (!ttnn.device) -> tensor<1x512x2560xf32, #ttnn_layout13> loc(#loc)
        %9 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>, fill_value = 7.812500e-03 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512x8>}> : (!ttnn.device) -> tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20> loc(#loc)
        %10 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512x8x128>}> : (!ttnn.device) -> tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21> loc(#loc)
        %11 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>, fill_value = 7.812500e-03 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512x32>}> : (!ttnn.device) -> tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20> loc(#loc)
        %12 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512x32x128>}> : (!ttnn.device) -> tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21> loc(#loc)
        %13 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>, fill_value = 3.9100647E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512>}> : (!ttnn.device) -> tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout22> loc(#loc)
        %14 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512x2560>}> : (!ttnn.device) -> tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout18> loc(#loc)
        %15 = "ttnn.typecast"(%arg18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2560xbf16, #ttnn_layout7>) -> tensor<2560xf32, #ttnn_layout23> loc(#loc26)
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<2560xbf16, #ttnn_layout7>) -> () loc(#loc26)
        %16 = "ttnn.reshape"(%15) <{shape = [1 : i32, 1 : i32, 2560 : i32]}> : (tensor<2560xf32, #ttnn_layout23>) -> tensor<1x1x2560xf32, #ttnn_layout24> loc(#loc26)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<2560xf32, #ttnn_layout23>) -> () loc(#loc26)
        %17 = "ttnn.typecast"(%arg6) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x2560xbf16, #ttnn_layout6>) -> tensor<1x512x2560xf32, #ttnn_layout13> loc(#loc27)
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<1x512x2560xbf16, #ttnn_layout6>) -> () loc(#loc27)
        %18 = "ttnn.typecast"(%arg15) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<128xbf16, #ttnn_layout11>) -> tensor<128xf32, #ttnn_layout25> loc(#loc28)
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<128xbf16, #ttnn_layout11>) -> () loc(#loc28)
        %19 = "ttnn.reshape"(%18) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout25>) -> tensor<1x1x1x128xf32, #ttnn_layout26> loc(#loc28)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<128xf32, #ttnn_layout25>) -> () loc(#loc28)
        %20 = "ttnn.permute"(%19) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x1x128xf32, #ttnn_layout26>) -> tensor<1x1x1x128xf32, #ttnn_layout26> loc(#loc28)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn_layout26>) -> () loc(#loc28)
        %21 = "ttnn.typecast"(%arg7) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2560xbf16, #ttnn_layout7>) -> tensor<2560xf32, #ttnn_layout23> loc(#loc29)
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<2560xbf16, #ttnn_layout7>) -> () loc(#loc29)
        %22 = "ttnn.reshape"(%21) <{shape = [1 : i32, 2560 : i32]}> : (tensor<2560xf32, #ttnn_layout23>) -> tensor<1x2560xf32, #ttnn_layout27> loc(#loc29)
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<2560xf32, #ttnn_layout23>) -> () loc(#loc29)
        %23 = "ttnn.pow"(%6, %14) : (tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout18>, tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout18>) -> tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout18> loc(#loc25)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout18>) -> () loc(#loc25)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout18>) -> () loc(#loc25)
        %24 = "ttnn.typecast"(%23) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout18>) -> tensor<1x512x2560xbf16, #ttnn_layout6> loc(#loc139)
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout18>) -> () loc(#loc139)
        %25 = "ttnn.sum"(%24) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x512x2560xbf16, #ttnn_layout6>) -> tensor<1x512xbf16, #ttnn_layout28> loc(#loc30)
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x512x2560xbf16, #ttnn_layout6>) -> () loc(#loc30)
        %26 = "ttnn.typecast"(%25) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512xbf16, #ttnn_layout28>) -> tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout22> loc(#loc139)
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x512xbf16, #ttnn_layout28>) -> () loc(#loc139)
        %27 = "ttnn.multiply"(%26, %13) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout22>, tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout22>) -> tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout22> loc(#loc31)
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout22>) -> () loc(#loc31)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout22>) -> () loc(#loc31)
        %28 = "ttnn.reshape"(%27) <{shape = [512 : i32, 1 : i32]}> : (tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout22>) -> tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout29> loc(#loc31)
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout22>) -> () loc(#loc31)
        %29 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32]}> : (tensor<!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout17>) -> tensor<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout30> loc(#loc162)
        %30 = "ttnn.add"(%28, %29) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout29>, tensor<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout30>) -> tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout29> loc(#loc33)
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout30>) -> () loc(#loc33)
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout29>) -> () loc(#loc33)
        %31 = "ttnn.rsqrt"(%30) : (tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout29>) -> tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout29> loc(#loc34)
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout29>) -> () loc(#loc34)
        %32 = "ttnn.typecast"(%31) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout29>) -> tensor<512x1xf32, #ttnn_layout31> loc(#loc35)
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout29>) -> () loc(#loc35)
        %33 = "ttnn.reshape"(%17) <{shape = [512 : i32, 2560 : i32]}> : (tensor<1x512x2560xf32, #ttnn_layout13>) -> tensor<512x2560xf32, #ttnn_layout32> loc(#loc153)
        %34 = "ttnn.multiply"(%33, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<512x2560xf32, #ttnn_layout32>, tensor<512x1xf32, #ttnn_layout31>) -> tensor<512x2560xf32, #ttnn_layout32> loc(#loc36)
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<512x2560xf32, #ttnn_layout32>) -> () loc(#loc36)
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<512x1xf32, #ttnn_layout31>) -> () loc(#loc36)
        %35 = "ttnn.multiply"(%22, %34) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x2560xf32, #ttnn_layout27>, tensor<512x2560xf32, #ttnn_layout32>) -> tensor<512x2560xf32, #ttnn_layout32> loc(#loc37)
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<512x2560xf32, #ttnn_layout32>) -> () loc(#loc37)
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x2560xf32, #ttnn_layout27>) -> () loc(#loc37)
        %36 = "ttnn.typecast"(%35) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<512x2560xf32, #ttnn_layout32>) -> tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout33> loc(#loc38)
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<512x2560xf32, #ttnn_layout32>) -> () loc(#loc38)
        %37 = "ttnn.matmul"(%36, %4) <{transpose_a = false, transpose_b = false}> : (tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout33>, tensor<2560x4096x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout16>) -> tensor<512x4096x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout34> loc(#loc23)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<2560x4096x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout16>) -> () loc(#loc23)
        %38 = "ttnn.reshape"(%37) <{shape = [1 : i32, 512 : i32, 32 : i32, 128 : i32]}> : (tensor<512x4096x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout34>) -> tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21> loc(#loc39)
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<512x4096x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout34>) -> () loc(#loc39)
        %39 = "ttnn.typecast"(%38) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> tensor<1x512x32x128xf32, #ttnn_layout35> loc(#loc40)
        %40 = "ttnn.permute"(%39) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x512x32x128xf32, #ttnn_layout35>) -> tensor<1x32x512x128xf32, #ttnn_layout36> loc(#loc40)
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x512x32x128xf32, #ttnn_layout35>) -> () loc(#loc40)
        %41 = "ttnn.pow"(%38, %12) : (tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>, tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21> loc(#loc41)
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> () loc(#loc41)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> () loc(#loc41)
        %42 = "ttnn.typecast"(%41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> tensor<1x512x32x128xbf16, #ttnn_layout37> loc(#loc141)
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> () loc(#loc141)
        %43 = "ttnn.sum"(%42) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x512x32x128xbf16, #ttnn_layout37>) -> tensor<1x512x32xbf16, #ttnn_layout38> loc(#loc42)
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x512x32x128xbf16, #ttnn_layout37>) -> () loc(#loc42)
        %44 = "ttnn.typecast"(%43) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512x32xbf16, #ttnn_layout38>) -> tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20> loc(#loc141)
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x512x32xbf16, #ttnn_layout38>) -> () loc(#loc141)
        %45 = "ttnn.multiply"(%44, %11) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20>, tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20>) -> tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20> loc(#loc43)
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20>) -> () loc(#loc43)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20>) -> () loc(#loc43)
        %46 = "ttnn.reshape"(%45) <{shape = [1 : i32, 512 : i32, 32 : i32, 1 : i32]}> : (tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20>) -> tensor<1x512x32x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout39> loc(#loc43)
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20>) -> () loc(#loc43)
        %47 = "ttnn.permute"(%46) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x512x32x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout39>) -> tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout40> loc(#loc43)
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x512x32x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout39>) -> () loc(#loc43)
        %48 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout17>) -> tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout41> loc(#loc44)
        %49 = "ttnn.permute"(%48) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout41>) -> tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout41> loc(#loc163)
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout41>) -> () loc(#loc163)
        %50 = "ttnn.add"(%47, %49) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout40>, tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout41>) -> tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout40> loc(#loc46)
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout41>) -> () loc(#loc46)
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout40>) -> () loc(#loc46)
        %51 = "ttnn.rsqrt"(%50) : (tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout40>) -> tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout40> loc(#loc47)
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout40>) -> () loc(#loc47)
        %52 = "ttnn.typecast"(%51) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout40>) -> tensor<1x32x512x1xf32, #ttnn_layout42> loc(#loc48)
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout40>) -> () loc(#loc48)
        %53 = "ttnn.multiply"(%40, %52) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x32x512x128xf32, #ttnn_layout36>, tensor<1x32x512x1xf32, #ttnn_layout42>) -> tensor<1x32x512x128xf32, #ttnn_layout36> loc(#loc49)
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x32x512x1xf32, #ttnn_layout42>) -> () loc(#loc49)
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x32x512x128xf32, #ttnn_layout36>) -> () loc(#loc49)
        %54 = "ttnn.multiply"(%20, %53) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xf32, #ttnn_layout26>, tensor<1x32x512x128xf32, #ttnn_layout36>) -> tensor<1x32x512x128xf32, #ttnn_layout36> loc(#loc50)
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x32x512x128xf32, #ttnn_layout36>) -> () loc(#loc50)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn_layout26>) -> () loc(#loc50)
        %55 = "ttnn.typecast"(%54) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x32x512x128xf32, #ttnn_layout36>) -> tensor<1x32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout43> loc(#loc51)
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x32x512x128xf32, #ttnn_layout36>) -> () loc(#loc51)
        %56 = "ttnn.typecast"(%55) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout43>) -> tensor<1x32x512x128xf32, #ttnn_layout36> loc(#loc52)
        %57 = "ttnn.reshape"(%56) <{shape = [32 : i32, 512 : i32, 128 : i32]}> : (tensor<1x32x512x128xf32, #ttnn_layout36>) -> tensor<32x512x128xf32, #ttnn_layout44> loc(#loc52)
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x32x512x128xf32, #ttnn_layout36>) -> () loc(#loc52)
        %58 = "ttnn.typecast"(%arg10) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512xsi32, #ttnn_layout9>) -> tensor<1x512xf32, #ttnn_layout19> loc(#loc53)
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<1x512xsi32, #ttnn_layout9>) -> () loc(#loc53)
        %59 = "ttnn.reshape"(%58) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512xf32, #ttnn_layout19>) -> tensor<1x1x512xf32, #ttnn_layout45> loc(#loc53)
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x512xf32, #ttnn_layout19>) -> () loc(#loc53)
        %60 = "ttnn.matmul"(%arg11, %59) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn_layout10>, tensor<1x1x512xf32, #ttnn_layout45>) -> tensor<1x64x512xf32, #ttnn_layout46> loc(#loc54)
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x1x512xf32, #ttnn_layout45>) -> () loc(#loc54)
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<1x64x1xf32, #ttnn_layout10>) -> () loc(#loc54)
        %61 = "ttnn.permute"(%60) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x512xf32, #ttnn_layout46>) -> tensor<1x512x64xf32, #ttnn_layout47> loc(#loc55)
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x64x512xf32, #ttnn_layout46>) -> () loc(#loc55)
        %62 = "ttnn.concat"(%61, %61) <{dim = 2 : si32}> : (tensor<1x512x64xf32, #ttnn_layout47>, tensor<1x512x64xf32, #ttnn_layout47>) -> tensor<1x512x128xf32, #ttnn_layout48> loc(#loc56)
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x512x64xf32, #ttnn_layout47>) -> () loc(#loc56)
        %63 = "ttnn.cos"(%62) : (tensor<1x512x128xf32, #ttnn_layout48>) -> tensor<1x512x128xf32, #ttnn_layout48> loc(#loc57)
        %64 = "ttnn.multiply"(%57, %63) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x512x128xf32, #ttnn_layout44>, tensor<1x512x128xf32, #ttnn_layout48>) -> tensor<32x512x128xf32, #ttnn_layout44> loc(#loc58)
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<32x512x128xf32, #ttnn_layout44>) -> () loc(#loc58)
        %65 = "ttnn.slice_static"(%55) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 32 : i32, 512 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout43>) -> tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout49> loc(#loc59)
        %66 = "ttnn.neg"(%65) : (tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout49>) -> tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout49> loc(#loc60)
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout49>) -> () loc(#loc60)
        %67 = "ttnn.reshape"(%66) <{shape = [32 : i32, 512 : i32, 64 : i32]}> : (tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout49>) -> tensor<32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout50> loc(#loc60)
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout49>) -> () loc(#loc60)
        %68 = "ttnn.slice_static"(%55) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 32 : i32, 512 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout43>) -> tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout49> loc(#loc61)
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout43>) -> () loc(#loc61)
        %69 = "ttnn.reshape"(%68) <{shape = [32 : i32, 512 : i32, 64 : i32]}> : (tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout49>) -> tensor<32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout50> loc(#loc62)
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout49>) -> () loc(#loc62)
        %70 = "ttnn.concat"(%67, %69) <{dim = 2 : si32}> : (tensor<32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout50>, tensor<32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout50>) -> tensor<32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout51> loc(#loc62)
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout50>) -> () loc(#loc62)
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout50>) -> () loc(#loc62)
        %71 = "ttnn.typecast"(%70) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout51>) -> tensor<32x512x128xf32, #ttnn_layout44> loc(#loc63)
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout51>) -> () loc(#loc63)
        %72 = "ttnn.sin"(%62) : (tensor<1x512x128xf32, #ttnn_layout48>) -> tensor<1x512x128xf32, #ttnn_layout48> loc(#loc64)
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x512x128xf32, #ttnn_layout48>) -> () loc(#loc64)
        %73 = "ttnn.multiply"(%71, %72) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x512x128xf32, #ttnn_layout44>, tensor<1x512x128xf32, #ttnn_layout48>) -> tensor<32x512x128xf32, #ttnn_layout44> loc(#loc65)
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<32x512x128xf32, #ttnn_layout44>) -> () loc(#loc65)
        %74 = "ttnn.add"(%64, %73) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x512x128xf32, #ttnn_layout44>, tensor<32x512x128xf32, #ttnn_layout44>) -> tensor<32x512x128xf32, #ttnn_layout44> loc(#loc66)
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<32x512x128xf32, #ttnn_layout44>) -> () loc(#loc66)
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<32x512x128xf32, #ttnn_layout44>) -> () loc(#loc66)
        %75 = "ttnn.typecast"(%arg13) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<128xbf16, #ttnn_layout11>) -> tensor<128xf32, #ttnn_layout25> loc(#loc67)
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<128xbf16, #ttnn_layout11>) -> () loc(#loc67)
        %76 = "ttnn.reshape"(%75) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn_layout25>) -> tensor<1x1x1x128xf32, #ttnn_layout26> loc(#loc67)
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<128xf32, #ttnn_layout25>) -> () loc(#loc67)
        %77 = "ttnn.permute"(%76) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x1x128xf32, #ttnn_layout26>) -> tensor<1x1x1x128xf32, #ttnn_layout26> loc(#loc67)
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn_layout26>) -> () loc(#loc67)
        %78 = "ttnn.matmul"(%36, %3) <{transpose_a = false, transpose_b = false}> : (tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout33>, tensor<2560x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout14>) -> tensor<512x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout52> loc(#loc22)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<2560x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout14>) -> () loc(#loc22)
        %79 = "ttnn.reshape"(%78) <{shape = [1 : i32, 512 : i32, 8 : i32, 128 : i32]}> : (tensor<512x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout52>) -> tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21> loc(#loc68)
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<512x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout52>) -> () loc(#loc68)
        %80 = "ttnn.typecast"(%79) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> tensor<1x512x8x128xf32, #ttnn_layout35> loc(#loc69)
        %81 = "ttnn.permute"(%80) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x512x8x128xf32, #ttnn_layout35>) -> tensor<1x8x512x128xf32, #ttnn_layout53> loc(#loc69)
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x512x8x128xf32, #ttnn_layout35>) -> () loc(#loc69)
        %82 = "ttnn.pow"(%79, %10) : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>, tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21> loc(#loc70)
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> () loc(#loc70)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> () loc(#loc70)
        %83 = "ttnn.typecast"(%82) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> tensor<1x512x8x128xbf16, #ttnn_layout37> loc(#loc143)
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> () loc(#loc143)
        %84 = "ttnn.sum"(%83) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x512x8x128xbf16, #ttnn_layout37>) -> tensor<1x512x8xbf16, #ttnn_layout38> loc(#loc71)
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x512x8x128xbf16, #ttnn_layout37>) -> () loc(#loc71)
        %85 = "ttnn.typecast"(%84) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512x8xbf16, #ttnn_layout38>) -> tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20> loc(#loc143)
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<1x512x8xbf16, #ttnn_layout38>) -> () loc(#loc143)
        %86 = "ttnn.multiply"(%85, %9) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20>, tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20>) -> tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20> loc(#loc72)
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20>) -> () loc(#loc72)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20>) -> () loc(#loc72)
        %87 = "ttnn.reshape"(%86) <{shape = [1 : i32, 512 : i32, 8 : i32, 1 : i32]}> : (tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20>) -> tensor<1x512x8x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout39> loc(#loc72)
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout20>) -> () loc(#loc72)
        %88 = "ttnn.permute"(%87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x512x8x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout39>) -> tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout54> loc(#loc72)
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<1x512x8x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout39>) -> () loc(#loc72)
        %89 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout17>) -> tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout41> loc(#loc73)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout17>) -> () loc(#loc73)
        %90 = "ttnn.permute"(%89) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout41>) -> tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout41> loc(#loc164)
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout41>) -> () loc(#loc164)
        %91 = "ttnn.add"(%88, %90) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout54>, tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout41>) -> tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout54> loc(#loc75)
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout41>) -> () loc(#loc75)
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout54>) -> () loc(#loc75)
        %92 = "ttnn.rsqrt"(%91) : (tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout54>) -> tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout54> loc(#loc76)
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout54>) -> () loc(#loc76)
        %93 = "ttnn.typecast"(%92) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout54>) -> tensor<1x8x512x1xf32, #ttnn_layout55> loc(#loc77)
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout54>) -> () loc(#loc77)
        %94 = "ttnn.multiply"(%81, %93) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x512x128xf32, #ttnn_layout53>, tensor<1x8x512x1xf32, #ttnn_layout55>) -> tensor<1x8x512x128xf32, #ttnn_layout53> loc(#loc78)
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x8x512x1xf32, #ttnn_layout55>) -> () loc(#loc78)
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x8x512x128xf32, #ttnn_layout53>) -> () loc(#loc78)
        %95 = "ttnn.multiply"(%77, %94) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xf32, #ttnn_layout26>, tensor<1x8x512x128xf32, #ttnn_layout53>) -> tensor<1x8x512x128xf32, #ttnn_layout53> loc(#loc79)
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x8x512x128xf32, #ttnn_layout53>) -> () loc(#loc79)
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn_layout26>) -> () loc(#loc79)
        %96 = "ttnn.typecast"(%95) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x8x512x128xf32, #ttnn_layout53>) -> tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout56> loc(#loc80)
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x8x512x128xf32, #ttnn_layout53>) -> () loc(#loc80)
        %97 = "ttnn.typecast"(%96) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout56>) -> tensor<1x8x512x128xf32, #ttnn_layout53> loc(#loc81)
        %98 = "ttnn.reshape"(%63) <{shape = [1 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x512x128xf32, #ttnn_layout48>) -> tensor<1x1x512x128xf32, #ttnn_layout57> loc(#loc82)
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x512x128xf32, #ttnn_layout48>) -> () loc(#loc82)
        %99 = "ttnn.multiply"(%97, %98) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x512x128xf32, #ttnn_layout53>, tensor<1x1x512x128xf32, #ttnn_layout57>) -> tensor<1x8x512x128xf32, #ttnn_layout53> loc(#loc83)
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x1x512x128xf32, #ttnn_layout57>) -> () loc(#loc83)
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x8x512x128xf32, #ttnn_layout53>) -> () loc(#loc83)
        %100 = "ttnn.reshape"(%99) <{shape = [1 : i32, 8 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x512x128xf32, #ttnn_layout53>) -> tensor<1x8x1x512x128xf32, #ttnn_layout58> loc(#loc83)
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x8x512x128xf32, #ttnn_layout53>) -> () loc(#loc83)
        %101 = "ttnn.slice_static"(%96) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 512 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout56>) -> tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout59> loc(#loc84)
        %102 = "ttnn.neg"(%101) : (tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout59>) -> tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout59> loc(#loc85)
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout59>) -> () loc(#loc85)
        %103 = "ttnn.reshape"(%102) <{shape = [1 : i32, 8 : i32, 1 : i32, 512 : i32, 64 : i32]}> : (tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout59>) -> tensor<1x8x1x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout60> loc(#loc85)
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout59>) -> () loc(#loc85)
        %104 = "ttnn.slice_static"(%96) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 512 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout56>) -> tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout59> loc(#loc86)
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout56>) -> () loc(#loc86)
        %105 = "ttnn.reshape"(%104) <{shape = [1 : i32, 8 : i32, 1 : i32, 512 : i32, 64 : i32]}> : (tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout59>) -> tensor<1x8x1x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout60> loc(#loc87)
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout59>) -> () loc(#loc87)
        %106 = "ttnn.concat"(%103, %105) <{dim = 4 : si32}> : (tensor<1x8x1x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout60>, tensor<1x8x1x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout60>) -> tensor<1x8x1x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout61> loc(#loc87)
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x8x1x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout60>) -> () loc(#loc87)
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x8x1x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout60>) -> () loc(#loc87)
        %107 = "ttnn.typecast"(%106) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout61>) -> tensor<1x8x1x512x128xf32, #ttnn_layout58> loc(#loc88)
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x8x1x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout61>) -> () loc(#loc88)
        %108 = "ttnn.reshape"(%72) <{shape = [1 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x512x128xf32, #ttnn_layout48>) -> tensor<1x1x512x128xf32, #ttnn_layout57> loc(#loc89)
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x512x128xf32, #ttnn_layout48>) -> () loc(#loc89)
        %109 = "ttnn.repeat"(%108) <{repeat_dims = #ttnn.shape<1x8x1x1>}> : (tensor<1x1x512x128xf32, #ttnn_layout57>) -> tensor<1x8x512x128xf32, #ttnn_layout53> loc(#loc89)
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x1x512x128xf32, #ttnn_layout57>) -> () loc(#loc89)
        %110 = "ttnn.reshape"(%109) <{shape = [1 : i32, 8 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x512x128xf32, #ttnn_layout53>) -> tensor<1x8x1x512x128xf32, #ttnn_layout58> loc(#loc150)
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x8x512x128xf32, #ttnn_layout53>) -> () loc(#loc150)
        %111 = "ttnn.multiply"(%107, %110) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x512x128xf32, #ttnn_layout58>, tensor<1x8x1x512x128xf32, #ttnn_layout58>) -> tensor<1x8x1x512x128xf32, #ttnn_layout58> loc(#loc91)
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x8x1x512x128xf32, #ttnn_layout58>) -> () loc(#loc91)
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x8x1x512x128xf32, #ttnn_layout58>) -> () loc(#loc91)
        %112 = "ttnn.add"(%100, %111) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x512x128xf32, #ttnn_layout58>, tensor<1x8x1x512x128xf32, #ttnn_layout58>) -> tensor<1x8x1x512x128xf32, #ttnn_layout58> loc(#loc92)
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x8x1x512x128xf32, #ttnn_layout58>) -> () loc(#loc92)
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x8x1x512x128xf32, #ttnn_layout58>) -> () loc(#loc92)
        %113 = "ttnn.repeat"(%112) <{repeat_dims = #ttnn.shape<1x1x4x1x1>}> : (tensor<1x8x1x512x128xf32, #ttnn_layout58>) -> tensor<1x8x4x512x128xf32, #ttnn_layout62> loc(#loc90)
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x8x1x512x128xf32, #ttnn_layout58>) -> () loc(#loc90)
        %114 = "ttnn.reshape"(%113) <{shape = [1 : i32, 32 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x4x512x128xf32, #ttnn_layout62>) -> tensor<1x32x512x128xf32, #ttnn_layout36> loc(#loc93)
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x8x4x512x128xf32, #ttnn_layout62>) -> () loc(#loc93)
        %115 = "ttnn.permute"(%114) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x32x512x128xf32, #ttnn_layout36>) -> tensor<1x32x128x512xf32, #ttnn_layout63> loc(#loc94)
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x32x512x128xf32, #ttnn_layout36>) -> () loc(#loc94)
        %116 = "ttnn.reshape"(%115) <{shape = [32 : i32, 128 : i32, 512 : i32]}> : (tensor<1x32x128x512xf32, #ttnn_layout63>) -> tensor<32x128x512xf32, #ttnn_layout64> loc(#loc95)
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x32x128x512xf32, #ttnn_layout63>) -> () loc(#loc95)
        %117 = "ttnn.matmul"(%74, %116) <{transpose_a = false, transpose_b = false}> : (tensor<32x512x128xf32, #ttnn_layout44>, tensor<32x128x512xf32, #ttnn_layout64>) -> tensor<32x512x512xf32, #ttnn_layout65> loc(#loc96)
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<32x128x512xf32, #ttnn_layout64>) -> () loc(#loc96)
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<32x512x128xf32, #ttnn_layout44>) -> () loc(#loc96)
        %118 = "ttnn.reshape"(%117) <{shape = [1 : i32, 32 : i32, 512 : i32, 512 : i32]}> : (tensor<32x512x512xf32, #ttnn_layout65>) -> tensor<1x32x512x512xf32, #ttnn_layout66> loc(#loc97)
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<32x512x512xf32, #ttnn_layout65>) -> () loc(#loc97)
        %119 = "ttnn.reshape"(%arg9) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout>) -> tensor<1x1x1x1xf32, #ttnn_layout67> loc(#loc98)
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<f32, #ttnn_layout>) -> () loc(#loc98)
        %120 = "ttnn.multiply"(%118, %119) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x32x512x512xf32, #ttnn_layout66>, tensor<1x1x1x1xf32, #ttnn_layout67>) -> tensor<1x32x512x512xf32, #ttnn_layout66> loc(#loc99)
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn_layout67>) -> () loc(#loc99)
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x32x512x512xf32, #ttnn_layout66>) -> () loc(#loc99)
        %121 = "ttnn.slice_static"(%2) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 512 : i32, 512 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x512x513x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout15>) -> tensor<1x1x512x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout68> loc(#loc21)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x512x513x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout15>) -> () loc(#loc21)
        %122 = "ttnn.typecast"(%121) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x512x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout68>) -> tensor<1x1x512x512xf32, #ttnn_layout69> loc(#loc100)
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x1x512x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout68>) -> () loc(#loc100)
        %123 = "ttnn.add"(%120, %122) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x32x512x512xf32, #ttnn_layout66>, tensor<1x1x512x512xf32, #ttnn_layout69>) -> tensor<1x32x512x512xf32, #ttnn_layout66> loc(#loc101)
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x1x512x512xf32, #ttnn_layout69>) -> () loc(#loc101)
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x32x512x512xf32, #ttnn_layout66>) -> () loc(#loc101)
        %124 = "ttnn.softmax"(%123) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x32x512x512xf32, #ttnn_layout66>) -> tensor<1x32x512x512xf32, #ttnn_layout66> loc(#loc102)
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x32x512x512xf32, #ttnn_layout66>) -> () loc(#loc102)
        %125 = "ttnn.reshape"(%124) <{shape = [32 : i32, 512 : i32, 512 : i32]}> : (tensor<1x32x512x512xf32, #ttnn_layout66>) -> tensor<32x512x512xf32, #ttnn_layout65> loc(#loc103)
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x32x512x512xf32, #ttnn_layout66>) -> () loc(#loc103)
        %126 = "ttnn.matmul"(%36, %1) <{transpose_a = false, transpose_b = false}> : (tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout33>, tensor<2560x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout14>) -> tensor<512x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout52> loc(#loc20)
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout33>) -> () loc(#loc20)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2560x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout14>) -> () loc(#loc20)
        %127 = "ttnn.reshape"(%126) <{shape = [1 : i32, 512 : i32, 8 : i32, 128 : i32]}> : (tensor<512x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout52>) -> tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21> loc(#loc104)
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<512x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout52>) -> () loc(#loc104)
        %128 = "ttnn.permute"(%127) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout56> loc(#loc105)
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout21>) -> () loc(#loc105)
        %129 = "ttnn.reshape"(%128) <{shape = [1 : i32, 8 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout56>) -> tensor<1x8x1x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout61> loc(#loc106)
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout56>) -> () loc(#loc106)
        %130 = "ttnn.typecast"(%129) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout61>) -> tensor<1x8x1x512x128xf32, #ttnn_layout58> loc(#loc107)
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x8x1x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn_layout61>) -> () loc(#loc107)
        %131 = "ttnn.repeat"(%130) <{repeat_dims = #ttnn.shape<1x1x4x1x1>}> : (tensor<1x8x1x512x128xf32, #ttnn_layout58>) -> tensor<1x8x4x512x128xf32, #ttnn_layout62> loc(#loc107)
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x8x1x512x128xf32, #ttnn_layout58>) -> () loc(#loc107)
        %132 = "ttnn.reshape"(%131) <{shape = [32 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x4x512x128xf32, #ttnn_layout62>) -> tensor<32x512x128xf32, #ttnn_layout44> loc(#loc107)
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x8x4x512x128xf32, #ttnn_layout62>) -> () loc(#loc107)
        %133 = "ttnn.matmul"(%125, %132) <{transpose_a = false, transpose_b = false}> : (tensor<32x512x512xf32, #ttnn_layout65>, tensor<32x512x128xf32, #ttnn_layout44>) -> tensor<32x512x128xf32, #ttnn_layout44> loc(#loc108)
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<32x512x128xf32, #ttnn_layout44>) -> () loc(#loc108)
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<32x512x512xf32, #ttnn_layout65>) -> () loc(#loc108)
        %134 = "ttnn.reshape"(%133) <{shape = [1 : i32, 32 : i32, 512 : i32, 128 : i32]}> : (tensor<32x512x128xf32, #ttnn_layout44>) -> tensor<1x32x512x128xf32, #ttnn_layout36> loc(#loc109)
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<32x512x128xf32, #ttnn_layout44>) -> () loc(#loc109)
        %135 = "ttnn.concatenate_heads"(%134) : (tensor<1x32x512x128xf32, #ttnn_layout36>) -> tensor<1x512x4096xf32, #ttnn_layout70> loc(#loc110)
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x32x512x128xf32, #ttnn_layout36>) -> () loc(#loc110)
        %136 = "ttnn.reshape"(%135) <{shape = [512 : i32, 4096 : i32]}> : (tensor<1x512x4096xf32, #ttnn_layout70>) -> tensor<512x4096xf32, #ttnn_layout71> loc(#loc110)
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x512x4096xf32, #ttnn_layout70>) -> () loc(#loc110)
        %137 = "ttnn.matmul"(%136, %arg3) <{transpose_a = false, transpose_b = false}> : (tensor<512x4096xf32, #ttnn_layout71>, tensor<4096x2560xbf16, #ttnn_layout3>) -> tensor<512x2560xf32, #ttnn_layout32> loc(#loc111)
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<512x4096xf32, #ttnn_layout71>) -> () loc(#loc111)
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<4096x2560xbf16, #ttnn_layout3>) -> () loc(#loc111)
        %138 = "ttnn.reshape"(%137) <{shape = [1 : i32, 512 : i32, 2560 : i32]}> : (tensor<512x2560xf32, #ttnn_layout32>) -> tensor<1x512x2560xf32, #ttnn_layout13> loc(#loc112)
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<512x2560xf32, #ttnn_layout32>) -> () loc(#loc112)
        %139 = "ttnn.add"(%17, %138) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x2560xf32, #ttnn_layout13>, tensor<1x512x2560xf32, #ttnn_layout13>) -> tensor<1x512x2560xf32, #ttnn_layout13> loc(#loc113)
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x512x2560xf32, #ttnn_layout13>) -> () loc(#loc113)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x512x2560xf32, #ttnn_layout13>) -> () loc(#loc113)
        %140 = "ttnn.typecast"(%arg16) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2560xbf16, #ttnn_layout7>) -> tensor<2560xf32, #ttnn_layout23> loc(#loc114)
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<2560xbf16, #ttnn_layout7>) -> () loc(#loc114)
        %141 = "ttnn.reshape"(%140) <{shape = [1 : i32, 2560 : i32]}> : (tensor<2560xf32, #ttnn_layout23>) -> tensor<1x2560xf32, #ttnn_layout27> loc(#loc114)
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<2560xf32, #ttnn_layout23>) -> () loc(#loc114)
        %142 = "ttnn.pow"(%139, %8) : (tensor<1x512x2560xf32, #ttnn_layout13>, tensor<1x512x2560xf32, #ttnn_layout13>) -> tensor<1x512x2560xf32, #ttnn_layout13> loc(#loc115)
        %143 = "ttnn.sum"(%142) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x512x2560xf32, #ttnn_layout13>) -> tensor<1x512xf32, #ttnn_layout19> loc(#loc116)
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x512x2560xf32, #ttnn_layout13>) -> () loc(#loc116)
        %144 = "ttnn.multiply"(%143, %7) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512xf32, #ttnn_layout19>, tensor<1x512xf32, #ttnn_layout19>) -> tensor<1x512xf32, #ttnn_layout19> loc(#loc117)
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x512xf32, #ttnn_layout19>) -> () loc(#loc117)
        %145 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn_layout>) -> tensor<1x1x1xf32, #ttnn_layout72> loc(#loc118)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<f32, #ttnn_layout>) -> () loc(#loc118)
        %146 = "ttnn.repeat"(%145) <{repeat_dims = #ttnn.shape<1x512x1>}> : (tensor<1x1x1xf32, #ttnn_layout72>) -> tensor<1x512x1xf32, #ttnn_layout73> loc(#loc118)
        %147 = "ttnn.reshape"(%146) <{shape = [1 : i32, 512 : i32]}> : (tensor<1x512x1xf32, #ttnn_layout73>) -> tensor<1x512xf32, #ttnn_layout19> loc(#loc117)
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x512x1xf32, #ttnn_layout73>) -> () loc(#loc117)
        %148 = "ttnn.add"(%144, %147) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512xf32, #ttnn_layout19>, tensor<1x512xf32, #ttnn_layout19>) -> tensor<1x512xf32, #ttnn_layout19> loc(#loc119)
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x512xf32, #ttnn_layout19>) -> () loc(#loc119)
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x512xf32, #ttnn_layout19>) -> () loc(#loc119)
        %149 = "ttnn.rsqrt"(%148) : (tensor<1x512xf32, #ttnn_layout19>) -> tensor<1x512xf32, #ttnn_layout19> loc(#loc120)
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x512xf32, #ttnn_layout19>) -> () loc(#loc120)
        %150 = "ttnn.reshape"(%149) <{shape = [512 : i32, 1 : i32]}> : (tensor<1x512xf32, #ttnn_layout19>) -> tensor<512x1xf32, #ttnn_layout31> loc(#loc120)
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x512xf32, #ttnn_layout19>) -> () loc(#loc120)
        %151 = "ttnn.reshape"(%139) <{shape = [512 : i32, 2560 : i32]}> : (tensor<1x512x2560xf32, #ttnn_layout13>) -> tensor<512x2560xf32, #ttnn_layout32> loc(#loc151)
        %152 = "ttnn.multiply"(%151, %150) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<512x2560xf32, #ttnn_layout32>, tensor<512x1xf32, #ttnn_layout31>) -> tensor<512x2560xf32, #ttnn_layout32> loc(#loc122)
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<512x2560xf32, #ttnn_layout32>) -> () loc(#loc122)
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<512x1xf32, #ttnn_layout31>) -> () loc(#loc122)
        %153 = "ttnn.multiply"(%141, %152) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x2560xf32, #ttnn_layout27>, tensor<512x2560xf32, #ttnn_layout32>) -> tensor<512x2560xf32, #ttnn_layout32> loc(#loc123)
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<512x2560xf32, #ttnn_layout32>) -> () loc(#loc123)
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x2560xf32, #ttnn_layout27>) -> () loc(#loc123)
        %154 = "ttnn.matmul"(%153, %arg17) <{transpose_a = false, transpose_b = false}> : (tensor<512x2560xf32, #ttnn_layout32>, tensor<2560x9728xbf16, #ttnn_layout2>) -> tensor<512x9728xf32, #ttnn_layout74> loc(#loc124)
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<2560x9728xbf16, #ttnn_layout2>) -> () loc(#loc124)
        %155 = "ttnn.sigmoid"(%154) : (tensor<512x9728xf32, #ttnn_layout74>) -> tensor<512x9728xf32, #ttnn_layout74> loc(#loc125)
        %156 = "ttnn.multiply"(%154, %155) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<512x9728xf32, #ttnn_layout74>, tensor<512x9728xf32, #ttnn_layout74>) -> tensor<512x9728xf32, #ttnn_layout74> loc(#loc126)
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<512x9728xf32, #ttnn_layout74>) -> () loc(#loc126)
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<512x9728xf32, #ttnn_layout74>) -> () loc(#loc126)
        %157 = "ttnn.matmul"(%153, %arg2) <{transpose_a = false, transpose_b = false}> : (tensor<512x2560xf32, #ttnn_layout32>, tensor<2560x9728xbf16, #ttnn_layout2>) -> tensor<512x9728xf32, #ttnn_layout74> loc(#loc127)
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<512x2560xf32, #ttnn_layout32>) -> () loc(#loc127)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<2560x9728xbf16, #ttnn_layout2>) -> () loc(#loc127)
        %158 = "ttnn.multiply"(%156, %157) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<512x9728xf32, #ttnn_layout74>, tensor<512x9728xf32, #ttnn_layout74>) -> tensor<512x9728xf32, #ttnn_layout74> loc(#loc128)
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<512x9728xf32, #ttnn_layout74>) -> () loc(#loc128)
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<512x9728xf32, #ttnn_layout74>) -> () loc(#loc128)
        %159 = "ttnn.matmul"(%158, %arg1) <{transpose_a = false, transpose_b = false}> : (tensor<512x9728xf32, #ttnn_layout74>, tensor<9728x2560xbf16, #ttnn_layout1>) -> tensor<512x2560xf32, #ttnn_layout32> loc(#loc129)
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<512x9728xf32, #ttnn_layout74>) -> () loc(#loc129)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<9728x2560xbf16, #ttnn_layout1>) -> () loc(#loc129)
        %160 = "ttnn.reshape"(%159) <{shape = [1 : i32, 512 : i32, 2560 : i32]}> : (tensor<512x2560xf32, #ttnn_layout32>) -> tensor<1x512x2560xf32, #ttnn_layout13> loc(#loc130)
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<512x2560xf32, #ttnn_layout32>) -> () loc(#loc130)
        %161 = "ttnn.add"(%139, %160) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x2560xf32, #ttnn_layout13>, tensor<1x512x2560xf32, #ttnn_layout13>) -> tensor<1x512x2560xf32, #ttnn_layout13> loc(#loc131)
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<1x512x2560xf32, #ttnn_layout13>) -> () loc(#loc131)
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x512x2560xf32, #ttnn_layout13>) -> () loc(#loc131)
        %162 = "ttnn.pow"(%161, %8) : (tensor<1x512x2560xf32, #ttnn_layout13>, tensor<1x512x2560xf32, #ttnn_layout13>) -> tensor<1x512x2560xf32, #ttnn_layout13> loc(#loc132)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x512x2560xf32, #ttnn_layout13>) -> () loc(#loc132)
        %163 = "ttnn.sum"(%162) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x512x2560xf32, #ttnn_layout13>) -> tensor<1x512xf32, #ttnn_layout19> loc(#loc133)
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x512x2560xf32, #ttnn_layout13>) -> () loc(#loc133)
        %164 = "ttnn.multiply"(%163, %7) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512xf32, #ttnn_layout19>, tensor<1x512xf32, #ttnn_layout19>) -> tensor<1x512xf32, #ttnn_layout19> loc(#loc134)
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x512xf32, #ttnn_layout19>) -> () loc(#loc134)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x512xf32, #ttnn_layout19>) -> () loc(#loc134)
        %165 = "ttnn.reshape"(%164) <{shape = [1 : i32, 512 : i32, 1 : i32]}> : (tensor<1x512xf32, #ttnn_layout19>) -> tensor<1x512x1xf32, #ttnn_layout73> loc(#loc134)
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<1x512xf32, #ttnn_layout19>) -> () loc(#loc134)
        %166 = "ttnn.add"(%165, %145) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x1xf32, #ttnn_layout73>, tensor<1x1x1xf32, #ttnn_layout72>) -> tensor<1x512x1xf32, #ttnn_layout73> loc(#loc135)
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<1x512x1xf32, #ttnn_layout73>) -> () loc(#loc135)
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x1x1xf32, #ttnn_layout72>) -> () loc(#loc135)
        %167 = "ttnn.rsqrt"(%166) : (tensor<1x512x1xf32, #ttnn_layout73>) -> tensor<1x512x1xf32, #ttnn_layout73> loc(#loc136)
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<1x512x1xf32, #ttnn_layout73>) -> () loc(#loc136)
        %168 = "ttnn.multiply"(%161, %167) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x2560xf32, #ttnn_layout13>, tensor<1x512x1xf32, #ttnn_layout73>) -> tensor<1x512x2560xf32, #ttnn_layout13> loc(#loc137)
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<1x512x1xf32, #ttnn_layout73>) -> () loc(#loc137)
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x512x2560xf32, #ttnn_layout13>) -> () loc(#loc137)
        %169 = "ttnn.multiply"(%16, %168) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x2560xf32, #ttnn_layout24>, tensor<1x512x2560xf32, #ttnn_layout13>) -> tensor<1x512x2560xf32, #ttnn_layout13> loc(#loc138)
        "ttnn.deallocate"(%168) <{force = false}> : (tensor<1x512x2560xf32, #ttnn_layout13>) -> () loc(#loc138)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x2560xf32, #ttnn_layout24>) -> () loc(#loc138)
        return %169 : tensor<1x512x2560xf32, #ttnn_layout13> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc20 = loc("dot.53")
#loc21 = loc("slice.73")
#loc22 = loc("dot.95")
#loc23 = loc("dot.172")
#loc24 = loc("broadcast.37")
#loc25 = loc("power.17")
#loc26 = loc("convert.345")
#loc27 = loc("convert.266")
#loc28 = loc("convert.207")
#loc29 = loc("convert.48")
#loc30 = loc("reduce.24")
#loc31 = loc("multiply.33")
#loc32 = loc("reshape.171")
#loc33 = loc("add.38")
#loc34 = loc("rsqrt.39")
#loc35 = loc("convert.40")
#loc36 = loc("multiply.44")
#loc37 = loc("multiply.50")
#loc38 = loc("convert.51")
#loc39 = loc("reshape.174")
#loc40 = loc("convert.200")
#loc41 = loc("power.176")
#loc42 = loc("reduce.183")
#loc43 = loc("multiply.192")
#loc44 = loc("broadcast.196")
#loc45 = loc("transpose.211")
#loc46 = loc("add.197")
#loc47 = loc("rsqrt.198")
#loc48 = loc("convert.199")
#loc49 = loc("multiply.203")
#loc50 = loc("multiply.209")
#loc51 = loc("convert.210")
#loc52 = loc("convert.220")
#loc53 = loc("convert.83")
#loc54 = loc("dot.84")
#loc55 = loc("transpose.85")
#loc56 = loc("concatenate.86")
#loc57 = loc("cosine.144")
#loc58 = loc("multiply.223")
#loc59 = loc("slice.213")
#loc60 = loc("negate.214")
#loc61 = loc("slice.212")
#loc62 = loc("concatenate.215")
#loc63 = loc("convert.216")
#loc64 = loc("sine.87")
#loc65 = loc("multiply.219")
#loc66 = loc("add.226")
#loc67 = loc("convert.130")
#loc68 = loc("reshape.97")
#loc69 = loc("convert.123")
#loc70 = loc("power.99")
#loc71 = loc("reduce.106")
#loc72 = loc("multiply.115")
#loc73 = loc("broadcast.119")
#loc74 = loc("transpose.134")
#loc75 = loc("add.120")
#loc76 = loc("rsqrt.121")
#loc77 = loc("convert.122")
#loc78 = loc("multiply.126")
#loc79 = loc("multiply.132")
#loc80 = loc("convert.133")
#loc81 = loc("convert.148")
#loc82 = loc("broadcast.150")
#loc83 = loc("multiply.151")
#loc84 = loc("slice.136")
#loc85 = loc("negate.137")
#loc86 = loc("slice.135")
#loc87 = loc("concatenate.138")
#loc88 = loc("convert.139")
#loc89 = loc("broadcast.141")
#loc90 = loc("broadcast.162")
#loc91 = loc("multiply.142")
#loc92 = loc("add.154")
#loc93 = loc("reshape.163")
#loc94 = loc("transpose.164")
#loc95 = loc("reshape.166")
#loc96 = loc("dot.229")
#loc97 = loc("reshape.230")
#loc98 = loc("broadcast.231")
#loc99 = loc("multiply.232")
#loc100 = loc("convert.233")
#loc101 = loc("add.238")
#loc102 = loc("divide.255")
#loc103 = loc("reshape.257")
#loc104 = loc("reshape.55")
#loc105 = loc("transpose.56")
#loc106 = loc("broadcast.64")
#loc107 = loc("convert.258")
#loc108 = loc("dot.259")
#loc109 = loc("reshape.260")
#loc110 = loc("reshape.263")
#loc111 = loc("dot.264")
#loc112 = loc("reshape.265")
#loc113 = loc("add.269")
#loc114 = loc("convert.298")
#loc115 = loc("power.271")
#loc116 = loc("reduce.278")
#loc117 = loc("multiply.287")
#loc118 = loc("broadcast.291")
#loc119 = loc("add.292")
#loc120 = loc("rsqrt.293")
#loc121 = loc("reshape.305")
#loc122 = loc("multiply.296")
#loc123 = loc("multiply.300")
#loc124 = loc("dot.306")
#loc125 = loc("logistic.308")
#loc126 = loc("multiply.309")
#loc127 = loc("dot.302")
#loc128 = loc("multiply.310")
#loc129 = loc("dot.312")
#loc130 = loc("reshape.313")
#loc131 = loc("add.316")
#loc132 = loc("power.318")
#loc133 = loc("reduce.325")
#loc134 = loc("multiply.334")
#loc135 = loc("add.339")
#loc136 = loc("rsqrt.340")
#loc137 = loc("multiply.343")
#loc138 = loc("multiply.347")
#loc139 = loc("reduce.24_workaround"(#loc30))
#loc140 = loc("reshape.171_tm0"(#loc32))
#loc141 = loc("reduce.183_workaround"(#loc42))
#loc142 = loc("transpose.211_tm0"(#loc45))
#loc143 = loc("reduce.106_workaround"(#loc71))
#loc144 = loc("transpose.134_tm0"(#loc74))
#loc145 = loc("broadcast.162_tm1"(#loc90))
#loc146 = loc("reshape.305_tm1"(#loc121))
#loc147 = loc("reshape.171_tm0_tm1"(#loc140))
#loc148 = loc("transpose.211_tm0_tm1"(#loc142))
#loc149 = loc("transpose.134_tm0_tm1"(#loc144))
#loc150 = loc("broadcast.162_tm1_tm1"(#loc145))
#loc151 = loc("reshape.305_tm1_tm0"(#loc146))
#loc152 = loc("reshape.171_tm0_tm1_tm1"(#loc147))
#loc153 = loc("reshape.171_tm0_tm1_tm0"(#loc147))
#loc154 = loc("transpose.211_tm0_tm1_tm1"(#loc148))
#loc155 = loc("transpose.134_tm0_tm1_tm1"(#loc149))
#loc156 = loc("reshape.171_tm0_tm1_tm1_tm0"(#loc152))
#loc157 = loc("transpose.211_tm0_tm1_tm1_tm0"(#loc154))
#loc158 = loc("transpose.134_tm0_tm1_tm1_tm0"(#loc155))
#loc159 = loc("reshape.171_tm0_tm1_tm1_tm0_tm0"(#loc156))
#loc160 = loc("transpose.211_tm0_tm1_tm1_tm0_tm0"(#loc157))
#loc161 = loc("transpose.134_tm0_tm1_tm1_tm0_tm0"(#loc158))
#loc162 = loc("reshape.171_tm0_tm1_tm1_tm0_tm0_tm1"(#loc159))
#loc163 = loc("transpose.211_tm0_tm1_tm1_tm0_tm0_tm1"(#loc160))
#loc164 = loc("transpose.134_tm0_tm1_tm1_tm0_tm0_tm1"(#loc161))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc("dot.53")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %1 = "ttnn.typecast"(%arg4) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<2560x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<2560x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.53")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<2560x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.53")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %2 = "ttnn.typecast"(%arg8) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x1x512x513xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x17x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x512x513x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x17x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.73")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<1x1x512x513xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x17x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("slice.73")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %3 = "ttnn.typecast"(%arg12) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<2560x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<2560x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.95")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<2560x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.95")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %4 = "ttnn.typecast"(%arg14) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<2560x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<2560x4096x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.172")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<2560x4096xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x128x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.172")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %5 = "ttnn.typecast"(%arg5) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.37")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.37")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %6 = "ttnn.typecast"(%arg6) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512x2560xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.17")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %7 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.906250e-04 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512>}> : (!ttnn.device) -> tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %8 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512x2560>}> : (!ttnn.device) -> tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %9 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>, fill_value = 7.812500e-03 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512x8>}> : (!ttnn.device) -> tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %10 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512x8x128>}> : (!ttnn.device) -> tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %11 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>, fill_value = 7.812500e-03 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512x32>}> : (!ttnn.device) -> tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %12 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512x32x128>}> : (!ttnn.device) -> tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %13 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>, fill_value = 3.9100647E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512>}> : (!ttnn.device) -> tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %14 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512x2560>}> : (!ttnn.device) -> tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc(unknown)
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %15 = "ttnn.typecast"(%arg18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2560xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<2560xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.345")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<2560xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.345")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %16 = "ttnn.reshape"(%15) <{shape = [1 : i32, 1 : i32, 2560 : i32]}> : (tensor<2560xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.345")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%15) <{force = false}> : (tensor<2560xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.345")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %17 = "ttnn.typecast"(%arg6) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x2560xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.266")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<1x512x2560xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.266")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %18 = "ttnn.typecast"(%arg15) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<128xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.207")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<128xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.207")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %19 = "ttnn.reshape"(%18) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.207")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%18) <{force = false}> : (tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.207")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %20 = "ttnn.permute"(%19) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.207")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.207")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %21 = "ttnn.typecast"(%arg7) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2560xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<2560xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.48")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<2560xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.48")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %22 = "ttnn.reshape"(%21) <{shape = [1 : i32, 2560 : i32]}> : (tensor<2560xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.48")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%21) <{force = false}> : (tensor<2560xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.48")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %23 = "ttnn.pow"(%6, %14) : (tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.17")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.17")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1014 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.17")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1014 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %24 = "ttnn.typecast"(%23) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x2560xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.24_workaround"("reduce.24"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1014 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.24_workaround"("reduce.24"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %25 = "ttnn.sum"(%24) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x512x2560xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.24")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x512x2560xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.24")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %26 = "ttnn.typecast"(%25) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.24_workaround"("reduce.24"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1014 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x512xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.24_workaround"("reduce.24"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %27 = "ttnn.multiply"(%26, %13) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.33")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.33")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.33")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %28 = "ttnn.reshape"(%27) <{shape = [512 : i32, 1 : i32]}> : (tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.33")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%27) <{force = false}> : (tensor<1x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.33")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %29 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32]}> : (tensor<!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.171_tm0_tm1_tm1_tm0_tm0_tm1"("reshape.171_tm0_tm1_tm1_tm0_tm0"("reshape.171_tm0_tm1_tm1_tm0"("reshape.171_tm0_tm1_tm1"("reshape.171_tm0_tm1"("reshape.171_tm0"("reshape.171")))))))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %30 = "ttnn.add"(%28, %29) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.38")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.38")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%28) <{force = false}> : (tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.38")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %31 = "ttnn.rsqrt"(%30) : (tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.39")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%30) <{force = false}> : (tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.39")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %32 = "ttnn.typecast"(%31) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.40")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%31) <{force = false}> : (tensor<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.40")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %33 = "ttnn.reshape"(%17) <{shape = [512 : i32, 2560 : i32]}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.171_tm0_tm1_tm0"("reshape.171_tm0_tm1"("reshape.171_tm0"("reshape.171"))))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %34 = "ttnn.multiply"(%33, %32) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.44")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%33) <{force = false}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.44")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%32) <{force = false}> : (tensor<512x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.44")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %35 = "ttnn.multiply"(%22, %34) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.50")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%34) <{force = false}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.50")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.50")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %36 = "ttnn.typecast"(%35) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.51")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%35) <{force = false}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.51")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %37 = "ttnn.matmul"(%36, %4) <{transpose_a = false, transpose_b = false}> : (tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<2560x4096x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x4096x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.172")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1014 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%4) <{force = false}> : (tensor<2560x4096x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.172")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %38 = "ttnn.reshape"(%37) <{shape = [1 : i32, 512 : i32, 32 : i32, 128 : i32]}> : (tensor<512x4096x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.174")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%37) <{force = false}> : (tensor<512x4096x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.174")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %39 = "ttnn.typecast"(%38) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x32x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.200")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %40 = "ttnn.permute"(%39) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x512x32x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.200")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x512x32x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.200")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %41 = "ttnn.pow"(%38, %12) : (tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.176")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.176")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.176")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %42 = "ttnn.typecast"(%41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.183_workaround"("reduce.183"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x512x32x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.183_workaround"("reduce.183"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %43 = "ttnn.sum"(%42) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x512x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.183")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x512x32x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.183")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %44 = "ttnn.typecast"(%43) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.183_workaround"("reduce.183"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x512x32xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.183_workaround"("reduce.183"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %45 = "ttnn.multiply"(%44, %11) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.192")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.192")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.192")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %46 = "ttnn.reshape"(%45) <{shape = [1 : i32, 512 : i32, 32 : i32, 1 : i32]}> : (tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x32x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.192")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x512x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.192")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %47 = "ttnn.permute"(%46) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x512x32x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.192")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x512x32x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.192")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %48 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.196")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %49 = "ttnn.permute"(%48) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.211_tm0_tm1_tm1_tm0_tm0_tm1"("transpose.211_tm0_tm1_tm1_tm0_tm0"("transpose.211_tm0_tm1_tm1_tm0"("transpose.211_tm0_tm1_tm1"("transpose.211_tm0_tm1"("transpose.211_tm0"("transpose.211")))))))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.211_tm0_tm1_tm1_tm0_tm0_tm1"("transpose.211_tm0_tm1_tm1_tm0_tm0"("transpose.211_tm0_tm1_tm1_tm0"("transpose.211_tm0_tm1_tm1"("transpose.211_tm0_tm1"("transpose.211_tm0"("transpose.211")))))))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %50 = "ttnn.add"(%47, %49) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.197")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.197")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.197")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %51 = "ttnn.rsqrt"(%50) : (tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.198")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.198")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %52 = "ttnn.typecast"(%51) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.199")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x32x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.199")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %53 = "ttnn.multiply"(%40, %52) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.203")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x32x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.203")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.203")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %54 = "ttnn.multiply"(%20, %53) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.209")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.209")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.209")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %55 = "ttnn.typecast"(%54) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.210")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.210")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %56 = "ttnn.typecast"(%55) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.220")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %57 = "ttnn.reshape"(%56) <{shape = [32 : i32, 512 : i32, 128 : i32]}> : (tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.220")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.220")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %58 = "ttnn.typecast"(%arg10) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.83")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<1x512xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.83")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %59 = "ttnn.reshape"(%58) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.83")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.83")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %60 = "ttnn.matmul"(%arg11, %59) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.84")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x1x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.84")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.84")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %61 = "ttnn.permute"(%60) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.85")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x64x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.85")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %62 = "ttnn.concat"(%61, %61) <{dim = 2 : si32}> : (tensor<1x512x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.86")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x512x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.86")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %63 = "ttnn.cos"(%62) : (tensor<1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("cosine.144")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %64 = "ttnn.multiply"(%57, %63) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.223")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%57) <{force = false}> : (tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.223")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %65 = "ttnn.slice_static"(%55) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 32 : i32, 512 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.213")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %66 = "ttnn.neg"(%65) : (tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.214")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.214")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %67 = "ttnn.reshape"(%66) <{shape = [32 : i32, 512 : i32, 64 : i32]}> : (tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.214")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.214")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %68 = "ttnn.slice_static"(%55) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 32 : i32, 512 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.212")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("slice.212")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %69 = "ttnn.reshape"(%68) <{shape = [32 : i32, 512 : i32, 64 : i32]}> : (tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.215")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.215")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %70 = "ttnn.concat"(%67, %69) <{dim = 2 : si32}> : (tensor<32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.215")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%69) <{force = false}> : (tensor<32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.215")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%67) <{force = false}> : (tensor<32x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.215")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %71 = "ttnn.typecast"(%70) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.216")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%70) <{force = false}> : (tensor<32x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.216")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %72 = "ttnn.sin"(%62) : (tensor<1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("sine.87")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("sine.87")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %73 = "ttnn.multiply"(%71, %72) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.219")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%71) <{force = false}> : (tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.219")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1003 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %74 = "ttnn.add"(%64, %73) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.226")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%73) <{force = false}> : (tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.226")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1003 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%64) <{force = false}> : (tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.226")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %75 = "ttnn.typecast"(%arg13) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<128xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.130")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<128xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.130")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %76 = "ttnn.reshape"(%75) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.130")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%75) <{force = false}> : (tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.130")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %77 = "ttnn.permute"(%76) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.130")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.130")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %78 = "ttnn.matmul"(%36, %3) <{transpose_a = false, transpose_b = false}> : (tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<2560x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.95")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%3) <{force = false}> : (tensor<2560x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.95")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %79 = "ttnn.reshape"(%78) <{shape = [1 : i32, 512 : i32, 8 : i32, 128 : i32]}> : (tensor<512x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.97")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%78) <{force = false}> : (tensor<512x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.97")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %80 = "ttnn.typecast"(%79) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x8x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.123")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %81 = "ttnn.permute"(%80) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x512x8x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.123")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x512x8x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.123")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1004 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %82 = "ttnn.pow"(%79, %10) : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.99")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.99")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.99")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1014 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %83 = "ttnn.typecast"(%82) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.106_workaround"("reduce.106"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1014 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.106_workaround"("reduce.106"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %84 = "ttnn.sum"(%83) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x512x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x8xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.106")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x512x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.106")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %85 = "ttnn.typecast"(%84) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512x8xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.106_workaround"("reduce.106"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%84) <{force = false}> : (tensor<1x512x8xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.106_workaround"("reduce.106"))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %86 = "ttnn.multiply"(%85, %9) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.115")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%85) <{force = false}> : (tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.115")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.115")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %87 = "ttnn.reshape"(%86) <{shape = [1 : i32, 512 : i32, 8 : i32, 1 : i32]}> : (tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x8x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.115")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%86) <{force = false}> : (tensor<1x512x8x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.115")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %88 = "ttnn.permute"(%87) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x512x8x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.115")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%87) <{force = false}> : (tensor<1x512x8x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.115")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %89 = "ttnn.reshape"(%5) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%5) <{force = false}> : (tensor<!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.119")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %90 = "ttnn.permute"(%89) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.134_tm0_tm1_tm1_tm0_tm0_tm1"("transpose.134_tm0_tm1_tm1_tm0_tm0"("transpose.134_tm0_tm1_tm1_tm0"("transpose.134_tm0_tm1_tm1"("transpose.134_tm0_tm1"("transpose.134_tm0"("transpose.134")))))))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.134_tm0_tm1_tm1_tm0_tm0_tm1"("transpose.134_tm0_tm1_tm1_tm0_tm0"("transpose.134_tm0_tm1_tm1_tm0"("transpose.134_tm0_tm1_tm1"("transpose.134_tm0_tm1"("transpose.134_tm0"("transpose.134")))))))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %91 = "ttnn.add"(%88, %90) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.120")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x1x1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.120")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%88) <{force = false}> : (tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.120")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %92 = "ttnn.rsqrt"(%91) : (tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.121")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.121")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %93 = "ttnn.typecast"(%92) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.122")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x8x512x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.122")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %94 = "ttnn.multiply"(%81, %93) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.126")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x8x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.126")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.126")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %95 = "ttnn.multiply"(%77, %94) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.132")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.132")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.132")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %96 = "ttnn.typecast"(%95) <{dtype = #ttcore.supportedDataTypes<bfp_bf8>}> : (tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.133")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.133")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %97 = "ttnn.typecast"(%96) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.148")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %98 = "ttnn.reshape"(%63) <{shape = [1 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.150")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.150")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %99 = "ttnn.multiply"(%97, %98) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.151")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.151")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.151")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %100 = "ttnn.reshape"(%99) <{shape = [1 : i32, 8 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.151")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.151")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %101 = "ttnn.slice_static"(%96) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 512 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.136")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %102 = "ttnn.neg"(%101) : (tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.137")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.137")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %103 = "ttnn.reshape"(%102) <{shape = [1 : i32, 8 : i32, 1 : i32, 512 : i32, 64 : i32]}> : (tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("negate.137")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("negate.137")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %104 = "ttnn.slice_static"(%96) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 512 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.135")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("slice.135")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %105 = "ttnn.reshape"(%104) <{shape = [1 : i32, 8 : i32, 1 : i32, 512 : i32, 64 : i32]}> : (tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.138")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x8x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.138")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %106 = "ttnn.concat"(%103, %105) <{dim = 4 : si32}> : (tensor<1x8x1x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("concatenate.138")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x8x1x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.138")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x8x1x512x64x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x2x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("concatenate.138")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %107 = "ttnn.typecast"(%106) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.139")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x8x1x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.139")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %108 = "ttnn.reshape"(%72) <{shape = [1 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.141")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.141")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %109 = "ttnn.repeat"(%108) <{repeat_dims = #ttnn.shape<1x8x1x1>}> : (tensor<1x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.141")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.141")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %110 = "ttnn.reshape"(%109) <{shape = [1 : i32, 8 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.162_tm1_tm1"("broadcast.162_tm1"("broadcast.162")))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x8x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.162_tm1_tm1"("broadcast.162_tm1"("broadcast.162")))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %111 = "ttnn.multiply"(%107, %110) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.142")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.142")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.142")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %112 = "ttnn.add"(%100, %111) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.154")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.154")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.154")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %113 = "ttnn.repeat"(%112) <{repeat_dims = #ttnn.shape<1x1x4x1x1>}> : (tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x4x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 16384 + d1 * 2048 + d2 * 512 + d3, d4), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.162")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.162")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %114 = "ttnn.reshape"(%113) <{shape = [1 : i32, 32 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x4x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 16384 + d1 * 2048 + d2 * 512 + d3, d4), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.163")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x8x4x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 16384 + d1 * 2048 + d2 * 512 + d3, d4), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.163")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %115 = "ttnn.permute"(%114) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x128x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 128 + d2, d3), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.164")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.164")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %116 = "ttnn.reshape"(%115) <{shape = [32 : i32, 128 : i32, 512 : i32]}> : (tensor<1x32x128x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 128 + d2, d3), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x128x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.166")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x32x128x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 128 + d2, d3), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.166")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %117 = "ttnn.matmul"(%74, %116) <{transpose_a = false, transpose_b = false}> : (tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<32x128x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.229")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%116) <{force = false}> : (tensor<32x128x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.229")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 21 MB FreePerBank: 1005 MB ContiguousFreePerBank: 998 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%74) <{force = false}> : (tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.229")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1008 MB ContiguousFreePerBank: 998 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %118 = "ttnn.reshape"(%117) <{shape = [1 : i32, 32 : i32, 512 : i32, 512 : i32]}> : (tensor<32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.230")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1009 MB ContiguousFreePerBank: 998 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%117) <{force = false}> : (tensor<32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.230")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1009 MB ContiguousFreePerBank: 998 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %119 = "ttnn.reshape"(%arg9) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.231")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1009 MB ContiguousFreePerBank: 998 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.231")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1009 MB ContiguousFreePerBank: 998 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %120 = "ttnn.multiply"(%118, %119) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.232")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1009 MB ContiguousFreePerBank: 998 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.232")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 22 MB FreePerBank: 1004 MB ContiguousFreePerBank: 998 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.232")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 22 MB FreePerBank: 1004 MB ContiguousFreePerBank: 998 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %121 = "ttnn.slice_static"(%2) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 512 : i32, 512 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x512x513x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x17x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x512x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x16x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("slice.73")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 2005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1x512x513x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x17x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("slice.73")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %122 = "ttnn.typecast"(%121) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x512x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x16x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.233")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x1x512x512x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x16x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.233")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %123 = "ttnn.add"(%120, %122) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.238")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x1x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 * 512 + d2, d3), <1x1>, memref<16x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.238")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 22 MB FreePerBank: 1003 MB ContiguousFreePerBank: 998 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.238")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 22 MB FreePerBank: 1004 MB ContiguousFreePerBank: 998 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %124 = "ttnn.softmax"(%123) <{dimension = 3 : si32, numericStable = true}> : (tensor<1x32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("divide.255")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1007 MB ContiguousFreePerBank: 998 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("divide.255")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 22 MB FreePerBank: 1004 MB ContiguousFreePerBank: 998 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %125 = "ttnn.reshape"(%124) <{shape = [32 : i32, 512 : i32, 512 : i32]}> : (tensor<1x32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.257")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 2005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.257")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 2005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %126 = "ttnn.matmul"(%36, %1) <{transpose_a = false, transpose_b = false}> : (tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<2560x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.53")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 2005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%36) <{force = false}> : (tensor<512x2560x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.53")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%1) <{force = false}> : (tensor<2560x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.53")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %127 = "ttnn.reshape"(%126) <{shape = [1 : i32, 512 : i32, 8 : i32, 128 : i32]}> : (tensor<512x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.55")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%126) <{force = false}> : (tensor<512x1024x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x32x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.55")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %128 = "ttnn.permute"(%127) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("transpose.56")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x512x8x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("transpose.56")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %129 = "ttnn.reshape"(%128) <{shape = [1 : i32, 8 : i32, 1 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.64")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x8x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 512 + d2, d3), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.64")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %130 = "ttnn.typecast"(%129) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.258")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x8x1x512x128x!ttcore.tile<32x32, bfp_bf8>, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, bfp_bf8>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.258")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %131 = "ttnn.repeat"(%130) <{repeat_dims = #ttnn.shape<1x1x4x1x1>}> : (tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x4x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 16384 + d1 * 2048 + d2 * 512 + d3, d4), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.258")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x8x1x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 4096 + d1 * 512 + d2 * 512 + d3, d4), <1x1>, memref<128x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.258")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %132 = "ttnn.reshape"(%131) <{shape = [32 : i32, 512 : i32, 128 : i32]}> : (tensor<1x8x4x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 16384 + d1 * 2048 + d2 * 512 + d3, d4), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.258")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x8x4x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 16384 + d1 * 2048 + d2 * 512 + d3, d4), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.258")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %133 = "ttnn.matmul"(%125, %132) <{transpose_a = false, transpose_b = false}> : (tensor<32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.259")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%132) <{force = false}> : (tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.259")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 21 MB FreePerBank: 1005 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%125) <{force = false}> : (tensor<32x512x512xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.259")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %134 = "ttnn.reshape"(%133) <{shape = [1 : i32, 32 : i32, 512 : i32, 128 : i32]}> : (tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.260")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%133) <{force = false}> : (tensor<32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.260")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %135 = "ttnn.concatenate_heads"(%134) : (tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.263")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x32x512x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 512 + d2, d3), <1x1>, memref<512x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.263")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %136 = "ttnn.reshape"(%135) <{shape = [512 : i32, 4096 : i32]}> : (tensor<1x512x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.263")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x512x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.263")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %137 = "ttnn.matmul"(%136, %arg3) <{transpose_a = false, transpose_b = false}> : (tensor<512x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.264")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%136) <{force = false}> : (tensor<512x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.264")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<4096x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.264")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1012 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %138 = "ttnn.reshape"(%137) <{shape = [1 : i32, 512 : i32, 2560 : i32]}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.265")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1012 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%137) <{force = false}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.265")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1012 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %139 = "ttnn.add"(%17, %138) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.269")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1012 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.269")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.269")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %140 = "ttnn.typecast"(%arg16) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<2560xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<2560xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1013 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<2560xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %141 = "ttnn.reshape"(%140) <{shape = [1 : i32, 2560 : i32]}> : (tensor<2560xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%140) <{force = false}> : (tensor<2560xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("convert.298")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %142 = "ttnn.pow"(%139, %8) : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.271")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %143 = "ttnn.sum"(%142) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.278")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.278")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %144 = "ttnn.multiply"(%143, %7) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.287")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.287")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %145 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.291")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("broadcast.291")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %146 = "ttnn.repeat"(%145) <{repeat_dims = #ttnn.shape<1x512x1>}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("broadcast.291")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %147 = "ttnn.reshape"(%146) <{shape = [1 : i32, 512 : i32]}> : (tensor<1x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.287")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.287")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %148 = "ttnn.add"(%144, %147) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.292")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.292")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.292")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %149 = "ttnn.rsqrt"(%148) : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.293")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.293")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %150 = "ttnn.reshape"(%149) <{shape = [512 : i32, 1 : i32]}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.293")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.293")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %151 = "ttnn.reshape"(%139) <{shape = [512 : i32, 2560 : i32]}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.305_tm1_tm0"("reshape.305_tm1"("reshape.305")))
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %152 = "ttnn.multiply"(%151, %150) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.296")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%151) <{force = false}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.296")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%150) <{force = false}> : (tensor<512x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.296")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %153 = "ttnn.multiply"(%141, %152) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.300")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%152) <{force = false}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.300")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.300")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %154 = "ttnn.matmul"(%153, %arg17) <{transpose_a = false, transpose_b = false}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<2560x9728xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x304x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<2560x9728xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x304x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.306")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %155 = "ttnn.sigmoid"(%154) : (tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("logistic.308")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %156 = "ttnn.multiply"(%154, %155) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.309")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1006 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%155) <{force = false}> : (tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.309")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 22 MB FreePerBank: 1004 MB ContiguousFreePerBank: 999 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%154) <{force = false}> : (tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.309")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1006 MB ContiguousFreePerBank: 999 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %157 = "ttnn.matmul"(%153, %arg2) <{transpose_a = false, transpose_b = false}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<2560x9728xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x304x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.302")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1010 MB ContiguousFreePerBank: 999 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%153) <{force = false}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.302")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1006 MB ContiguousFreePerBank: 999 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<2560x9728xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<80x304x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.302")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1006 MB ContiguousFreePerBank: 999 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %158 = "ttnn.multiply"(%156, %157) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.310")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1006 MB ContiguousFreePerBank: 999 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%157) <{force = false}> : (tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.310")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 21 MB FreePerBank: 1004 MB ContiguousFreePerBank: 999 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%156) <{force = false}> : (tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.310")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 20 MB FreePerBank: 1006 MB ContiguousFreePerBank: 999 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %159 = "ttnn.matmul"(%158, %arg1) <{transpose_a = false, transpose_b = false}> : (tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<9728x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<304x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("dot.312")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 2007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%158) <{force = false}> : (tensor<512x9728xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x304x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.312")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 19 MB FreePerBank: 1007 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<9728x2560xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<304x80x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("dot.312")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %160 = "ttnn.reshape"(%159) <{shape = [1 : i32, 512 : i32, 2560 : i32]}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reshape.313")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%159) <{force = false}> : (tensor<512x2560xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reshape.313")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %161 = "ttnn.add"(%139, %160) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.316")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%160) <{force = false}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.316")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 18 MB FreePerBank: 1008 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.316")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %162 = "ttnn.pow"(%161, %8) : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("power.318")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("power.318")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %163 = "ttnn.sum"(%162) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("reduce.325")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("reduce.325")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %164 = "ttnn.multiply"(%163, %7) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.334")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.334")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.334")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %165 = "ttnn.reshape"(%164) <{shape = [1 : i32, 512 : i32, 1 : i32]}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.334")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%164) <{force = false}> : (tensor<1x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.334")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %166 = "ttnn.add"(%165, %145) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("add.339")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%165) <{force = false}> : (tensor<1x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.339")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("add.339")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %167 = "ttnn.rsqrt"(%166) : (tensor<1x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("rsqrt.340")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%166) <{force = false}> : (tensor<1x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("rsqrt.340")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %168 = "ttnn.multiply"(%161, %167) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.343")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%167) <{force = false}> : (tensor<1x512x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.343")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.343")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: %169 = "ttnn.multiply"(%16, %168) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> loc("multiply.347")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1011 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%168) <{force = false}> : (tensor<1x512x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 512 + d1, d2), <1x1>, memref<16x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.347")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 17 MB FreePerBank: 1009 MB ContiguousFreePerBank: 1001 MB
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | Executing operation: "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x1x2560xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x80x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> () loc("multiply.347")
[32m            RuntimeTTNN[0m | [1m[38;5;69m    INFO[0m | KCM DRAM View - Banks: 12 Size: 1023 MB TotalBytesAllocatedPerBank: 16 MB FreePerBank: 1010 MB ContiguousFreePerBank: 1001 MB

Results for output 0:
  PCC: 0.7157, threshold: 0.9 âŽ (assert_pcc == False)
  ATOL: 62.5646, threshold: 0.01 âŽ (assert_atol == False)

FAILED

=================================== FAILURES ===================================
_______ test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval] _______

test_entry = {'path': '/localdev/ssalice/tt-torch/third_party/tt_forge_models/qwen_3/embedding/pytorch/loader.py', 'variant_info': ...ING_4B: 'embedding_4b'>, <class 'tt-forge-models.qwen_3.embedding.pytorch.loader.ModelLoader'>, <enum 'ModelVariant'>)}
mode = 'eval', op_by_op = None
record_property = <function record_property.<locals>._original_record_property at 0x7fb502a377e0>
test_metadata = <tests.runner.test_utils.ModelTestConfig object at 0x7fb5c5a40410>

    @pytest.mark.parametrize(
        "mode",
        ["eval"],
    )
    @pytest.mark.parametrize(
        "op_by_op",
        [None],
        ids=["full"],
        # [OpByOpBackend.STABLEHLO, OpByOpBackend.TORCH, None],
        # ids=["op_by_op_stablehlo", "op_by_op_torch", "full"],
    )
    @pytest.mark.parametrize(
        "test_entry",
        test_entries,
        ids=create_test_id_generator(MODELS_ROOT),
    )
    def test_all_models(test_entry, mode, op_by_op, record_property, test_metadata):
        loader_path = test_entry["path"]
        variant_info = test_entry["variant_info"]
    
        # Ensure per-model requirements are installed, and roll back after the test
        with RequirementsManager.for_loader(loader_path):
            # FIXME - Consider cleaning this up, avoid call to import_model_loader_and_variant.
            if variant_info:
                # Unpack the tuple we stored earlier
                variant, ModelLoader, ModelVariant = variant_info
            else:
                # For models without variants
                ModelLoader, _ = import_model_loader_and_variant(loader_path, MODELS_ROOT)
                variant = None
    
            cc = CompilerConfig()
            cc.enable_consteval = True
            cc.consteval_parameters = True
            #cc.arg_type_map_override = True
    
            if op_by_op:
                cc.compile_depth = CompileDepth.EXECUTE_OP_BY_OP
                cc.op_by_op_backend = op_by_op
    
            # Use the variant from the test_entry parameter
            loader = ModelLoader(variant=variant)
    
            # Get model name from the ModelLoader's ModelInfo
            model_info = ModelLoader.get_model_info(variant=variant)
            print(f"model_name: {model_info.name} status: {test_metadata.status}")
    
            if test_metadata.status == ModelStatus.NOT_SUPPORTED_SKIP:
                skip_full_eval_test(
                    record_property,
                    cc,
                    model_info.name,
                    bringup_status=test_metadata.skip_bringup_status,
                    reason=test_metadata.skip_reason,
                    model_group=model_info.group,
                    forge_models_test=True,
                )
    
            tester = DynamicTester(
                model_info.name,
                mode,
                loader=loader,
                model_info=model_info,
                compiler_config=cc,
                record_property_handle=record_property,
                forge_models_test=True,
                **test_metadata.to_tester_args(),
            )
    
            results = tester.test_model()
>           tester.finalize()

tests/runner/test_models.py:94: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.runner.test_utils.DynamicTester object at 0x7fb5029a5690>

    def finalize(self):
        # to be called at the end of the test
    
        ModelTester.filter_nan_inf_for_record(self.record_tag_cache, "pccs")
        ModelTester.filter_nan_inf_for_record(self.record_tag_cache, "atols")
    
        ModelTester.record_aggregate_model_metric(self.record_tag_cache, "pccs")
        ModelTester.record_aggregate_model_metric(self.record_tag_cache, "atols")
    
        # FE standardization - pack a single PCC & ATOL into the tag record
        # use cached metrics. Guaranteed to be init'd to the default value
        self.record_tag_cache["pcc"] = self.record_tag_cache["min_pccs"]
        self.record_tag_cache["atol"] = self.record_tag_cache["max_atols"]
    
        # Compile depth is remapped post-execution to FE-standardized format
        # based on actual execution result.
        self.record_tag_cache["bringup_status"] = ModelTester.remap_compile_depth(
            self.compiler_config.compile_depth,
            self.record_tag_cache["min_pccs"],
            self.required_pcc,
        )
        self.record_property("tags", self.record_tag_cache)
    
        # Assert any deferred verification failures after all reporting is complete
        # This allows pytest to write junitxml during teardown even if the test fails
        if self.verification_failure_msg:
>           assert False, self.verification_failure_msg
                   ^^^^^
E           AssertionError: PCC check failed. Required: 0.9, Got lowest pcc 0.7157465505048731
E           Detail:
E           	output 0:0.7157465505048731 [req > 0.9]

tests/utils.py:843: AssertionError
=============================== warnings summary ===============================
install/torch_mlir/extras/onnx_importer.py:748
  /localdev/ssalice/tt-torch/install/torch_mlir/extras/onnx_importer.py:748: DeprecationWarning: invalid escape sequence '\w'
    return re.sub("[^\w\.]", "_", name)

<frozen importlib._bootstrap>:241
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute

<frozen importlib._bootstrap>:241
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute

env/venv/lib/python3.11/site-packages/hippynn/_settings_setup.py:14
  /localdev/ssalice/tt-torch/env/venv/lib/python3.11/site-packages/hippynn/_settings_setup.py:14: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives
    from distutils.util import strtobool

tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval]
tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval]
tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval]
  /localdev/ssalice/tt-torch/tt_torch/dynamo/experimental/xla_backend.py:769: DeprecationWarning: Use torch_xla.sync instead
    xm.mark_step()

tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval]
  /localdev/ssalice/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/experimental/const_fold.py:264: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer
    new_node = root_const_gm.graph.get_attr(in_node.target)

tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval]
  /localdev/ssalice/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___layers__modules__0___self_attn_q_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval]
  /localdev/ssalice/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___layers__modules__0___self_attn_k_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval]
  /localdev/ssalice/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___layers__modules__0___self_attn_v_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval]
  /localdev/ssalice/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___layers__modules__0___self_attn_o_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval]
  /localdev/ssalice/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___layers__modules__0___mlp_gate_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval]
  /localdev/ssalice/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___layers__modules__0___mlp_up_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval]
  /localdev/ssalice/tt-torch/env/venv/lib/python3.11/site-packages/torch/fx/graph.py:1316: UserWarning: Failed to fetch module L__self___layers__modules__0___mlp_down_proj!
    warnings.warn(f"Failed to fetch module {module_path}!")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/runner/test_models.py::test_all_models[qwen_3/embedding/pytorch-embedding_4b-full-eval] - AssertionError: PCC check failed. Required: 0.9, Got lowest pcc 0.7157465505048731
Detail:
	output 0:0.7157465505048731 [req > 0.9]
================== 1 failed, 15 warnings in 112.34s (0:01:52) ==================
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
